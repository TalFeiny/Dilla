        async def fetch_single_company(company: str) -> Dict:
            """Fetch data for a single company - SEMI-SEQUENTIAL: funding first for context, then others"""
            from bs4 import BeautifulSoup
            import re
            
            # PHASE 1: Run funding search first to get context
            funding_query = f'{company} raised seed series million funding'
            cached_funding = await self._get_cached_tavily_result(funding_query, "funding_search")
            
            if cached_funding:
                funding_search = cached_funding
            else:
                funding_search = await self.executor.execute_tavily({
                    "query": funding_query,
                    "search_depth": "advanced",
                    "max_results": 10,
                    "include_raw_content": True,
                    "exclude_domains": ["tracxn.com", "getlatka.com"]
                })
                await self._cache_tavily_result(funding_query, "funding_search", funding_search)
            
            # Extract context from funding results
            website_url_from_funding = None
            sector = None
            
            if funding_search.get('success'):
                for result in funding_search.get('data', {}).get('results', [])[:3]:  # Check top 3 results
                    raw_html = result.get('raw_content', '')
                    content = result.get('content', '').lower()
                    
                    # Extract website URLs from funding article HTML
                    if raw_html and not website_url_from_funding:
                        try:
                            soup = BeautifulSoup(raw_html, 'html.parser')
                            # Look for links that might be the company website
                            for link in soup.find_all('a', href=True):
                                href = link['href']
                                # Check if it looks like a company website
                                company_clean = company.lower().replace(' ', '').replace('-', '')
                                if (company_clean in href.lower() and 
                                    any(tld in href for tld in ['.com', '.io', '.ai', '.co', '.app']) and
                                    not any(skip in href for skip in ['twitter.com', 'linkedin.com', 'facebook.com', 'crunchbase.com'])):
                                    website_url_from_funding = href
                                    logger.info(f"Found {company} website in funding article: {website_url_from_funding}")
                                    break
                        except Exception as e:
                            logger.debug(f"Error parsing funding HTML: {e}")
                    
                    # Extract sector/industry from content
                    if not sector:
                        sectors = ['fintech', 'saas', 'ai', 'ml', 'healthcare', 'healthtech', 'ecommerce', 
                                  'security', 'cybersecurity', 'edtech', 'proptech', 'insurtech', 'biotech',
                                  'logistics', 'supply chain', 'marketplace', 'payments', 'crypto', 'web3']
                        for s in sectors:
                            if s in content:
                                sector = s
                                logger.info(f"Identified {company} sector: {sector}")
                                break
            
            # PHASE 2: Run remaining searches in parallel with context
            tasks = []
            
            # 1. General search
            company_query = f'{company} startup company'
            cached_metrics = await self._get_cached_tavily_result(company_query, "company_search")
            
            async def search_company():
                if cached_metrics:
                    return cached_metrics
                result = await self.executor.execute_tavily({
                    "query": company_query,
                    "search_depth": "advanced", 
                    "max_results": 10,
                    "include_raw_content": True,
                    "exclude_domains": ["tracxn.com", "getlatka.com"]
                })
                await self._cache_tavily_result(company_query, "company_search", result)
                return result
            tasks.append(search_company())
            
            # 2. Website search WITH CONTEXT from funding
            async def scrape_website():
                try:
                    company_name_clean = company.lower().replace(' ', '').replace('-', '').replace('_', '')
                    company_website_url = None
                    
                    # If we found website URL in funding article, try to fetch it directly
                    if website_url_from_funding:
                        logger.info(f"Using website URL from funding article: {website_url_from_funding}")
                        # Clean the URL
                        if not website_url_from_funding.startswith('http'):
                            website_url_from_funding = f'https://{website_url_from_funding}'
                        
                        # Try to extract content from the found website
                        scrape_result = await self.executor.execute_tavily_extract({
                            "urls": [website_url_from_funding]
                        })
                        
                        if scrape_result.get("success"):
                            # Verify it's the right website
                            raw_text = ""
                            if "results" in scrape_result.get("data", {}):
                                for result in scrape_result["data"]["results"]:
                                    raw_text += result.get("raw_content", "")
                            elif "raw_content" in scrape_result.get("data", {}):
                                raw_text = scrape_result["data"]["raw_content"]
                            
                            # Quick verification
                            if company.lower() in raw_text.lower()[:2000]:
                                logger.info(f"Verified {website_url_from_funding} as {company}'s website")
                                
                                website_info = {
                                    "raw_content": raw_text[:5000],
                                    "url": website_url_from_funding,
                                    "has_pricing": "pricing" in raw_text.lower() or "price" in raw_text.lower(),
                                    "has_customers": "customer" in raw_text.lower() or "client" in raw_text.lower(),
                                    "has_team": "team" in raw_text.lower() or "founder" in raw_text.lower(),
                                    "has_about": "about" in raw_text.lower()
                                }
                                
                                # Extract pricing details
                                pricing_info = self._extract_pricing_from_text(raw_text)
                                if pricing_info:
                                    website_info["pricing"] = pricing_info
                                
                                # Extract business model
                                business_model = self._extract_business_model_from_text(raw_text)
                                if business_model:
                                    website_info["business_model"] = business_model
                                
                                return {
                                    "website_data": scrape_result,
                                    "website_url": website_url_from_funding,
                                    "extracted_info": website_info
                                }
                    
                    # Otherwise, search for website with enhanced query using context
                    website_query = f'"{company}" {sector or ""} official website startup technology -tourism -travel -florence -italy -dictionary -wikipedia -pet -dog'
                    cached_website = await self._get_cached_tavily_result(website_query, "website_search")
                    
                    if cached_website:
                        search_result = cached_website
                    else:
                        search_result = await self.executor.execute_tavily({
                            "query": website_query,
                            "max_results": 5,
                            "include_raw_content": True,
                            "exclude_domains": ["tracxn.com", "getlatka.com", "crunchbase.com"]
                        })
                        await self._cache_tavily_result(website_query, "website_search", search_result)
                    
                    if search_result.get("success") and search_result.get("data", {}).get("results"):
                        # Score and rank potential websites
                        website_candidates = []
                        
                        for result in search_result["data"]["results"]:
                            url = result.get("url", "")
                            title = result.get("title", "").lower()
                            content = result.get("content", "").lower()
                            raw_content = result.get("raw_content", "")
                            
                            # Skip bad domains
                            if any(bad in url.lower() for bad in [
                                'dictionary', 'webster', 'wikipedia', 'wiki', 
                                'tracxn.com', 'getlatka.com', 'crunchbase.com',
                                'linkedin.com', 'twitter.com', 'facebook.com',
                                'tourism', 'travel', 'florence', 'italy', 'pet', 'dog'
                            ]):
                                continue
                            
                            # Score the URL
                            score = 0
                            url_lower = url.lower()
                            
                            # High priority: Direct company domain
                            company_variations = [
                                company_name_clean,
                                company.lower().replace(' ', '-'),
                                company.lower().replace(' ', '_'),
                                company.lower().replace(' ', '')
                            ]
                            
                            for variant in company_variations:
                                if variant in url_lower:
                                    domain_match = re.search(r'https?://(?:www\.)?([^/]+)', url_lower)
                                    if domain_match:
                                        domain = domain_match.group(1)
                                        if domain.startswith(variant) or domain == f"{variant}.com" or domain == f"{variant}.io":
                                            score += 100
                                            break
                                        elif f"{variant}." in domain or f".{variant}" in domain:
                                            score += 70
                                            break
                                        elif f"/{variant}" in url_lower:
                                            score += 20
                                            break
                            
                            # Boost if sector matches
                            if sector and sector in content:
                                score += 30
                            
                            # Check title relevance
                            if company_name_clean in title:
                                score += 20
                            if 'official' in title or 'home' in title:
                                score += 15
                            
                            # Check content indicators
                            if any(indicator in content for indicator in ['about us', 'our mission', 'our team', 'pricing', 'sign up', 'get started']):
                                score += 25
                            
                            # Penalize certain patterns
                            if any(bad in title or bad in content[:200] for bad in ['dictionary', 'definition', 'tourism', 'travel guide', 'vacation', 'pet care', 'dog breed']):
                                score -= 50
                            
                            if score > 0:
                                website_candidates.append({
                                    'url': url,
                                    'score': score,
                                    'title': result.get("title", ""),
                                    'content': content,
                                    'raw_content': raw_content
                                })
                        
                        # Sort by score and pick the best one
                        if website_candidates:
                            website_candidates.sort(key=lambda x: x['score'], reverse=True)
                            best_candidate = website_candidates[0]
                            
                            if best_candidate['score'] >= 50:
                                company_website_url = best_candidate['url'].split('?')[0]
                                logger.info(f"Found {company} website with score {best_candidate['score']}: {company_website_url}")
                                
                                # Extract content from the website
                                scrape_result = await self.executor.execute_tavily_extract({
                                    "urls": [company_website_url]
                                })
                                
                                if scrape_result.get("success"):
                                    raw_text = ""
                                    if "results" in scrape_result.get("data", {}):
                                        for result in scrape_result["data"]["results"]:
                                            raw_text += result.get("raw_content", "")
                                    elif "raw_content" in scrape_result.get("data", {}):
                                        raw_text = scrape_result["data"]["raw_content"]
                                    
                                    website_info = {
                                        "raw_content": raw_text[:5000],
                                        "url": company_website_url,
                                        "has_pricing": "pricing" in raw_text.lower() or "price" in raw_text.lower(),
                                        "has_customers": "customer" in raw_text.lower() or "client" in raw_text.lower(),
                                        "has_team": "team" in raw_text.lower() or "founder" in raw_text.lower(),
                                        "has_about": "about" in raw_text.lower()
                                    }
                                    
                                    pricing_info = self._extract_pricing_from_text(raw_text)
                                    if pricing_info:
                                        website_info["pricing"] = pricing_info
                                    
                                    business_model = self._extract_business_model_from_text(raw_text)
                                    if business_model:
                                        website_info["business_model"] = business_model
                                    
                                    return {
                                        "website_data": scrape_result,
                                        "website_url": company_website_url,
                                        "extracted_info": website_info
                                    }
                    
                    return {}
                    
                except Exception as e:
                    logger.warning(f"Website scraping failed for {company}: {e}")
                    return {
                        "website_data": None,
                        "website_url": None,
                        "extracted_info": {
                            "error": str(e),
                            "raw_content": "",
                            "has_pricing": False,
                            "has_customers": False,
                            "has_team": False
                        }
                    }
            
            tasks.append(scrape_website())
            
            # 3. Database search
            async def db_search():
                try:
                    result = supabase_service.search_companies(company, limit=1) if supabase_service else []
                    return result[0] if result else {}
                except Exception as e:
                    logger.warning(f"Database search failed for {company}: {e}")
                    return {}
            
            tasks.append(db_search())
            
            # Execute phase 2 searches in parallel
            phase2_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            general_search = phase2_results[0] if not isinstance(phase2_results[0], Exception) else {}
            website_data = phase2_results[1] if not isinstance(phase2_results[1], Exception) else {}
            db_result = phase2_results[2] if not isinstance(phase2_results[2], Exception) else {}
            
            # Extract citations from all sources
            citation_ids = []
            
            # Citations from general search
            if general_search and general_search.get("success"):
                data = general_search.get("data", {})
                for result in data.get("results", []):
                    url = result.get("url", "")
                    title = result.get("title", "")
                    snippet = result.get("content", "")[:200]
                    
                    citation_id = self.citation_manager.add_citation(
                        source=url,
                        date=datetime.now().isoformat(),
                        content=result.get("content", "")[:500],
                        metadata={
                            "title": title,
                            "type": "General Research",
                            "snippet": snippet,
                            "formatted": f"[{title}]({url})",
                            "html": f'<a href="{url}" target="_blank">{title}</a>',
                            "relevance": "high" if company.lower() in snippet.lower() else "medium"
                        }
                    )
                    citation_ids.append(citation_id)
            
            # Citations from funding search
            if funding_search and funding_search.get("success"):
                data = funding_search.get("data", {})
                for result in data.get("results", []):
                    citation_id = self.citation_manager.add_citation(
                        source=result.get("url", "Unknown URL"),
                        date=datetime.now().isoformat(),
                        content=result.get("content", "")[:500],
                        metadata={
                            "title": result.get("title", ""),
                            "type": "Funding Research",
                            "snippet": result.get("content", "")[:200]
                        }
                    )
                    citation_ids.append(citation_id)
            
            # Citation from website scrape
            if website_data.get("website_url"):
                citation_id = self.citation_manager.add_citation(
                    source=website_data["website_url"],
                    date=datetime.now().isoformat(),
                    content=f"Official website for {company}",
                    metadata={
                        "title": f"{company} Official Website",
                        "type": "Website Scrape"
                    }
                )
                citation_ids.append(citation_id)
            
            return {
                "company": company,
                "general_search": general_search,
                "funding_data": funding_search,
                "website": website_data,
                "database": db_result,
                "citation_ids": citation_ids,
                "context": {
                    "sector": sector,
                    "website_from_funding": website_url_from_funding
                }
            }