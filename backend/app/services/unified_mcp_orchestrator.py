"""
Unified MCP Orchestrator - Agentic orchestration with skill system
Combines MCP tools (Tavily, Firecrawl) with 36+ skills for comprehensive analysis
"""

import asyncio
import aiohttp
import importlib
import logging
import json
from copy import deepcopy
from typing import Dict, List, Any, Optional, AsyncGenerator, Union, Tuple
from datetime import datetime, timedelta
from decimal import Decimal
from enum import Enum
from dataclasses import dataclass, field
import math
import os
import re
import random

# Track guarded import errors so the module still loads even if dependencies fail
# Critical = orchestrator cannot function; Non-critical = gracefully degrade
CRITICAL_IMPORT_ERRORS: Dict[str, Exception] = {}
NON_CRITICAL_IMPORT_ERRORS: Dict[str, Exception] = {}

# All LLM calls go through ModelRouter - no direct imports
from app.core.config import settings

# --- Critical imports (orchestrator cannot function without these) ---
try:
    from app.services.structured_data_extractor import StructuredDataExtractor
except Exception as exc:  # pragma: no cover - defensive import guard
    CRITICAL_IMPORT_ERRORS["StructuredDataExtractor"] = exc
    StructuredDataExtractor = None  # type: ignore[assignment]

try:
    from app.services.intelligent_gap_filler import IntelligentGapFiller
except Exception as exc:  # pragma: no cover - defensive import guard
    CRITICAL_IMPORT_ERRORS["IntelligentGapFiller"] = exc
    IntelligentGapFiller = None  # type: ignore[assignment]

try:
    from app.services import valuation_engine_service as valuation_module
except Exception as exc:  # pragma: no cover - defensive import guard
    CRITICAL_IMPORT_ERRORS["valuation_engine_service"] = exc
    valuation_module = None  # type: ignore[assignment]

try:
    from app.services.valuation_engine_service import (
        ValuationRequest,
        Stage,
        ValuationMethod
    )
except Exception as exc:  # pragma: no cover - defensive import guard
    CRITICAL_IMPORT_ERRORS["valuation_engine_service_symbols"] = exc
    ValuationRequest = Stage = ValuationMethod = None  # type: ignore[assignment]

# Import centralized data validator
try:
    from app.services.data_validator import (
        ensure_numeric, safe_divide, safe_get_value,
        safe_multiply, validate_company_data, safe_get
    )
except Exception as exc:  # pragma: no cover - defensive import guard
    CRITICAL_IMPORT_ERRORS["data_validator"] = exc
    ensure_numeric = safe_divide = safe_get_value = None  # type: ignore[assignment]
    safe_multiply = validate_company_data = safe_get = None  # type: ignore[assignment]

try:
    from app.services.pre_post_cap_table import PrePostCapTable
except Exception as exc:  # pragma: no cover - defensive import guard
    CRITICAL_IMPORT_ERRORS["PrePostCapTable"] = exc
    PrePostCapTable = None  # type: ignore[assignment]

try:
    from app.services.advanced_cap_table import CapTableCalculator
except Exception as exc:  # pragma: no cover - defensive import guard
    CRITICAL_IMPORT_ERRORS["CapTableCalculator"] = exc
    CapTableCalculator = None  # type: ignore[assignment]

try:
    from app.services.citation_manager import CitationManager
except Exception as exc:  # pragma: no cover - defensive import guard
    CRITICAL_IMPORT_ERRORS["CitationManager"] = exc
    CitationManager = None  # type: ignore[assignment]

try:
    from app.services.ownership_return_analyzer import OwnershipReturnAnalyzer, InvestmentType
except Exception as exc:  # pragma: no cover - defensive import guard
    CRITICAL_IMPORT_ERRORS["OwnershipReturnAnalyzer"] = exc
    OwnershipReturnAnalyzer = InvestmentType = None  # type: ignore[assignment]

try:
    from app.services.comprehensive_deal_analyzer import ComprehensiveDealAnalyzer
except Exception as exc:  # pragma: no cover - defensive import guard
    CRITICAL_IMPORT_ERRORS["ComprehensiveDealAnalyzer"] = exc
    ComprehensiveDealAnalyzer = None  # type: ignore[assignment]

try:
    from app.core.error_handler import (
        DillaErrorHandler, RetryConfig, RetryStrategy, with_retry, error_handler as global_error_handler
    )
except Exception as exc:  # pragma: no cover - defensive import guard
    CRITICAL_IMPORT_ERRORS["DillaErrorHandler"] = exc
    DillaErrorHandler = RetryConfig = RetryStrategy = with_retry = None  # type: ignore[assignment]
    global_error_handler = None  # type: ignore[assignment]

# --- Non-critical imports (gracefully degrade, log warning) ---
try:
    from app.services.matrix_query_orchestrator import MatrixQueryOrchestrator
    MATRIX_QUERY_ORCHESTRATOR_AVAILABLE = True
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["MatrixQueryOrchestrator"] = exc
    MatrixQueryOrchestrator = None  # type: ignore[assignment]
    MATRIX_QUERY_ORCHESTRATOR_AVAILABLE = False

try:
    from app.services.chart_renderer_service import chart_renderer
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["chart_renderer_service"] = exc
    chart_renderer = None  # type: ignore[assignment]

try:
    from app.services.chart_data_service import (
        format_sankey_chart,
        format_side_by_side_sankey_chart,
        format_probability_cloud_chart,
        format_heatmap_chart,
        format_waterfall_chart,
        format_bar_chart,
        format_line_chart,
        format_pie_chart,
    )
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["chart_format_helpers"] = exc
    format_sankey_chart = format_side_by_side_sankey_chart = None  # type: ignore[assignment]
    format_probability_cloud_chart = format_heatmap_chart = None  # type: ignore[assignment]
    format_waterfall_chart = format_bar_chart = None  # type: ignore[assignment]
    format_line_chart = format_pie_chart = None  # type: ignore[assignment]

try:
    from app.utils.formatters import DeckFormatter
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["DeckFormatter"] = exc
    DeckFormatter = None  # type: ignore[assignment]

try:
    from app.services.config_loader import ConfigLoader
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["ConfigLoader"] = exc
    ConfigLoader = None  # type: ignore[assignment]

try:
    from app.services.fund_modeling_service import FundModelingService
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["FundModelingService"] = exc
    FundModelingService = None  # type: ignore[assignment]

try:
    from app.services.nl_scenario_composer import NLScenarioComposer
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["NLScenarioComposer"] = exc
    NLScenarioComposer = None  # type: ignore[assignment]

try:
    from app.services.company_history_analysis_service import CompanyHistoryAnalysisService
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["CompanyHistoryAnalysisService"] = exc
    CompanyHistoryAnalysisService = None  # type: ignore[assignment]

try:
    from app.services.fpa_regression_service import FPARegressionService
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["FPARegressionService"] = exc
    FPARegressionService = None  # type: ignore[assignment]

try:
    from app.services.fpa_query_classifier import FPAQueryClassifier
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["FPAQueryClassifier"] = exc
    FPAQueryClassifier = None  # type: ignore[assignment]

# --- Phase 3: Wire additional services as agent tools ---
try:
    from app.services.waterfall_advanced import AdvancedWaterfallCalculator
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["AdvancedWaterfallCalculator"] = exc
    AdvancedWaterfallCalculator = None  # type: ignore[assignment]

try:
    from app.services.advanced_debt_structures_service import AdvancedDebtStructures
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["AdvancedDebtStructures"] = exc
    AdvancedDebtStructures = None  # type: ignore[assignment]

try:
    from app.services.market_intelligence_service import MarketIntelligenceService
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["MarketIntelligenceService"] = exc
    MarketIntelligenceService = None  # type: ignore[assignment]

try:
    from app.services.revenue_projection_service import RevenueProjectionService
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["RevenueProjectionService"] = exc
    RevenueProjectionService = None  # type: ignore[assignment]

try:
    from app.services.financial_calculator import FinancialCalculator
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["FinancialCalculator"] = exc
    FinancialCalculator = None  # type: ignore[assignment]

try:
    from app.services.enhanced_compliance_service import EnhancedComplianceService
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["EnhancedComplianceService"] = exc
    EnhancedComplianceService = None  # type: ignore[assignment]

try:
    from app.services.ma_workflow_service import MAWorkflowService
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["MAWorkflowService"] = exc
    MAWorkflowService = None  # type: ignore[assignment]

try:
    from app.services.deck_quality_validator import DeckQualityValidator
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["DeckQualityValidator"] = exc
    DeckQualityValidator = None  # type: ignore[assignment]

try:
    from app.services.slide_content_optimizer import SlideContentOptimizer
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["SlideContentOptimizer"] = exc
    SlideContentOptimizer = None  # type: ignore[assignment]

try:
    from app.services.formula_evaluator import FormulaEvaluator
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["FormulaEvaluator"] = exc
    FormulaEvaluator = None  # type: ignore[assignment]

try:
    from app.services.arithmetic_engine import ArithmeticEngine
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["ArithmeticEngine"] = exc
    ArithmeticEngine = None  # type: ignore[assignment]

try:
    from app.services.nl_matrix_controller import NLMatrixController
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["NLMatrixController"] = exc
    NLMatrixController = None  # type: ignore[assignment]

try:
    from app.services.lightweight_memo_service import LightweightMemoService
    from app.services.memo_templates import MEMO_TEMPLATES, INTENT_TO_TEMPLATE
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["LightweightMemoService"] = exc
    logger.error("[MEMO] Failed to import LightweightMemoService: %s", exc, exc_info=True)
    LightweightMemoService = None  # type: ignore[assignment]
    MEMO_TEMPLATES = {}  # type: ignore[assignment]
    INTENT_TO_TEMPLATE = {}  # type: ignore[assignment]

try:
    from app.services.session_state import (
        SessionState, TaskPlanner, CompletionChecker, Scoreboard, Task as PlannedTask,
        SessionMemo, AgentTaskTracker,
    )
except Exception as exc:  # pragma: no cover - defensive import guard
    NON_CRITICAL_IMPORT_ERRORS["SessionState"] = exc
    SessionState = TaskPlanner = CompletionChecker = Scoreboard = PlannedTask = SessionMemo = AgentTaskTracker = None  # type: ignore[assignment]

MODEL_ROUTER_IMPORT_ERROR: Optional[Exception] = None
try:
    from app.services.model_router import ModelRouter, ModelCapability, get_model_router
except Exception as import_error:
    MODEL_ROUTER_IMPORT_ERROR = import_error
    ModelRouter = None  # type: ignore[assignment]
    get_model_router = None  # type: ignore[assignment]
    
    class ModelCapability(Enum):
        """Fallback enum so module load succeeds when ModelRouter import fails"""
        ANALYSIS = "analysis"
        CODE = "code"
        STRUCTURED = "structured"
        CREATIVE = "creative"
        FAST = "fast"
        CHEAP = "cheap"

logger = logging.getLogger(__name__)

# ---------------------------------------------------------------------------
# Fund defaults — single source of truth for fallback values
# ---------------------------------------------------------------------------
DEFAULT_FUND_SIZE = 260_000_000  # $260M Series A-C fund
DEFAULT_FUND_STRATEGY = "Series A-C"
DEFAULT_FUND_YEAR = 4

ORCHESTRATOR_READINESS_STATE: Dict[str, Any] = {
    "ready": False,
    "error": "UnifiedMCPOrchestrator not initialized"
}


def _update_readiness_state(ready: bool, error: Optional[str] = None) -> None:
    ORCHESTRATOR_READINESS_STATE["ready"] = ready
    ORCHESTRATOR_READINESS_STATE["error"] = error


def get_orchestrator_readiness() -> Dict[str, Any]:
    """Expose readiness state without instantiating the orchestrator."""
    return {
        "ready": ORCHESTRATOR_READINESS_STATE["ready"],
        "error": ORCHESTRATOR_READINESS_STATE["error"]
    }


# ---------------------------------------------------------------------------
# Hydration helpers — pure functions for _hydrate_shared_data_from_companies
# ---------------------------------------------------------------------------

def _num(val: Any) -> float:
    """Coerce a value to float, extracting .value from InferenceResult if needed."""
    if val is None:
        return 0.0
    if hasattr(val, "value"):
        val = val.value
    try:
        return float(val)
    except (TypeError, ValueError):
        return 0.0


_STAGE_GROWTH: Dict[str, float] = {
    "Pre-Seed": 3.0, "Seed": 2.5, "Series A": 1.5,
    "Series B": 0.80, "Series C": 0.50, "Series D": 0.30,
    "Series E": 0.20, "Growth": 0.15, "IPO": 0.10,
}

_STAGE_FOUNDER_PCT: Dict[str, float] = {
    "Pre-Seed": 0.85, "Seed": 0.70, "Series A": 0.45,
    "Series B": 0.30, "Series C": 0.22, "Series D": 0.18,
    "Series E": 0.15, "Growth": 0.12, "IPO": 0.08,
}

_STAGE_ROUND_COUNT: Dict[str, int] = {
    "Pre-Seed": 1, "Seed": 1, "Series A": 2,
    "Series B": 3, "Series C": 4, "Series D": 5,
    "Series E": 6, "Growth": 7, "IPO": 8,
}


def _stage_default_growth(stage: Optional[str]) -> float:
    if not stage:
        return 1.0
    return _STAGE_GROWTH.get(stage, 1.0)


def _stage_founder_ownership(stage: Optional[str]) -> float:
    if not stage:
        return 0.40
    return _STAGE_FOUNDER_PCT.get(stage, 0.40)


def _stage_round_count(stage: Optional[str]) -> int:
    if not stage:
        return 2
    return _STAGE_ROUND_COUNT.get(stage, 2)


class OutputFormat(Enum):
    """Output format types"""
    STRUCTURED = "structured"
    JSON = "json"
    MARKDOWN = "markdown"
    SPREADSHEET = "spreadsheet"
    DECK = "deck"
    MATRIX = "matrix"


class SkillCategory(Enum):
    """Categories of skills"""
    DATA_GATHERING = "data_gathering"
    ANALYSIS = "analysis"
    GENERATION = "generation"
    FORMATTING = "formatting"


# Phase 6: Maps grid skill name -> (action_id from cell_action_registry, column_hint)
# Covers all registry actions that can run per matrix row. Add new actions here.
GRID_ACTION_MAP: Dict[str, Tuple[str, str]] = {
    # Financial formulas
    "grid-run-irr": ("financial.irr", "value"),
    "grid-run-npv": ("financial.npv", "value"),
    "grid-run-moic": ("financial.moic", "value"),
    "grid-run-cagr": ("financial.cagr", "value"),
    # Valuation
    "grid-run-valuation": ("valuation_engine.auto", "valuation"),
    "grid-run-pwerm": ("valuation_engine.pwerm", "valuation"),
    "grid-run-dcf": ("valuation_engine.dcf", "valuation"),
    "grid-run-opm": ("valuation_engine.opm", "valuation"),
    "grid-run-waterfall-valuation": ("valuation_engine.waterfall", "valuation"),
    "grid-run-recent-transaction": ("valuation_engine.recent_transaction", "valuation"),
    "grid-run-cost-method": ("valuation_engine.cost_method", "valuation"),
    "grid-run-milestone": ("valuation_engine.milestone", "valuation"),
    # Revenue & charts
    "grid-run-revenue-projection": ("revenue_projection.build", "revenue"),
    "grid-run-chart": ("chart_intelligence.generate", "value"),
    # NAV
    "grid-run-nav": ("nav.calculate", "valuation"),
    "grid-run-nav-company": ("nav.calculate_company", "valuation"),
    "grid-run-nav-portfolio": ("nav.calculate_portfolio", "value"),
    "grid-run-nav-timeseries": ("nav.timeseries_company", "valuation"),
    # Fund & portfolio (fund-level)
    "grid-run-fund-metrics": ("fund_metrics.calculate", "value"),
    "grid-run-followon": ("followon_strategy.recommend", "value"),
    "grid-run-portfolio-nav": ("portfolio.total_nav", "value"),
    "grid-run-portfolio-invested": ("portfolio.total_invested", "value"),
    "grid-run-dpi": ("portfolio.dpi", "value"),
    "grid-run-tvpi": ("portfolio.tvpi", "value"),
    "grid-run-dpi-sankey": ("portfolio.dpi_sankey", "value"),
    "grid-run-portfolio-optimize": ("portfolio.optimize", "value"),
    "grid-run-nav-timeseries-fund": ("nav.timeseries", "value"),
    "grid-run-nav-forecast": ("nav.forecast", "value"),
    # Market
    "grid-find-comparables": ("market.find_comparables", "value"),
    "grid-run-market-timing": ("market.timing_analysis", "value"),
    "grid-run-investment-readiness": ("market.investment_readiness", "value"),
    "grid-run-sector-landscape": ("market.sector_landscape", "value"),
    # Document
    "grid-run-document-extract": ("document.extract", "value"),
    "grid-run-document-analyze": ("document.analyze", "value"),
    # Waterfall & cap table
    "grid-run-waterfall": ("waterfall.calculate", "valuation"),
    "grid-run-waterfall-breakpoints": ("waterfall.breakpoints", "value"),
    "grid-run-waterfall-exit": ("waterfall.exit_scenarios", "value"),
    "grid-run-cap-table": ("cap_table.calculate", "value"),
    "grid-run-cap-table-ownership": ("cap_table.ownership", "value"),
    "grid-run-cap-table-dilution": ("cap_table.dilution", "value"),
    # Ownership
    "grid-run-ownership": ("ownership.analyze", "value"),
    "grid-run-return-scenarios": ("ownership.return_scenarios", "value"),
    # M&A
    "grid-run-ma-acquisition": ("ma.model_acquisition", "value"),
    "grid-run-ma-transactions": ("ma.transactions", "value"),
    "grid-run-ma-synergy": ("ma.synergy", "value"),
    # Skills (orchestrator)
    "grid-fetch-company-data": ("skill.company_data_fetch", "valuation"),
    "grid-run-funding-aggregation": ("skill.funding_aggregation", "value"),
    "grid-run-market-research": ("skill.market_research", "value"),
    "grid-run-competitive": ("skill.competitive_analysis", "value"),
    "grid-run-skill-valuation": ("skill.valuation_engine", "valuation"),
    "grid-run-skill-pwerm": ("skill.pwerm_calculator", "valuation"),
    "grid-run-financial-analysis": ("skill.financial_analysis", "value"),
    "grid-run-scenario-analysis": ("skill.scenario_analysis", "value"),
    "grid-run-deal-comparison": ("skill.deal_comparison", "value"),
    "grid-run-cap-table-gen": ("skill.cap_table_generation", "value"),
    "grid-run-exit-modeling": ("skill.exit_modeling", "value"),
    "grid-run-deck": ("skill.deck_storytelling", "value"),
    "grid-run-excel": ("skill.excel_generation", "value"),
    "grid-run-memo": ("skill.memo_generation", "value"),
    "grid-run-skill-chart": ("skill.chart_generation", "value"),
    "grid-run-portfolio-analysis": ("skill.portfolio_analysis", "value"),
    "grid-run-fund-metrics-skill": ("skill.fund_metrics_calculator", "value"),
    "grid-run-stage-analysis": ("skill.stage_analysis", "value"),
    # Scoring & gap filler
    "grid-run-scoring": ("scoring.score_company", "value"),
    "grid-run-portfolio-dashboard": ("scoring.portfolio_dashboard", "value"),
    "grid-run-gap-impact": ("gap_filler.ai_impact", "value"),
    "grid-run-gap-filler": ("gap_filler.ai_valuation", "valuation"),
    "grid-run-gap-market": ("gap_filler.market_opportunity", "value"),
    "grid-run-gap-momentum": ("gap_filler.momentum", "value"),
    "grid-run-gap-fund-fit": ("gap_filler.fund_fit", "value"),
    # Scenario
    "grid-run-scenario-compose": ("scenario.compose", "value"),
    # Round modeling & report generation (delegated to orchestrator skills)
    "grid-run-round-modeling": ("skill.round_modeler", "value"),
    "grid-run-report": ("skill.report_generator", "value"),
    # Portfolio scenario & health
    "grid-run-portfolio-scenarios": ("skill.portfolio_scenario_modeler", "value"),
    "grid-run-company-health": ("skill.company_health_dashboard", "value"),
}

# Trigger keywords for each grid skill (prompt substring match when matrix_context present)
GRID_TRIGGER_MAP: Dict[str, List[str]] = {
    "grid-run-irr": ["irr", "internal rate of return"],
    "grid-run-npv": ["npv", "net present value"],
    "grid-run-moic": ["moic", "multiple on invested"],
    "grid-run-cagr": ["cagr", "compound annual growth"],
    "grid-run-valuation": ["valuation", "run valuation", "auto valuation", "value", "value for", "value @", "value acme", "run valuation for"],
    "grid-run-pwerm": ["pwerm", "run pwerm", "run pwerm for", "pwerm for"],
    "grid-run-dcf": ["dcf", "run dcf", "discounted cash flow"],
    "grid-run-opm": ["opm", "option pricing"],
    "grid-run-waterfall-valuation": ["waterfall valuation"],
    "grid-run-recent-transaction": ["recent transaction", "transaction valuation"],
    "grid-run-cost-method": ["cost method"],
    "grid-run-milestone": ["milestone valuation"],
    "grid-run-revenue-projection": ["revenue projection", "project revenue"],
    "grid-run-chart": ["generate chart", "chart from data"],
    "grid-run-nav": ["nav", "calculate nav", "net asset value"],
    "grid-run-nav-company": ["company nav"],
    "grid-run-nav-portfolio": ["portfolio nav", "calculate portfolio nav"],
    "grid-run-nav-timeseries": ["nav timeseries", "nav over time"],
    "grid-run-fund-metrics": ["fund metrics", "dpi", "tvpi", "irr"],
    "grid-run-followon": ["follow-on", "followon", "follow on strategy", "should we follow on", "extension", "sell", "pro rata", "pro-rata", "dilution"],
    "grid-run-portfolio-nav": ["portfolio nav", "total nav"],
    "grid-run-portfolio-invested": ["total invested", "invested capital"],
    "grid-run-dpi": ["dpi", "distributed to paid"],
    "grid-run-tvpi": ["tvpi", "total value to paid"],
    "grid-run-dpi-sankey": ["dpi sankey", "sankey"],
    "grid-run-portfolio-optimize": ["portfolio optimize", "optimize portfolio"],
    "grid-run-nav-timeseries-fund": ["nav timeseries", "nav over time"],
    "grid-run-nav-forecast": ["nav forecast"],
    "grid-find-comparables": ["comparables", "comps", "find comps"],
    "grid-run-market-timing": ["market timing", "timing analysis"],
    "grid-run-investment-readiness": ["investment readiness"],
    "grid-run-sector-landscape": ["sector landscape"],
    "grid-run-document-extract": ["extract document", "extract document for", "parse document", "document extract"],
    "grid-run-document-analyze": ["analyze document", "document analyze"],
    "grid-run-waterfall": ["waterfall", "liquidation waterfall"],
    "grid-run-waterfall-breakpoints": ["waterfall breakpoints", "breakpoints"],
    "grid-run-waterfall-exit": ["exit scenario waterfall"],
    "grid-run-cap-table": ["cap table", "calculate cap table"],
    "grid-run-cap-table-ownership": ["ownership", "calculate ownership"],
    "grid-run-cap-table-dilution": ["dilution path", "dilution"],
    "grid-run-ownership": ["ownership analyze", "ownership scenarios"],
    "grid-run-return-scenarios": ["return scenarios"],
    "grid-run-ma-acquisition": ["ma acquisition", "model acquisition"],
    "grid-run-ma-transactions": ["ma transactions", "ma search"],
    "grid-run-ma-synergy": ["ma synergy", "synergy"],
    "grid-fetch-company-data": ["enrich", "fetch data", "company data", "fetch company"],
    "grid-run-funding-aggregation": ["funding aggregation", "funding history"],
    "grid-run-market-research": ["market research", "tam", "sector research"],
    "grid-run-competitive": ["competitive", "competitors", "competitor analysis"],
    "grid-run-skill-valuation": ["skill valuation", "valuation engine"],
    "grid-run-skill-pwerm": ["skill pwerm"],
    "grid-run-financial-analysis": ["financial analysis", "financial metrics"],
    "grid-run-scenario-analysis": ["scenario analysis", "monte carlo", "sensitivity"],
    "grid-run-deal-comparison": ["deal comparison", "compare deals"],
    "grid-run-cap-table-gen": ["cap table gen", "generate cap table"],
    "grid-run-exit-modeling": ["exit modeling", "exit model"],
    "grid-run-deck": ["generate deck", "deck", "presentation"],
    "grid-run-excel": ["excel", "spreadsheet", "generate excel"],
    "grid-run-memo": ["memo", "investment memo", "generate memo"],
    "grid-run-skill-chart": ["skill chart", "chart generation"],
    "grid-run-portfolio-analysis": ["portfolio analysis"],
    "grid-run-fund-metrics-skill": ["fund metrics calculator"],
    "grid-run-stage-analysis": ["stage analysis"],
    "grid-run-scoring": ["score company", "score companies"],
    "grid-run-portfolio-dashboard": ["portfolio dashboard", "dashboard"],
    "grid-run-gap-impact": ["ai impact", "gap impact"],
    "grid-run-gap-filler": ["gap fill", "infer", "fill gaps", "ai valuation"],
    "grid-run-gap-market": ["market opportunity", "gap market"],
    "grid-run-gap-momentum": ["momentum", "company momentum"],
    "grid-run-gap-fund-fit": ["fund fit", "fund fit scoring"],
    "grid-run-scenario-compose": ["scenario compose", "what if", "what happens", "stress test", "scenario"],
    "grid-run-portfolio-scenarios": ["portfolio scenarios", "fund scenarios", "fund return scenarios", "what if portfolio"],
    "grid-run-company-health": ["company health", "portfolio health", "health dashboard", "growth decay", "runway analysis"],
    "grid-run-round-modeling": ["series d", "series c", "next round", "model round", "round modeling"],
    "grid-run-report": ["generate report", "lp report", "follow-on memo", "gp deck", "quarterly report"],
}


# ---------------------------------------------------------------------------
# Agent Tool Registry — wraps existing services as callable tools for the
# ReAct agent loop.  Each tool has a handler (method name on the orchestrator),
# a compact description for the LLM router, a cost tier, and a timeout.
# ---------------------------------------------------------------------------

@dataclass
class AgentTool:
    name: str
    description: str        # ≤80 chars — shown to cheap routing LLM
    handler: str            # method name on UnifiedMCPOrchestrator
    input_schema: dict      # JSON-serializable hint for LLM
    cost_tier: str = "free" # "free" (no LLM) | "cheap" | "expensive"
    timeout_ms: int = 30_000


AGENT_TOOLS: list[AgentTool] = [
    AgentTool(
        name="query_portfolio",
        description="Filter/aggregate portfolio grid data by column, company, or metric.",
        handler="_tool_query_portfolio",
        input_schema={"query": "str", "filters": "dict?", "columns": "list[str]?"},
    ),
    AgentTool(
        name="query_documents",
        description="Search uploaded fund documents by keyword or metric.",
        handler="_tool_query_documents",
        input_schema={"query": "str", "company_id": "str?", "doc_type": "str?"},
        cost_tier="free",
    ),
    AgentTool(
        name="calculate_fund_metrics",
        description="Calculate fund-level NAV, IRR, DPI, TVPI, pacing.",
        handler="_tool_fund_metrics",
        input_schema={"metrics": "list[str]", "fund_id": "str?"},
    ),
    AgentTool(
        name="run_valuation",
        description="Run PWERM/DCF/OPM/comparables valuation for a company.",
        handler="_tool_valuation",
        input_schema={"company_id": "str", "method": "str?"},
        cost_tier="expensive",
        timeout_ms=60_000,
    ),
    AgentTool(
        name="run_scenario",
        description="Model a what-if scenario on portfolio or company.",
        handler="_tool_scenario",
        input_schema={"scenario_description": "str", "affected_companies": "list[str]?"},
        cost_tier="cheap",
        timeout_ms=45_000,
    ),
    AgentTool(
        name="generate_chart",
        description="Generate chart config: bar, line, scatter, sankey, waterfall, probability_cloud, revenue_multiples_scatter.",
        handler="_tool_chart",
        input_schema={"chart_type": "str", "data_source": "str", "title": "str?"},
    ),
    AgentTool(
        name="web_search",
        description="Search web via Tavily for comparables, market data, currency rates, news.",
        handler="_tool_web_search",
        input_schema={"query": "str", "search_depth": "str?"},
        cost_tier="cheap",
        timeout_ms=15_000,
    ),
    AgentTool(
        name="suggest_grid_edit",
        description="Suggest a cell edit on the portfolio grid (accept/reject flow).",
        handler="_tool_suggest_edit",
        input_schema={"company": "str", "column": "str", "value": "any", "reasoning": "str"},
    ),
    AgentTool(
        name="suggest_action",
        description="Suggest an action item, warning, or insight. Persisted to DB for accept/reject.",
        handler="_tool_suggest_action",
        input_schema={"type": "str", "title": "str", "description": "str", "priority": "str?", "company_id": "str?"},
    ),
    AgentTool(
        name="write_to_memo",
        description="Append sections to the working memo/LP report.",
        handler="_tool_write_memo",
        input_schema={"sections": "list[dict]"},
    ),
    AgentTool(
        name="fetch_company_data",
        description="Fetch company data from web (Tavily + extraction). Use for any company name — fetches funding, revenue, team, market data.",
        handler="_tool_fetch_company",
        input_schema={"company_name": "str"},
        cost_tier="expensive",
        timeout_ms=90_000,
    ),
    AgentTool(
        name="run_fpa",
        description="Run FP&A: forecast, stress test, sensitivity, regression.",
        handler="_tool_fpa",
        input_schema={"query": "str", "type": "str?"},
        cost_tier="cheap",
        timeout_ms=45_000,
    ),
    AgentTool(
        name="parse_accounts",
        description="Parse raw financial accounts (P&L, balance sheet, cash flow) from text or doc.",
        handler="_tool_parse_accounts",
        input_schema={"text": "str?", "document_id": "str?", "format": "str?"},
        cost_tier="expensive",
        timeout_ms=45_000,
    ),
    AgentTool(
        name="fx_check",
        description="Check FX rates and compute currency impact on portfolio companies. Use when companies have non-USD revenue.",
        handler="_tool_fx_check",
        input_schema={"base_currency": "str?"},
        cost_tier="cheap",
        timeout_ms=10_000,
    ),
    AgentTool(
        name="generate_deck",
        description="Generate investment deck/presentation from company data already in shared_data. Call after fetching companies and running analyses.",
        handler="_tool_generate_deck",
        input_schema={"title": "str?"},
        cost_tier="expensive",
        timeout_ms=120_000,
    ),
    AgentTool(
        name="generate_memo",
        description="Generate memo/report from data already in shared_data. Types: ic_memo, followon, lp_report, gp_strategy, comparison, bespoke_lp, fund_analysis, ownership_analysis, plan_memo. Auto-detects type from prompt if not specified.",
        handler="_tool_generate_memo",
        input_schema={"memo_type": "str?", "prompt": "str?"},
        cost_tier="expensive",
        timeout_ms=90_000,
    ),
    AgentTool(
        name="run_skill",
        description="Run a registered analysis skill by name. Available: valuation-engine, cap-table-generator, exit-modeler, scenario-generator, portfolio-analyzer, fund-metrics-calculator, followon-strategy, regression-analyzer, monte-carlo-simulator, sensitivity-analyzer, competitive-intelligence, market-sourcer, deck-quality-validator, formula-evaluator, arithmetic-engine, company-history-analyzer, waterfall-calculator, debt-converter, market-landscape, revenue-projector, compliance-checker, ma-modeler.",
        handler="_tool_run_skill",
        input_schema={"skill": "str", "inputs": "dict?"},
        cost_tier="expensive",
        timeout_ms=90_000,
    ),
    # ------------------------------------------------------------------
    # Dedicated tools — surface key services directly for the LLM router
    # so it doesn't need the run_skill indirection for common tasks.
    # ------------------------------------------------------------------
    AgentTool(
        name="run_portfolio_health",
        description="Portfolio health dashboard: growth decay, burn/runway, funding trajectory, signals for every company.",
        handler="_tool_portfolio_health",
        input_schema={"fund_id": "str?"},
        cost_tier="expensive",
        timeout_ms=90_000,
    ),
    AgentTool(
        name="run_followon_strategy",
        description="Analyze follow-on / pro-rata / extend-or-sell decisions for portfolio companies.",
        handler="_tool_followon",
        input_schema={"company": "str?", "fund_id": "str?"},
        cost_tier="expensive",
        timeout_ms=60_000,
    ),
    AgentTool(
        name="run_round_modeling",
        description="Model next funding round: dilution, waterfall, valuation step-up, capital required.",
        handler="_tool_round_modeling",
        input_schema={"company": "str", "round_type": "str?", "raise_amount": "float?"},
        cost_tier="expensive",
        timeout_ms=60_000,
    ),
    AgentTool(
        name="run_exit_modeling",
        description="Model exit scenarios with fund ownership impact: IPO, M&A, secondary at various multiples.",
        handler="_tool_exit_modeling",
        input_schema={"company": "str", "exit_values": "list[float]?"},
        cost_tier="expensive",
        timeout_ms=60_000,
    ),
    AgentTool(
        name="run_regression",
        description="Run regression, Monte Carlo, sensitivity, or time-series forecast. Set type param.",
        handler="_tool_regression",
        input_schema={"type": "str", "company": "str?", "metric": "str?", "inputs": "dict?"},
        cost_tier="cheap",
        timeout_ms=45_000,
    ),
    AgentTool(
        name="generate_plan_memo",
        description="Generate a plan memo — resumable context document capturing research findings, execution steps, and data snapshot for cross-session use.",
        handler="_tool_generate_plan_memo",
        input_schema={"prompt": "str?"},
        cost_tier="expensive",
        timeout_ms=90_000,
    ),
    AgentTool(
        name="run_report",
        description="Generate LP quarterly report, follow-on investment memo, or GP strategy report.",
        handler="_tool_report",
        input_schema={"type": "str", "fund_id": "str?", "company": "str?"},
        cost_tier="expensive",
        timeout_ms=90_000,
    ),

    # ------------------------------------------------------------------
    # Phase 8: Proactive enrichment, search/extract combos, projections
    # ------------------------------------------------------------------
    AgentTool(
        name="enrich_company_proactive",
        description="Auto-fetch and enrich a company mentioned without @. Searches web, extracts structured data, pushes suggestions to grid.",
        handler="_tool_enrich_proactive",
        input_schema={"company_name": "str", "push_to_grid": "bool?"},
        cost_tier="expensive",
        timeout_ms=90_000,
    ),
    AgentTool(
        name="build_company_list",
        description=(
            "LEGACY — prefer source_companies with discover_web=true instead. "
            "Delegates to source_companies internally. Kept for backward compatibility."
        ),
        handler="_tool_build_company_list",
        input_schema={"criteria": "str", "sector": "str?", "stage": "str?", "geography": "str?", "max_results": "int?"},
        cost_tier="expensive",
        timeout_ms=120_000,
    ),
    AgentTool(
        name="run_projection",
        description="Run revenue/ARR projection with growth decay curves, scenario bands (bull/base/bear), and time-to-milestone estimates.",
        handler="_tool_run_projection",
        input_schema={"company": "str?", "metric": "str?", "years": "int?", "scenarios": "bool?"},
        cost_tier="cheap",
        timeout_ms=45_000,
    ),
    AgentTool(
        name="search_and_extract",
        description="Targeted web search + structured extraction combo. Searches for specific data points (funding round, revenue, team, customers) and returns extracted structured data.",
        handler="_tool_search_extract",
        input_schema={"query": "str", "extract_fields": "list[str]?", "company": "str?"},
        cost_tier="cheap",
        timeout_ms=30_000,
    ),
    AgentTool(
        name="enrich_sparse_grid",
        description="Auto-detect companies with sparse/missing data in the grid and generate suggestions to fill gaps using web search + stage benchmarks.",
        handler="_tool_enrich_sparse_grid",
        input_schema={"fund_id": "str?", "min_empty_fields": "int?"},
        cost_tier="expensive",
        timeout_ms=120_000,
    ),
    AgentTool(
        name="resolve_data_gaps",
        description="Auto-detect and fill missing company data using benchmarks + parallel searches. Run FIRST when companies have sparse data.",
        handler="_tool_resolve_gaps",
        input_schema={"companies": "list[str]?", "needed_fields": "list[str]?"},
        cost_tier="expensive",
        timeout_ms=60_000,
    ),
    AgentTool(
        name="enrich_field",
        description="Search for specific data fields for a company. Works with ANY column: arr, burnRate, description, headcount, grossMargin, valuation, competitors, etc. Uses column names to build targeted searches.",
        handler="_tool_enrich_field",
        input_schema={"company_name": "str", "fields": "list[str]", "context": "str?"},
        cost_tier="cheap",
        timeout_ms=30_000,
    ),
    AgentTool(
        name="search_field_for_companies",
        description="Search for a specific field (e.g. founders, ARR, team_size, valuation) across multiple companies at once. Use when the user asks to fill a column or find data for several companies. Example: 'find cofounder names for Mercury, Ramp, Deel'.",
        handler="_tool_search_field_for_companies",
        input_schema={"field": "str", "company_names": "list[str]", "context": "str?"},
        cost_tier="cheap",
        timeout_ms=60_000,
    ),

    # ------------------------------------------------------------------
    # Phase 3: 26 new tools wiring existing services
    # ------------------------------------------------------------------

    # --- Cap Table & Ownership ---
    AgentTool(
        name="cap_table_evolution",
        description="Track dilution through ALL funding rounds with Sankey visualization.",
        handler="_tool_cap_table_evolution",
        input_schema={"company": "str", "include_sankey": "bool?"},
        cost_tier="expensive",
        timeout_ms=60_000,
    ),
    AgentTool(
        name="liquidation_waterfall",
        description="Model liquidation waterfall at specific exit values with investor distributions.",
        handler="_tool_liquidation_waterfall",
        input_schema={"company": "str", "exit_value": "float", "exit_type": "str?"},
        cost_tier="expensive",
        timeout_ms=60_000,
    ),
    AgentTool(
        name="anti_dilution_modeling",
        description="Model ratchet/broad-based anti-dilution scenarios on cap table.",
        handler="_tool_anti_dilution",
        input_schema={"company": "str", "new_round_price": "float?", "mechanism": "str?"},
        cost_tier="expensive",
        timeout_ms=45_000,
    ),
    AgentTool(
        name="debt_conversion_modeling",
        description="Model SAFEs, convertible notes, and debt conversion triggers.",
        handler="_tool_debt_conversion",
        input_schema={"company": "str", "trigger_valuation": "float?"},
        cost_tier="expensive",
        timeout_ms=45_000,
    ),

    # --- Scenario & Stress Testing ---
    AgentTool(
        name="stress_test_portfolio",
        description="Portfolio-wide shock modeling: rates, revenue, multiples across all holdings.",
        handler="_tool_stress_test_portfolio",
        input_schema={"shock_type": "str", "magnitude": "float?", "affected_companies": "list[str]?"},
        cost_tier="expensive",
        timeout_ms=90_000,
    ),
    AgentTool(
        name="world_model_scenario",
        description="Multi-factor scenario with propagated effects across company/market model.",
        handler="_tool_world_model_scenario",
        input_schema={"company": "str", "factor_changes": "dict"},
        cost_tier="expensive",
        timeout_ms=60_000,
    ),
    AgentTool(
        name="monte_carlo_portfolio",
        description="Monte Carlo simulation across entire portfolio with probability distributions.",
        handler="_tool_monte_carlo_portfolio",
        input_schema={"iterations": "int?", "variables": "dict?"},
        cost_tier="expensive",
        timeout_ms=90_000,
    ),
    AgentTool(
        name="sensitivity_matrix",
        description="2D sensitivity analysis on any two variables: revenue × multiple, growth × discount rate.",
        handler="_tool_sensitivity_matrix",
        input_schema={"company": "str", "var_x": "str", "var_y": "str", "range_x": "list[float]?", "range_y": "list[float]?"},
        cost_tier="cheap",
        timeout_ms=45_000,
    ),

    # --- Scenario Trees & Cash Flow Planning ---
    AgentTool(
        name="run_scenario_tree",
        description="Build branching scenario tree: multi-company growth paths, round predictions, fund DPI impact per path.",
        handler="_tool_scenario_tree",
        input_schema={"query": "str", "companies": "list[str]?", "years": "int?"},
        cost_tier="expensive",
        timeout_ms=60_000,
    ),
    AgentTool(
        name="run_cash_flow_model",
        description="Build full P&L / cash flow model: revenue, COGS, opex, EBITDA, FCF, runway, funding gap.",
        handler="_tool_cash_flow_model",
        input_schema={"company": "str", "years": "int?", "growth_overrides": "list[float]?"},
        cost_tier="cheap",
        timeout_ms=30_000,
    ),

    AgentTool(
        name="run_bull_bear_base",
        description="Build bull/bear/base scenario analysis for one or more companies. 3-path projection with probability-weighted outcomes.",
        handler="_tool_bull_bear_base",
        input_schema={"companies": "list[str]", "years": "int?"},
        cost_tier="expensive",
        timeout_ms=60_000,
    ),
    AgentTool(
        name="apply_macro_shock",
        description="Apply a macro event (recession, rate hike, regulation, tariff, pandemic, ai_winter) to an existing scenario tree and see impact on portfolio NAV/DPI.",
        handler="_tool_macro_shock",
        input_schema={"shock_type": "str", "magnitude": "float?", "start_year": "int?"},
        cost_tier="cheap",
        timeout_ms=30_000,
    ),
    AgentTool(
        name="portfolio_snapshot",
        description="Get probability-weighted portfolio state (NAV, DPI, TVPI, per-company revenue) at a specific future year from scenario analysis.",
        handler="_tool_portfolio_snapshot",
        input_schema={"year": "int"},
        cost_tier="cheap",
        timeout_ms=15_000,
    ),
    AgentTool(
        name="three_scenario_cash_flow",
        description="Build bull/base/bear P&L models side by side for a company.",
        handler="_tool_three_scenario_cash_flow",
        input_schema={"company": "str", "years": "int?"},
        cost_tier="cheap",
        timeout_ms=30_000,
    ),

    # --- Portfolio Operations ---
    AgentTool(
        name="add_company_to_portfolio",
        description="Fetch, validate, and suggest adding a company to portfolio (requires approval).",
        handler="_tool_add_company",
        input_schema={"company_name": "str", "stage": "str?", "check_size": "float?"},
        cost_tier="expensive",
        timeout_ms=90_000,
    ),
    AgentTool(
        name="bulk_operation",
        description="Batch valuations, health checks, or data refreshes across portfolio.",
        handler="_tool_bulk_operation",
        input_schema={"operation": "str", "companies": "list[str]?"},
        cost_tier="expensive",
        timeout_ms=120_000,
    ),
    AgentTool(
        name="portfolio_comparison",
        description="Side-by-side comparison of N companies on specified metrics.",
        handler="_tool_portfolio_comparison",
        input_schema={"companies": "list[str]", "metrics": "list[str]?"},
        cost_tier="cheap",
        timeout_ms=45_000,
    ),
    AgentTool(
        name="graduation_rates",
        description="Stage progression analysis: how companies move through funding stages.",
        handler="_tool_graduation_rates",
        input_schema={"fund_id": "str?"},
        cost_tier="cheap",
        timeout_ms=30_000,
    ),

    # --- Market & Intelligence ---
    AgentTool(
        name="market_landscape",
        description="Competitive landscape mapping: sector, geography, stage, timing, competitors.",
        handler="_tool_market_landscape",
        input_schema={"sector": "str", "geography": "str?", "stage": "str?"},
        cost_tier="expensive",
        timeout_ms=60_000,
    ),
    AgentTool(
        name="market_timing",
        description="Assess market timing for entry/exit: hot, cooling, cold, or emerging.",
        handler="_tool_market_timing",
        input_schema={"sector": "str", "geography": "str?"},
        cost_tier="cheap",
        timeout_ms=30_000,
    ),
    AgentTool(
        name="company_history",
        description="Full company history analysis with funding rounds, investors, DPI Sankey.",
        handler="_tool_company_history",
        input_schema={"company": "str", "fund_id": "str?"},
        cost_tier="expensive",
        timeout_ms=60_000,
    ),

    # --- Financial Modeling ---
    AgentTool(
        name="revenue_projection",
        description="Project revenue with quality-adjusted decay curves and growth modeling.",
        handler="_tool_revenue_projection",
        input_schema={"company": "str", "years": "int?", "growth_rate": "float?"},
        cost_tier="cheap",
        timeout_ms=30_000,
    ),
    AgentTool(
        name="fund_deployment_model",
        description="Model fund J-curve, pacing, reserve allocation, and deployment strategy.",
        handler="_tool_fund_deployment",
        input_schema={"fund_id": "str?", "scenarios": "list[dict]?"},
        cost_tier="expensive",
        timeout_ms=60_000,
    ),
    AgentTool(
        name="financial_calculator",
        description="On-demand NPV, IRR, PMT, PV, FV calculations. Specify function and inputs.",
        handler="_tool_financial_calc",
        input_schema={"function": "str", "inputs": "dict"},
        cost_tier="free",
        timeout_ms=10_000,
    ),
    AgentTool(
        name="fx_portfolio_impact",
        description="Full portfolio FX exposure analysis across all non-USD holdings.",
        handler="_tool_fx_portfolio_impact",
        input_schema={"base_currency": "str?", "shock_pct": "float?"},
        cost_tier="cheap",
        timeout_ms=30_000,
    ),

    # --- Compliance & Reporting ---
    AgentTool(
        name="compliance_check",
        description="Check filing requirements, generate Form ADV, AIFMD, KYC/AML documentation.",
        handler="_tool_compliance_check",
        input_schema={"check_type": "str?", "advisor_info": "dict?"},
        cost_tier="cheap",
        timeout_ms=30_000,
    ),
    AgentTool(
        name="ma_workflow",
        description="M&A deal structure analysis: synergies, integration risk, returns.",
        handler="_tool_ma_workflow",
        input_schema={"acquirer": "str", "target": "str", "deal_type": "str?", "deal_price": "float?"},
        cost_tier="expensive",
        timeout_ms=60_000,
    ),

    # --- Generation (varying types) ---
    AgentTool(
        name="generate_ic_memo",
        description="Generate investment committee memo: thesis, market, financials, cap table, risks.",
        handler="_tool_generate_ic_memo",
        input_schema={"company": "str", "prompt": "str?"},
        cost_tier="expensive",
        timeout_ms=90_000,
    ),
    AgentTool(
        name="generate_followon_memo",
        description="Generate follow-on investment analysis: position, performance, pro-rata, recommendation.",
        handler="_tool_generate_followon_memo",
        input_schema={"company": "str", "prompt": "str?"},
        cost_tier="expensive",
        timeout_ms=90_000,
    ),
    AgentTool(
        name="generate_lp_report",
        description="Generate quarterly LP report: portfolio summary, per-company updates, fund metrics.",
        handler="_tool_generate_lp_report",
        input_schema={"fund_id": "str?", "quarter": "str?"},
        cost_tier="expensive",
        timeout_ms=90_000,
    ),
    AgentTool(
        name="generate_gp_update",
        description="Generate GP strategy update: deployment pacing, pipeline, market views.",
        handler="_tool_generate_gp_update",
        input_schema={"fund_id": "str?"},
        cost_tier="expensive",
        timeout_ms=90_000,
    ),
    AgentTool(
        name="generate_comparison_report",
        description="Generate side-by-side investment comparison report for 2+ companies.",
        handler="_tool_generate_comparison_report",
        input_schema={"companies": "list[str]", "prompt": "str?"},
        cost_tier="expensive",
        timeout_ms=90_000,
    ),

    # ------------------------------------------------------------------
    # Lightweight diligence & portfolio enrichment
    # ------------------------------------------------------------------
    AgentTool(
        name="lightweight_diligence",
        description="Quick single-search company lookup. Use fetch_company_data only for @ or explicit deep dive.",
        handler="_execute_lightweight_diligence",
        input_schema={"company_name": "str"},
        cost_tier="cheap",
        timeout_ms=30_000,
    ),
    AgentTool(
        name="enrich_portfolio",
        description="Analyze full grid: identify missing fields, compute stage/sector/geo distribution, flag gaps. Auto-persists suggestions.",
        handler="_execute_enrich_portfolio",
        input_schema={"fund_id": "str?"},
        cost_tier="cheap",
        timeout_ms=45_000,
    ),
    AgentTool(
        name="search_companies_db",
        description="Search 1k+ companies database by name/sector/description. Use before web fetch.",
        handler="_tool_search_companies_db",
        input_schema={"query": "str", "limit": "int?"},
        cost_tier="free",
        timeout_ms=10_000,
    ),
    AgentTool(
        name="add_company_to_matrix",
        description="Add company from rich DB to matrix grid with pre-populated data.",
        handler="_tool_add_company_to_matrix",
        input_schema={"company_id": "str?", "company_name": "str?"},
        cost_tier="free",
        timeout_ms=15_000,
    ),

    # ------------------------------------------------------------------
    # Company list reasoning, @ cell enrichment, inline todos
    # ------------------------------------------------------------------
    AgentTool(
        name="reason_company_list",
        description="Given a set of companies, produce ranked reasoning: why each is relevant, gaps, and next steps.",
        handler="_tool_reason_company_list",
        input_schema={"companies": "list[str]", "objective": "str?"},
        cost_tier="cheap",
        timeout_ms=30_000,
    ),
    AgentTool(
        name="enrich_cell",
        description="Deep-enrich a single company cell by name (no @ prefix needed): fetch latest data, infer missing fields, return suggestion. Works with any company name from the grid.",
        handler="_tool_enrich_cell",
        input_schema={"company": "str", "column": "str", "current_value": "any?"},
        cost_tier="cheap",
        timeout_ms=30_000,
    ),
    AgentTool(
        name="emit_todo",
        description="Emit an inline todo/action-item into the response stream. Todos appear in chat and feed into the suggestions panel.",
        handler="_tool_emit_todo",
        input_schema={"title": "str", "description": "str?", "priority": "str?", "company": "str?", "due": "str?"},
        cost_tier="free",
        timeout_ms=5_000,
    ),
    AgentTool(
        name="sync_crm",
        description="Sync companies from matrix to CRM (Attio/Affinity via MCP). Returns sync result.",
        handler="_tool_sync_crm",
        input_schema={"companies": "list[str]?", "direction": "str?"},
        cost_tier="cheap",
        timeout_ms=45_000,
    ),

    # ------------------------------------------------------------------
    # Granular company search tools — 1 Tavily search each, auto-suggest
    # ------------------------------------------------------------------
    AgentTool(
        name="search_company_funding",
        description="Search for a company's funding history, valuation, investors, stage. 1 focused web search + extraction. Auto-suggests grid edits.",
        handler="_tool_search_company_funding",
        input_schema={"company_name": "str"},
        cost_tier="cheap",
        timeout_ms=30_000,
    ),
    AgentTool(
        name="search_company_product",
        description="Search for a company's product, business model, pricing, target market. 1 focused web search + extraction. Auto-suggests grid edits.",
        handler="_tool_search_company_product",
        input_schema={"company_name": "str"},
        cost_tier="cheap",
        timeout_ms=30_000,
    ),
    AgentTool(
        name="search_company_team",
        description="Search for a company's founders, leadership, team size, headcount. 1 focused web search + extraction. Auto-suggests grid edits.",
        handler="_tool_search_company_team",
        input_schema={"company_name": "str"},
        cost_tier="cheap",
        timeout_ms=30_000,
    ),
    AgentTool(
        name="search_company_market",
        description="Search for a company's competitors, TAM, market position, industry trends. 1 focused web search + extraction. Auto-suggests grid edits.",
        handler="_tool_search_company_market",
        input_schema={"company_name": "str"},
        cost_tier="cheap",
        timeout_ms=30_000,
    ),
    AgentTool(
        name="analyze_financials",
        description="Infer/compute financial metrics: gross margin, burn rate, runway, growth projection, Rule of 40, capital efficiency. Uses stage benchmarks + available data. Auto-suggests grid edits.",
        handler="_tool_analyze_financials",
        input_schema={"company_name": "str"},
        cost_tier="free",
        timeout_ms=10_000,
    ),

    # ------------------------------------------------------------------
    # Batch operations — parallel execution across multiple companies
    # ------------------------------------------------------------------
    AgentTool(
        name="batch_valuate",
        description="Run valuations across multiple companies in parallel batches of 5. Faster than individual run_valuation calls.",
        handler="_tool_batch_valuate",
        input_schema={"companies": "list[str]", "method": "str?"},
        cost_tier="expensive",
        timeout_ms=120_000,
    ),
    AgentTool(
        name="batch_enrich",
        description="Enrich multiple companies in parallel: benchmarks + web search + gap fill. Wrapper around resolve_data_gaps for explicit batch calls.",
        handler="_tool_batch_enrich",
        input_schema={"companies": "list[str]", "fields": "list[str]?"},
        cost_tier="expensive",
        timeout_ms=120_000,
    ),

    # ------------------------------------------------------------------
    # Currency conversion — FX normalization for multi-currency portfolios
    # ------------------------------------------------------------------
    AgentTool(
        name="convert_currency",
        description="Convert all monetary values in shared_data to target currency. Applies FX rates to revenue, valuation, funding for all companies.",
        handler="_tool_convert_currency",
        input_schema={"target_currency": "str", "source_currency": "str?"},
        cost_tier="cheap",
        timeout_ms=15_000,
    ),

    # ------------------------------------------------------------------
    # CRM granular operations — search, log, pipeline
    # ------------------------------------------------------------------
    AgentTool(
        name="crm_search",
        description="Search CRM (Attio/Affinity) for companies, deals, or notes matching a query.",
        handler="_tool_crm_search",
        input_schema={"query": "str", "entity_type": "str?"},
        cost_tier="cheap",
        timeout_ms=15_000,
    ),
    AgentTool(
        name="crm_log_interaction",
        description="Log a meeting, call, email, or note to CRM for a company.",
        handler="_tool_crm_log_interaction",
        input_schema={"company": "str", "note_type": "str", "content": "str", "title": "str?"},
        cost_tier="cheap",
        timeout_ms=15_000,
    ),
    AgentTool(
        name="crm_pipeline_update",
        description="Update deal pipeline stage in CRM. Stages: sourced, screening, dd, ic_review, term_sheet, closed, passed.",
        handler="_tool_crm_pipeline_update",
        input_schema={"company": "str", "stage": "str", "deal_value": "float?", "notes": "str?"},
        cost_tier="cheap",
        timeout_ms=15_000,
    ),
    # ------------------------------------------------------------------
    # Memo canvas — primary output tool for incremental analysis
    # ------------------------------------------------------------------
    AgentTool(
        name="write_to_memo",
        description="Write a section to the analysis memo (prose + optional chart/table). Streams to user in real-time.",
        handler="_tool_write_to_memo",
        input_schema={
            "section_title": "str?",
            "text": "str",
            "chart_type": "str?",
            "chart_data": "dict?",
            "table": "dict?",
        },
        cost_tier="free",
        timeout_ms=5_000,
    ),

    # ------------------------------------------------------------------
    # Micro-skill tools — self-contained, agent-callable portfolio loops
    # ------------------------------------------------------------------
    AgentTool(
        name="run_followon_analysis",
        description=(
            "Loop every company in the portfolio grid and flag who needs follow-on capital. "
            "Checks runway vs burn against stage benchmarks. Companies with <9mo runway are "
            "flagged URGENT; 9-15mo flagged WATCH. Auto-persists suggestions to grid badges. "
            "Use when asked 'who needs capital', 'runway check', or 'follow-on'."
        ),
        handler="_tool_run_followon_analysis",
        input_schema={"fund_id": "str?", "runway_threshold_months": "int?"},
        cost_tier="cheap",
        timeout_ms=60_000,
    ),
    AgentTool(
        name="run_benchmark_scan",
        description=(
            "Run stage benchmark analysis across ALL portfolio companies: stage fit, ARR vs benchmark, "
            "burn rate, growth outliers, valuation coherence. Emits per-company grid suggestions for "
            "any field that is missing or significantly out of range. Use for 'fill in missing data', "
            "'benchmark all companies', or 'what fields are missing from my portfolio'."
        ),
        handler="_tool_run_portfolio_health",
        input_schema={"fund_id": "str?", "fields": "list[str]?"},
        cost_tier="cheap",
        timeout_ms=60_000,
    ),
    AgentTool(
        name="enrich_sparse_companies",
        description=(
            "Find companies with 3+ missing core fields and enrich them: Tier 1 instant benchmarks "
            "then Tier 2 parallel web searches to fill gaps. Emits grid suggestions with sources. "
            "Use when grid has many blank cells or user asks 'fill in the blanks' / 'enrich my portfolio'."
        ),
        handler="_tool_enrich_sparse_companies",
        input_schema={"fund_id": "str?", "companies": "list[str]?", "fields": "list[str]?", "min_missing": "int?"},
        cost_tier="expensive",
        timeout_ms=90_000,
    ),

    # ------------------------------------------------------------------
    # Sourcing & List Engine — DB query + rubric-driven web discovery + scoring
    # ------------------------------------------------------------------
    AgentTool(
        name="source_companies",
        description=(
            "Query, filter, score, and rank companies from the database. Fast — no web calls by default. "
            "Populate filters directly as structured JSON — do NOT pass a natural-language query string. "
            "Filter keys (all optional): sector (str, ilike), stage (str, e.g. 'Series B'), "
            "geography (str, matches hq_location), keyword (str, searches name+description+sector+category), "
            "business_model (str), round_name (str), arr_min/arr_max (number), "
            "valuation_min/valuation_max (number), funding_min/funding_max (number), "
            "founded_after/founded_before (int year), has_arr (bool), "
            "raised_within_months (int, companies that raised within N months), "
            "latest_round_date_after/latest_round_date_before (str date, e.g. '2024-01-01'). "
            "sort_by: name|arr|valuation|total_funding|growth_rate|employee_count|founded_year|burn_rate|runway. "
            "Set enrich_top_n to run microskills (benchmarks + Tavily searches + valuations) on the top N results. "
            "Set persist_results=true to upsert enriched results back to the DB. "
            "Set discover_web=true to search the web for NEW companies when DB results are thin "
            "(< min_web_threshold, default 5). Web discovery uses LLM-generated Tavily queries, "
            "extracts company names, enriches via parallel fetch, normalises to DB schema, "
            "scores with the same rubric, deduplicates, and merges into the ranked list. "
            "Pass thesis (str) to guide web query generation — reuse the thesis from generate_rubric."
        ),
        handler="_tool_source_companies",
        input_schema={
            "filters": "dict",
            "sort_by": "str?",
            "sort_desc": "bool?",
            "display": "str?",
            "max_results": "int?",
            "target_stage": "str?",
            "custom_weights": "dict?",
            "enrich_top_n": "int?",
            "persist_results": "bool?",
            "discover_web": "bool?",
            "min_web_threshold": "int?",
            "thesis": "str?",
        },
        cost_tier="cheap",
        timeout_ms=120_000,  # 15s DB-only, up to 120s with web discovery
    ),
    # ------------------------------------------------------------------
    # Rubric generation — turn thesis into scoring weights before sourcing
    # ------------------------------------------------------------------
    AgentTool(
        name="generate_rubric",
        description=(
            "Turn a natural-language investment thesis into a scoring rubric with custom weights and filters. "
            "Call this BEFORE source_companies when the user describes an investment thesis "
            "(e.g. 'Series A B2B SaaS in fintech, $3-10M ARR, >100% NRR'). "
            "Returns: weights dict, filters dict, target_stage, description, "
            "intent (dealflow/acquirer/gtm_leads/lp_investor/service_provider/talent), "
            "entity_type (startup/company/investor/person), search_context, extraction_hint. "
            "Flow: generate_rubric → source_companies(custom_weights=rubric.weights, filters=rubric.filters, "
            "discover_web=true, thesis=<original request>). "
            "The rubric auto-classifies intent and adapts the entire sourcing pipeline."
        ),
        handler="_tool_generate_rubric",
        input_schema={
            "thesis": "str",
            "weight_overrides": "dict?",
            "target_stage": "str?",
            "filters": "dict?",
        },
        cost_tier="cheap",
        timeout_ms=5_000,
    ),
]

# Quick lookup by name (includes ALL tools — agent-visible + internal)
AGENT_TOOL_MAP: dict[str, AgentTool] = {t.name: t for t in AGENT_TOOLS}


# ---------------------------------------------------------------------------
# Tool Wiring: declarative dependency graph for automatic prerequisite
# resolution.  Each tool declares the shared_data keys it *requires* and
# the keys it *produces*.  Before executing any tool, the wiring layer
# checks shared_data for missing requires and recursively runs whichever
# tool produces the missing key.  Parallel where independent.
# ---------------------------------------------------------------------------

TOOL_WIRING: dict[str, dict] = {
    # ── Core data fetching ─────────────────────────────────────────────
    "fetch_company_data": {
        "requires": [],
        "produces": ["companies"],
    },
    "lightweight_diligence": {
        "requires": [],
        "produces": ["companies"],
    },
    "search_companies_db": {
        "requires": [],
        "produces": [],
    },
    "add_company_to_matrix": {
        "requires": [],
        "produces": [],
    },
    "enrich_company_proactive": {
        "requires": [],
        "produces": ["companies"],
    },
    "build_company_list": {
        "requires": [],
        "produces": ["companies"],
    },
    "source_companies": {
        "requires": [],
        "produces": ["companies"],
    },
    "generate_rubric": {
        "requires": [],
        "produces": [],
    },
    "search_and_extract": {
        "requires": [],
        "produces": ["companies"],
    },

    # ── Granular company search (no prereqs, feed into companies) ──────
    "search_company_funding": {
        "requires": [],
        "produces": ["companies"],
    },
    "search_company_product": {
        "requires": [],
        "produces": ["companies"],
    },
    "search_company_team": {
        "requires": [],
        "produces": ["companies"],
    },
    "search_company_market": {
        "requires": [],
        "produces": ["companies"],
    },
    "analyze_financials": {
        "requires": ["companies"],
        "produces": [],
    },

    # ── Valuation & analysis ───────────────────────────────────────────
    "run_valuation": {
        "requires": ["companies"],
        "produces": ["valuations", "scenario_analysis"],
    },
    "run_scenario": {
        "requires": ["companies"],
        "produces": ["scenario_analysis"],
    },
    "run_fpa": {
        "requires": ["companies"],
        "produces": [],
    },

    # ── Cap table & ownership (internal — wiring-only) ─────────────────
    "cap_table_evolution": {
        "requires": ["companies"],
        "produces": ["cap_table_history"],
    },
    "liquidation_waterfall": {
        "requires": ["companies"],
        "produces": [],
    },
    "anti_dilution_modeling": {
        "requires": ["companies"],
        "produces": [],
    },
    "debt_conversion_modeling": {
        "requires": ["companies"],
        "produces": [],
    },

    # ── Financial modeling (internal — wiring-only) ────────────────────
    "revenue_projection": {
        "requires": ["companies"],
        "produces": ["revenue_projections"],
    },
    "run_projection": {
        "requires": ["companies"],
        "produces": ["revenue_projections"],
    },

    # ── Scenario trees & cash flow ────────────────────────────────────
    "run_scenario_tree": {
        "requires": ["companies"],
        "produces": ["scenario_analysis"],
    },
    "run_cash_flow_model": {
        "requires": ["companies"],
        "produces": [],
    },

    # ── Scenario & stress testing (internal — wiring-only) ─────────────
    "stress_test_portfolio": {
        "requires": ["companies"],
        "produces": [],
    },
    "world_model_scenario": {
        "requires": ["companies"],
        "produces": [],
    },
    "monte_carlo_portfolio": {
        "requires": ["companies"],
        "produces": [],
    },
    "sensitivity_matrix": {
        "requires": ["companies"],
        "produces": [],
    },

    # ── Portfolio operations ───────────────────────────────────────────
    "run_portfolio_health": {
        "requires": ["companies"],
        "produces": ["portfolio_health"],
    },
    "run_followon_strategy": {
        "requires": ["companies"],
        "produces": ["followon_strategy"],
    },
    "run_exit_modeling": {
        "requires": ["companies"],
        "produces": ["exit_modeling"],
    },
    "run_round_modeling": {
        "requires": ["companies"],
        "produces": [],
    },
    "run_regression": {
        "requires": ["companies"],
        "produces": [],
    },

    # ── Generation ──
    # Memo and deck declare heavier prereqs so the wiring layer auto-resolves
    # valuations, cap tables, and scenarios BEFORE generation starts.
    # _hydrate_shared_data_from_companies provides lightweight fallbacks for
    # any keys the real tools fail to populate.
    "generate_memo": {
        "requires": ["companies", "valuations", "cap_table_history", "scenario_analysis"],
        "produces": ["memo_artifacts"],
    },
    "generate_deck": {
        "requires": ["companies", "valuations", "cap_table_history", "scenario_analysis"],
        "produces": ["deck_slides"],
    },
    "generate_ic_memo": {
        "requires": ["companies"],
        "produces": ["memo_artifacts"],
    },
    "generate_followon_memo": {
        "requires": ["companies", "followon_strategy"],
        "produces": ["memo_artifacts"],
    },
    "generate_lp_report": {
        "requires": ["companies", "fund_metrics", "portfolio_health"],
        "produces": ["memo_artifacts"],
    },
    "generate_gp_update": {
        "requires": ["companies", "fund_metrics", "portfolio_health"],
        "produces": ["memo_artifacts"],
    },
    "generate_comparison_report": {
        "requires": ["companies"],
        "produces": ["memo_artifacts"],
    },
    "generate_plan_memo": {
        "requires": ["companies"],
        "produces": ["memo_artifacts"],
    },
    "run_report": {
        "requires": ["companies"],
        "produces": ["memo_artifacts"],
    },

    # ── Fund metrics & intelligence ────────────────────────────────────
    "calculate_fund_metrics": {
        "requires": [],
        "produces": ["fund_metrics"],
    },
    "query_portfolio": {
        "requires": [],
        "produces": ["companies"],
    },
    "query_documents": {
        "requires": [],
        "produces": [],
    },
    "portfolio_comparison": {
        "requires": ["companies"],
        "produces": [],
    },
    "graduation_rates": {
        "requires": [],
        "produces": [],
    },
    "bulk_operation": {
        "requires": ["companies"],
        "produces": [],
    },
    "add_company_to_portfolio": {
        "requires": [],
        "produces": ["companies"],
    },
    "enrich_portfolio": {
        "requires": [],
        "produces": ["companies"],
    },
    "enrich_sparse_grid": {
        "requires": [],
        "produces": ["companies"],
    },
    "resolve_data_gaps": {
        "requires": ["companies"],
        "produces": ["companies"],
    },
    "enrich_cell": {
        "requires": [],
        "produces": [],
    },

    # ── Market & intelligence ──────────────────────────────────────────
    "market_landscape": {
        "requires": [],
        "produces": [],
    },
    "market_timing": {
        "requires": [],
        "produces": [],
    },
    "company_history": {
        "requires": [],
        "produces": [],
    },

    # ── Financial calculators ──────────────────────────────────────────
    "financial_calculator": {
        "requires": [],
        "produces": [],
    },
    "fund_deployment_model": {
        "requires": [],
        "produces": [],
    },
    "fx_check": {
        "requires": [],
        "produces": [],
    },
    "fx_portfolio_impact": {
        "requires": [],
        "produces": [],
    },

    # ── Compliance & M&A ───────────────────────────────────────────────
    "compliance_check": {
        "requires": [],
        "produces": [],
    },
    "ma_workflow": {
        "requires": [],
        "produces": [],
    },

    # ── Misc agent tools ───────────────────────────────────────────────
    "web_search": {
        "requires": [],
        "produces": [],
    },
    "suggest_grid_edit": {
        "requires": [],
        "produces": [],
    },
    "suggest_action": {
        "requires": [],
        "produces": [],
    },
    "write_to_memo": {
        "requires": [],
        "produces": [],
    },
    "generate_chart": {
        "requires": [],
        "produces": [],
    },
    "parse_accounts": {
        "requires": [],
        "produces": [],
    },
    "run_skill": {
        "requires": [],
        "produces": [],
    },
    "reason_company_list": {
        "requires": ["companies"],
        "produces": [],
    },
    "emit_todo": {
        "requires": [],
        "produces": [],
    },
    "write_to_memo": {
        "requires": [],
        "produces": [],
    },
    "sync_crm": {
        "requires": [],
        "produces": [],
    },

    # ── Batch operations ──────────────────────────────────────────────
    "batch_valuate": {
        "requires": ["companies"],
        "produces": ["valuations"],
    },
    "batch_enrich": {
        "requires": [],
        "produces": ["companies"],
    },
    "convert_currency": {
        "requires": ["companies"],
        "produces": [],
    },
}

# Reverse index: shared_data key → which tool produces it.
# Used by _resolve_prerequisites to know what to call for missing data.
_PRODUCED_BY: dict[str, str] = {}
for _tool_name, _wiring in TOOL_WIRING.items():
    for _key in _wiring.get("produces", []):
        if _key not in _PRODUCED_BY:
            _PRODUCED_BY[_key] = _tool_name

# ---------------------------------------------------------------------------
# Prompt-injected vs callable-only tools.
# _INTERNAL_TOOLS are NOT in the agent prompt (saves tokens, avoids
# duplicates) but remain fully callable via _execute_tool() and wiring.
# Only true duplicates/meta-dispatchers belong here — every real tool
# is modular and the agent picks what it needs, no forced workflows.
# ---------------------------------------------------------------------------

_INTERNAL_TOOLS: set[str] = {
    # Memo sub-types — duplicates of generate_memo(memo_type=...)
    "generate_ic_memo",
    "generate_followon_memo",
    "generate_lp_report",
    "generate_gp_update",
    "generate_comparison_report",
    "generate_plan_memo",
    # Meta-dispatchers
    "run_skill",
    "run_report",
    # Internal reasoning step
    "reason_company_list",
}

# Agent-visible tools: everything NOT in _INTERNAL_TOOLS
AGENT_VISIBLE_TOOLS: list[AgentTool] = [t for t in AGENT_TOOLS if t.name not in _INTERNAL_TOOLS]

# Quick lookup for agent-visible tools only (used by agent loop)
AGENT_VISIBLE_TOOL_MAP: dict[str, AgentTool] = {t.name: t for t in AGENT_VISIBLE_TOOLS}

# Full map — every tool is callable via _execute_tool regardless of visibility
ALL_TOOL_MAP: dict[str, AgentTool] = {t.name: t for t in AGENT_TOOLS}

# ---------------------------------------------------------------------------
# Intent-based tool scoping — EXPLICIT per-intent tool lists.
#
# Each intent gets exactly the 5-12 tools it actually needs.  No category
# unions, no snowballing.  All tools remain callable via _execute_tool()
# and wiring regardless — this only controls what appears in the agent prompt.
#
# Design principles:
#   1. Every tool in a list must be plausibly the FIRST tool an agent picks.
#   2. Prerequisite tools (e.g. fetch_company_data for run_valuation) are
#      auto-resolved by TOOL_WIRING — they don't need to be in the prompt.
#   3. "core" grab-bag is gone.  Each intent gets curated picks.
#   4. When in doubt, leave a tool OUT.  The agent can't misuse what it
#      can't see, and wiring will still pull it in when needed.
# ---------------------------------------------------------------------------

INTENT_TOOLS: dict[str, list[str]] = {
    # --- Lookup & enrichment ---
    "company_lookup": [
        "fetch_company_data",       # primary action
        "search_and_extract",       # web research fallback
        "query_portfolio",          # check if already in portfolio
        "lightweight_diligence",    # quick snapshot
        "add_company_to_matrix",    # user may want to save it
        "emit_todo",                # track follow-ups
    ],
    "enrichment": [
        "enrich_cell",              # single-cell fill
        "enrich_sparse_grid",       # bulk grid fill
        "resolve_data_gaps",        # targeted gap filling
        "suggest_grid_edit",        # propose edits
        "query_portfolio",          # read current state
        "web_search",               # supplemental research
    ],
    "grid_edit": [
        "suggest_grid_edit",        # primary action
        "enrich_cell",              # fill during edit
        "resolve_data_gaps",        # fix gaps found during edit
        "query_portfolio",          # read current grid
        "suggest_action",           # propose next steps
    ],

    # --- Analysis ---
    "valuation": [
        "run_valuation",            # primary action (auto-fetches company)
        "cap_table_evolution",      # ownership context
        "run_scenario",             # exit scenarios
        "generate_chart",           # visualize results
        "query_portfolio",          # portfolio context
        "analyze_financials",       # financial detail
    ],
    "scenario": [
        "run_scenario",             # primary action
        "run_scenario_tree",        # branching scenario trees
        "run_bull_bear_base",       # bull/bear/base scenarios
        "apply_macro_shock",        # macro event impact analysis
        "portfolio_snapshot",       # point-in-time portfolio state
        "three_scenario_cash_flow", # bull/base/bear P&L models
        "run_projection",           # revenue projections
        "run_fpa",                  # FP&A: forecast, stress test
        "run_regression",           # regression, monte carlo, sensitivity
        "sensitivity_matrix",       # sensitivity analysis
        "monte_carlo_portfolio",    # probabilistic modeling
        "run_cash_flow_model",      # P&L / cash flow modeling
        "generate_chart",           # visualize
        "query_portfolio",          # portfolio context
    ],
    "forecast": [
        "run_fpa",                  # primary: FP&A forecast, stress test
        "run_bull_bear_base",       # bull/bear/base scenario modeling
        "three_scenario_cash_flow", # 3-scenario P&L
        "run_regression",           # regression, monte carlo, sensitivity, time-series
        "run_projection",           # revenue projections with decay
        "run_scenario_tree",        # branching growth scenarios
        "run_cash_flow_model",      # full P&L / cash flow model
        "sensitivity_matrix",       # 2D sensitivity analysis
        "monte_carlo_portfolio",    # probabilistic portfolio modeling
        "run_scenario",             # scenario composition
        "generate_chart",           # visualize results (tornado, histogram, etc.)
        "query_portfolio",          # portfolio context for company data
    ],
    # Aliases — route to same toolset as "forecast" / "scenario"
    "regression": [
        "run_regression",           # primary: regression analysis
        "run_fpa",                  # broader FP&A context
        "run_projection",           # revenue projections
        "generate_chart",           # visualize regression results
        "query_portfolio",          # portfolio context
    ],
    "sensitivity": [
        "sensitivity_matrix",       # primary: 2D sensitivity analysis
        "run_regression",           # sensitivity via regression tool
        "run_fpa",                  # broader FP&A context
        "generate_chart",           # tornado chart visualization
        "query_portfolio",          # portfolio context
    ],
    "monte_carlo": [
        "monte_carlo_portfolio",    # primary: MC simulation
        "run_regression",           # MC via regression tool
        "run_projection",           # revenue projections
        "generate_chart",           # histogram visualization
        "query_portfolio",          # portfolio context
    ],
    "comparison": [
        "query_portfolio",          # read companies to compare
        "run_valuation",            # value each company
        "generate_chart",           # comparison charts
        "portfolio_comparison",     # side-by-side metrics
        "generate_deck",            # output as deck
        "generate_memo",            # output as memo
    ],

    # --- Generation ---
    "memo": [
        "generate_memo",            # primary action (auto-resolves prereqs)
        "generate_chart",           # embed charts
        "write_to_memo",            # append to existing memo
        "query_portfolio",          # read company data
        "web_search",               # supplemental research
        "emit_todo",                # track follow-ups
    ],
    "deck": [
        "generate_deck",            # primary action (auto-resolves prereqs)
        "generate_chart",           # custom charts for slides
        "query_portfolio",          # read company data
        "run_valuation",            # ensure valuations exist
        "cap_table_evolution",      # cap table slides
        "web_search",               # supplemental research
    ],

    # --- Portfolio & fund ---
    "portfolio": [
        "query_portfolio",          # primary read
        "run_portfolio_health",     # health dashboard
        "calculate_fund_metrics",   # DPI, TVPI, IRR
        "enrich_portfolio",         # fill missing data
        "generate_chart",           # portfolio charts
        "run_followon_strategy",    # follow-on analysis
        "emit_todo",                # action items
    ],
    "sourcing": [
        "generate_rubric",          # thesis → weights + filters (call first)
        "source_companies",         # primary action — DB query + web discovery + scoring
        "query_portfolio",          # portfolio context
        "generate_chart",           # visualize results
        "suggest_grid_edit",        # push to grid
    ],
    "market": [
        "market_landscape",         # primary action
        "source_companies",         # fast DB lookup for market mapping
        "market_timing",            # market timing analysis
        "web_search",               # supplemental research
        "search_and_extract",       # structured extraction
        "lightweight_diligence",    # quick company scans
        "generate_chart",           # market maps
    ],

    # --- Specialized ---
    "fx": [
        "convert_currency",         # single conversion
        "fx_check",                 # rate lookup
        "fx_portfolio_impact",      # portfolio-wide FX exposure
        "query_portfolio",          # read portfolio for FX calc
    ],

    # --- Fallback: intentionally broader but still curated ---
    "general": [
        "fetch_company_data",       # most common need
        "query_portfolio",          # read state
        "web_search",               # research
        "suggest_action",           # help user decide
        "generate_chart",           # visualize anything
        "run_valuation",            # common analysis
        "generate_memo",            # common output
        "emit_todo",                # track items
    ],
}


def get_tools_for_intent(intent: str) -> list[AgentTool]:
    """Return agent-visible tools for a classified intent.

    Uses INTENT_TOOLS for explicit per-intent scoping.  Falls back to
    the 'general' list for unknown intents (never dumps everything).
    All tools remain callable via _execute_tool() and wiring regardless.
    """
    tool_names = INTENT_TOOLS.get(intent, INTENT_TOOLS["general"])
    tool_set = set(tool_names)
    # Preserve declaration order from INTENT_TOOLS for prompt consistency
    return [
        t for name in tool_names
        for t in [AGENT_VISIBLE_TOOL_MAP.get(name)]
        if t is not None
    ]


# ---------------------------------------------------------------------------
# Phase 2: Flexible intent classification & plan-based chaining
# ---------------------------------------------------------------------------

@dataclass
class QueryClassification:
    """Flexible intent classification — NOT a rigid enum.

    The LLM classifies the intent as a free-form string; keyword
    matching is only a fast-path fallback.
    """
    complexity: str  # "simple" | "dealflow" | "complex"
    intent: str  # Free-form, e.g. "portfolio_overview", "company_research", etc.
    suggested_chain: Optional[List[str]] = None  # Tool names in suggested order
    needs_portfolio: bool = False  # Touches existing portfolio data
    needs_external: bool = False  # Needs to fetch external (non-portfolio) company data
    confidence: float = 1.0


@dataclass
class PlanStep:
    """A single step in an execution plan."""
    id: str
    label: str
    tool: str
    inputs: Dict[str, Any] = field(default_factory=dict)
    depends_on: List[str] = field(default_factory=list)  # Step IDs this depends on
    status: str = "pending"  # "pending" | "running" | "done" | "failed" | "skipped"
    output: Optional[Any] = None


@dataclass
class ExecutionPlan:
    """Lightweight execution plan — can be auto-generated or from templates."""
    intent: str
    steps: List[PlanStep] = field(default_factory=list)
    context: Dict[str, Any] = field(default_factory=dict)  # Accumulated results
    source: str = "auto"  # "template" | "llm" | "auto"


@dataclass
class Artifact:
    """Unified artifact produced by the agent — charts, tables, memos, etc."""
    type: str       # "memo_section", "chart", "grid_command", "suggestion", "table", "cap_table", "scenario_result", "plan"
    action: str     # "append", "replace", "pin", "suggest"
    data: Dict[str, Any] = field(default_factory=dict)
    target: Optional[str] = None  # "memo", "grid", "chat", "docs_panel"
    requires_approval: bool = False


@dataclass
class SessionPlan:
    """In-memory execution plan with lifecycle tracking.

    Lives in shared_data — expires with the session.  Optionally serialized
    to the documents table for cross-session resume (see PlanContext).
    """
    plan_id: str
    intent: str
    prompt: str                             # Original user prompt
    steps: List[PlanStep] = field(default_factory=list)
    status: Dict[str, str] = field(default_factory=dict)  # step_id → status
    results: Dict[str, Any] = field(default_factory=dict)  # step_id → output
    created_at: str = ""                    # ISO timestamp
    updated_at: str = ""                    # ISO timestamp
    source: str = "auto"                    # "template" | "llm" | "auto"

    def __post_init__(self):
        now = datetime.utcnow().isoformat()
        if not self.created_at:
            self.created_at = now
        if not self.updated_at:
            self.updated_at = now
        # Ensure status map is in sync with steps
        for step in self.steps:
            if step.id not in self.status:
                self.status[step.id] = step.status

    def mark_running(self, step_id: str):
        self.status[step_id] = "running"
        for s in self.steps:
            if s.id == step_id:
                s.status = "running"
        self.updated_at = datetime.utcnow().isoformat()

    def mark_done(self, step_id: str, output: Any = None):
        self.status[step_id] = "done"
        if output is not None:
            self.results[step_id] = output
        for s in self.steps:
            if s.id == step_id:
                s.status = "done"
                s.output = output
        self.updated_at = datetime.utcnow().isoformat()

    def mark_failed(self, step_id: str, error: str = ""):
        self.status[step_id] = "failed"
        self.results[step_id] = {"error": error}
        for s in self.steps:
            if s.id == step_id:
                s.status = "failed"
        self.updated_at = datetime.utcnow().isoformat()

    def mark_skipped(self, step_id: str):
        self.status[step_id] = "skipped"
        for s in self.steps:
            if s.id == step_id:
                s.status = "skipped"
        self.updated_at = datetime.utcnow().isoformat()

    def next_pending(self) -> Optional[PlanStep]:
        for s in self.steps:
            if s.status == "pending":
                return s
        return None

    def runnable_steps(self) -> List[PlanStep]:
        """Return all pending steps whose dependencies are met (done/skipped)."""
        resolved = {sid for sid, st in self.status.items() if st in ("done", "skipped")}
        return [
            s for s in self.steps
            if s.status == "pending" and all(d in resolved for d in s.depends_on)
        ]

    def is_complete(self) -> bool:
        return all(st in ("done", "skipped", "failed") for st in self.status.values())

    def to_dict(self) -> Dict[str, Any]:
        return {
            "plan_id": self.plan_id,
            "intent": self.intent,
            "prompt": self.prompt,
            "steps": [
                {"id": s.id, "label": s.label, "tool": s.tool, "inputs": s.inputs,
                 "depends_on": s.depends_on, "status": s.status}
                for s in self.steps
            ],
            "results": {k: self._safe_serialize(v) for k, v in self.results.items()},
            "created_at": self.created_at,
            "updated_at": self.updated_at,
            "source": self.source,
        }

    @staticmethod
    def _safe_serialize(v: Any) -> Any:
        """Best-effort JSON-safe representation."""
        if isinstance(v, dict):
            return v
        try:
            json.dumps(v)
            return v
        except (TypeError, ValueError):
            return str(v)

    def persist_to_db(self, fund_id: Optional[str] = None) -> bool:
        """Persist SessionPlan to documents table for cross-session resume."""
        try:
            supabase_url = settings.SUPABASE_URL
            supabase_key = settings.SUPABASE_SERVICE_ROLE_KEY or settings.SUPABASE_ANON_KEY
            if not supabase_url or not supabase_key:
                return False
            from supabase import create_client
            sb = create_client(supabase_url, supabase_key)
            plan_data = self.to_dict()
            sb.table("processed_documents").upsert({
                "id": self.plan_id,
                "document_type": "session_plan",
                "title": f"Plan: {self.prompt[:80]}",
                "fund_id": fund_id,
                "extracted_data": plan_data,
                "processing_status": "complete" if self.is_complete() else "in_progress",
            }, on_conflict="id").execute()
            logger.info(f"[PLAN] Persisted SessionPlan {self.plan_id} to DB")
            return True
        except Exception as e:
            logger.warning(f"[PLAN] Failed to persist SessionPlan: {e}")
            return False

    @classmethod
    def load_from_db(cls, plan_id: str) -> Optional["SessionPlan"]:
        """Load a SessionPlan from the documents table."""
        try:
            supabase_url = settings.SUPABASE_URL
            supabase_key = settings.SUPABASE_SERVICE_ROLE_KEY or settings.SUPABASE_ANON_KEY
            if not supabase_url or not supabase_key:
                return None
            from supabase import create_client
            sb = create_client(supabase_url, supabase_key)
            result = sb.table("processed_documents").select("extracted_data").eq(
                "id", plan_id
            ).eq("document_type", "session_plan").limit(1).execute()
            if not result.data:
                return None
            plan_data = result.data[0].get("extracted_data", {})
            steps = [
                PlanStep(
                    id=s["id"], label=s["label"], tool=s["tool"],
                    inputs=s.get("inputs", {}),
                    depends_on=s.get("depends_on", []),
                    status=s.get("status", "pending"),
                )
                for s in plan_data.get("steps", [])
            ]
            plan = cls(
                plan_id=plan_data.get("plan_id", plan_id),
                intent=plan_data.get("intent", ""),
                prompt=plan_data.get("prompt", ""),
                steps=steps,
                results=plan_data.get("results", {}),
                source=plan_data.get("source", "db"),
            )
            return plan
        except Exception as e:
            logger.warning(f"[PLAN] Failed to load SessionPlan {plan_id}: {e}")
            return None

    @classmethod
    def load_recent(cls, fund_id: Optional[str] = None, limit: int = 3) -> List["SessionPlan"]:
        """Load recent incomplete SessionPlans for a fund."""
        try:
            supabase_url = settings.SUPABASE_URL
            supabase_key = settings.SUPABASE_SERVICE_ROLE_KEY or settings.SUPABASE_ANON_KEY
            if not supabase_url or not supabase_key:
                return []
            from supabase import create_client
            sb = create_client(supabase_url, supabase_key)
            query = sb.table("processed_documents").select("extracted_data").eq(
                "document_type", "session_plan"
            ).eq("processing_status", "in_progress").order(
                "created_at", desc=True
            ).limit(limit)
            if fund_id:
                query = query.eq("fund_id", fund_id)
            result = query.execute()
            plans = []
            for row in (result.data or []):
                plan_data = row.get("extracted_data", {})
                steps = [
                    PlanStep(
                        id=s["id"], label=s["label"], tool=s["tool"],
                        inputs=s.get("inputs", {}),
                        depends_on=s.get("depends_on", []),
                        status=s.get("status", "pending"),
                    )
                    for s in plan_data.get("steps", [])
                ]
                plans.append(cls(
                    plan_id=plan_data.get("plan_id", ""),
                    intent=plan_data.get("intent", ""),
                    prompt=plan_data.get("prompt", ""),
                    steps=steps,
                    results=plan_data.get("results", {}),
                    source="db",
                ))
            return plans
        except Exception as e:
            logger.warning(f"[PLAN] Failed to load recent plans: {e}")
            return []


@dataclass
class PlanContext:
    """Accumulated knowledge that bridges plan-mode to execution-mode — and
    across sessions when serialized to the documents table.

    Within a session: lives in shared_data['plan_context'].
    Across sessions: serialized as a single JSON blob into the existing
    ``documents`` table with ``document_type = 'session_context'``.
    """
    session_id: str
    findings: Dict[str, Any] = field(default_factory=dict)      # research results keyed by entity/topic
    fetched_data: Dict[str, Any] = field(default_factory=dict)   # company data, metrics already retrieved
    reasoning: List[str] = field(default_factory=list)           # chain of reasoning steps
    entity_refs: Dict[str, str] = field(default_factory=dict)    # resolved entity names → IDs
    artifacts: List[Dict[str, Any]] = field(default_factory=list)  # serialized Artifact dicts
    plan: Optional[Dict[str, Any]] = None                        # serialized SessionPlan
    updated_at: str = ""

    def __post_init__(self):
        if not self.updated_at:
            self.updated_at = datetime.utcnow().isoformat()

    def record_finding(self, key: str, value: Any):
        self.findings[key] = value
        self.updated_at = datetime.utcnow().isoformat()

    def record_reasoning(self, step: str):
        self.reasoning.append(step)
        self.updated_at = datetime.utcnow().isoformat()

    def to_dict(self) -> Dict[str, Any]:
        return {
            "session_id": self.session_id,
            "findings": self.findings,
            "fetched_data": self.fetched_data,
            "reasoning": self.reasoning,
            "entity_refs": self.entity_refs,
            "artifacts": self.artifacts,
            "plan": self.plan,
            "updated_at": self.updated_at,
        }

    @classmethod
    def from_dict(cls, d: Dict[str, Any]) -> "PlanContext":
        return cls(
            session_id=d.get("session_id", ""),
            findings=d.get("findings", {}),
            fetched_data=d.get("fetched_data", {}),
            reasoning=d.get("reasoning", []),
            entity_refs=d.get("entity_refs", {}),
            artifacts=d.get("artifacts", []),
            plan=d.get("plan"),
            updated_at=d.get("updated_at", ""),
        )


# Plan templates: optional guidance for the LLM, not rigid constraints.
# Keys are common intent patterns; values are suggested tool chains.
PLAN_TEMPLATES: Dict[str, List[str]] = {
    "company_research": ["fetch_company_data", "run_valuation", "generate_chart"],
    "portfolio_overview": ["query_portfolio", "run_portfolio_health", "calculate_fund_metrics", "generate_chart"],
    "company_deep_dive": ["query_portfolio", "run_portfolio_health", "run_valuation", "run_exit_modeling", "generate_chart"],
    "ic_memo": ["fetch_company_data", "run_valuation", "cap_table_evolution", "generate_ic_memo"],
    "comparison": ["fetch_company_data", "run_valuation", "portfolio_comparison", "generate_comparison_report"],
    "stress_test": ["query_portfolio", "stress_test_portfolio", "run_regression", "generate_chart"],
    "exit_analysis": ["query_portfolio", "run_exit_modeling", "liquidation_waterfall", "generate_chart"],
    "deck_generation": ["fetch_company_data", "run_valuation", "cap_table_evolution", "generate_deck"],
    "lp_report": ["query_portfolio", "calculate_fund_metrics", "run_portfolio_health", "generate_lp_report"],
    "round_modeling": ["query_portfolio", "run_round_modeling", "anti_dilution_modeling", "generate_chart"],
    "followon_decision": ["query_portfolio", "run_followon_strategy", "run_round_modeling", "generate_followon_memo"],
    "fund_metrics": ["calculate_fund_metrics", "query_portfolio", "generate_chart"],
    # Phase 3: New plan templates using new tools
    "cap_table_analysis": ["fetch_company_data", "cap_table_evolution", "liquidation_waterfall", "generate_chart"],
    "ma_analysis": ["fetch_company_data", "ma_workflow", "run_valuation", "generate_chart"],
    "market_research": ["market_landscape", "market_timing", "web_search", "generate_chart"],
    "fund_deployment": ["calculate_fund_metrics", "fund_deployment_model", "revenue_projection", "generate_chart"],
    "compliance_review": ["compliance_check", "query_portfolio", "generate_report"],
    "scenario_analysis": ["fetch_company_data", "world_model_scenario", "sensitivity_matrix", "generate_chart"],
    "portfolio_stress": ["query_portfolio", "stress_test_portfolio", "monte_carlo_portfolio", "generate_chart"],
    "gp_update": ["query_portfolio", "calculate_fund_metrics", "fund_deployment_model", "generate_gp_update"],
    # Phase 8: New plan templates
    "proactive_enrichment": ["enrich_company_proactive", "suggest_grid_edit", "generate_chart"],
    "company_list_building": ["build_company_list", "run_valuation", "generate_memo"],
    "projection_analysis": ["query_portfolio", "run_projection", "generate_chart"],
    "forecast_analysis": ["run_fpa", "run_regression", "run_projection", "sensitivity_matrix", "generate_chart"],
    "revenue_forecast": ["query_portfolio", "run_fpa", "run_projection", "generate_chart"],
    "monte_carlo_analysis": ["query_portfolio", "monte_carlo_portfolio", "generate_chart"],
    "sensitivity_analysis": ["query_portfolio", "run_regression", "sensitivity_matrix", "generate_chart"],
    "search_extract_combo": ["search_and_extract", "run_valuation", "suggest_grid_edit"],
    "sparse_grid_enrich": ["enrich_sparse_grid", "suggest_grid_edit"],
    "sourcing": ["generate_rubric", "source_companies", "generate_chart"],
    "sourcing_with_web": ["generate_rubric", "source_companies", "build_company_list", "generate_memo"],
    "market_mapping": ["generate_rubric", "source_companies", "build_company_list", "web_search", "generate_memo"],
    # Phase 9: Gap-resolution-first templates
    "followon_analysis": ["resolve_data_gaps", "run_valuation", "cap_table_evolution", "run_followon_strategy", "run_round_modeling", "run_projection", "generate_memo"],
    "enrichment_first": ["resolve_data_gaps", "suggest_grid_edit", "generate_memo"],
    "company_list_dynamic": ["build_company_list", "resolve_data_gaps", "run_valuation", "generate_memo", "generate_chart"],
}


@dataclass
class SkillChainNode:
    """Represents a single node in the skill execution chain"""
    skill: str
    purpose: str
    inputs: Dict[str, Any] = field(default_factory=dict)
    parallel_group: int = 0
    depends_on: List[str] = field(default_factory=list)
    result: Optional[Dict[str, Any]] = None
    status: str = "pending"
    required: bool = True  # If False, failure is non-fatal — chain continues


class UnifiedMCPOrchestrator:
    """
    Agentic orchestrator combining MCP tools with skill system
    Features:
    - Self-decomposing: Claude analyzes and plans execution
    - Self-routing: Automatically chooses best skills
    - Self-correcting: Handles missing data gracefully
    - Self-optimizing: Parallel execution when possible
    - Self-formatting: Adapts output to requested format
    """
    
    def __init__(self):
        # Track orchestrator readiness for higher-level health checks
        self._is_ready: bool = False
        self._readiness_error: Optional[str] = None
        _update_readiness_state(False, "Initializing UnifiedMCPOrchestrator")
        
        # Use ModelRouter as the single entry point for all LLM calls
        # This ensures consistent error handling, rate limiting, and fallback
        if MODEL_ROUTER_IMPORT_ERROR or not get_model_router or ModelRouter is None:
            self._readiness_error = (
                f"ModelRouter failed to import: {MODEL_ROUTER_IMPORT_ERROR}"
            )
            _update_readiness_state(False, self._readiness_error)
            logger.critical(
                "[ORCHESTRATOR_INIT] ❌ Unable to import ModelRouter. "
                "Unified orchestrator cannot start.",
                exc_info=MODEL_ROUTER_IMPORT_ERROR
            )
            raise RuntimeError(self._readiness_error) from MODEL_ROUTER_IMPORT_ERROR
        
        try:
            self.model_router = get_model_router()
        except Exception as router_error:
            self._readiness_error = f"ModelRouter initialization error: {router_error}"
            _update_readiness_state(False, self._readiness_error)
            logger.critical(
                "[ORCHESTRATOR_INIT] ❌ Failed to initialize ModelRouter instance.",
                exc_info=router_error
            )
            raise

        if NON_CRITICAL_IMPORT_ERRORS:
            nc_details = "; ".join(
                f"{name}: {exc}" for name, exc in NON_CRITICAL_IMPORT_ERRORS.items()
            )
            logger.warning(
                "[ORCHESTRATOR_INIT] ⚠️ Non-critical dependencies unavailable (degraded): %s",
                nc_details,
            )

        if CRITICAL_IMPORT_ERRORS:
            error_details = "; ".join(
                f"{name}: {exc}" for name, exc in CRITICAL_IMPORT_ERRORS.items()
            )
            self._readiness_error = (
                f"Critical orchestrator dependencies failed to import: {error_details}"
            )
            _update_readiness_state(False, self._readiness_error)
            logger.critical(
                "[ORCHESTRATOR_INIT] ❌ Missing critical orchestrator dependencies.",
                extra={"dependency_errors": error_details}
            )
            raise RuntimeError(self._readiness_error)
        
        self.tavily_api_key = settings.TAVILY_API_KEY
        self.session = None
        
        # Debug logging with clear warnings
        if self.tavily_api_key:
            logger.info(f"✅ [ORCHESTRATOR] Tavily API key configured: {self.tavily_api_key[:10]}...{self.tavily_api_key[-4:] if len(self.tavily_api_key) > 14 else '***'}")
        else:
            logger.warning("🔴 [ORCHESTRATOR] TAVILY_API_KEY is MISSING - Tavily searches will fail!")
            logger.warning("🔴 [ORCHESTRATOR] Set TAVILY_API_KEY in environment or .env file")
        logger.info(f"[ORCHESTRATOR] Using ModelRouter for all LLM calls")
        
        # Caches
        self._tavily_cache = {}
        self._company_cache = {}
        
        # Services
        self.data_extractor = StructuredDataExtractor()
        self.gap_filler = IntelligentGapFiller()
        self.valuation_engine = self._load_valuation_engine()
        self.cap_table_service = PrePostCapTable()
        self.advanced_cap_table = CapTableCalculator()
        self.citation_manager = CitationManager()
        self.ownership_analyzer = OwnershipReturnAnalyzer()
        self.comprehensive_deal_analyzer = ComprehensiveDealAnalyzer()

        # Fund modeling + scenario services
        self.fund_modeling = FundModelingService() if FundModelingService else None
        self.nl_scenario_composer = NLScenarioComposer() if NLScenarioComposer else None
        self.company_history_service = CompanyHistoryAnalysisService() if CompanyHistoryAnalysisService else None

        # Phase 3: Additional services for new agent tools
        self.market_intelligence = MarketIntelligenceService() if MarketIntelligenceService else None
        self.ma_service = MAWorkflowService() if MAWorkflowService else None
        self.compliance_service = EnhancedComplianceService() if EnhancedComplianceService else None
        self.financial_calculator = FinancialCalculator() if FinancialCalculator else None

        # Error handler for retry + circuit breaker
        self.error_handler = global_error_handler

        # Matrix Query Orchestrator for portfolio-aware document queries
        if MATRIX_QUERY_ORCHESTRATOR_AVAILABLE and MatrixQueryOrchestrator:
            try:
                self.matrix_query_orchestrator = MatrixQueryOrchestrator()
                logger.info("[ORCHESTRATOR_INIT] ✅ MatrixQueryOrchestrator initialized")
            except Exception as e:
                logger.warning(f"[ORCHESTRATOR_INIT] ⚠️ Failed to initialize MatrixQueryOrchestrator: {e}")
                self.matrix_query_orchestrator = None
        else:
            self.matrix_query_orchestrator = None
            logger.warning("[ORCHESTRATOR_INIT] ⚠️ MatrixQueryOrchestrator not available")
        
        # Skill Registry
        self.skills = self._initialize_skill_registry()
        
        # Shared data store for skill communication
        self.shared_data = {}
        
        # Synchronization locks for thread-safe data updates
        self.shared_data_lock = asyncio.Lock()
        self.citation_lock = asyncio.Lock()

        self._is_ready = True
        _update_readiness_state(True, None)
        logger.info("[ORCHESTRATOR_INIT] ✅ UnifiedMCPOrchestrator ready")

        # Stage ordering for cap table inference
        self._STAGE_ORDER = [
            "Pre-seed", "Seed", "Series A", "Series B", "Series C", "Series D", "Series E"
        ]
        self._ROUND_KEYWORDS = [
            ("Pre-seed", ["pre-seed", "pre seed", "angel", "accelerator", "friends and family"]),
            ("Seed", ["seed", "seed round"]),
            ("Series A", ["series a", "round a"]),
            ("Series B", ["series b", "round b"]),
            ("Series C", ["series c", "round c"]),
            ("Series D", ["series d", "round d", "growth", "late stage", "mezzanine"]),
            ("Series E", ["series e", "series f", "series g", "series h", "series i", "series j", "series k", "series l", "pre-ipo"])
        ]

    def _load_valuation_engine(self):
        """Ensure the latest valuation engine implementation is loaded."""
        # Use imported classes directly instead of modifying globals
        # This avoids "cannot access local variable" errors
        if valuation_module is None:
            raise RuntimeError("valuation_engine_service module is unavailable due to import failure")
        try:
            # Try to reload for development, but keep original imports if it fails
            module = importlib.reload(valuation_module)
            logger.info("[SERVICE_RELOAD] valuation_engine_service reloaded to pick up latest PWERM schema")
            return module.ValuationEngineService()
        except Exception as exc:
            logger.warning(f"[SERVICE_RELOAD] Unable to reload valuation_engine_service: {exc}, using original imports")
            return valuation_module.ValuationEngineService()

    def _build_system_prompt(self, task_instruction: str) -> str:
        """Build a system prompt with dynamic fund context instead of hardcoded values."""
        fund_ctx = self.shared_data.get("fund_context", {})
        fund_size = fund_ctx.get("fund_size", fund_ctx.get("fundSize", DEFAULT_FUND_SIZE))
        strategy = fund_ctx.get("strategy", DEFAULT_FUND_STRATEGY)
        remaining = fund_ctx.get("remaining_capital", fund_ctx.get("remainingCapital"))
        fund_year = fund_ctx.get("fund_year", fund_ctx.get("fundYear", DEFAULT_FUND_YEAR))
        check_min = fund_ctx.get("check_size_min", 5_000_000)
        check_max = fund_ctx.get("check_size_max", 20_000_000)
        target_ownership = fund_ctx.get("target_ownership_pct", 10)

        size_str = f"${fund_size / 1e6:.0f}M" if fund_size else f"${DEFAULT_FUND_SIZE / 1e6:.0f}M"
        fund_line = f"{size_str} {strategy} fund"
        if remaining:
            fund_line += f", ${remaining / 1e6:.0f}M remaining to deploy"
        if fund_year:
            fund_line += f", Year {fund_year}"

        return (
            f"You are an investment analyst agent for a {fund_line} "
            f"(${check_min / 1e6:.0f}–${check_max / 1e6:.0f}M checks, target {target_ownership}% entry ownership, 1/3 reserves for follow-on). "
            "Reason from evidence to decision — never from question to summary.\n\n"
            "RULES:\n"
            "- DATA FIRST: if revenue, ARR, or valuation is missing, fetch it before writing. Never write with gaps.\n"
            "- Lead with the number or fact that changes the decision. No preamble.\n"
            "- Mark inferred data: 'ARR ~$8M (inferred from headcount + stage, 70% confidence)'.\n"
            "- NEVER say 'no data available' — present estimates with ranges and confidence.\n"
            "- Every financial comparison gets a chart. Every multi-company comparison gets a table.\n"
            "- Cite sources inline: 'ARR $8M ([TechCrunch Jan 2025](url))'. Not as a footer list.\n\n"
            "TOOLS:\n"
            "- Data: company-data-fetcher, market-sourcer, competitive-intelligence, search-extract-combo, sparse-grid-enricher\n"
            "- Valuation: valuation-engine (DCF/comps/cost/milestone), pwerm-calculator, waterfall-calculator, cap-table-generator, exit-modeler, round-modeler, followon-strategy, debt-converter\n"
            "- Analysis: scenario-generator, financial-analyzer, deal-comparer, team-comparison, monte-carlo-simulator, sensitivity-analyzer, time-series-forecaster, revenue-projector\n"
            "- Portfolio: portfolio-analyzer, fund-metrics-calculator, bulk-valuation, fund-analyzer, followon-deep-dive\n"
            "- Grid: nl-matrix-controller — edit cells, write enrichment back to matrix (ARR, valuation, ownership, runway)\n"
            "- Memos (memo-writer, memo_type=...): ic_memo, followon, followon_deep_dive, lp_report, lp_quarterly_enhanced, gp_strategy, "
            "comparison, competitive_landscape, diligence_memo, market_dynamics, team_comparison, ownership_analysis, "
            "portfolio_construction, pipeline_review, fund_analysis, bespoke_lp, comparable_analysis, company_list, citation_report\n"
            "- Charts (chart-generator): probability_cloud (return distribution p10–p90), waterfall (exit proceeds/NAV), "
            "bar_comparison, scatter_multiples (multiple vs growth), cap_table_sankey, revenue_forecast, "
            "dpi_sankey, bull_bear_base, radar_comparison (team scoring)\n"
            "- Deck: deck-storytelling (16–18 slide investment presentation)\n\n"
            "MEMO RULE: For formal deliverables (IC memo, LP report, comparison) — fetch data → run valuations → generate cap table THEN call memo-writer. "
            "Use write_to_memo only for exploratory inline analysis.\n"
            "CHART RULE: Charts are analytical tools. State what the chart reveals before rendering it.\n"
            "SOURCING RULE: When asked to find, source, or build a list of companies/acquirers/leads/investors/LPs:\n"
            "  1. Call generate_rubric(thesis=<user's request>) — returns weights, filters, intent, entity_type, search_context, extraction_hint.\n"
            "  2. Call source_companies(filters=rubric.filters, custom_weights=rubric.weights, "
            "discover_web=true, thesis=<user's request>, min_web_threshold=5, persist_results=true).\n"
            "  3. The rubric auto-classifies intent (dealflow/acquirer/gtm_leads/lp_investor/service_provider/talent) "
            "and adapts search queries, entity extraction, and scoring accordingly.\n"
            "  4. For sourcing, prefer source_companies with discover_web=true over build_company_list.\n\n"
            f"{task_instruction}"
        )

    def _initialize_skill_registry(self) -> Dict[str, Dict[str, Any]]:
        """Initialize the skill registry with 36+ skills"""
        return {
            # Data Gathering Skills
            "company-data-fetcher": {
                "category": SkillCategory.DATA_GATHERING,
                "handler": self._execute_company_fetch,
                "description": "Fetch company metrics, funding, team"
            },
            "funding-aggregator": {
                "category": SkillCategory.DATA_GATHERING,
                "handler": self._execute_funding_aggregation,
                "description": "Aggregate funding history"
            },
            "market-sourcer": {
                "category": SkillCategory.DATA_GATHERING,
                "handler": self._execute_market_research,
                "description": "Market analysis, TAM, trends"
            },
            "source-companies": {
                "category": SkillCategory.DATA_GATHERING,
                "handler": self._tool_source_companies,
                "description": "Query, filter, score, rank companies from DB with optional web discovery"
            },
            "generate-rubric": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._tool_generate_rubric,
                "description": "Turn investment thesis into scoring weights + filters"
            },
            "competitive-intelligence": {
                "category": SkillCategory.DATA_GATHERING,
                "handler": self._execute_competitive_analysis,
                "description": "Competitor analysis"
            },
            
            # Analysis Skills
            "valuation-engine": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_valuation,
                "description": "DCF, comparables valuation"
            },
            "pwerm-calculator": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_pwerm,
                "description": "PWERM valuation"
            },
            "financial-analyzer": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_financial_analysis,
                "description": "Ratios, projections"
            },
            "scenario-generator": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_scenario_analysis,
                "description": "Monte Carlo, sensitivity"
            },
            "deal-comparer": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_deal_comparison,
                "description": "Multi-company comparison"
            },
            
            # Generation Skills
            "deck-storytelling": {
                "category": SkillCategory.GENERATION,
                "handler": self._execute_deck_generation,
                "description": "Presentation generation"
            },
            "excel-generator": {
                "category": SkillCategory.GENERATION,
                "handler": self._execute_excel_generation,
                "description": "Spreadsheet creation"
            },
            "memo-writer": {
                "category": SkillCategory.GENERATION,
                "handler": self._execute_memo_generation,
                "description": "Document generation"
            },
            "chart-generator": {
                "category": SkillCategory.GENERATION,
                "handler": self._execute_chart_generation,
                "description": "Data visualization"
            },
            
            # Cap Table & Fund Management Skills
            "cap-table-generator": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_cap_table_generation,
                "description": "Generate cap tables with ownership"
            },
            "portfolio-analyzer": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_portfolio_analysis,
                "description": "Analyze fund portfolio performance"
            },
            "fund-metrics-calculator": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_fund_metrics,
                "description": "Calculate DPI, TVPI, IRR"
            },
            "stage-analyzer": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_stage_analysis,
                "description": "Multi-stage investment analysis"
            },
            "exit-modeler": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_exit_modeling,
                "description": "Model exit scenarios and returns"
            },
            "followon-strategy": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_followon_strategy,
                "description": "Analyze follow-on, extension, sell decisions for portfolio companies"
            },
            "round-modeler": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_round_modeling,
                "description": "Model next round (Series D, etc.) with dilution and waterfall"
            },
            "report-generator": {
                "category": SkillCategory.GENERATION,
                "handler": self._execute_report_generation,
                "description": "Generate LP quarterly, follow-on memo, or GP strategy reports"
            },

            # FPA / Modeling Skills - regression, forecast, Monte Carlo, sensitivity
            "regression-analyzer": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_regression_analysis,
                "description": "Linear regression, curve fitting, R-squared analysis"
            },
            "time-series-forecaster": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_time_series_forecast,
                "description": "Time series forecast with confidence intervals"
            },
            "growth-decay-forecaster": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_growth_decay_forecast,
                "description": "Exponential growth/decay modeling and half-life"
            },
            "monte-carlo-simulator": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_monte_carlo,
                "description": "Monte Carlo simulation with probability distributions"
            },
            "sensitivity-analyzer": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_sensitivity_analysis,
                "description": "Sensitivity/tornado analysis on key variables"
            },
            "fund-analyzer": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_fund_analysis,
                "description": "Comprehensive fund analysis with follow-on strategy"
            },
            "portfolio-scenario-modeler": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_portfolio_scenario_modeling,
                "description": "Model fund return scenarios: what if company A exits at 5x while company B bridges"
            },
            "company-health-dashboard": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_company_health_dashboard,
                "description": "Portfolio health: growth decay, burn/runway, funding trajectory, signals"
            },

            # Phase 3: Expanded skill registry — wire remaining services
            "deck-quality-validator": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_deck_quality_validation,
                "description": "Validate deck quality: hardcoded defaults, estimation markers, consistency"
            },
            "slide-content-optimizer": {
                "category": SkillCategory.GENERATION,
                "handler": self._execute_slide_optimization,
                "description": "Optimize slide text, bullets, metrics for presentation"
            },
            "formula-evaluator": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_formula_evaluation,
                "description": "Evaluate Excel-like formulas with cell references and functions"
            },
            "arithmetic-engine": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_arithmetic,
                "description": "Sum, avg, median, percentile, rank, stdev and other calculations"
            },
            "company-history-analyzer": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_company_history,
                "description": "Full company history with funding rounds, investors, DPI Sankey"
            },
            "nl-matrix-controller": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_nl_matrix_command,
                "description": "Natural language to matrix commands: show columns, filter, sort"
            },
            "waterfall-calculator": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_waterfall_calculation,
                "description": "Liquidation waterfall with investor distributions at exit"
            },
            "debt-converter": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_debt_conversion,
                "description": "Model SAFE/convertible note conversions and debt instruments"
            },
            "market-landscape": {
                "category": SkillCategory.DATA_GATHERING,
                "handler": self._execute_market_landscape,
                "description": "Competitive landscape by sector, geography, stage"
            },
            "revenue-projector": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_revenue_projection,
                "description": "Revenue projection with quality-adjusted decay curves"
            },
            "compliance-checker": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_compliance_check,
                "description": "Filing requirements, Form ADV, regulatory calendar"
            },
            "ma-modeler": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_ma_modeling,
                "description": "M&A deal modeling with synergies and integration risk"
            },

            # Phase 7: Enhanced Agent Skills — bulk ops, LP queries, team comparison
            "bulk-valuation": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_bulk_valuation,
                "description": "Run valuation engine across all portfolio companies in parallel"
            },
            "multi-enrich": {
                "category": SkillCategory.DATA_GATHERING,
                "handler": self._execute_multi_enrich,
                "description": "Enrich multiple companies with dynamic Tavily searches based on data gaps"
            },
            "lp-query-response": {
                "category": SkillCategory.GENERATION,
                "handler": self._execute_lp_query_response,
                "description": "Answer LP questions with fund data, portfolio metrics, DPI Sankey"
            },
            "team-comparison": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_team_comparison,
                "description": "Compare founding teams across 2-4 companies with radar scoring"
            },
            "followon-deep-dive": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_followon_deep_dive,
                "description": "Deep follow-on analysis with cap table evolution and breakpoints"
            },
            "competitive-landscape-memo": {
                "category": SkillCategory.GENERATION,
                "handler": self._execute_competitive_landscape_memo,
                "description": "Generate competitive landscape memo with scatter positioning"
            },

            # Phase 8: Proactive enrichment, search/extract, projections, company lists
            "proactive-enricher": {
                "category": SkillCategory.DATA_GATHERING,
                "handler": self._execute_proactive_enrich,
                "description": "Auto-fetch companies mentioned without @ and push enriched data to grid"
            },
            "company-list-builder": {
                "category": SkillCategory.DATA_GATHERING,
                "handler": self._execute_company_list_search,
                "description": "Search for companies by criteria and build enriched deal flow list"
            },
            "search-extract-combo": {
                "category": SkillCategory.DATA_GATHERING,
                "handler": self._execute_search_extract,
                "description": "Targeted web search + structured extraction for specific data points"
            },
            "projection-modeler": {
                "category": SkillCategory.ANALYSIS,
                "handler": self._execute_projection_model,
                "description": "Revenue/ARR projection with decay curves and scenario bands"
            },
            "sparse-grid-enricher": {
                "category": SkillCategory.DATA_GATHERING,
                "handler": self._execute_sparse_grid_enrich,
                "description": "Auto-detect and fill sparse grid data via web search + benchmarks"
            },

            # Phase 6: Grid skills - emit grid_commands for frontend to run via onRunService
            # Generated from cell_action_registry - covers all registry action_ids
            **self._build_grid_skill_registry()
        }
    
    def _build_grid_skill_registry(self) -> Dict[str, Dict[str, Any]]:
        """Build grid-run-* skill registry from GRID_ACTION_MAP (cell_action_registry alignment)."""
        registry = {}
        for skill_name, (action_id, col_hint) in GRID_ACTION_MAP.items():
            registry[skill_name] = {
                "category": SkillCategory.ANALYSIS,
                "handler": self._make_grid_run_handler(action_id, col_hint),
                "description": f"Emit run {action_id} per company row"
            }
        return registry
    
    def _make_grid_run_handler(self, action_id: str, column_hint: str = "value"):
        """Return async handler that emits grid_commands for given action_id."""
        async def handler(inputs: Dict[str, Any]) -> Dict[str, Any]:
            return self._emit_grid_run_commands(action_id, column_hint)
        return handler
    
    def _emit_grid_run_commands(
        self,
        action_id: str,
        column_hint: Optional[str] = None,
        entities_override: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Build grid_commands for run actions. Used by grid-run-* skills."""
        matrix_ctx = self.shared_data.get("matrix_context") or {}
        if not matrix_ctx or not (matrix_ctx.get("rowIds") or matrix_ctx.get("row_ids")):
            return {"grid_commands": []}
        entities = entities_override
        if entities is None:
            companies = self.shared_data.get("companies") or []
            names = [c.get("company") or c.get("company_name") for c in companies if isinstance(c, dict)]
            entities = {"companies": names} if names else {}
        targets = self._get_target_row_ids(matrix_ctx, entities)
        columns = matrix_ctx.get("columns") or []
        col = self._column_id_for_field(matrix_ctx, column_hint or "value")
        if not col and columns:
            col = columns[0].get("id") or columns[0].get("columnId")
        if not col:
            col = "default"
        commands = [
            {"action": "run", "rowId": rid, "columnId": col, "actionId": action_id}
            for rid, _ in targets
        ]
        return {"grid_commands": commands}
    
    async def _execute_grid_analysis(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analyze the grid: gaps, trends, outliers. Reads matrix_context/gridSnapshot,
        calls LLM, returns gaps, outliers, suggested_actions (and optional grid_commands).
        """
        matrix_ctx = self.shared_data.get("matrix_context") or {}
        grid_snapshot = matrix_ctx.get("gridSnapshot") or matrix_ctx.get("rows") or []
        columns = matrix_ctx.get("columns") or []
        row_ids = matrix_ctx.get("rowIds") or matrix_ctx.get("row_ids") or []
        company_names = matrix_ctx.get("companyNames") or matrix_ctx.get("company_names") or []
        if not grid_snapshot and not (row_ids and company_names):
            return {
                "grid_analysis": {
                    "gaps": [],
                    "outliers": [],
                    "suggested_actions": [],
                    "explanation": "No grid data in context. Send matrix context with rowIds, companyNames, and optionally gridSnapshot (rows with cells) to analyze the grid."
                }
            }
        # Build a compact summary for the LLM (cap ~3KB)
        rows_summary = []
        if isinstance(grid_snapshot, list):
            for r in grid_snapshot[:50]:
                name = r.get("companyName") or r.get("company_name") or ""
                cells = r.get("cells") or r.get("cellValues") or {}
                rows_summary.append({"company": name, "cells": dict(list(cells.items())[:15])})
        else:
            for i in range(min(len(row_ids), len(company_names), 50)):
                rows_summary.append({
                    "rowId": row_ids[i] if i < len(row_ids) else "",
                    "company": company_names[i] if i < len(company_names) else "",
                    "cells": {}
                })
        col_names = [c.get("name") or c.get("id") or c.get("label") or "" for c in columns[:30]]
        prompt = f"""Analyze this portfolio/spreadsheet grid.

<grid_rows>
{json.dumps(rows_summary, default=str)[:4000]}
</grid_rows>

<column_names>
{json.dumps(col_names)}
</column_names>

Identify:
1. gaps: missing or empty key fields (e.g. valuation, ARR, revenue, funding_rounds) and for which companies.
2. outliers: values that look unusually high/low compared to the rest (with company name).
3. suggested_actions: 1-5 concrete actions (e.g. "Run valuation for Company X", "Enrich funding history for rows missing funding_rounds").

RESPOND WITH JUST THE JSON. NO MARKDOWN FENCES. NO EXPLANATION.
{{"gaps": [{{"company": "...", "field": "...", "suggestion": "..."}}], "outliers": [{{"company": "...", "field": "...", "value": "...", "note": "..."}}], "suggested_actions": ["...", "..."], "summary": "1-2 sentence summary"}}
JUST THE JSON:"""
        try:
            result = await self.model_router.get_completion(
                prompt=prompt,
                system_prompt="You are a JSON extraction machine. Return ONLY raw JSON. No markdown. No fences. No explanation.",
                capability=ModelCapability.STRUCTURED,
                max_tokens=1200,
                temperature=0,
                json_mode=True,
                fallback_enabled=True
            )
            content = (result.get("response") or "").strip()
            from app.services.micro_skills.search_skills import _parse_llm_json
            analysis = _parse_llm_json(content, "grid_analyzer", "grid")
            if not analysis or not isinstance(analysis, dict):
                analysis = {"gaps": [], "outliers": [], "suggested_actions": [], "summary": content[:500]}
        except Exception as e:
            logger.warning(f"[GRID_ANALYZER] LLM analysis failed: {e}")
            analysis = {"gaps": [], "outliers": [], "suggested_actions": [], "summary": str(e)}

        # ── Auto-enrich: if we found gaps, run gap resolver on those companies ──
        gap_companies = list({g.get("company", "") for g in analysis.get("gaps", []) if g.get("company")})
        enrichment_result = None
        if gap_companies:
            logger.info(f"[GRID_ANALYZER] Auto-enriching {len(gap_companies)} companies with gaps: {gap_companies[:10]}")
            try:
                enrichment_result = await self._tool_resolve_gaps({
                    "companies": gap_companies,
                })
                analysis["auto_enriched"] = {
                    "companies": gap_companies,
                    "fields_filled": enrichment_result.get("total_fields_filled", 0),
                    "skills_run": enrichment_result.get("skills_run", []),
                }
                analysis["grid_commands"] = enrichment_result.get("grid_commands", [])
                logger.info(f"[GRID_ANALYZER] Auto-enriched {len(gap_companies)} companies, filled {enrichment_result.get('total_fields_filled', 0)} fields")
            except Exception as enrich_err:
                logger.warning(f"[GRID_ANALYZER] Auto-enrich failed: {enrich_err}")
                analysis["auto_enriched"] = {"error": str(enrich_err)}

        return {
            "grid_analysis": analysis,
            **({"enrichment": enrichment_result} if enrichment_result else {}),
        }
    
    async def process_request(
        self,
        prompt: Union[str, Dict[str, Any]],
        output_format: str = "analysis",
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Process a request synchronously
        Can accept either individual parameters or a dict containing them
        """
        logger.info(f"[ORCHESTRATOR] process_request called with output_format: {output_format}")
        
        # Handle dict input for backward compatibility
        if isinstance(prompt, dict):
            # Check if this is a direct skill invocation
            skill_name = prompt.get("skill")
            if skill_name:
                logger.info(f"[ORCHESTRATOR] Direct skill invocation detected: {skill_name}")
                
                # Map old skill names to new ones for backwards compatibility
                skill_mapping = {
                    "generate_deck": "deck-storytelling",
                    "generate_investment_deck": "deck-storytelling",
                    # Add other mappings as needed
                }
                skill_name = skill_mapping.get(skill_name, skill_name)
                logger.info(f"[ORCHESTRATOR] Mapped skill name: {skill_name}")
                
                # Execute skill directly
                inputs = prompt.get("inputs", {})
                
                # CRITICAL FIX: Populate shared_data with companies and fund_context
                # Store fund_context in shared_data if provided (check both prompt and inputs)
                fund_context = prompt.get("fund_context") or inputs.get("fund_context")
                if fund_context:
                    async with self.shared_data_lock:
                        self.shared_data["fund_context"] = fund_context
                    logger.info(f"[ORCHESTRATOR] Stored fund_context in shared_data")
                
                # Check for companies in both prompt (top level) and inputs
                companies_input = prompt.get("companies") or inputs.get("companies")
                if companies_input:
                    logger.info(f"[ORCHESTRATOR] Companies provided: {len(companies_input)} companies found")
                    
                    # Check if companies are just names (strings) that need fetching
                    if companies_input and isinstance(companies_input[0], str):
                        logger.info(f"[ORCHESTRATOR] Companies are strings, need to fetch data")
                        
                        # CRITICAL FIX: Actually fetch the companies instead of just building a prompt
                        logger.info(f"[ORCHESTRATOR] Fetching data for {len(companies_input)} companies: {companies_input}")
                        
                        # Fetch each company
                        fetched_companies = []
                        for company_str in companies_input:
                            # Remove @ symbol if present
                            company_name = company_str.replace('@', '').strip()
                            logger.info(f"[ORCHESTRATOR] Fetching company: '{company_name}' (handle: '{company_str}')")
                            
                            try:
                                # Use the company fetch skill directly
                                fetch_result = await self._execute_company_fetch({
                                    'company': company_name,
                                    'prompt_handle': company_str
                                })
                                
                                if fetch_result and fetch_result.get('companies'):
                                    logger.info(f"[ORCHESTRATOR] Successfully fetched {len(fetch_result['companies'])} results for '{company_name}'")
                                    fetched_companies.extend(fetch_result['companies'])
                                else:
                                    logger.warning(f"[ORCHESTRATOR] No data returned for company '{company_name}'")
                            except Exception as e:
                                logger.error(f"[ORCHESTRATOR] Failed to fetch company '{company_name}': {e}")
                                # Continue with other companies even if one fails
                        
                        if not fetched_companies:
                            logger.error(f"[ORCHESTRATOR] Failed to fetch any company data")
                            return {"success": False, "error": "Failed to fetch company data"}
                        
                        logger.info(f"[ORCHESTRATOR] Fetched {len(fetched_companies)} total companies")
                        
                        # Ensure all have inferred data and validate
                        enriched_companies = await self._ensure_companies_have_inferred_data(fetched_companies)
                        # Validate all company data to prevent None errors
                        enriched_companies = [validate_company_data(c) for c in enriched_companies]
                        logger.info(f"[ORCHESTRATOR] Enriched and validated {len(enriched_companies)} companies with inferred data")
                        
                        # Store in shared_data
                        async with self.shared_data_lock:
                            self.shared_data["companies"] = enriched_companies
                            logger.critical(f"[ORCHESTRATOR] 🟡🟡🟡 Stored {len(enriched_companies)} companies in shared_data 🟡🟡🟡")
                            logger.info(f"[ORCHESTRATOR] Stored {len(enriched_companies)} companies in shared_data")
                        
                        # NOW call the deck generation skill with the fetched data
                        if skill_name in self.skills:
                            handler = self.skills[skill_name]["handler"]
                            logger.info(f"[ORCHESTRATOR] Executing skill directly: {skill_name} with {len(enriched_companies)} companies")
                            result = await handler(inputs)
                            
                            # For deck generation, return clean structure
                            if skill_name == "deck-storytelling" and result.get('format') == 'deck':
                                logger.info(f"[ORCHESTRATOR] Deck result has {len(result.get('slides', []))} slides")
                                # Return deck data directly with success flag
                                return {
                                    "success": True,
                                    "format": "deck",
                                    "slides": result.get('slides', []),
                                    "theme": result.get('theme', 'professional'),
                                    "metadata": result.get('metadata', {}),
                                    "citations": result.get('citations', []),
                                    "charts": result.get('charts', []),
                                    "companies": result.get('companies', [])
                                }
                            
                            return {"success": True, **result}
                        else:
                            return {"success": False, "error": f"Unknown skill: {skill_name}"}
                    else:
                        # Companies are already data objects, ensure they have inferred data
                        logger.info(f"[ORCHESTRATOR] Companies are data objects, ensuring inferred data before adding to shared_data")
                        
                        # CRITICAL: Ensure all companies have inferred data
                        enriched_companies = await self._ensure_companies_have_inferred_data(companies_input)
                        
                        async with self.shared_data_lock:
                            self.shared_data["companies"] = enriched_companies
                        
                        # Call skill directly if we have data
                        if skill_name in self.skills:
                            handler = self.skills[skill_name]["handler"]
                            logger.info(f"[ORCHESTRATOR] Executing skill directly: {skill_name}")
                            result = await handler(inputs)
                            
                            # For deck generation, return clean structure
                            if skill_name == "deck-storytelling" and result.get('format') == 'deck':
                                logger.info(f"[ORCHESTRATOR] Deck result has {len(result.get('slides', []))} slides")
                                # Return deck data directly with success flag
                                return {
                                    "success": True,
                                    "format": "deck",
                                    "slides": result.get('slides', []),
                                    "theme": result.get('theme', 'professional'),
                                    "metadata": result.get('metadata', {}),
                                    "citations": result.get('citations', []),
                                    "charts": result.get('charts', []),
                                    "companies": result.get('companies', [])
                                }
                            
                            return {"success": True, **result}
                        else:
                            return {"success": False, "error": f"Unknown skill: {skill_name}"}
                else:
                    # Check if companies are in the prompt or inputs even if not in the conditional above
                    companies_input = prompt.get("companies") or inputs.get("companies")
                    if companies_input:
                        logger.info(f"[ORCHESTRATOR] Found {len(companies_input)} companies in request, processing...")
                        
                        # Ensure companies have inferred data
                        enriched_companies = await self._ensure_companies_have_inferred_data(companies_input)
                        
                        # Store in shared_data
                        async with self.shared_data_lock:
                            self.shared_data["companies"] = enriched_companies
                        logger.info(f"[ORCHESTRATOR] Stored {len(enriched_companies)} enriched companies in shared_data")
                    
                    # Execute the skill
                    if skill_name in self.skills:
                        handler = self.skills[skill_name]["handler"]
                        logger.info(f"[ORCHESTRATOR] Executing skill: {skill_name} (companies in shared_data: {len(self.shared_data.get('companies', []))})")
                        result = await handler(inputs)
                        return {"success": True, **result}
                    else:
                        return {"success": False, "error": f"Unknown skill: {skill_name}"}
            else:
                actual_prompt = prompt.get("prompt", "")
                output_format = prompt.get("output_format", "analysis")
                context = prompt.get("context", None)
        else:
            actual_prompt = prompt
        
        logger.info(f"[ORCHESTRATOR] actual_prompt: {actual_prompt[:100] if actual_prompt else 'None'}...")
        logger.info(f"[ORCHESTRATOR] final output_format: {output_format}")
        
        result = None
        error_message = None
        
        # Add detailed logging for stream processing
        logger.info(f"[PROCESS_REQUEST] Starting stream processing for format: {output_format}")
        
        try:
            async for update in self.process_request_stream(actual_prompt, output_format, context):
                logger.info(f"[PROCESS_REQUEST] Stream update type: {update.get('type')}")
                
                if update.get("type") == "complete":
                    result = update.get("result", {})
                    logger.info(f"[PROCESS_REQUEST] ✅ Captured complete result from stream")
                    logger.info(f"[PROCESS_REQUEST] ✅ Result type: {type(result)}")
                    logger.info(f"[PROCESS_REQUEST] ✅ Result keys: {list(result.keys()) if isinstance(result, dict) else 'not_dict'}")
                    if isinstance(result, dict):
                        logger.info(f"[PROCESS_REQUEST] ✅ Result format: {result.get('format')}")
                        slides_data = result.get('slides') or []
                        logger.info(f"[PROCESS_REQUEST] ✅ Result slides count: {len(slides_data)}")
                        if slides_data:
                            logger.info(f"[PROCESS_REQUEST] ✅ First slide ID: {slides_data[0].get('id') if slides_data else 'None'}")
                            logger.info(f"[PROCESS_REQUEST] ✅ Slide IDs: {[s.get('id') for s in slides_data[:3]]}")
                elif update.get("type") == "error":
                    error_message = update.get("error")
                    logger.error(f"[PROCESS_REQUEST] ❌ Error from stream: {error_message}")
                elif update.get("type") == "progress":
                    logger.info(f"[PROCESS_REQUEST] Progress: {update.get('stage')} - {update.get('message')}")
                    
        except Exception as e:
            logger.error(f"[PROCESS_REQUEST] Stream processing failed: {e}")
            error_message = str(e)
            result = None
        
        # Add debug logging for deck format
        logger.info(f"[PROCESS_REQUEST] Final result analysis:")
        logger.info(f"[PROCESS_REQUEST] output_format: {output_format}")
        logger.info(f"[PROCESS_REQUEST] result type: {type(result)}")
        logger.info(f"[PROCESS_REQUEST] result keys: {list(result.keys()) if isinstance(result, dict) else 'not_dict'}")
        if isinstance(result, dict):
            logger.info(f"[PROCESS_REQUEST] result.format: {result.get('format')}")
            slides_data = result.get('slides') or []
            logger.info(f"[PROCESS_REQUEST] result.slides count: {len(slides_data)}")
            if slides_data:
                logger.info(f"[PROCESS_REQUEST] Slide preview: {[s.get('id', 'no-id') for s in slides_data[:3]]}")
        
        # Return in the format the endpoint expects
        if result and not result.get("error"):
            logger.info(f"[PROCESS_REQUEST] Returning success with result format: {result.get('format') if isinstance(result, dict) else 'not_dict'}")
            
            # For deck format, return slides directly in the response structure
            if isinstance(result, dict) and result.get('format') == 'deck':
                logger.info(f"[PROCESS_REQUEST] ✅ Deck format detected, returning slides directly")
                logger.info(f"[PROCESS_REQUEST] ✅ Slides count: {len(result.get('slides', []))}")
                
                # Return deck data directly with slides at top level
                # Ensure slides is always an array (never None)
                slides = result.get('slides') or []
                if not isinstance(slides, list):
                    slides = []
                resp = {
                    "success": True,
                    "format": "deck",
                    "slides": slides,
                    "theme": result.get('theme', 'professional'),
                    "metadata": result.get('metadata', {}),
                    "citations": result.get('citations', []),
                    "charts": result.get('charts', []),
                    "companies": result.get('companies', []),
                    "warnings": result.get('warnings', []),
                    "result": result,  # Keep original for backward compatibility
                    "results": result
                }
                return resp
            else:
                # For other formats, use the original structure
                logger.info(f"[PROCESS_REQUEST] Non-deck format, using original structure")
                resp = {"success": True, "result": result, "results": result}
                if isinstance(result, dict) and result.get("warnings"):
                    resp["warnings"] = result["warnings"]
                return resp
        else:
            # Use the captured error message if available, otherwise fall back to generic message
            error = error_message or result.get('error') if result else 'No result generated'
            logger.error(f"[PROCESS_REQUEST] Returning error: {error}")
            return {"success": False, "error": error}
    
    # ------------------------------------------------------------------
    # Agent Loop: complexity classifier, tool executor, ReAct loop
    # ------------------------------------------------------------------

    async def _classify_intent(self, prompt: str, entities: Optional[Dict[str, Any]] = None,
                                context: Optional[Dict[str, Any]] = None,
                                grid_fingerprint: str = "") -> QueryClassification:
        """LLM-based intent classification with minimal keyword fast-paths.

        Returns a QueryClassification with free-form intent (NOT a rigid enum).
        Only 3 true fast-paths remain (metric regex, memo polish, memo save).
        Everything else goes to one LLM call with full grid fingerprint context.
        """
        lower = prompt.lower().strip()
        has_at = "@" in prompt
        company_names = (entities or {}).get("companies", [])
        has_companies = bool(has_at or company_names)

        # --- Fast path 1: simple single-metric regex (no LLM needed) ---
        simple_patterns = [
            r"^what('s| is) (our|the|my) (dpi|tvpi|irr|nav|fund size)",
            r"^how many (companies|positions|investments)",
        ]
        if any(re.match(p, lower) for p in simple_patterns):
            return QueryClassification(
                complexity="simple",
                intent="metric_lookup",
                needs_portfolio=True,
                confidence=0.95,
            )

        # --- Fast path 2: memo polish (user refining an existing memo) ---
        has_memo_artifacts = bool(self.shared_data.get("memo_artifacts"))
        if has_memo_artifacts and any(s in lower for s in [
            "refine this", "edit the memo", "change the risk",
            "update the memo", "polish the memo", "rewrite the",
            "fix the memo", "adjust the", "revise the",
        ]):
            logger.info(f"[CLASSIFY] Detected memo polish request: {prompt[:80]}")
            return QueryClassification(
                complexity="simple",
                intent="memo_polish",
                needs_portfolio=False,
                confidence=0.9,
            )

        # --- Fast path 3: save memo ---
        if has_memo_artifacts and any(s in lower for s in [
            "save this memo", "save the memo", "persist this",
            "save this report", "save the report",
        ]):
            logger.info(f"[CLASSIFY] Detected memo save request: {prompt[:80]}")
            return QueryClassification(
                complexity="simple",
                intent="memo_save",
                needs_portfolio=False,
                confidence=0.95,
            )

        # --- Store feedback/corrections (but still let LLM classify) ---
        feedback_signals = [
            "no,", "not that", "wrong", "instead",
            "don't", "try again", "that's not",
            "actually,", "i meant", "not what i asked",
        ]
        if any(lower.startswith(s) or f" {s}" in f" {lower}" for s in feedback_signals):
            existing_corrections = self.shared_data.get("session_corrections", [])
            existing_corrections.append({"correction": prompt, "timestamp": datetime.now().isoformat()})
            self.shared_data["session_corrections"] = existing_corrections[-10:]
            logger.info(f"[CLASSIFY] Stored feedback/correction: {prompt[:80]}")

        # --- LLM classification with grid fingerprint context ---
        try:
            tool_names = ", ".join(t.name for t in AGENT_VISIBLE_TOOLS)
            template_names = ", ".join(PLAN_TEMPLATES.keys())

            # Build grid state context for the classifier
            state_context = grid_fingerprint or "STATE: no grid fingerprint available"

            classify_prompt = f"""Classify this investment query. Return JSON only.

Query: {prompt}
Companies mentioned: {company_names or "none"}
Has @mentions: {has_at}

Grid state:
{state_context}

Available tools: {tool_names}
Known workflow patterns: {template_names}

Return:
{{
  "complexity": "simple|complex",
  "intent": "<free-form intent description>",
  "suggested_chain": ["tool1", "tool2"],
  "needs_portfolio": true/false,
  "needs_external": true/false,
  "confidence": 0.0-1.0
}}

Rules:
- "simple" = ONLY for single metric lookups that need no tools (e.g. "what is our DPI?")
- "complex" = EVERYTHING ELSE — any query that needs enrichment, search, valuation, memo, comparison, gap filling, or analysis. When in doubt, classify as complex.
- If grid state shows GAPS or missing data and the query relates to those companies, suggest resolve_data_gaps in chain
- If the user asks about a company and grid shows it's missing data, suggest fetch_company_data → run_valuation
- If the user asks for a memo/report, suggest resolve_data_gaps → run_valuation → generate_memo
- needs_portfolio = query touches existing portfolio data
- needs_external = query requires fetching new data from web
- suggested_chain = ordered list of tools to call"""

            result = await self.model_router.get_completion(
                prompt=classify_prompt,
                system_prompt="Classify investment queries. Return valid JSON only. Bias toward 'complex' — the agent loop has 80+ tools and handles everything better than a single-shot answer.",
                capability=ModelCapability.FAST,
                max_tokens=200,
                temperature=0.0,
                json_mode=True,
                caller_context="intent_classification",
            )
            content = result.get("response", "{}") if isinstance(result, dict) else str(result)
            parsed = json.loads(content)

            complexity = parsed.get("complexity", "complex")
            # Normalize: "dealflow" → "complex" (agent loop handles both)
            if complexity not in ("simple",):
                complexity = "complex"

            return QueryClassification(
                complexity=complexity,
                intent=parsed.get("intent", "unknown"),
                suggested_chain=parsed.get("suggested_chain"),
                needs_portfolio=parsed.get("needs_portfolio", False),
                needs_external=parsed.get("needs_external", False),
                confidence=parsed.get("confidence", 0.7),
            )
        except Exception as e:
            logger.warning(f"[CLASSIFY] LLM classification failed: {e}, defaulting to complex")

        # --- Fallback: default to complex (agent loop handles everything) ---
        return QueryClassification(
            complexity="complex",
            intent="general",
            needs_portfolio=True,
            needs_external=has_companies,
            confidence=0.5,
        )

    def _assess_complexity(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> str:
        """Backward-compatible sync wrapper. Prefer _classify_intent() for richer data."""
        lower = prompt.lower().strip()

        simple_patterns = [
            r"^what('s| is) (our|the|my) (dpi|tvpi|irr|nav|fund size)",
            r"^how many (companies|positions|investments)",
        ]
        if any(re.match(p, lower) for p in simple_patterns):
            return "simple"

        # Default: everything goes to complex (agent loop)
        return "complex"

    async def _direct_dispatch(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> Optional[Dict[str, Any]]:
        """Handle truly simple queries with one tool call. Returns None to upgrade to agent loop."""
        lower = prompt.lower()
        fund_ctx = self.shared_data.get("fund_context", {})

        if any(w in lower for w in ["dpi", "tvpi", "irr", "nav", "fund size", "fund metrics"]):
            result = await self._tool_fund_metrics({
                "metrics": ["nav", "irr", "dpi", "tvpi"],
                "fund_id": fund_ctx.get("fundId"),
            })
            return {"content": self._format_fund_metrics_text(result), "format": "analysis"}

        # Everything else → return None to signal upgrade to agent loop
        logger.info(f"[DIRECT_DISPATCH] No simple handler matched, upgrading to agent loop")
        return None

    def _build_grid_context_text(self, max_rows: int = 30, max_cols: int = 15) -> str:
        """Build a compact text representation of the portfolio grid for LLM context."""
        matrix_ctx = self.shared_data.get("matrix_context") or {}
        grid_snapshot = matrix_ctx.get("gridSnapshot") or {}
        grid_rows = grid_snapshot.get("rows", []) if isinstance(grid_snapshot, dict) else grid_snapshot if isinstance(grid_snapshot, list) else []
        if not grid_rows:
            return ""

        columns = matrix_ctx.get("columns") or []
        col_names = [c.get("name") or c.get("id") for c in columns[:max_cols]] if columns else []

        lines = [f"Portfolio Grid ({len(grid_rows)} companies):"]
        if col_names:
            lines.append(f"Columns: {', '.join(col_names)}")
        lines.append("")

        for row in grid_rows[:max_rows]:
            name = row.get("companyName") or row.get("company_name") or "Unknown"
            cells = row.get("cells") or row.get("cellValues") or {}
            # Pick the most useful cells (cap at max_cols)
            cell_parts = []
            for k, v in list(cells.items())[:max_cols]:
                if v is not None and v != "" and v != "N/A":
                    cell_parts.append(f"{k}={v}")
            if cell_parts:
                lines.append(f"- {name}: {', '.join(cell_parts)}")
            else:
                lines.append(f"- {name}")

        return "\n".join(lines)

    def _build_portfolio_intelligence(self) -> str:
        """Build dense portfolio context for LLM injection (~500-800 tokens).

        Produces real portfolio analysis (not a spreadsheet dump):
        - Per-company: description, sector, stage, geography, lead/follow
        - Portfolio-level: sector concentration, stage distribution, geographic mix, gaps
        """
        matrix_ctx = self.shared_data.get("matrix_context") or {}
        grid_snapshot = matrix_ctx.get("gridSnapshot") or {}
        grid_rows = (
            grid_snapshot.get("rows", [])
            if isinstance(grid_snapshot, dict)
            else grid_snapshot if isinstance(grid_snapshot, list) else []
        )
        if not grid_rows:
            return ""

        # --- Per-company context ---
        companies_ctx = []
        sector_counts: dict[str, list[str]] = {}
        stage_counts: dict[str, int] = {}
        geo_counts: dict[str, int] = {}

        for row in grid_rows:
            name = row.get("companyName") or row.get("company_name") or "Unknown"
            cells = row.get("cells") or row.get("cellValues") or {}

            # Extract key fields from cell values (handle dict or raw)
            def _cv(key_patterns: list[str]) -> str:
                for k, v in cells.items():
                    k_lower = k.lower()
                    for pat in key_patterns:
                        if pat in k_lower:
                            val = v.get("value", v) if isinstance(v, dict) else v
                            return str(val) if val and val != "N/A" else ""
                return ""

            sector = _cv(["sector", "vertical", "industry"]) or "Unknown"
            stage = _cv(["stage", "round", "series"]) or "Unknown"
            geo = _cv(["geo", "hq", "location", "country"]) or "Unknown"
            description = _cv(["description", "business", "what_they_do"])
            lead = _cv(["lead", "investment_lead"])
            arr = _cv(["arr", "revenue"])
            valuation = _cv(["valuation", "post_money"])

            # Build per-company line
            parts = [f"[{stage}]"]
            if sector and sector != "Unknown":
                parts.append(f"sector={sector}")
            if geo and geo != "Unknown":
                parts.append(f"geo={geo}")
            if lead:
                parts.append(f"lead={lead}")
            if arr:
                parts.append(f"arr={arr}")
            if valuation:
                parts.append(f"val={valuation}")
            if description:
                parts.append(f"— {description[:80]}")
            companies_ctx.append(f"  {name}: {' '.join(parts)}")

            # Aggregate for portfolio-level analysis
            sector_key = sector if sector != "Unknown" else "Unclassified"
            sector_counts.setdefault(sector_key, []).append(name)
            stage_counts[stage] = stage_counts.get(stage, 0) + 1
            geo_key = geo if geo != "Unknown" else "Unclassified"
            geo_counts[geo_key] = geo_counts.get(geo_key, 0) + 1

        total = len(grid_rows)
        lines = [f"PORTFOLIO ({total} companies):"]
        lines.extend(companies_ctx)

        # --- Portfolio-level analysis ---
        lines.append("")
        lines.append("PORTFOLIO ANALYSIS:")

        # Sector concentration
        if sector_counts:
            sorted_sectors = sorted(sector_counts.items(), key=lambda x: -len(x[1]))
            sector_parts = []
            for s, names in sorted_sectors[:5]:
                pct = len(names) / total * 100
                sector_parts.append(f"{s}: {len(names)} ({pct:.0f}%) [{', '.join(names[:3])}{'...' if len(names) > 3 else ''}]")
            lines.append(f"  Sectors: {'; '.join(sector_parts)}")

        # Stage distribution
        if stage_counts:
            stage_parts = [f"{s}: {c}" for s, c in sorted(stage_counts.items(), key=lambda x: -x[1])]
            lines.append(f"  Stages: {', '.join(stage_parts)}")

        # Geographic mix
        if geo_counts:
            geo_parts = [f"{g}: {c} ({c/total*100:.0f}%)" for g, c in sorted(geo_counts.items(), key=lambda x: -x[1])[:5]]
            lines.append(f"  Geography: {', '.join(geo_parts)}")

        # Gaps/flags
        if sector_counts:
            top_sector = sorted_sectors[0]
            if len(top_sector[1]) / total > 0.35:
                lines.append(f"  FLAG: Heavy concentration in {top_sector[0]} ({len(top_sector[1])}/{total} companies)")

        # Auto-detect sparse columns — tells the agent what data is missing
        sparse_fields = {"sector": 0, "description": 0, "arr": 0, "valuation": 0, "stage": 0, "headcount": 0}
        for row in grid_rows:
            cells = row.get("cells") or row.get("cellValues") or {}
            for field_key in sparse_fields:
                has_value = False
                for k, v in cells.items():
                    if field_key in k.lower():
                        val = v.get("value", v) if isinstance(v, dict) else v
                        if val and val != "N/A" and val != "Unknown" and val != "":
                            has_value = True
                            break
                if not has_value:
                    sparse_fields[field_key] += 1

        sparse_alerts = []
        for field, empty_count in sparse_fields.items():
            if empty_count > total * 0.5:
                sparse_alerts.append(f"{field}: {empty_count}/{total} empty")
                self.shared_data["needs_auto_enrich"] = True

        if sparse_alerts:
            lines.append(f"  SPARSE DATA: {'; '.join(sparse_alerts)}")
            lines.append("  → Agent should call resolve_data_gaps to auto-fill before answering user questions")
            self.shared_data["auto_enrich_fields"] = [f for f, c in sparse_fields.items() if c > total * 0.5]

        # --- Fund metrics context (TVPI, DPI, IRR, deployed) ---
        fund_metrics = self.shared_data.get("fund_metrics", {})
        perf = fund_metrics.get("metrics", fund_metrics) if isinstance(fund_metrics, dict) else {}
        if perf:
            fm_parts = []
            if perf.get("tvpi"): fm_parts.append(f"TVPI={perf['tvpi']:.2f}x")
            if perf.get("dpi"): fm_parts.append(f"DPI={perf['dpi']:.2f}x")
            if perf.get("irr"): fm_parts.append(f"IRR={perf['irr']:.1f}%")
            if perf.get("total_invested"): fm_parts.append(f"invested=${perf['total_invested']/1e6:.0f}M")
            if perf.get("total_nav"): fm_parts.append(f"NAV=${perf['total_nav']/1e6:.0f}M")
            if fm_parts:
                lines.append(f"\nFUND METRICS: {', '.join(fm_parts)}")

        fund_ctx = self.shared_data.get("fund_context", {})
        if fund_ctx:
            fc_parts = []
            if fund_ctx.get("fund_size"): fc_parts.append(f"size=${fund_ctx['fund_size']/1e6:.0f}M")
            if fund_ctx.get("remaining_capital"): fc_parts.append(f"remaining=${fund_ctx['remaining_capital']/1e6:.0f}M")
            if fund_ctx.get("strategy"): fc_parts.append(f"strategy={fund_ctx['strategy']}")
            if fc_parts and not perf:
                lines.append(f"\nFUND CONTEXT: {', '.join(fc_parts)}")

        # --- Enriched company data from shared_data (beyond grid) ---
        sd_companies = self.shared_data.get("companies", [])
        if sd_companies:
            lines.append(f"\nENRICHED COMPANIES ({len(sd_companies)}):")
            for c in sd_companies[:8]:
                name = c.get("company") or c.get("name") or "Unknown"
                parts = []
                rev = c.get("revenue") or c.get("arr") or c.get("inferred_revenue")
                if rev and isinstance(rev, (int, float)) and rev > 0:
                    parts.append(f"rev=${rev/1e6:.1f}M")
                val = c.get("valuation")
                if val and isinstance(val, (int, float)) and val > 0:
                    parts.append(f"val=${val/1e6:.0f}M")
                growth = c.get("growth_rate") or c.get("revenue_growth_annual_pct")
                if growth and isinstance(growth, (int, float)):
                    g_pct = growth * 100 if growth < 5 else growth
                    parts.append(f"growth={g_pct:.0f}%")
                stage = c.get("stage")
                if stage: parts.append(f"stage={stage}")
                investors = c.get("investors") or c.get("key_investors")
                if investors and isinstance(investors, list):
                    inv_names = investors[:3] if isinstance(investors[0], str) else [i.get("name", "") for i in investors[:3]]
                    parts.append(f"investors=[{', '.join(inv_names)}]")
                desc = c.get("product_description") or c.get("description") or ""
                if desc:
                    parts.append(f"— {desc[:60]}")
                lines.append(f"  {name}: {' '.join(parts)}")

        # --- Available analysis data for memo enrichment ---
        available_data_keys = []
        for key in ("cap_table_history", "scenario_analysis", "exit_modeling",
                     "followon_strategy", "portfolio_health", "revenue_projections",
                     "fund_deployment"):
            if self.shared_data.get(key):
                available_data_keys.append(key)
        if available_data_keys:
            lines.append(f"\nAVAILABLE ANALYSIS: {', '.join(available_data_keys)}")
            lines.append("  → Use these in generate_memo for rich charts and data-driven sections")

        return "\n".join(lines)

    async def _execute_lightweight_diligence(self, tool_input: dict) -> dict:
        """Quick single-search company lookup. 1 Tavily search + 1 LLM extraction.

        Skips StructuredDataExtractor + IntelligentGapFiller for speed.
        Caches result in shared_data['companies'] with source='lightweight'.
        """
        company_name = tool_input.get("company_name") or tool_input.get("company", "")
        if not company_name:
            return {"error": "company_name is required"}

        company_name = company_name.strip().lstrip("@")

        # Check cache first
        for c in self.shared_data.get("companies", []):
            c_name = (c.get("name") or c.get("companyName") or "").lower()
            if company_name.lower() == c_name or company_name.lower() in c_name:
                return {"company": c, "source": "cached"}

        # Check grid context
        resolved = await self._resolve_company(company_name, allow_external=False)
        if resolved:
            return {"company": resolved, "source": "portfolio"}

        # Single Tavily search
        try:
            search_result = await self._execute_tool("web_search", {
                "query": f"{company_name} startup funding valuation revenue investors",
                "search_depth": "basic",
            })
            search_text = json.dumps(search_result.get("results", search_result), default=str)[:4000]
        except Exception as e:
            logger.warning(f"[LIGHTWEIGHT_DILIGENCE] Search failed for {company_name}: {e}")
            search_text = ""

        # Single LLM extraction call
        extraction_prompt = f"""Extract company data for {company_name} from this search result:

{search_text}

Return JSON with these fields (use null if unknown):
{{
  "name": "{company_name}",
  "description": "What they actually do (factual, no buzzwords)",
  "stage": "Seed/Series A/B/C/D+",
  "revenue_estimate": "number or null",
  "valuation": "number or null",
  "total_funding": "number or null",
  "team_size": "number or null",
  "investors": ["list of known investors"],
  "recent_news": "Most recent notable event",
  "hq_location": "City, Country",
  "sector": "Industry vertical"
}}"""

        try:
            extraction = await self.model_router.get_completion(
                prompt=extraction_prompt,
                system_prompt="Extract factual company data. No speculation. Return valid JSON only.",
                capability=ModelCapability.FAST,
                max_tokens=500,
                temperature=0.0,
                json_mode=True,
                caller_context="lightweight_diligence_extract",
            )
            raw = extraction.get("response", "{}") if isinstance(extraction, dict) else str(extraction)
            company_data = json.loads(raw) if isinstance(raw, str) else raw
        except Exception as e:
            logger.warning(f"[LIGHTWEIGHT_DILIGENCE] Extraction failed for {company_name}: {e}")
            company_data = {"name": company_name, "error": str(e)}

        company_data["source"] = "lightweight"

        # Cache in shared_data
        async with self.shared_data_lock:
            existing = self.shared_data.get("companies", [])
            existing.append(company_data)
            self.shared_data["companies"] = existing

        # Auto-generate diligence memo if we got real data
        memo_result = None
        if not company_data.get("error"):
            try:
                from app.services.lightweight_memo_service import LightweightMemoService
                memo_svc = LightweightMemoService(
                    model_router=self.model_router,
                    shared_data=self.shared_data,
                )
                memo_result = await memo_svc.generate(
                    prompt=f"Initial diligence on {company_name}",
                    memo_type="diligence_memo",
                )
                # Store in memo_artifacts
                async with self.shared_data_lock:
                    artifacts = self.shared_data.get("memo_artifacts", [])
                    artifacts.append(memo_result)
                    self.shared_data["memo_artifacts"] = artifacts
            except Exception as e:
                logger.warning(f"[LIGHTWEIGHT_DILIGENCE] Memo generation failed for {company_name}: {e}")

        # Return memo_sections so the agent loop collector picks them up for inline display
        memo_sections = memo_result.get("sections", []) if isinstance(memo_result, dict) else []
        return {"company": company_data, "source": "lightweight", "memo": memo_result,
                "memo_sections": memo_sections}

    async def _execute_enrich_portfolio(self, tool_input: dict) -> dict:
        """Read full grid, use IntelligentGapFiller to estimate missing fields per company.

        Uses stage benchmarks + time since funding to infer empty cells.
        Scores each company via score_fund_fit().
        Returns structured summary + per-company gap report + estimates.
        Stores in shared_data['portfolio_enrichment'].
        """
        matrix_ctx = self.shared_data.get("matrix_context") or {}
        grid_snapshot = matrix_ctx.get("gridSnapshot") or {}
        grid_rows = (
            grid_snapshot.get("rows", [])
            if isinstance(grid_snapshot, dict)
            else grid_snapshot if isinstance(grid_snapshot, list) else []
        )
        if not grid_rows:
            return {"error": "No portfolio grid data available"}

        # Fund context for scoring
        fund_ctx = self.shared_data.get("fund_context", {})
        fund_scoring_ctx = {
            "fund_size": fund_ctx.get("fundSize") or fund_ctx.get("fund_size") or DEFAULT_FUND_SIZE,
            "fund_year": fund_ctx.get("fundYear") or fund_ctx.get("fund_year") or 3,
            "portfolio_count": len(grid_rows),
            "deployed_capital": fund_ctx.get("deployedCapital") or fund_ctx.get("deployed_capital"),
            "remaining_capital": fund_ctx.get("remainingCapital") or fund_ctx.get("remaining_capital"),
        }

        enriched_companies = []
        gap_report = []
        stage_dist: dict[str, int] = {}
        sector_dist: dict[str, int] = {}
        total_arr = 0.0
        companies_with_arr = 0

        for row in grid_rows:
            name = row.get("companyName") or row.get("company_name") or "Unknown"
            cells = row.get("cells") or row.get("cellValues") or {}

            # Build company_data dict from cells
            company_data: dict[str, any] = {"name": name}
            missing_fields = []
            for k, v in cells.items():
                val = v.get("value", v) if isinstance(v, dict) else v
                if val is None or val == "" or val == "N/A":
                    missing_fields.append(k)
                else:
                    company_data[k] = val

            stage = company_data.get("stage") or company_data.get("round") or "Unknown"
            sector = company_data.get("sector") or company_data.get("vertical") or "Unknown"
            stage_dist[stage] = stage_dist.get(stage, 0) + 1
            sector_dist[sector] = sector_dist.get(sector, 0) + 1

            # Track ARR
            arr_val = company_data.get("arr") or company_data.get("revenue")
            if arr_val:
                try:
                    total_arr += float(str(arr_val).replace("$", "").replace(",", "").replace("M", "e6").replace("B", "e9"))
                    companies_with_arr += 1
                except (ValueError, TypeError):
                    pass

            # Use IntelligentGapFiller to estimate missing values
            inferred = {}
            if missing_fields and self.gap_filler and stage != "Unknown":
                try:
                    inferred = await self.gap_filler.infer_from_stage_benchmarks(
                        company_data=company_data,
                        missing_fields=missing_fields,
                    )
                except Exception as e:
                    logger.warning(f"[ENRICH] Gap filler failed for {name}: {e}")

            # Score company fit
            fund_fit = {}
            if self.gap_filler:
                try:
                    fund_fit = self.gap_filler.score_fund_fit(
                        company_data=company_data,
                        inferred_data=inferred,
                        context=fund_scoring_ctx,
                    )
                except Exception as e:
                    logger.warning(f"[ENRICH] Fund fit scoring failed for {name}: {e}")

            # Build enriched entry
            inferred_summary = {}
            for field, result in inferred.items():
                val = result.value if hasattr(result, "value") else result
                conf = result.confidence if hasattr(result, "confidence") else None
                inferred_summary[field] = {"value": val, "confidence": conf}

            enriched_companies.append({
                "name": name,
                "missing_fields": missing_fields,
                "inferred": inferred_summary,
                "fund_fit_score": fund_fit.get("overall_score"),
                "fund_fit_action": fund_fit.get("action"),
            })

            if missing_fields:
                gap_report.append({
                    "company": name,
                    "missing_fields": missing_fields,
                    "count": len(missing_fields),
                    "inferred_count": len(inferred_summary),
                })

        # --- Step 2: Cross-reference against rich companies DB ---
        db_fill_count = 0
        try:
            from app.services.portfolio_service import PortfolioService
            ps = PortfolioService()
            for ec in enriched_companies:
                if not ec["missing_fields"]:
                    continue
                db_company = await ps.get_company_by_name(ec["name"])
                if db_company and isinstance(db_company, dict):
                    for field in ec["missing_fields"]:
                        db_val = db_company.get(field)
                        if db_val and db_val not in (None, "", "N/A"):
                            if field not in ec["inferred"]:
                                ec["inferred"][field] = {"value": db_val, "confidence": 0.85, "source": "companies_db"}
                                db_fill_count += 1
        except Exception as e:
            logger.warning(f"[ENRICH] DB cross-reference failed: {e}")

        # --- Step 3: Emit suggest_grid_edit for every inferred value ---
        from app.services.micro_skills.suggestion_emitter import FIELD_TO_COLUMN as _ENRICH_F2C
        suggestion_count = 0
        fund_id = fund_ctx.get("fundId") or fund_ctx.get("fund_id")
        for ec in enriched_companies:
            row_id = ec.get("row_id", ec["name"])
            for field, inf in ec.get("inferred", {}).items():
                val = inf.get("value") if isinstance(inf, dict) else inf
                conf = inf.get("confidence", 0.5) if isinstance(inf, dict) else 0.5
                source = inf.get("source", "stage_benchmark") if isinstance(inf, dict) else "stage_benchmark"
                col_id = _ENRICH_F2C.get(field, field)  # Map backend field → frontend column ID
                if val is not None and val != "":
                    # Persist each inferred value as a pending suggestion
                    if fund_id:
                        try:
                            supabase_url = settings.SUPABASE_URL
                            supabase_key = settings.SUPABASE_SERVICE_ROLE_KEY or settings.SUPABASE_ANON_KEY
                            if supabase_url and supabase_key:
                                from supabase import create_client
                                sb = create_client(supabase_url, supabase_key)
                                sb.table("pending_suggestions").upsert({
                                    "fund_id": fund_id,
                                    "company_id": row_id,
                                    "column_id": col_id,
                                    "suggested_value": {"value": val},
                                    "source_service": f"agent.enrich_portfolio.{source}",
                                    "reasoning": f"Inferred from {source} (confidence: {conf:.0%})",
                                    "metadata": {"tool": "enrich_portfolio", "confidence": conf, "source": source},
                                }, on_conflict="fund_id,company_id,column_id").execute()
                                suggestion_count += 1
                        except Exception as e:
                            logger.warning(f"[ENRICH] Failed to persist suggestion for {ec['name']}.{field}: {e}")

        # --- Step 4: Business-model-aware action suggestions ---
        biz_model_suggestions = self._generate_business_model_suggestions(enriched_companies, fund_id)

        total = len(grid_rows)
        enrichment = {
            "total_companies": total,
            "stage_distribution": stage_dist,
            "sector_distribution": sector_dist,
            "avg_arr": total_arr / companies_with_arr if companies_with_arr else None,
            "companies_with_arr": companies_with_arr,
            "gap_report": sorted(gap_report, key=lambda x: -x["count"]),
            "most_incomplete": gap_report[:5] if gap_report else [],
            "completeness_pct": round((1 - len([g for g in gap_report if g["count"] > 0]) / total) * 100, 1) if total else 0,
            "enriched_companies": enriched_companies,
            "suggestions_persisted": suggestion_count,
            "db_fills": db_fill_count,
            "business_model_suggestions": biz_model_suggestions,
        }

        # Store in shared_data
        async with self.shared_data_lock:
            self.shared_data["portfolio_enrichment"] = enrichment

        # Build inline memo sections summarizing enrichment results
        memo_sections = self._build_enrichment_memo_sections(enrichment)
        enrichment["memo_sections"] = memo_sections

        return enrichment

    def _generate_business_model_suggestions(self, enriched_companies: list, fund_id: Optional[str]) -> list:
        """Generate business-model-aware action suggestions for each company.

        Identifies business model from sector/description and suggests appropriate
        analyses: CAPM for high-capex, pref stack for late-stage, debt for asset-backed, etc.
        """
        BIZ_MODEL_RULES = {
            "high_capex": {
                "keywords": ["space", "aerospace", "defense", "hardware", "manufacturing", "semiconductor",
                             "data center", "infrastructure", "nuclear", "energy", "mining", "construction"],
                "suggestions": [
                    {"type": "action_item", "title": "Run CAPM-based valuation", "description": "High capex business with tangible assets — DCF with CAPM discount rate more appropriate than revenue multiples. Use risk-free rate + beta * equity risk premium.", "priority": "high"},
                    {"type": "action_item", "title": "Asset-backed debt opportunity", "description": "Physical assets (facilities, equipment, launch vehicles) enable asset-backed lending. Model debt capacity and interest coverage ratio.", "priority": "high"},
                    {"type": "insight", "title": "Negative gross margin trajectory", "description": "Gross margin likely negative initially due to R&D/capex, but high payoff once operational with sticky government/enterprise contracts. Model the J-curve.", "priority": "medium"},
                    {"type": "action_item", "title": "Run preference stack analysis", "description": "Complex capital structure likely — model liquidation preferences, participation rights, and waterfall for next round.", "priority": "medium"},
                ],
            },
            "european_sovereignty": {
                "keywords": ["europe", "eu", "german", "french", "uk", "european", "sovereignty", "defense"],
                "suggestions": [
                    {"type": "insight", "title": "European sovereignty theme premium", "description": "Strategic importance to EU autonomy adds premium. EU defense spending increasing 2%+ of GDP. ESA budget growing. Government contract stickiness provides revenue predictability.", "priority": "high"},
                    {"type": "action_item", "title": "Map EU funding programs", "description": "Check eligibility for EU Defence Fund, ESA contracts, Horizon Europe, IPCEI. These are non-dilutive capital sources.", "priority": "medium"},
                ],
            },
            "saas_recurring": {
                "keywords": ["saas", "software", "platform", "subscription", "recurring", "cloud"],
                "suggestions": [
                    {"type": "action_item", "title": "Run revenue multiples comparables", "description": "SaaS company — use ARR multiples with growth-adjusted NTM. Check Rule of 40 (growth + FCF margin).", "priority": "medium"},
                ],
            },
            "marketplace": {
                "keywords": ["marketplace", "platform", "two-sided", "exchange", "trading"],
                "suggestions": [
                    {"type": "action_item", "title": "Benchmark take rate", "description": "Marketplace — take rate is the key metric. Search for comparable marketplace take rates in this vertical.", "priority": "medium"},
                ],
            },
        }

        all_suggestions = []
        for ec in enriched_companies:
            name = ec.get("name", "Unknown")
            # Build text to match against
            sector = (ec.get("sector") or "").lower()
            description = (ec.get("description") or "").lower()
            geo = (ec.get("geo") or ec.get("geography") or ec.get("hq") or "").lower()
            match_text = f"{sector} {description} {geo} {name.lower()}"

            matched_models = set()
            for model_name, rule in BIZ_MODEL_RULES.items():
                if any(kw in match_text for kw in rule["keywords"]):
                    matched_models.add(model_name)

            for model_name in matched_models:
                rule = BIZ_MODEL_RULES[model_name]
                for sugg_template in rule["suggestions"]:
                    suggestion = {
                        **sugg_template,
                        "company_id": name,
                        "description": f"[{name}] {sugg_template['description']}",
                    }
                    all_suggestions.append(suggestion)

                    # Persist to DB
                    if fund_id:
                        try:
                            supabase_url = settings.SUPABASE_URL
                            supabase_key = settings.SUPABASE_SERVICE_ROLE_KEY or settings.SUPABASE_ANON_KEY
                            if supabase_url and supabase_key:
                                from supabase import create_client
                                sb = create_client(supabase_url, supabase_key)
                                sb.table("pending_suggestions").upsert({
                                    "fund_id": fund_id,
                                    "company_id": name,
                                    "column_id": f"_action_{sugg_template['type']}",
                                    "suggested_value": suggestion,
                                    "source_service": f"agent.biz_model.{model_name}",
                                    "reasoning": sugg_template["description"],
                                    "metadata": {"tool": "enrich_portfolio", "biz_model": model_name, "priority": sugg_template["priority"]},
                                }, on_conflict="fund_id,company_id,column_id").execute()
                        except Exception as e:
                            logger.warning(f"[ENRICH] Failed to persist biz model suggestion for {name}: {e}")

        return all_suggestions

    def _build_enrichment_memo_sections(self, enrichment: dict) -> list:
        """Build compact memo sections summarizing portfolio enrichment for inline chat."""
        sections: list = []
        total = enrichment.get("total_companies", 0)
        completeness = enrichment.get("completeness_pct", 0)
        suggestions = enrichment.get("suggestions_persisted", 0)
        db_fills = enrichment.get("db_fills", 0)

        sections.append({"type": "heading2", "content": "Portfolio Enrichment Summary"})

        overview_items = [
            f"{total} companies analyzed",
            f"{completeness:.0f}% data completeness",
            f"{suggestions} field suggestions generated",
        ]
        if db_fills > 0:
            overview_items.append(f"{db_fills} fields filled from company database")
        avg_arr = enrichment.get("avg_arr")
        if avg_arr:
            overview_items.append(f"Average ARR: ${avg_arr / 1e6:,.1f}M")
        sections.append({"type": "list", "items": overview_items})

        # Top gaps
        gap_report = enrichment.get("gap_report", [])
        if gap_report:
            sections.append({"type": "heading2", "content": "Top Data Gaps"})
            gap_items = []
            for g in gap_report[:5]:
                name = g.get("company", "Unknown")
                count = g.get("count", 0)
                inferred = g.get("inferred_count", 0)
                gap_items.append(f"**{name}**: {count} missing fields ({inferred} inferred)")
            sections.append({"type": "list", "items": gap_items})

        # Stage distribution
        stage_dist = enrichment.get("stage_distribution", {})
        if stage_dist:
            stage_items = [f"{stage}: {count}" for stage, count in sorted(stage_dist.items(), key=lambda x: -x[1])]
            sections.append({"type": "heading2", "content": "Stage Distribution"})
            sections.append({"type": "list", "items": stage_items})

        # Business model suggestions summary
        biz_suggestions = enrichment.get("business_model_suggestions", [])
        if biz_suggestions:
            sections.append({"type": "heading2", "content": "Business Model Insights"})
            biz_items = [f"**{s.get('title', '')}** ({s.get('company_id', '')})" for s in biz_suggestions[:6]]
            sections.append({"type": "list", "items": biz_items})

        return sections

    async def _single_shot_answer(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Single LLM call for simple queries that don't map to a specific tool."""
        grid_text = self._build_portfolio_intelligence()

        memo_ctx = self.shared_data.get("agent_context", {}).get("memo_sections", [])
        memo_text = self._serialize_memo_sections(memo_ctx, limit=10) if memo_ctx else ""

        augmented = f"""{grid_text}
{('Memo context: ' + memo_text[:2000]) if memo_text else ''}

Question: {prompt}

Answer using specific company names and numbers from the portfolio grid above."""

        response = await self.model_router.get_completion(
            prompt=augmented,
            system_prompt=self._build_system_prompt("Answer with specific numbers — company names, dollar amounts, multiples. When grid data is sparse, estimate using stage benchmarks and state your confidence. Never say 'no data available' — provide ranges. End with 1-2 concrete next steps."),
            capability=ModelCapability.ANALYSIS,
            max_tokens=1000,
            temperature=0.2,
            caller_context="single_shot_answer",
        )
        content = response.get("response", "") if isinstance(response, dict) else str(response)
        return {"content": content, "format": "analysis"}

    def _format_fund_metrics_text(self, result: Dict[str, Any]) -> str:
        """Format fund metrics into readable markdown."""
        if "error" in result:
            return f"Unable to calculate fund metrics: {result['error']}"
        metrics = result.get("metrics", result)
        lines = ["**Fund Metrics**\n"]
        for key, val in metrics.items():
            if isinstance(val, (int, float)):
                if key in ("nav", "fund_size", "total_invested"):
                    lines.append(f"- **{key.upper()}**: ${val/1e6:,.1f}M")
                elif key in ("dpi", "tvpi", "irr"):
                    lines.append(f"- **{key.upper()}**: {val:.2f}x" if key != "irr" else f"- **{key.upper()}**: {val:.1%}")
                else:
                    lines.append(f"- **{key}**: {val:,.2f}")
            elif val is not None:
                lines.append(f"- **{key}**: {val}")
        return "\n".join(lines)

    # ------------------------------------------------------------------
    # Tool wiring: automatic prerequisite resolution
    # ------------------------------------------------------------------

    async def _resolve_prerequisites(
        self,
        tool_name: str,
        tool_input: dict,
        _resolving: set | None = None,
    ) -> None:
        """Ensure all required shared_data keys exist before running *tool_name*.

        For each missing key, look up which tool produces it (via _PRODUCED_BY)
        and run that tool first.  Recursion is cycle-safe via *_resolving*.
        Independent prerequisites are resolved in parallel.
        """
        wiring = TOOL_WIRING.get(tool_name)
        if not wiring:
            return

        if _resolving is None:
            _resolving = set()

        # If "companies" is required but empty, try populating from grid context
        requires = wiring.get("requires", [])
        if "companies" in requires:
            companies = self.shared_data.get("companies")
            if not companies:
                self._populate_companies_from_grid()

        missing: list[str] = []
        for key in requires:
            val = self.shared_data.get(key)
            # Treat empty lists/dicts as missing too
            if val is None or val == [] or val == {}:
                missing.append(key)

        if not missing:
            return

        logger.info(f"[WIRING] {tool_name} missing prerequisites: {missing}")

        # Group missing keys by producing tool so we can parallelize
        producer_tasks: dict[str, list[str]] = {}  # producer_tool → [keys]
        for key in missing:
            producer = _PRODUCED_BY.get(key)
            if not producer:
                logger.debug(f"[WIRING] No producer registered for '{key}', skipping")
                continue
            if producer in _resolving:
                logger.warning(f"[WIRING] Cycle detected: {producer} already resolving, skipping '{key}'")
                continue
            producer_tasks.setdefault(producer, []).append(key)

        if not producer_tasks:
            return

        # Mark these producers as in-flight to prevent cycles
        _resolving.add(tool_name)

        async def _run_producer(producer_tool: str, keys: list[str]) -> None:
            """Run a single producer tool to populate the given shared_data keys."""
            logger.info(f"[WIRING] Auto-resolving {keys} via {producer_tool}")
            # Build minimal inputs for the producer from current context
            producer_inputs = self._build_producer_inputs(producer_tool, tool_input)
            # Recurse: the producer may itself have prerequisites
            await self._resolve_prerequisites(producer_tool, producer_inputs, _resolving)
            # Now execute the producer
            result = await self._execute_tool_raw(producer_tool, producer_inputs)
            if isinstance(result, dict) and "error" not in result:
                self._persist_outputs(producer_tool, result)
            else:
                err = result.get("error", "unknown") if isinstance(result, dict) else str(result)
                logger.warning(f"[WIRING] Producer {producer_tool} failed: {err}")

        # Run independent producers in parallel
        await asyncio.gather(
            *[_run_producer(p, ks) for p, ks in producer_tasks.items()],
            return_exceptions=True,
        )

        _resolving.discard(tool_name)

    def _build_producer_inputs(self, producer_tool: str, caller_inputs: dict) -> dict:
        """Build sensible default inputs for a producer tool from context.

        Uses the caller's inputs + shared_data to supply company names, fund IDs, etc.
        """
        inputs: dict = {}

        # Extract company name from caller inputs or shared_data
        company = (
            caller_inputs.get("company")
            or caller_inputs.get("company_name")
            or caller_inputs.get("company_id")
        )
        if not company:
            companies = self.shared_data.get("companies", [])
            if companies:
                first = companies[0]
                company = first.get("name") or first.get("company", "")

        fund_id = (
            caller_inputs.get("fund_id")
            or self.shared_data.get("fund_context", {}).get("fund_id")
            or self.shared_data.get("fund_context", {}).get("fundId")
        )

        # Map producer tools to their expected input shapes
        if producer_tool == "fetch_company_data":
            inputs["company_name"] = company or ""
        elif producer_tool in ("cap_table_evolution", "revenue_projection", "run_projection"):
            inputs["company"] = company or ""
            if producer_tool in ("revenue_projection", "run_projection"):
                inputs["years"] = 5
        elif producer_tool in ("run_valuation",):
            inputs["company_id"] = company or ""
        elif producer_tool in ("run_scenario",):
            inputs["scenario_description"] = "base case exit scenarios"
        elif producer_tool in ("calculate_fund_metrics",):
            inputs["metrics"] = ["nav", "irr", "dpi", "tvpi"]
            if fund_id:
                inputs["fund_id"] = fund_id
        elif producer_tool in ("run_portfolio_health",):
            if fund_id:
                inputs["fund_id"] = fund_id
        elif producer_tool in ("run_followon_strategy",):
            inputs["company"] = company or ""
            if fund_id:
                inputs["fund_id"] = fund_id
        elif producer_tool in ("run_exit_modeling",):
            inputs["company"] = company or ""
        else:
            # Generic: pass company if the tool schema mentions it
            tool_def = AGENT_TOOL_MAP.get(producer_tool)
            if tool_def:
                schema = tool_def.input_schema or {}
                if "company" in schema:
                    inputs["company"] = company or ""
                if "company_name" in schema:
                    inputs["company_name"] = company or ""
                if "fund_id" in schema and fund_id:
                    inputs["fund_id"] = fund_id

        return inputs

    def _persist_outputs(self, tool_name: str, result: dict) -> None:
        """Store tool outputs into shared_data based on TOOL_WIRING produces.

        Handles merging logic: 'companies' lists are merged by name,
        dict-keyed outputs (cap_table_history, revenue_projections) are
        merged by company key, and everything else is overwritten.
        """
        wiring = TOOL_WIRING.get(tool_name)
        if not wiring:
            return

        produces = wiring.get("produces", [])
        if not produces:
            return

        for key in produces:
            value = result.get(key)

            # Many tools don't return their output under the produces key name
            # directly — they store it in shared_data themselves (e.g.,
            # cap_table_evolution stores to shared_data["cap_table_history"]
            # inside its handler).  Only persist if we have an explicit value
            # AND shared_data doesn't already have it from the handler.
            if value is None:
                continue

            existing = self.shared_data.get(key)

            if key == "companies" and isinstance(value, list):
                # Merge company lists by name
                if not isinstance(existing, list):
                    existing = []
                existing_names = {
                    (c.get("name") or c.get("company", "")).lower()
                    for c in existing
                }
                for c in value:
                    cname = (c.get("name") or c.get("company", "")).lower()
                    if cname and cname not in existing_names:
                        existing.append(c)
                        existing_names.add(cname)
                self.shared_data[key] = existing
            elif isinstance(value, dict) and isinstance(existing, dict):
                # Merge dict outputs (e.g., cap_table_history keyed by company)
                existing.update(value)
                self.shared_data[key] = existing
            else:
                self.shared_data[key] = value

        logger.debug(f"[WIRING] Persisted outputs for {tool_name}: {produces}")

    def _populate_companies_from_grid(self) -> None:
        """Populate shared_data['companies'] from matrix grid context.

        Called by _resolve_prerequisites when 'companies' is required but
        empty — extracts company data from the grid snapshot so downstream
        tools have something to work with even without an explicit fetch.
        """
        matrix_ctx = self.shared_data.get("matrix_context") or {}
        grid_snapshot = matrix_ctx.get("gridSnapshot") or {}
        grid_rows = (
            grid_snapshot.get("rows", [])
            if isinstance(grid_snapshot, dict)
            else grid_snapshot if isinstance(grid_snapshot, list) else []
        )
        if not grid_rows:
            return

        companies: list[dict] = []
        for row in grid_rows[:10]:
            name = row.get("companyName") or row.get("company_name") or ""
            cells = row.get("cells") or row.get("cellValues") or {}
            if not name:
                continue

            def _cv(key_patterns: list[str]):
                for k, v in cells.items():
                    for pat in key_patterns:
                        if pat in k.lower():
                            val = v.get("value", v) if isinstance(v, dict) else v
                            return val if val and val != "N/A" else None
                return None

            company_obj = {
                "company": name, "name": name,
                "stage": _cv(["stage", "round", "series"]),
                "sector": _cv(["sector", "vertical", "industry"]),
                "revenue": _cv(["arr", "revenue"]),
                "valuation": _cv(["valuation", "post_money"]),
                "investors": _cv(["investors", "lead_investor"]),
                "description": _cv(["description", "business"]),
                "growth_rate": _cv(["growth", "arr_growth"]),
                "team_size": _cv(["headcount", "team_size", "employees"]),
            }
            # Clean numeric fields
            for nf in ("revenue", "valuation", "growth_rate", "team_size"):
                v = company_obj.get(nf)
                if isinstance(v, str):
                    try:
                        company_obj[nf] = float(
                            v.replace("$", "").replace(",", "")
                            .replace("M", "e6").replace("B", "e9").replace("%", "")
                        )
                    except (ValueError, TypeError):
                        company_obj[nf] = None
            companies.append(company_obj)

        if companies:
            self.shared_data["companies"] = companies
            logger.info(f"[WIRING] Auto-populated {len(companies)} companies from grid")

    async def _execute_tool_raw(self, tool_name: str, tool_input: dict, max_retries: int = 2) -> dict:
        """Low-level tool dispatch — NO prerequisite resolution (avoids recursion).

        This is the original _execute_tool logic: lookup handler, run with
        timeout + retry.  Called by both _execute_tool (after prereqs) and
        _resolve_prerequisites (to avoid infinite loops).
        """
        tool_def = AGENT_TOOL_MAP.get(tool_name)
        if not tool_def:
            return {"error": f"Unknown tool: {tool_name}"}
        handler = getattr(self, tool_def.handler, None)
        if not handler:
            return {"error": f"Handler not found: {tool_def.handler}"}

        last_error = None
        for attempt in range(max_retries + 1):
            try:
                result = await asyncio.wait_for(
                    handler(tool_input),
                    timeout=tool_def.timeout_ms / 1000,
                )
                return result
            except asyncio.TimeoutError:
                last_error = f"Timeout after {tool_def.timeout_ms}ms"
            except Exception as e:
                last_error = str(e)
                if "rate_limit" in str(e).lower() or "429" in str(e):
                    delay = (2 ** attempt) + random.uniform(0, 1)
                    logger.warning(f"[AGENT_TOOL] Rate limited on {tool_name}, retry in {delay:.1f}s")
                    await asyncio.sleep(delay)
                elif attempt < max_retries:
                    await asyncio.sleep(0.5)
                else:
                    break

        logger.error(f"[AGENT_TOOL] {tool_name} failed after {max_retries + 1} attempts: {last_error}")
        return {"error": f"{tool_name} failed after {max_retries + 1} attempts: {last_error}"}

    async def _execute_tool(self, tool_name: str, tool_input: dict, max_retries: int = 2) -> dict:
        """Dispatch to tool handler with automatic prerequisite resolution.

        1. Check TOOL_WIRING for required shared_data keys
        2. For any missing key, auto-run the producer tool (recursive, parallel)
        3. Execute the actual tool
        4. Persist outputs to shared_data per TOOL_WIRING produces
        """
        # Step 1+2: resolve prerequisites
        await self._resolve_prerequisites(tool_name, tool_input)

        # Step 3: execute the tool itself
        result = await self._execute_tool_raw(tool_name, tool_input, max_retries)

        # Step 4: persist outputs (only if handler didn't already)
        if isinstance(result, dict) and "error" not in result:
            self._persist_outputs(tool_name, result)

        return result

    # ------------------------------------------------------------------
    # Tool handler methods — thin adapters around existing services
    # ------------------------------------------------------------------

    def _extract_numeric(self, cells: dict, *keys: str) -> float:
        """Pull a numeric value from grid cells (handles raw or {value:X} dicts)."""
        for k in keys:
            raw = cells.get(k)
            if isinstance(raw, dict):
                raw = raw.get("value")
            if raw is not None:
                try:
                    return float(raw)
                except (ValueError, TypeError):
                    pass
        return 0.0

    def _extract_str(self, cells: dict, *keys: str) -> str:
        """Pull a string value from grid cells (handles raw or {value:X} dicts)."""
        for k in keys:
            raw = cells.get(k)
            if isinstance(raw, dict):
                raw = raw.get("value")
            if raw:
                return str(raw)
        return ""

    async def _tool_query_portfolio(self, inputs: dict) -> dict:
        """Query/filter the portfolio grid. Primary: frontend grid context. Fallback: MatrixQueryOrchestrator."""
        try:
            query = inputs.get("query", "")
            filters = inputs.get("filters") or {}
            fund_ctx = self.shared_data.get("fund_context", {})
            matrix_ctx = self.shared_data.get("matrix_context") or {}
            grid_snapshot = matrix_ctx.get("gridSnapshot") or {}
            grid_rows = grid_snapshot.get("rows", []) if isinstance(grid_snapshot, dict) else grid_snapshot if isinstance(grid_snapshot, list) else []

            # PRIMARY PATH: Read directly from the frontend grid context
            if grid_rows:
                rows_out = []
                for row in grid_rows[:50]:
                    name = row.get("companyName") or row.get("company_name") or ""
                    cells = row.get("cells") or row.get("cellValues") or {}
                    row_id = row.get("rowId") or row.get("row_id") or ""
                    rows_out.append({"rowId": row_id, "companyName": name, "cells": dict(list(cells.items())[:20])})

                # Apply simple filters if provided
                if filters:
                    stage_filter = filters.get("stage") or filters.get("investment_stage")
                    if stage_filter:
                        stage_lower = stage_filter.lower()
                        rows_out = [r for r in rows_out if stage_lower in str(r.get("cells", {}).get("stage", "")).lower() or stage_lower in str(r.get("cells", {}).get("investment_stage", "")).lower()]
                    name_filter = filters.get("name") or filters.get("company_name")
                    if name_filter:
                        name_lower = name_filter.lower()
                        rows_out = [r for r in rows_out if name_lower in r.get("companyName", "").lower()]

                columns = matrix_ctx.get("columns") or []
                col_names = [c.get("name") or c.get("id") for c in columns[:20]] if columns else []
                summary = f"Portfolio grid: {len(rows_out)} companies"
                if col_names:
                    summary += f", columns: {', '.join(col_names[:10])}"
                logger.info(f"[TOOL] query_portfolio served from grid context: {len(rows_out)} rows")
                return {"rows": rows_out, "summary": summary, "columns": col_names, "source": "grid_context"}

            # FALLBACK PATH: MatrixQueryOrchestrator (Supabase)
            if MATRIX_QUERY_ORCHESTRATOR_AVAILABLE and self.matrix_query_orchestrator:
                mqo = self.matrix_query_orchestrator
                result = await mqo.process_matrix_query(
                    query,
                    fund_id=fund_ctx.get("fundId"),
                    context={"gridSnapshot": grid_snapshot, "filters": filters},
                )
                return {"rows": result.get("rows", []), "summary": result.get("summary", ""), "source": "supabase"}

            # LAST RESORT: Try portfolio_service (with or without fund_id)
            try:
                from app.services.portfolio_service import portfolio_service
                fund_id = fund_ctx.get("fundId") or fund_ctx.get("fund_id")
                if fund_id:
                    portfolio = await portfolio_service.get_portfolio(fund_id)
                else:
                    portfolio = await portfolio_service.get_all_companies()
                companies = portfolio.get("companies", [])
                if companies:
                    return {"rows": companies, "summary": f"Portfolio: {len(companies)} companies from DB", "source": "portfolio_service"}
            except Exception as ps_err:
                logger.debug(f"[TOOL] portfolio_service fallback failed: {ps_err}")

            return {"summary": "No portfolio data found. Ensure the grid is loaded or database is connected.", "rows": []}
        except Exception as e:
            logger.warning(f"[TOOL] query_portfolio failed: {e}")
            return {"summary": f"Query failed: {e}", "rows": []}

    async def _tool_query_documents(self, inputs: dict) -> dict:
        """Search uploaded documents."""
        try:
            from app.services.document_query_service import DocumentQueryService
            dqs = DocumentQueryService()
            fund_id = self.shared_data.get("fund_context", {}).get("fundId")
            company_id = inputs.get("company_id")
            query_text = inputs.get("query", "")

            # Route via detect_query_type for best method
            query_type = dqs.detect_query_type(query_text)
            if str(query_type) in ("MatrixQueryType.METRIC", "metric", "financial"):
                docs = dqs.query_by_metric(
                    query_text,
                    fund_id=fund_id,
                    company_id=company_id,
                )
            else:
                docs = dqs.query_portfolio_documents(
                    fund_id=fund_id,
                    company_ids=[company_id] if company_id else None,
                    document_types=None,
                )
            doc_list = docs if isinstance(docs, list) else docs.get("results", docs.get("documents", []))
            return {"documents": doc_list[:10], "count": len(doc_list)}
        except Exception as e:
            logger.warning(f"[TOOL] query_documents failed: {e}")
            return {"documents": [], "count": 0, "error": str(e)}

    async def _tool_fund_metrics(self, inputs: dict) -> dict:
        """Calculate fund-level metrics via FundModelingService."""
        try:
            if not self.fund_modeling:
                return {"error": "FundModelingService not available"}
            fms = self.fund_modeling
            fund_ctx = self.shared_data.get("fund_context", {})
            fund_id = inputs.get("fund_id") or fund_ctx.get("fundId")
            metrics_list = inputs.get("metrics", ["nav", "irr", "dpi", "tvpi"])
            result = await fms.calculate_fund_metrics(fund_id)
            if not isinstance(result, dict):
                return {"metrics": {}}
            # Filter to requested metrics client-side
            metrics_data = result.get("metrics", result)
            if metrics_list:
                filtered = {k: v for k, v in result.items() if k in metrics_list or k in ("fund_id", "as_of", "metrics", "portfolio", "investments")}
                return {"metrics": filtered if filtered else result}
            return {"metrics": result}
        except Exception as e:
            logger.warning(f"[TOOL] fund_metrics failed: {e}")
            return {"error": str(e)}

    async def _tool_valuation(self, inputs: dict) -> dict:
        """Run valuation via ValuationEngineService.value_company()."""
        try:
            if not self.valuation_engine:
                return {"error": "ValuationEngineService not available"}
            ves = self.valuation_engine
            # Accept company_id, company, or company_name
            company_id = inputs.get("company_id") or inputs.get("company") or inputs.get("company_name", "")
            # Build company_data from grid snapshot
            grid = self.shared_data.get("matrix_context", {}).get("gridSnapshot", {})
            company_data = {}
            for row in grid.get("rows", []):
                if row.get("rowId") == company_id or row.get("companyName", "").lower() == company_id.lower():
                    cells = row.get("cells", {})
                    company_data = {
                        "name": self._extract_str(cells, "name", "companyName") or row.get("companyName", company_id),
                        "revenue": self._extract_numeric(cells, "arr", "revenue"),
                        "growth_rate": self._extract_numeric(cells, "growthRate", "growth_rate"),
                        "funding_stage": self._extract_str(cells, "fundingStage", "stage"),
                        "total_funding": self._extract_numeric(cells, "totalFunding", "total_funding"),
                        "valuation": self._extract_numeric(cells, "valuation", "currentValuation"),
                    }
                    break
            if not company_data:
                company_data = {"name": company_id}
            result = await ves.value_company(
                company_data,
                method=inputs.get("method", "auto"),
            )
            val_result = result if isinstance(result, dict) else {"valuation": result}

            # Auto-emit suggestion to grid if valuation produced a number
            val_amount = val_result.get("valuation") or val_result.get("fair_value") or val_result.get("equity_value")
            fund_id = self.shared_data.get("fund_context", {}).get("fundId")
            if val_amount and fund_id and company_id:
                try:
                    supabase_url = settings.SUPABASE_URL
                    supabase_key = settings.SUPABASE_SERVICE_ROLE_KEY or settings.SUPABASE_ANON_KEY
                    if supabase_url and supabase_key:
                        from supabase import create_client
                        sb = create_client(supabase_url, supabase_key)
                        method_used = val_result.get("method", inputs.get("method", "auto"))
                        sb.table("pending_suggestions").upsert({
                            "fund_id": fund_id,
                            "company_id": company_id,
                            "column_id": "valuation",
                            "suggested_value": val_amount if isinstance(val_amount, (int, float)) else str(val_amount),
                            "source_service": f"valuation_engine.{method_used}",
                            "reasoning": f"Valuation via {method_used}: {val_result.get('summary', '')}",
                            "metadata": {"tool": "run_valuation", "method": method_used},
                        }, on_conflict="fund_id,company_id,column_id").execute()
                except Exception as e:
                    logger.warning(f"[TOOL] Failed to persist valuation suggestion: {e}")

            # Emit suggestion in return value so frontend sees it immediately
            if val_amount:
                method_label = val_result.get('method', 'auto')
                summary_text = val_result.get('summary', '')
                val_result["suggestion"] = {
                    "company": company_data.get("name", company_id),
                    "column": "currentValuation",
                    "value": val_amount if isinstance(val_amount, (int, float)) else str(val_amount),
                    "source": f"valuation_engine.{method_label}",
                    "reasoning": f"Valuation via {method_label}: {summary_text}",
                }
                val_result["grid_commands"] = [{
                    "type": "suggest_edit",
                    "companyName": company_data.get("name", company_id),
                    "columnId": "valuation",
                    "value": val_amount if isinstance(val_amount, (int, float)) else str(val_amount),
                    "reasoning": f"Valuation via {method_label}: {summary_text}"[:200],
                    "confidence": 0.7,
                    "source": f"valuation_engine.{method_label}",
                }]

            return val_result
        except Exception as e:
            logger.warning(f"[TOOL] valuation failed: {e}")
            return {"error": str(e)}

    async def _tool_scenario(self, inputs: dict) -> dict:
        """Run scenario analysis.

        First tries NLScenarioComposer for natural language "what if" parsing.
        Falls back to direct ScenarioAnalyzer for keyword-based scenarios.
        """
        try:
            from app.services.scenario_analyzer import ScenarioAnalyzer, ScenarioType
            from app.services.nl_scenario_composer import NLScenarioComposer

            sa = ScenarioAnalyzer()
            nl_composer = self.nl_scenario_composer or NLScenarioComposer()
            model_id = inputs.get("model_id", "portfolio")
            description = inputs.get("scenario_description", "Ad-hoc scenario")
            fund_id = self.shared_data.get("fund_context", {}).get("fundId")

            # --- Try NL parsing first for "what if" style queries ---
            composed = await nl_composer.parse_what_if_query(description, fund_id=fund_id)

            if composed.events:
                logger.info(f"[TOOL] scenario: NL composer parsed {len(composed.events)} events")
                try:
                    wm_result = await nl_composer.compose_scenario_to_world_model(
                        composed_scenario=composed,
                        model_id=model_id,
                        fund_id=fund_id,
                    )
                    scenario = wm_result.get("scenario")
                    if scenario and scenario.get("id"):
                        exec_result = await sa.execute_scenario(scenario["id"])
                        result = exec_result if isinstance(exec_result, dict) else {"analysis": str(exec_result)}
                        result["nl_events"] = [
                            {"entity": e.entity_name, "type": e.event_type, "timing": e.timing}
                            for e in composed.events
                        ]
                        # Persist to shared_data (don't overwrite richer PWERM data)
                        async with self.shared_data_lock:
                            if not self.shared_data.get("scenario_analysis"):
                                self.shared_data["scenario_analysis"] = result
                        return result
                except Exception as nl_err:
                    logger.warning(f"[TOOL] NL scenario composition failed, falling back: {nl_err}")

            # --- Fallback: keyword-based scenario type detection ---
            desc_lower = description.lower()
            if any(w in desc_lower for w in ["stress", "worst", "crash", "downturn"]):
                s_type = ScenarioType.STRESS
            elif any(w in desc_lower for w in ["upside", "best", "bull"]):
                s_type = ScenarioType.UPSIDE
            elif any(w in desc_lower for w in ["downside", "bear"]):
                s_type = ScenarioType.DOWNSIDE
            else:
                s_type = ScenarioType.CUSTOM

            scenario = await sa.create_scenario(
                model_id=model_id,
                scenario_name=description[:50],
                scenario_type=s_type,
                description=description,
            )
            if scenario and scenario.get("id"):
                result = await sa.execute_scenario(scenario["id"])
                result = result if isinstance(result, dict) else {"analysis": str(result)}
                # Persist to shared_data (don't overwrite richer PWERM data)
                async with self.shared_data_lock:
                    if not self.shared_data.get("scenario_analysis"):
                        self.shared_data["scenario_analysis"] = result
                return result
            return {"analysis": "Scenario created but could not be executed", "scenario": scenario}
        except Exception as e:
            logger.warning(f"[TOOL] scenario failed: {e}")
            return {"error": str(e)}

    async def _tool_scenario_tree(self, inputs: dict) -> dict:
        """Build branching scenario tree from growth path queries or round planning."""
        try:
            from app.services.scenario_tree_service import ScenarioTreeService, GrowthPath
            from app.services.nl_scenario_composer import NLScenarioComposer

            query = inputs.get("query", "")
            years = inputs.get("years", 5)

            nl = self.nl_scenario_composer or NLScenarioComposer()
            svc = ScenarioTreeService()

            # Get known companies from shared_data
            known = [
                c.get("company_name", c.get("name", ""))
                for c in self.shared_data.get("companies", [])
            ]

            # Parse the query into growth paths
            company_paths = nl.parse_scenario_tree_query(query, known_companies=known)
            if not company_paths:
                return {"error": "Could not parse growth paths from query. Try: 'X grows at 30% then 20%, or Y at 50%, 40%, 30%'"}

            # Resolve base company data from shared_data
            base_data: dict = {}
            for cname in company_paths:
                for comp in self.shared_data.get("companies", []):
                    if comp.get("company_name", "").lower() == cname.lower() or comp.get("name", "").lower() == cname.lower():
                        base_data[cname] = comp
                        break
                if cname not in base_data:
                    base_data[cname] = {"company_name": cname, "revenue": 1_000_000, "stage": "Series A"}

            fund_ctx = self.shared_data.get("fund_context", {})
            fund_context = {
                "fund_size": fund_ctx.get("fundSize", 260_000_000),
                "total_invested": fund_ctx.get("totalInvested", 0),
                "distributions": fund_ctx.get("distributions", 0),
            }

            tree = svc.build_tree(company_paths, base_data, fund_context, years=years)
            tree_chart = svc.tree_to_chart_data(tree)
            paths_chart = svc.paths_to_line_chart_data(tree, metric="revenue")
            memo_sections = svc.tree_to_memo_sections(tree)

            # Fund-level DPI evaluation: evaluate tree paths in context of full portfolio
            fund_level_results = []
            try:
                from app.services.fund_modeling_service import fund_modeling_service
                portfolio = self.shared_data.get("companies", [])
                fund_level_results = fund_modeling_service.evaluate_scenario_tree_on_fund(
                    tree_paths=tree_chart["data"]["paths"],
                    portfolio_companies=portfolio,
                    fund_size=fund_context.get("fund_size", 260_000_000),
                    total_invested=fund_context.get("total_invested", 0),
                )
            except Exception as dpi_err:
                logger.warning(f"[TOOL] scenario_tree DPI evaluation failed: {dpi_err}")

            result = {
                "tree_chart": tree_chart,
                "paths_chart": paths_chart,
                "memo_sections": memo_sections,
                "expected_tvpi": tree_chart["data"]["expected_tvpi"],
                "sensitivity": tree_chart["data"]["sensitivity"],
                "num_paths": len(tree.paths),
                "companies": tree.companies,
                "fund_level_dpi": fund_level_results,
            }

            # Store all chart variants so chart dispatch can serve them
            all_charts = svc.to_all_charts(tree)

            async with self.shared_data_lock:
                self.shared_data["scenario_analysis"] = result
                self.shared_data["scenario_tree"] = tree
                self.shared_data["scenario_all_charts"] = all_charts

            return result
        except Exception as e:
            logger.warning(f"[TOOL] scenario_tree failed: {e}")
            return {"error": str(e)}

    async def _tool_cash_flow_model(self, inputs: dict) -> dict:
        """Build full P&L / cash flow model for a company."""
        try:
            from app.services.cash_flow_planning_service import CashFlowPlanningService

            svc = CashFlowPlanningService()
            company_name = inputs.get("company", "")
            years = inputs.get("years", 5)
            growth_overrides = inputs.get("growth_overrides")

            # Resolve company from shared_data
            company_data = None
            for comp in self.shared_data.get("companies", []):
                if comp.get("company_name", "").lower() == company_name.lower() or comp.get("name", "").lower() == company_name.lower():
                    company_data = comp
                    break

            if not company_data:
                company_data = {"company_name": company_name, "revenue": 1_000_000, "stage": "Series A"}

            model = svc.build_cash_flow_model(company_data, years=years, growth_overrides=growth_overrides)
            runway = svc.calculate_runway(model)
            funding_gap = svc.calculate_funding_gap(model)
            memo_sections = svc.to_memo_sections(company_name, model)

            # Waterfall for most recent projected year
            waterfall_chart = svc.to_waterfall_chart_data(model[-1]) if model else None

            return {
                "cash_flow_model": model,
                "runway": runway,
                "funding_gap": funding_gap,
                "waterfall_chart": waterfall_chart,
                "memo_sections": memo_sections,
                "company": company_name,
            }
        except Exception as e:
            logger.warning(f"[TOOL] cash_flow_model failed: {e}")
            return {"error": str(e)}


    async def _tool_bull_bear_base(self, inputs: dict) -> dict:
        """Build bull/bear/base scenarios for one or more companies."""
        try:
            from app.services.scenario_tree_service import ScenarioTreeService

            svc = ScenarioTreeService()
            companies = inputs.get("companies", [])
            years = inputs.get("years", 5)

            # Resolve company data
            companies_data = {}
            for cname in companies:
                for comp in self.shared_data.get("companies", []):
                    if comp.get("company_name", "").lower() == cname.lower() or comp.get("name", "").lower() == cname.lower():
                        companies_data[cname] = comp
                        break
                if cname not in companies_data:
                    companies_data[cname] = {"company_name": cname, "revenue": 1_000_000, "stage": "Series A"}

            fund_ctx = self.shared_data.get("fund_context", {})
            fund_context = {
                "fund_size": fund_ctx.get("fundSize", 260_000_000),
                "total_invested": fund_ctx.get("totalInvested", 0),
                "distributions": fund_ctx.get("distributions", 0),
            }

            if len(companies_data) == 1:
                cn = list(companies_data.keys())[0]
                tree = svc.build_bull_bear_base(cn, companies_data[cn], fund_context, years=years)
            else:
                tree = svc.build_portfolio_scenarios(companies_data, fund_context, years=years)

            charts = svc.to_all_charts(tree)
            memo = svc.tree_to_memo_sections(tree)

            # Fund-level DPI evaluation across full portfolio
            fund_level_results = []
            try:
                from app.services.fund_modeling_service import fund_modeling_service
                tree_chart = charts.get("scenario_tree", {})
                paths_data = tree_chart.get("data", {}).get("paths", []) if isinstance(tree_chart, dict) else []
                portfolio = self.shared_data.get("companies", [])
                fund_level_results = fund_modeling_service.evaluate_scenario_tree_on_fund(
                    tree_paths=paths_data,
                    portfolio_companies=portfolio,
                    fund_size=fund_context.get("fund_size", 260_000_000),
                    total_invested=fund_context.get("total_invested", 0),
                )
            except Exception as dpi_err:
                logger.warning(f"[TOOL] bull_bear_base DPI evaluation failed: {dpi_err}")

            result = {**charts, "memo_sections": memo, "companies": tree.companies, "num_paths": len(tree.paths), "fund_level_dpi": fund_level_results}

            async with self.shared_data_lock:
                self.shared_data["scenario_analysis"] = result
                self.shared_data["scenario_tree"] = tree

            return result
        except Exception as e:
            logger.warning(f"[TOOL] bull_bear_base failed: {e}")
            return {"error": str(e)}

    async def _tool_macro_shock(self, inputs: dict) -> dict:
        """Apply macro shock to existing scenario tree."""
        try:
            from app.services.scenario_tree_service import ScenarioTreeService

            tree = self.shared_data.get("scenario_tree")
            if not tree:
                return {"error": "No scenario tree found. Run bull_bear_base or scenario_tree first."}

            svc = ScenarioTreeService()
            shock_type = inputs.get("shock_type", "recession")
            magnitude = inputs.get("magnitude", 0.5)
            start_year = inputs.get("start_year", 1)

            shocked_tree = svc.apply_macro_shock(tree, shock_type, magnitude=magnitude, start_year=start_year)
            charts = svc.to_all_charts(shocked_tree)
            memo = svc.tree_to_memo_sections(shocked_tree)

            ev_before = tree.expected_value
            ev_after = shocked_tree.expected_value

            impact = {
                "shock_type": shock_type,
                "magnitude": magnitude,
                "nav_change_pct": ((ev_after.nav - ev_before.nav) / ev_before.nav * 100) if ev_before and ev_before.nav else 0,
                "tvpi_change": (ev_after.tvpi - ev_before.tvpi) if ev_before else 0,
            }

            result = {**charts, "memo_sections": memo, "impact": impact}

            async with self.shared_data_lock:
                self.shared_data["scenario_analysis"] = result
                self.shared_data["scenario_tree"] = shocked_tree

            return result
        except Exception as e:
            logger.warning(f"[TOOL] macro_shock failed: {e}")
            return {"error": str(e)}

    async def _tool_portfolio_snapshot(self, inputs: dict) -> dict:
        """Get point-in-time portfolio snapshot from scenario tree."""
        try:
            from app.services.scenario_tree_service import ScenarioTreeService

            tree = self.shared_data.get("scenario_tree")
            if not tree:
                return {"error": "No scenario tree found. Run bull_bear_base or scenario_tree first."}

            svc = ScenarioTreeService()
            year = inputs.get("year", 3)
            snapshot = svc.snapshot_at_year(tree, year)

            return {"snapshot": snapshot, "year": year}
        except Exception as e:
            logger.warning(f"[TOOL] portfolio_snapshot failed: {e}")
            return {"error": str(e)}

    async def _tool_three_scenario_cash_flow(self, inputs: dict) -> dict:
        """Build bull/base/bear P&L models side by side."""
        try:
            from app.services.cash_flow_planning_service import CashFlowPlanningService

            svc = CashFlowPlanningService()
            company_name = inputs.get("company", "")
            years = inputs.get("years", 5)

            company_data = None
            for comp in self.shared_data.get("companies", []):
                if comp.get("company_name", "").lower() == company_name.lower() or comp.get("name", "").lower() == company_name.lower():
                    company_data = comp
                    break
            if not company_data:
                company_data = {"company_name": company_name, "revenue": 1_000_000, "stage": "Series A"}

            models = svc.build_three_scenario_model(company_data, years=years)
            memo_sections = []
            for scenario_name, model in models.items():
                memo_sections.extend(svc.to_memo_sections(f"{company_name} ({scenario_name.title()})", model))

            return {"models": models, "memo_sections": memo_sections, "company": company_name}
        except Exception as e:
            logger.warning(f"[TOOL] three_scenario_cash_flow failed: {e}")
            return {"error": str(e)}

    async def _tool_chart(self, inputs: dict) -> dict:
        """Generate chart config via ChartDataService — dispatches to specific generators."""
        try:
            from app.services.chart_data_service import ChartDataService
            cds = ChartDataService()
            chart_type = inputs.get("chart_type", "bar")
            grid = self.shared_data.get("matrix_context", {}).get("gridSnapshot", {})

            # Extract company list from grid for multi-company charts
            companies = []
            for row in grid.get("rows", []):
                cells = row.get("cells", {})
                companies.append({
                    "name": self._extract_str(cells, "name", "companyName") or row.get("companyName", ""),
                    "revenue": self._extract_numeric(cells, "arr", "revenue"),
                    "valuation": self._extract_numeric(cells, "valuation", "currentValuation"),
                    "growth_rate": self._extract_numeric(cells, "growthRate"),
                    "funding_stage": self._extract_str(cells, "fundingStage"),
                    "total_funding": self._extract_numeric(cells, "totalFunding"),
                })

            # --- Merge enriched shared_data["companies"] into grid-extracted rows ---
            # The grid snapshot may have zeros/blanks for fields the agent already
            # fetched via search/fetch tools.  Overlay enriched values so charts
            # reflect the latest data the agent gathered.
            sd_companies = self.shared_data.get("companies", [])
            if sd_companies:
                _enriched_by_name: Dict[str, Dict] = {}
                for sc in sd_companies:
                    _ename = (sc.get("company") or sc.get("name") or "").strip().lower()
                    if _ename:
                        _enriched_by_name[_ename] = sc

                _FIELD_MAP = {
                    "revenue": ("revenue", "arr", "inferred_revenue"),
                    "valuation": ("valuation", "current_valuation", "inferred_valuation"),
                    "growth_rate": ("revenue_growth", "growth_rate", "yoy_growth"),
                    "total_funding": ("total_funding", "funding_total"),
                    "funding_stage": ("funding_stage", "stage", "latest_round"),
                }
                for comp in companies:
                    _ckey = (comp.get("name") or "").strip().lower()
                    enriched = _enriched_by_name.get(_ckey)
                    if not enriched:
                        continue
                    for dest, src_keys in _FIELD_MAP.items():
                        # Only overlay if the grid value is missing/zero
                        existing = comp.get(dest)
                        if existing and existing != 0 and existing != "":
                            continue
                        for sk in src_keys:
                            val = enriched.get(sk)
                            if val is not None and val != 0 and val != "":
                                comp[dest] = val
                                break

            chart_dispatch = {
                # ── Existing generators ──
                "revenue_multiples_scatter": lambda: cds.generate_revenue_multiples_scatter(grid),
                "revenue_multiple_scatter": lambda: cds.generate_revenue_multiple_scatter(companies),
                "revenue_treemap": lambda: cds.generate_revenue_treemap(companies),
                "revenue_growth_treemap": lambda: cds.generate_revenue_growth_treemap(companies),
                "probability_cloud": lambda: cds.generate_probability_cloud(companies[0] if companies else {}, 10_000_000),
                "path_to_100m": lambda: cds.generate_path_to_100m(companies),
                "cashflow": lambda: cds.generate_cashflow_projection(companies),
                "velocity_ranking": lambda: cds.generate_product_velocity_ranking(companies),
                "next_round_treemap": lambda: cds.generate_next_round_treemap(companies),
                # ── Newly wired generators (audit fix) ──
                "waterfall": lambda: cds.generate_waterfall(companies),
                "bar_comparison": lambda: cds.generate_bar_comparison(companies),
                "scatter_multiples": lambda: cds.generate_revenue_multiple_scatter(companies),
                "cap_table_sankey": lambda: cds.generate_cap_table_sankey(companies),
                "revenue_forecast": lambda: cds.generate_path_to_100m(companies),  # alias
                "dpi_sankey": lambda: cds.generate_dpi_sankey(companies),
                "bull_bear_base": lambda: cds.generate_bull_bear_base(companies),
                "radar_comparison": lambda: cds.generate_radar_comparison(companies),
                # ── Fund-level & advanced charts ──
                "heatmap": lambda: cds.generate_heatmap(companies),
                "cap_table_evolution": lambda: cds.generate_cap_table_evolution(companies),
                "stacked_bar": lambda: cds.generate_stacked_bar(companies),
                "market_map": lambda: cds.generate_market_map(companies),
                "nav_live": lambda: cds.generate_nav_live(companies),
                "fpa_stress_test": lambda: cds.generate_fpa_stress_test(companies),
                # ── Analytics-bridge charts (use stored results from shared_data) ──
                "sensitivity_tornado": lambda: cds.generate_sensitivity_tornado(
                    self.shared_data.get("fpa_result", {})),
                "regression_line": lambda: cds.generate_regression_line(
                    self.shared_data.get("fpa_result", {})),
                "monte_carlo_histogram": lambda: cds.generate_monte_carlo_histogram(
                    self.shared_data.get("monte_carlo_result", {})),
                "revenue_forecast_decay": lambda: cds.generate_revenue_forecast(
                    self.shared_data.get("revenue_projections", {}).get(
                        (companies[0].get("name", "") if companies else ""), [])),
                "fund_scenarios": lambda: cds.generate_fund_scenario_comparison(
                    self.shared_data.get("scenario_all_charts", {})),
            }

            generator = chart_dispatch.get(chart_type)
            if generator:
                config = generator()
                return {"chart_config": config} if config else {"error": f"No data for {chart_type}"}
            # Fallback: use ChartGenerationSkill for auto/unknown types
            skill = self.skills.get("chart-generator") or self.skills.get("chart_generation")
            if skill:
                skill_result = await skill.execute({
                    "chart_type": chart_type,
                    "companies": companies,
                    "data": grid,
                })
                charts = skill_result.get("charts", [])
                if charts:
                    return {"chart_config": charts[0]}
            # Last resort: revenue_multiples_scatter
            config = cds.generate_revenue_multiples_scatter(grid) if grid.get("rows") else None
            if config:
                return {"chart_config": config}
            return {"error": f"Unknown chart type: {chart_type}. Available: {list(chart_dispatch.keys())}"}
        except Exception as e:
            logger.warning(f"[TOOL] chart failed: {e}")
            return {"error": str(e)}

    async def _tool_web_search(self, inputs: dict) -> dict:
        """Search the web via existing Tavily integration."""
        try:
            raw = await self._tavily_search(inputs["query"])
            items = raw.get("results", []) if isinstance(raw, dict) else []
            # Compress: title + snippet + url, max 5 results
            return {
                "results": [
                    {
                        "title": r.get("title", ""),
                        "snippet": r.get("content", "")[:200],
                        "url": r.get("url", ""),
                    }
                    for r in items[:5]
                ]
            }
        except Exception as e:
            logger.warning(f"[TOOL] web_search failed: {e}")
            return {"results": [], "error": str(e)}

    async def _tool_search_companies_db(self, inputs: dict) -> dict:
        """Search the rich 1k+ companies database for existing data before web fetching."""
        query = inputs.get("query", "")
        if not query:
            return {"error": "query is required"}
        try:
            from app.services.portfolio_service import PortfolioService
            ps = PortfolioService()
            result = await ps.search_companies_db(query, limit=inputs.get("limit", 10))
            return result
        except Exception as e:
            logger.warning(f"[TOOL] search_companies_db failed: {e}")
            return {"companies": [], "error": str(e)}

    # ------------------------------------------------------------------
    # Sourcing Engine: fast DB query + math scoring, no LLM / no web
    # ------------------------------------------------------------------
    async def _tool_source_companies(self, inputs: dict) -> dict:
        """Query, filter, score, and rank companies from the database.

        Fast path — works on existing data (no web calls by default).
        When discover_web=true AND DB results are thin, enters the web
        discovery loop: rubric → LLM query gen → Tavily → extract names →
        enrich → score with same rubric → merge + dedupe → return.

        1. query_companies() — Supabase query with filters
        2. score_companies() — pure math scoring
        3. If discover_web and results < threshold → web discovery loop
        4. Optional microskills enrichment on top N
        5. Optional persistence back to DB
        6. Pick display mode and format response
        """
        from app.services.sourcing_service import (
            query_companies, score_companies,
            pick_display_mode, format_as_markdown_table,
            generate_rubric, build_query_gen_prompt,
            build_name_extraction_prompt,
            build_decomposition_prompt, stamp_queries_from_decomposition,
            build_bulk_extraction_prompt, build_rich_extraction_prompt,
            completeness_score, classify_enrichment_tier,
            merge_discovered_companies, build_round2_adaptive_queries,
            QUERY_INTENTS,
            build_rubric_classification_prompt, parse_llm_rubric,
            build_semantic_scoring_prompt,
        )

        filters = inputs.get("filters") or {}
        max_results = min(inputs.get("max_results") or 50, 200)
        sort_by = inputs.get("sort_by", "score")
        sort_desc = inputs.get("sort_desc", True)
        target_stage = inputs.get("target_stage")
        custom_weights = inputs.get("custom_weights")
        display = inputs.get("display")
        enrich_top_n = inputs.get("enrich_top_n", 0)  # enrich top N results via microskills
        persist_results = inputs.get("persist_results", False)
        # Web discovery params
        discover_web = inputs.get("discover_web", False)
        min_web_threshold = inputs.get("min_web_threshold", 5)
        thesis = inputs.get("thesis", "")

        try:
            # Extract target_stage from filters if not provided directly
            if not target_stage and filters.get("stage"):
                target_stage = filters["stage"]

            # --- Step 0: Always generate rubric first (before DB query) ---
            from app.services.model_router import ModelCapability
            rubric = None
            if thesis:
                try:
                    rubric_prompt = build_rubric_classification_prompt(thesis)
                    rubric_result = await self.model_router.get_completion(
                        prompt=rubric_prompt,
                        system_prompt="Classify this list-building request. Return valid JSON only.",
                        capability=ModelCapability.STRUCTURED,
                        max_tokens=600, temperature=0.2,
                        json_mode=True,
                        caller_context="source_companies_rubric_llm",
                    )
                    raw_rubric = rubric_result.get("response", "{}") if isinstance(rubric_result, dict) else str(rubric_result)
                    llm_rubric = json.loads(raw_rubric) if isinstance(raw_rubric, str) else raw_rubric
                    rubric = parse_llm_rubric(
                        llm_output=llm_rubric,
                        query=thesis,
                        weight_overrides=custom_weights,
                        target_stage=target_stage,
                        filters=filters,
                    )
                    logger.info(
                        "[source_companies] LLM rubric: intent=%s entity=%s sector=%s",
                        rubric.get("intent"), rubric.get("entity_type"),
                        rubric.get("filters", {}).get("sector"),
                    )
                except Exception as e:
                    logger.warning(f"[source_companies] LLM rubric failed, using pattern match: {e}")

            if rubric is None and thesis:
                rubric = generate_rubric(
                    thesis_description=thesis,
                    weight_overrides=custom_weights,
                    target_stage=target_stage,
                    filters=filters,
                )

            # --- Step 1: Merge rubric filters into DB query ---
            if rubric:
                rubric_filters = rubric.get("filters", {})
                for key in ("sector", "stage", "geography", "arr_min", "arr_max"):
                    if rubric_filters.get(key) and not filters.get(key):
                        filters[key] = rubric_filters[key]
                if rubric.get("target_stage") and not target_stage:
                    target_stage = rubric["target_stage"]
                if not custom_weights:
                    custom_weights = rubric.get("weights")

            # --- Step 2: DB query with enriched filters ---
            fund_id = getattr(self, "fund_id", None)
            db_result = await query_companies(
                filters=filters,
                sort_by=sort_by if sort_by != "score" else "name",
                sort_desc=sort_desc,
                limit=max_results * 2,  # fetch more, score will trim
                fund_id=fund_id,
            )

            companies = db_result.get("companies", [])

            # --- Step 3: Score with rubric context ---
            scored = score_companies(
                companies,
                weights=custom_weights,
                target_stage=target_stage,
                rubric=rubric,
            )

            # --- Step 4: Semantic LLM scoring on top candidates ---
            SEMANTIC_BATCH_SIZE = 20
            if rubric and thesis and len(scored) > 0:
                top_slice = scored[:SEMANTIC_BATCH_SIZE]
                try:
                    sem_prompt = build_semantic_scoring_prompt(thesis, top_slice)
                    sem_result = await self.model_router.get_completion(
                        prompt=sem_prompt,
                        system_prompt="Score each company's relevance. Return valid JSON only.",
                        capability=ModelCapability.STRUCTURED,
                        max_tokens=1500, temperature=0.1,
                        json_mode=True,
                        caller_context="source_companies_semantic_score",
                    )
                    raw_sem = sem_result.get("response", "{}") if isinstance(sem_result, dict) else str(sem_result)
                    parsed_sem = json.loads(raw_sem) if isinstance(raw_sem, str) else raw_sem
                    sem_scores = {s["index"]: s for s in parsed_sem.get("scores", [])}

                    # Blend: 70% quantitative + 30% semantic
                    SEMANTIC_WEIGHT = 0.30
                    for i, company in enumerate(top_slice):
                        sem_entry = sem_scores.get(i + 1)
                        if sem_entry:
                            sem_score_normalized = sem_entry["relevance"] * 10  # 0-10 → 0-100
                            old_score = company["score"]
                            company["score"] = round(
                                old_score * (1 - SEMANTIC_WEIGHT) + sem_score_normalized * SEMANTIC_WEIGHT, 1
                            )
                            company["score_breakdown"]["semantic_relevance"] = sem_entry["relevance"]
                            company["semantic_reason"] = sem_entry.get("reason", "")

                    # Re-sort after semantic adjustment
                    scored.sort(key=lambda x: x["score"], reverse=True)
                    for i, item in enumerate(scored):
                        item["rank"] = i + 1

                except Exception as e:
                    logger.warning(f"[source_companies] semantic scoring failed, using quant only: {e}")

            # Trim to max_results
            scored = scored[:max_results]

            # ── Web discovery loop ───────────────────────────────────
            # When discover_web=true and DB results are thin, the rubric
            # drives LLM-generated search queries → Tavily → name extraction
            # → enrichment → scoring → merge.  Up to 2 rounds.
            _web_discovery_info = None
            if discover_web and thesis and self.tavily_api_key:
                db_count = len(scored)
                top_score = scored[0].get("score", 0) if scored else 0
                needs_web = db_count < min_web_threshold or top_score < 40

                if needs_web:
                    try:
                        # Rubric already generated in Step 0 above — reuse it
                        existing_names = {c.get("name", "").lower() for c in scored}
                        all_web_companies: list = []
                        round1_subcategories: list = []
                        round_num = 0
                        completeness_fields = rubric.get("completeness_fields")

                        # ── Round 1: Decomposition + Template Stamping ──
                        # Phase A: LLM decomposes request into subcategories
                        decomposition = None
                        try:
                            decomp_prompt = build_decomposition_prompt(rubric)
                            decomp_result = await self.model_router.get_completion(
                                prompt=decomp_prompt,
                                system_prompt="Decompose the search into subcategories. Return valid JSON only.",
                                capability=ModelCapability.STRUCTURED,
                                max_tokens=400, temperature=0.3,
                                json_mode=True,
                                caller_context="source_companies_decompose",
                            )
                            raw_decomp = decomp_result.get("response", "{}") if isinstance(decomp_result, dict) else str(decomp_result)
                            decomposition = json.loads(raw_decomp) if isinstance(raw_decomp, str) else raw_decomp
                            round1_subcategories = decomposition.get("subcategories", [])
                        except Exception as e:
                            logger.warning(f"[source_companies] decomposition failed: {e}")
                            # Fallback: use rubric sector, or the full thesis as a single subcategory
                            fallback_sector = rubric.get("filters", {}).get("sector") or thesis.strip()
                            decomposition = {
                                "subcategories": [fallback_sector],
                                "known_companies": [],
                                "known_investors": [],
                            }
                            round1_subcategories = decomposition["subcategories"]

                        logger.info(
                            "[source_companies] decomposed into %d subcategories: %s",
                            len(round1_subcategories), round1_subcategories[:5],
                        )

                        # Phase B: Deterministic template stamping → intent-tagged queries
                        tagged_queries = stamp_queries_from_decomposition(
                            decomposition=decomposition,
                            rubric=rubric,
                            max_queries=10,
                        )

                        # Up to 2 search rounds
                        for round_num in (1, 2):
                            if round_num == 2:
                                # ── Round 2: Adaptive targeting of underrepresented subcategories ──
                                tagged_queries = build_round2_adaptive_queries(
                                    rubric=rubric,
                                    round1_results=all_web_companies,
                                    round1_subcategories=round1_subcategories,
                                    existing_names=[c.get("name", "") for c in all_web_companies + scored[:10]],
                                    max_queries=4,
                                )

                            if not tagged_queries:
                                break

                            web_queries = [tq["query"] for tq in tagged_queries]
                            # Build intent lookup: query string → intent tag
                            query_intent_map = {tq["query"]: tq.get("intent", "list_discovery") for tq in tagged_queries}
                            query_subcategory_map = {tq["query"]: tq.get("subcategory", "") for tq in tagged_queries}

                            logger.info(
                                "[source_companies] web discovery round %d: %d queries (intents: %s)",
                                round_num, len(web_queries),
                                [tq.get("intent") for tq in tagged_queries],
                            )

                            # 2. Run Tavily searches in parallel
                            async def _run_tavily(q: str) -> dict:
                                try:
                                    r = await self._tavily_search(q)
                                    results = r.get("results", []) if isinstance(r, dict) else []
                                    return {"query": q, "results": results}
                                except Exception:
                                    return {"query": q, "results": []}

                            search_tasks = [_run_tavily(q) for q in web_queries]
                            search_results = await asyncio.gather(*search_tasks)

                            # 3. Two-tier extraction routed by query intent
                            # Group results by extraction tier: bulk vs rich
                            bulk_snippets: list = []
                            rich_snippets: list = []
                            bulk_subcategories: list = []
                            rich_subcategories: list = []

                            for sr_batch in search_results:
                                q = sr_batch["query"]
                                intent_tag = query_intent_map.get(q, "list_discovery")
                                subcategory = query_subcategory_map.get(q, "")
                                tier_info = QUERY_INTENTS.get(intent_tag, {})
                                extraction_tier = tier_info.get("extraction_tier", "bulk")

                                for sr in sr_batch["results"][:5]:
                                    title = sr.get("title", "") or ""
                                    content = sr.get("content", "") or sr.get("snippet", "") or ""
                                    url = sr.get("url", "") or ""
                                    snippet_entry = f"Title: {title}\nURL: {url}\nSnippet: {content[:400]}"

                                    if extraction_tier == "rich":
                                        rich_snippets.append(snippet_entry)
                                        rich_subcategories.append(subcategory)
                                    else:
                                        bulk_snippets.append(snippet_entry)
                                        bulk_subcategories.append(subcategory)

                            if not bulk_snippets and not rich_snippets:
                                break

                            round_extracted: list = []  # companies extracted this round (dicts with metadata)

                            # Tier A: Bulk extraction — max name yield
                            if bulk_snippets:
                                snippet_text = "\n---\n".join(bulk_snippets[:30])
                                bulk_prompt = build_bulk_extraction_prompt(
                                    rubric=rubric,
                                    search_snippets=snippet_text,
                                    existing_names=list(existing_names),
                                )
                                try:
                                    ext_result = await self.model_router.get_completion(
                                        prompt=bulk_prompt,
                                        system_prompt="Extract ALL entity names from search results. Return valid JSON only.",
                                        capability=ModelCapability.STRUCTURED,
                                        max_tokens=2000, temperature=0.0,
                                        json_mode=True,
                                        caller_context=f"source_companies_bulk_r{round_num}",
                                    )
                                    raw_ext = ext_result.get("response", "{}") if isinstance(ext_result, dict) else str(ext_result)
                                    parsed_ext = json.loads(raw_ext) if isinstance(raw_ext, str) else raw_ext
                                    bulk_companies = parsed_ext.get("companies", [])
                                    # Normalize: ensure each entry is a dict
                                    for entry in bulk_companies:
                                        if isinstance(entry, str):
                                            entry = {"name": entry}
                                        if isinstance(entry, dict) and entry.get("name"):
                                            name_clean = entry["name"].strip()
                                            if 2 < len(name_clean) < 60 and name_clean.lower() not in existing_names:
                                                existing_names.add(name_clean.lower())
                                                entry["name"] = name_clean
                                                entry["source"] = "web_discovery"
                                                entry.setdefault("_extraction_tier", "bulk")
                                                round_extracted.append(entry)
                                except Exception as e:
                                    logger.warning(f"[source_companies] bulk extraction round {round_num} failed: {e}")

                            # Tier B: Rich extraction — fewer names but with funding/stage/investors
                            if rich_snippets:
                                snippet_text = "\n---\n".join(rich_snippets[:20])
                                rich_prompt = build_rich_extraction_prompt(
                                    rubric=rubric,
                                    search_snippets=snippet_text,
                                    existing_names=list(existing_names),
                                )
                                try:
                                    ext_result = await self.model_router.get_completion(
                                        prompt=rich_prompt,
                                        system_prompt="Extract entity names with metadata. Return valid JSON only.",
                                        capability=ModelCapability.STRUCTURED,
                                        max_tokens=2000, temperature=0.0,
                                        json_mode=True,
                                        caller_context=f"source_companies_rich_r{round_num}",
                                    )
                                    raw_ext = ext_result.get("response", "{}") if isinstance(ext_result, dict) else str(ext_result)
                                    parsed_ext = json.loads(raw_ext) if isinstance(raw_ext, str) else raw_ext
                                    rich_companies = parsed_ext.get("companies", [])
                                    for entry in rich_companies:
                                        if isinstance(entry, str):
                                            entry = {"name": entry}
                                        if isinstance(entry, dict) and entry.get("name"):
                                            name_clean = entry["name"].strip()
                                            if 2 < len(name_clean) < 60 and name_clean.lower() not in existing_names:
                                                existing_names.add(name_clean.lower())
                                                entry["name"] = name_clean
                                                entry["source"] = "web_discovery"
                                                entry.setdefault("_extraction_tier", "rich")
                                                round_extracted.append(entry)
                                except Exception as e:
                                    logger.warning(f"[source_companies] rich extraction round {round_num} failed: {e}")

                            logger.info(
                                "[source_companies] round %d: extracted %d companies (%d bulk, %d rich)",
                                round_num, len(round_extracted),
                                sum(1 for c in round_extracted if c.get("_extraction_tier") == "bulk"),
                                sum(1 for c in round_extracted if c.get("_extraction_tier") == "rich"),
                            )

                            if not round_extracted:
                                break

                            # 4. Dedupe + Merge BEFORE enrichment
                            # Merge all extracted companies so completeness gate sees best data
                            round_merged = merge_discovered_companies(round_extracted)

                            # 5. Completeness gate — route each company to skip/lightweight/full
                            remaining_slots = max(0, max_results - len(scored) - len(all_web_companies))
                            companies_to_process = round_merged[:min(remaining_slots, 15)]
                            semaphore = asyncio.Semaphore(3)

                            skip_companies: list = []
                            lightweight_names: list = []
                            full_enrich_names: list = []

                            for company in companies_to_process:
                                tier = classify_enrichment_tier(company, fields=completeness_fields)
                                if tier == "skip":
                                    skip_companies.append(company)
                                elif tier == "lightweight":
                                    lightweight_names.append(company)
                                else:
                                    full_enrich_names.append(company)

                            logger.info(
                                "[source_companies] round %d enrichment gate: %d skip, %d lightweight, %d full",
                                round_num, len(skip_companies), len(lightweight_names), len(full_enrich_names),
                            )

                            # 5a. Skip tier — already rich enough, add directly
                            for company in skip_companies:
                                company.setdefault("source", "web_discovery")
                                all_web_companies.append(company)

                            # 5b. Lightweight tier — 1 Tavily + 1 LLM call via _execute_lightweight_diligence
                            async def _enrich_lightweight(company: dict) -> Optional[dict]:
                                async with semaphore:
                                    try:
                                        result = await self._execute_lightweight_diligence({
                                            "company_name": company.get("name", ""),
                                        })
                                        cdata = result.get("company", {}) if isinstance(result, dict) else {}
                                        # Merge lightweight result with existing extraction data
                                        merged = dict(company)
                                        for field, val in cdata.items():
                                            if field.startswith("_"):
                                                continue
                                            existing = merged.get(field)
                                            if (existing is None or existing == "" or existing == []) and \
                                               val is not None and val != "" and val != []:
                                                merged[field] = val
                                        merged["source"] = "web_discovery"
                                        merged["_enrichment"] = "lightweight"
                                        return merged
                                    except Exception as e:
                                        logger.warning(f"[source_companies] lightweight enrich failed for '{company.get('name')}': {e}")
                                        company["source"] = "web_discovery"
                                        return company

                            # 5c. Full tier — 6 Tavily searches via _execute_company_fetch
                            async def _enrich_full(company: dict) -> Optional[dict]:
                                async with semaphore:
                                    try:
                                        name = company.get("name", "")
                                        result = await self._execute_company_fetch({
                                            "company": name,
                                            "fund_id": fund_id,
                                        })
                                        clist = result.get("companies", []) if isinstance(result, dict) else []
                                        cdata = clist[0] if clist else (result if isinstance(result, dict) else {})

                                        arr_val = cdata.get("arr") or cdata.get("revenue") or cdata.get("inferred_revenue")
                                        if hasattr(arr_val, "value"):
                                            arr_val = arr_val.value
                                        val_val = cdata.get("valuation") or cdata.get("inferred_valuation")
                                        if hasattr(val_val, "value"):
                                            val_val = val_val.value

                                        enriched = {
                                            "name": name,
                                            "company_id": cdata.get("id"),
                                            "sector": cdata.get("sector", "") or company.get("sector", ""),
                                            "stage": cdata.get("stage", "") or company.get("stage", ""),
                                            "description": (cdata.get("description") or cdata.get("product_description") or company.get("description", "") or "")[:200],
                                            "arr": arr_val,
                                            "valuation": val_val,
                                            "total_funding": cdata.get("total_funding") or cdata.get("total_raised") or company.get("last_funding_amount"),
                                            "employee_count": cdata.get("employee_count") or cdata.get("team_size") or company.get("employee_count"),
                                            "growth_rate": cdata.get("growth_rate"),
                                            "hq": cdata.get("hq_location") or cdata.get("geography", "") or company.get("hq", ""),
                                            "business_model": cdata.get("business_model", ""),
                                            "investors": cdata.get("investors") or company.get("investors", []),
                                            "source": "web_discovery",
                                            "_enrichment": "full",
                                        }
                                        return enriched
                                    except Exception as e:
                                        logger.warning(f"[source_companies] full enrich failed for '{company.get('name')}': {e}")
                                        company["source"] = "web_discovery"
                                        return company

                            # Run lightweight + full enrichment in parallel
                            all_enrich_tasks = (
                                [_enrich_lightweight(c) for c in lightweight_names] +
                                [_enrich_full(c) for c in full_enrich_names]
                            )
                            if all_enrich_tasks:
                                enriched_results = await asyncio.gather(*all_enrich_tasks)
                                for c in enriched_results:
                                    if c is not None:
                                        all_web_companies.append(c)

                            # Check if we have enough now
                            total_count = len(scored) + len(all_web_companies)
                            if total_count >= max_results or round_num == 2:
                                break

                        # 5. Score web-discovered companies with same rubric, merge + dedupe
                        if all_web_companies:
                            web_scored = score_companies(
                                all_web_companies,
                                weights=custom_weights or rubric.get("weights"),
                                target_stage=target_stage or rubric.get("target_stage"),
                            )

                            # Deduplicate: remove DB entries that overlap with web results (web has fresher data)
                            web_names_lower = {c.get("name", "").lower() for c in web_scored}
                            scored = [c for c in scored if c.get("name", "").lower() not in web_names_lower]

                            # Merge: combine DB + web, re-sort by score
                            scored = scored + web_scored
                            scored.sort(key=lambda x: x.get("score", 0), reverse=True)
                            scored = scored[:max_results]

                            # Re-assign ranks after merge
                            for i, item in enumerate(scored):
                                item["rank"] = i + 1

                            # Count actual sources in the final trimmed list
                            web_in_final = sum(1 for c in scored if c.get("source") == "web_discovery")
                            db_in_final = len(scored) - web_in_final

                            # Enrichment tier stats for cost tracking
                            skip_count = sum(1 for c in all_web_companies if c.get("_enrichment") is None or c.get("_extraction_tier") == "rich")
                            lw_count = sum(1 for c in all_web_companies if c.get("_enrichment") == "lightweight")
                            full_count = sum(1 for c in all_web_companies if c.get("_enrichment") == "full")

                            _web_discovery_info = {
                                "web_companies_found": len(all_web_companies),
                                "web_in_final_list": web_in_final,
                                "db_in_final_list": db_in_final,
                                "intent": rubric.get("intent", "dealflow"),
                                "entity_type": rubric.get("entity_type", "startup"),
                                "search_rounds": round_num,
                                "subcategories": round1_subcategories,
                                "enrichment_tiers": {
                                    "skipped": skip_count,
                                    "lightweight": lw_count,
                                    "full": full_count,
                                },
                            }
                            logger.info(
                                "[source_companies] web discovery complete: %d new %ss found (%d in final list), intent=%s",
                                len(all_web_companies), rubric.get("entity_type", "startup"),
                                web_in_final, rubric.get("intent", "dealflow"),
                            )

                    except Exception as e:
                        logger.warning(f"[source_companies] web discovery failed (non-fatal): {e}")

            # ── End web discovery loop ───────────────────────────────

            # Optional microskills enrichment on top N results
            _enrichment_info = None
            _enrich_grid_cmds = []
            if enrich_top_n and enrich_top_n > 0 and scored:
                try:
                    from app.services.micro_skills.gap_resolver import resolve_gaps

                    to_enrich = scored[:min(enrich_top_n, len(scored))]
                    fund_ctx = self.shared_data.get("fund_context", {})
                    _fund_id = fund_ctx.get("fundId", "")

                    async def tavily_fn(query: str) -> dict:
                        return await self._tavily_search(query)

                    async def llm_fn(prompt: str, system: str) -> str:
                        r = await self.model_router.get_completion(
                            prompt=prompt, system_prompt=system,
                            capability=ModelCapability.FAST,
                            max_tokens=400, temperature=0.0,
                            json_mode=True, caller_context="sourcing_enrich",
                        )
                        return r.get("response", "{}") if isinstance(r, dict) else str(r)

                    enrich_result = await resolve_gaps(
                        companies=to_enrich,
                        fund_id=_fund_id,
                        tavily_search_fn=tavily_fn if self.tavily_api_key else None,
                        llm_extract_fn=llm_fn,
                    )

                    _enrichment_info = {
                        "enriched_count": len(to_enrich),
                        "fields_filled": enrich_result.get("total_fields_filled", 0),
                        "skills_run": enrich_result.get("skills_run", []),
                    }
                    _enrich_grid_cmds = enrich_result.get("grid_commands", [])

                    # Re-score after enrichment (new data may change scores)
                    scored = score_companies(scored, weights=custom_weights, target_stage=target_stage)
                    scored = scored[:max_results]

                    logger.info(
                        "[source_companies] enriched top %d: %d fields filled via %s",
                        len(to_enrich),
                        enrich_result.get("total_fields_filled", 0),
                        enrich_result.get("skills_run", []),
                    )
                except Exception as e:
                    logger.warning(f"[source_companies] microskills enrichment failed: {e}")

            # Optional persistence of sourced results
            _persisted_count = 0
            if persist_results and scored:
                try:
                    from app.services.sourcing_service import upsert_sourced_companies
                    _persisted_count = await upsert_sourced_companies(scored, fund_id=fund_id)
                except Exception as e:
                    logger.warning(f"[source_companies] persist failed: {e}")

            # Pick display mode and format
            mode = pick_display_mode(len(scored), display)

            result = {
                "companies": scored,
                "count": len(scored),
                "display_mode": mode,
                "query_parsed": filters,
            }

            # Summary line
            stage_str = f" {target_stage}" if target_stage else ""
            sector_str = f" {filters.get('sector', '')}" if filters.get('sector') else ""
            top_name = scored[0]["name"] if scored else "none"
            top_score = scored[0].get("score", 0) if scored else 0
            result["summary"] = (
                f"Found {len(scored)}{stage_str}{sector_str} companies. "
                f"Top scorer: {top_name} ({top_score}/100)."
            )

            # Table format for table/ranked_list modes
            if mode in ("table", "ranked_list"):
                result["table"] = format_as_markdown_table(scored, include_score=True)

            # For grid_rows mode, emit add_row commands
            if mode == "grid_rows":
                result["grid_commands"] = [
                    {
                        "action": "add_row",
                        "company": c.get("name", ""),
                        "company_id": c.get("company_id", ""),
                        "data": {
                            "sector": c.get("sector", ""),
                            "stage": c.get("stage", ""),
                            "arr": c.get("arr"),
                            "valuation": c.get("valuation"),
                            "totalFunding": c.get("total_funding"),
                            "growthRate": c.get("growth_rate"),
                            "employeeCount": c.get("employee_count"),
                            "hq": c.get("hq", ""),
                            "businessModel": c.get("business_model", ""),
                            "foundedYear": c.get("founded"),
                            "tam": c.get("tam"),
                            "burnRate": c.get("burn_rate"),
                            "runwayMonths": c.get("runway_months"),
                            "latestRoundDate": c.get("latest_round_date", ""),
                        },
                    }
                    for c in scored
                ]

            # Attach enrichment, web discovery, and persistence metadata
            if _enrichment_info:
                result["enrichment"] = _enrichment_info
            if _web_discovery_info:
                result["web_discovery"] = _web_discovery_info
                # Update summary with actual final-list counts (not raw discovery totals)
                _web_final = _web_discovery_info.get("web_in_final_list", 0)
                _db_final = _web_discovery_info.get("db_in_final_list", len(scored))
                result["summary"] = (
                    f"Found {len(scored)}{stage_str}{sector_str} companies "
                    f"({_db_final} from DB, {_web_final} from web). "
                    f"Top scorer: {top_name} ({top_score}/100)."
                )
            if _enrich_grid_cmds:
                result["grid_commands"] = result.get("grid_commands", []) + _enrich_grid_cmds
            if _persisted_count:
                result["persisted_count"] = _persisted_count

            return result

        except Exception as e:
            logger.error(f"[TOOL] source_companies failed: {e}")
            return {"companies": [], "count": 0, "error": str(e)}

    async def _tool_generate_rubric(self, inputs: dict) -> dict:
        """Generate a scoring rubric from a natural-language query.

        LLM-first classification with pattern-match fallback.
        Returns intent, entity_type, search_context, completeness_fields
        alongside weights and filters — the full instruction set for sourcing.
        """
        from app.services.sourcing_service import (
            generate_rubric, build_rubric_classification_prompt, parse_llm_rubric,
        )
        from app.services.model_router import ModelCapability

        thesis = inputs.get("thesis", "")
        if not thesis.strip():
            return {"error": "A query description is required (e.g. 'Series A B2B SaaS in fintech' or 'top AI consulting firms in NYC')"}

        rubric = None
        try:
            rubric_prompt = build_rubric_classification_prompt(thesis)
            rubric_result = await self.model_router.get_completion(
                prompt=rubric_prompt,
                system_prompt="Classify this list-building request. Return valid JSON only.",
                capability=ModelCapability.FAST,
                max_tokens=600, temperature=0.2,
                json_mode=True,
                caller_context="generate_rubric_llm",
            )
            raw = rubric_result.get("response", "{}") if isinstance(rubric_result, dict) else str(rubric_result)
            llm_output = json.loads(raw) if isinstance(raw, str) else raw
            rubric = parse_llm_rubric(
                llm_output=llm_output,
                query=thesis,
                weight_overrides=inputs.get("weight_overrides"),
                target_stage=inputs.get("target_stage"),
                filters=inputs.get("filters"),
            )
        except Exception as e:
            logger.warning(f"[TOOL] generate_rubric LLM failed, using pattern match: {e}")

        if rubric is None:
            rubric = generate_rubric(
                thesis_description=thesis,
                weight_overrides=inputs.get("weight_overrides"),
                target_stage=inputs.get("target_stage"),
                filters=inputs.get("filters"),
            )

        logger.info(
            "[TOOL] generate_rubric: thesis=%s intent=%s entity=%s stage=%s weights=%s filters=%s",
            thesis[:60], rubric.get("intent"), rubric.get("entity_type"),
            rubric.get("target_stage"), rubric.get("weights"), rubric.get("filters"),
        )
        return rubric

    async def _tool_add_company_to_matrix(self, inputs: dict) -> dict:
        """Add a company from the rich DB to the portfolio matrix grid."""
        company_id = inputs.get("company_id", "")
        company_name = inputs.get("company_name", "")
        if not company_id and not company_name:
            return {"error": "company_id or company_name is required"}
        try:
            from app.services.portfolio_service import PortfolioService
            ps = PortfolioService()
            # Search by name if no ID
            if not company_id:
                result = await ps.search_companies_db(company_name, limit=1)
                companies = result.get("companies", [])
                if not companies:
                    return {"error": f"Company '{company_name}' not found in database. Use fetch_company_data instead."}
                company = companies[0]
            else:
                # Direct lookup by ID
                client = ps._client()
                if client:
                    resp = client.table("companies").select("*").eq("id", company_id).limit(1).execute()
                    if resp.data:
                        company = resp.data[0]
                    else:
                        return {"error": f"Company ID '{company_id}' not found"}
                else:
                    return {"error": "Database unavailable"}

            # Emit grid commands to add this company to the matrix
            grid_commands = []
            field_mapping = {
                "arr": company.get("arr") or company.get("current_arr_usd"),
                "valuation": company.get("valuation") or company.get("current_valuation_usd") or company.get("last_valuation_usd"),
                "stage": company.get("stage"),
                "sector": company.get("sector"),
                "totalFunding": company.get("total_funding") or company.get("total_funding_usd"),
                "growthRate": company.get("growth_rate"),
                "employeeCount": company.get("employee_count"),
                "burnRate": company.get("burn_rate") or company.get("burn_rate_monthly_usd"),
                "runwayMonths": company.get("runway_months"),
                "hq": company.get("hq") or company.get("hq_location"),
                "description": company.get("description"),
                "businessModel": company.get("business_model"),
            }

            row_id = company.get("name", company_name)
            for col, val in field_mapping.items():
                if val is not None and val != "" and val != "N/A":
                    grid_commands.append({
                        "action": "edit",
                        "rowId": row_id,
                        "columnId": col,
                        "value": val,
                        "reasoning": f"Pre-populated from companies database",
                    })

            return {
                "company": company,
                "grid_commands": grid_commands,
                "message": f"Added {row_id} to matrix with {len(grid_commands)} pre-populated fields from database",
            }
        except Exception as e:
            logger.warning(f"[TOOL] add_company_to_matrix failed: {e}")
            return {"error": str(e)}

    async def _tool_suggest_edit(self, inputs: dict) -> dict:
        """Return grid command AND persist to pending_suggestions for the accept/reject flow."""
        company_id = inputs.get("company")
        column_id = inputs.get("column")
        value = inputs.get("value")
        reasoning = inputs.get("reasoning", "")

        # Persist to pending_suggestions so the grid picks it up on refresh
        fund_id = self.shared_data.get("fund_context", {}).get("fundId")
        if fund_id and company_id and column_id:
            try:
                supabase_url = settings.SUPABASE_URL
                supabase_key = settings.SUPABASE_SERVICE_ROLE_KEY or settings.SUPABASE_ANON_KEY
                if supabase_url and supabase_key:
                    from supabase import create_client
                    sb = create_client(supabase_url, supabase_key)
                    sb.table("pending_suggestions").upsert({
                        "fund_id": fund_id,
                        "company_id": company_id,
                        "column_id": column_id,
                        "suggested_value": value,
                        "source_service": "agent.suggest_edit",
                        "reasoning": reasoning,
                        "metadata": {"tool": "suggest_grid_edit"},
                    }, on_conflict="fund_id,company_id,column_id").execute()
            except Exception as e:
                logger.warning(f"[TOOL] Failed to persist suggestion: {e}")

        return {
            "grid_commands": [{
                "action": "edit",
                "rowId": company_id,
                "columnId": column_id,
                "value": value,
                "reasoning": reasoning,
            }]
        }

    async def _tool_suggest_action(self, inputs: dict) -> dict:
        """Return an action suggestion (insight / warning / action item) AND persist to pending_suggestions."""
        VALID_TYPES = {"insight", "warning", "action_item", "follow_up"}
        suggestion_type = inputs.get("type", "insight")
        if suggestion_type not in VALID_TYPES:
            return {"error": f"Invalid suggestion type '{suggestion_type}'. Must be one of: {VALID_TYPES}"}

        suggestion = {
            "type": suggestion_type,
            "title": inputs.get("title", ""),
            "description": inputs.get("description", ""),
            "priority": inputs.get("priority", "medium"),
            "company_id": inputs.get("company_id", ""),
        }

        # Persist to pending_suggestions so it survives page refresh
        fund_id = self.shared_data.get("fund_context", {}).get("fundId")
        company_id = inputs.get("company_id", "") or inputs.get("company", "")
        if fund_id:
            try:
                supabase_url = settings.SUPABASE_URL
                supabase_key = settings.SUPABASE_SERVICE_ROLE_KEY or settings.SUPABASE_ANON_KEY
                if supabase_url and supabase_key:
                    from supabase import create_client
                    sb = create_client(supabase_url, supabase_key)
                    sb.table("pending_suggestions").upsert({
                        "fund_id": fund_id,
                        "company_id": company_id or "portfolio",
                        "column_id": f"_action_{suggestion_type}",
                        "suggested_value": suggestion,
                        "source_service": "agent.suggest_action",
                        "reasoning": inputs.get("description", ""),
                        "metadata": {"tool": "suggest_action", "priority": inputs.get("priority", "medium")},
                    }, on_conflict="fund_id,company_id,column_id").execute()
            except Exception as e:
                logger.warning(f"[TOOL] Failed to persist action suggestion: {e}")

        return {"suggestion": suggestion}

    async def _tool_write_memo(self, inputs: dict) -> dict:
        """Return memo sections for the frontend to append. No DB write here."""
        return {"memo_sections": inputs.get("sections", [])}

    # Mapping from fetched company data fields to grid column IDs
    _FIELD_TO_GRID_COLUMN: Dict[str, str] = {
        "description": "description",
        "product_description": "description",
        "arr": "arr",
        "revenue": "arr",
        "inferred_revenue": "arr",
        "valuation": "valuation",
        "funding_stage": "stage",
        "total_funding": "total_funding",
        "gross_margin": "gross_margin",
        "business_model": "business_model",
        "team_size": "team_size",
        "target_market": "target_market",
        "pricing_model": "pricing_model",
        "category": "sector",
        "revenue_growth": "revenue_growth",
        "hq_location": "hq_location",
        "founded_year": "founded_year",
        "investors": "investors",
        "fund_fit_score": "fund_fit_score",
    }

    def _auto_suggest_grid_edits(self, company_data: dict) -> list:
        """Compare fetched company data with grid snapshot and emit grid_commands for empty/stale cells.

        Returns a list of grid_command dicts ready to be collected as side effects.
        Also persists each suggestion to pending_suggestions in Supabase.
        """
        grid_commands: list = []
        company_name = (company_data.get("company") or company_data.get("name") or "").strip()
        if not company_name:
            return grid_commands

        # Find matching row in grid snapshot
        matrix_ctx = self.shared_data.get("matrix_context") or {}
        grid_snapshot = matrix_ctx.get("gridSnapshot") or {}
        grid_rows = grid_snapshot.get("rows", []) if isinstance(grid_snapshot, dict) else (
            grid_snapshot if isinstance(grid_snapshot, list) else []
        )

        matched_row = None
        matched_row_id = company_name  # fallback: use name as rowId
        for row in grid_rows:
            row_name = (row.get("companyName") or row.get("company_name") or "").lower()
            if company_name.lower() in row_name or row_name in company_name.lower():
                matched_row = row
                matched_row_id = row.get("rowId") or row.get("row_id") or company_name
                break

        existing_cells = {}
        if matched_row:
            existing_cells = matched_row.get("cells") or matched_row.get("cellValues") or {}

        # Track which grid columns we've already suggested (avoid duplicates)
        suggested_columns: set = set()
        fund_id = self.shared_data.get("fund_context", {}).get("fundId")

        for data_field, grid_col in self._FIELD_TO_GRID_COLUMN.items():
            if grid_col in suggested_columns:
                continue
            value = company_data.get(data_field)
            if value is None:
                continue
            # Skip empty strings/lists
            if isinstance(value, str) and not value.strip():
                continue
            if isinstance(value, list) and len(value) == 0:
                continue

            # Check if grid cell is empty or missing
            current = existing_cells.get(grid_col)
            is_empty = current is None or current == "" or current == 0
            if not is_empty:
                continue

            suggested_columns.add(grid_col)

            # Format investors list as comma-separated string
            display_value = ", ".join(value) if isinstance(value, list) else value

            reasoning = f"From web search: {data_field} for {company_name}"

            gc = {
                "action": "edit",
                "rowId": matched_row_id,
                "columnId": grid_col,
                "value": display_value,
                "reasoning": reasoning,
                "source_service": "agent.auto_enrich",
                "auto_apply": True,  # Bypass suggestion queue — agent enrichment applies directly
            }
            grid_commands.append(gc)

        if grid_commands:
            logger.info(f"[AUTO_SUGGEST] Generated {len(grid_commands)} grid suggestions for {company_name}")

        return grid_commands

    async def _tool_fetch_company(self, inputs: dict) -> dict:
        """Wrap existing _execute_company_fetch — dealflow pipeline stays intact.
        Returns rich fields for agent enrichment + inline memo_sections.
        AUTO-EMITS suggest_grid_edit for every empty grid cell where we have data."""
        try:
            result = await self._execute_company_fetch({
                "company": inputs["company_name"],
                "prompt_handle": f"@{inputs['company_name']}",
            })
            companies = result.get("companies", [])
            # Return enrichment-relevant fields so agent can suggest_grid_edit
            enrich_keys = [
                "name", "company", "description", "product_description",
                "revenue", "arr", "inferred_revenue", "valuation",
                "funding_stage", "total_funding", "investors",
                "gross_margin", "business_model", "team_size",
                "target_market", "pricing_model", "category",
                "revenue_growth", "hq_location", "founded_year",
                "fund_fit_score", "optimal_check_size",
            ]
            compressed = []
            for c in companies[:3]:
                row = {k: c.get(k) for k in enrich_keys if c.get(k) is not None}
                # Include key_metrics sub-dict if present
                km = c.get("key_metrics")
                if isinstance(km, dict):
                    row["key_metrics"] = km
                compressed.append(row)

            # Generate inline memo sections from fetched data
            memo_sections = self._build_company_memo_sections(compressed)

            # AUTO-SUGGEST: Compare with grid and emit grid_commands for empty cells
            all_grid_commands: list = []
            for c in compressed:
                cmds = self._auto_suggest_grid_edits(c)
                all_grid_commands.extend(cmds)

            out: Dict[str, Any] = {
                "companies": compressed,
                "memo_sections": memo_sections,
            }
            if all_grid_commands:
                out["grid_commands"] = all_grid_commands
                out["auto_suggestions_count"] = len(all_grid_commands)

            # Clear fetched companies from pending enrichment lists
            async with self.shared_data_lock:
                for c in compressed:
                    _cname_lower = (c.get("company") or c.get("name") or "").lower().strip("@")
                    if not _cname_lower:
                        continue
                    for _list_key in ("proactive_enrich_companies", "sparse_enrich_companies"):
                        _current = self.shared_data.get(_list_key, [])
                        if _current:
                            self.shared_data[_list_key] = [
                                x for x in _current if x.lower().strip("@") != _cname_lower
                            ]

            return out
        except Exception as e:
            logger.warning(f"[TOOL] fetch_company failed: {e}")
            return {"companies": [], "error": str(e)}

    def _build_company_memo_sections(self, companies: list) -> list:
        """Build compact memo sections from company data for inline chat display."""
        sections: list = []
        for c in companies:
            name = c.get("company") or c.get("name") or "Unknown"
            sections.append({"type": "heading2", "content": name})

            desc = c.get("product_description") or c.get("description")
            if desc:
                sections.append({"type": "paragraph", "content": desc})

            # Key metrics as a compact list
            items = []
            val = c.get("valuation")
            rev = c.get("revenue") or c.get("arr") or c.get("inferred_revenue")
            if val and isinstance(val, (int, float)):
                items.append(f"Valuation: ${val / 1e6:,.1f}M")
            if rev and isinstance(rev, (int, float)):
                items.append(f"Revenue: ${rev / 1e6:,.1f}M")
            gm = c.get("gross_margin")
            if gm is None:
                km = c.get("key_metrics") or {}
                gm = km.get("gross_margin")
            if gm and isinstance(gm, (int, float)):
                items.append(f"Gross Margin: {gm * 100 if gm < 1 else gm:.0f}%")
            funding = c.get("total_funding")
            if funding and isinstance(funding, (int, float)):
                items.append(f"Total Funding: ${funding / 1e6:,.1f}M")
            stage = c.get("funding_stage")
            if stage:
                items.append(f"Stage: {stage}")
            model = c.get("business_model")
            if model:
                items.append(f"Business Model: {model}")
            team = c.get("team_size")
            if team:
                items.append(f"Team Size: {team}")
            fit = c.get("fund_fit_score")
            if fit:
                items.append(f"Fund Fit: {fit}/100")

            if items:
                sections.append({"type": "list", "items": items})

        return sections

    # ------------------------------------------------------------------
    # Granular search tools — 1 Tavily search each, auto-suggest grid edits
    # ------------------------------------------------------------------

    async def _granular_search_and_extract(
        self,
        company_name: str,
        search_suffix: str,
        extract_fields: Dict[str, str],
        extraction_instructions: str,
    ) -> dict:
        """Shared helper for granular search tools.

        1. Single Tavily search with `company_name + search_suffix`
        2. LLM extraction for specific fields
        3. Auto-suggest grid edits for extracted data

        Args:
            company_name: Company to search for
            search_suffix: Appended to company name for the search query
            extract_fields: JSON schema fields to extract {field_name: type_hint}
            extraction_instructions: Extra guidance for the LLM extractor
        """
        company_name = company_name.strip().lstrip("@")
        if not company_name:
            return {"error": "company_name is required"}

        # Single Tavily search
        try:
            search_result = await self._execute_tool("web_search", {
                "query": f"{company_name} {search_suffix}",
                "search_depth": "basic",
            })
            search_text = json.dumps(search_result.get("results", search_result), default=str)[:4000]
        except Exception as e:
            logger.warning(f"[GRANULAR_SEARCH] Search failed for {company_name} ({search_suffix}): {e}")
            search_text = ""

        if not search_text or search_text == "[]":
            return {"company_name": company_name, "fields": {}, "message": "No search results found"}

        # Build extraction schema
        fields_json = json.dumps(extract_fields, indent=2)
        extraction_prompt = f"""Extract data for {company_name} from this search result:

{search_text}

{extraction_instructions}

Return JSON with ONLY these fields (use null if unknown):
{fields_json}"""

        try:
            extraction = await self.model_router.get_completion(
                prompt=extraction_prompt,
                system_prompt="Extract factual company data. No speculation. Return valid JSON only.",
                capability=ModelCapability.FAST,
                max_tokens=400,
                temperature=0.0,
                json_mode=True,
                caller_context="granular_search_extract",
            )
            raw = extraction.get("response", "{}") if isinstance(extraction, dict) else str(extraction)
            extracted = json.loads(raw) if isinstance(raw, str) else raw
        except Exception as e:
            logger.warning(f"[GRANULAR_SEARCH] Extraction failed for {company_name}: {e}")
            return {"company_name": company_name, "fields": {}, "error": str(e)}

        # Filter out null values
        fields = {k: v for k, v in extracted.items() if v is not None}

        # Auto-suggest grid edits
        grid_commands = self._auto_suggest_grid_edits({"name": company_name, **fields})

        # Build memo sections for this search
        items = []
        for k, v in fields.items():
            if isinstance(v, list):
                v = ", ".join(str(x) for x in v)
            if isinstance(v, (int, float)) and v > 1_000_000:
                items.append(f"**{k.replace('_', ' ').title()}**: ${v / 1e6:,.1f}M")
            elif isinstance(v, float) and v < 1:
                items.append(f"**{k.replace('_', ' ').title()}**: {v * 100:.0f}%")
            else:
                items.append(f"**{k.replace('_', ' ').title()}**: {v}")

        memo_sections = []
        if items:
            memo_sections = [
                {"type": "heading2", "content": f"{company_name} — {search_suffix.split()[0].title()}"},
                {"type": "list", "items": items},
            ]

        # Merge extracted fields into shared_data["companies"] so downstream
        # tools (valuation, cap table, follow-on) can access accumulated data.
        async with self.shared_data_lock:
            companies = self.shared_data.get("companies", [])
            existing = next(
                (c for c in companies if c.get("name", "").lower() == company_name.lower()),
                None,
            )
            if existing:
                # Merge new fields into existing entry (don't overwrite with None)
                for k, v in fields.items():
                    if v is not None:
                        existing[k] = v
            else:
                companies.append({"name": company_name, **fields})
            self.shared_data["companies"] = companies

            # Clear this company from pending enrichment lists so the agent
            # doesn't re-process it on subsequent iterations.
            _cname_lower = company_name.lower().strip("@")
            for _list_key in ("proactive_enrich_companies", "sparse_enrich_companies"):
                _current = self.shared_data.get(_list_key, [])
                if _current:
                    self.shared_data[_list_key] = [
                        c for c in _current if c.lower().strip("@") != _cname_lower
                    ]

        out: Dict[str, Any] = {
            "company_name": company_name,
            "fields": fields,
            "memo_sections": memo_sections,
        }
        if grid_commands:
            out["grid_commands"] = grid_commands
            out["auto_suggestions_count"] = len(grid_commands)

        return out

    async def _tool_search_company_funding(self, inputs: dict) -> dict:
        """Search for funding, valuation, investors, stage. 1 Tavily search."""
        return await self._granular_search_and_extract(
            company_name=inputs.get("company_name", ""),
            search_suffix="funding round valuation investors Series stage",
            extract_fields={
                "valuation": "number or null (in USD)",
                "total_funding": "number or null (in USD)",
                "funding_stage": "string: Seed/Series A/B/C/D+",
                "investors": "list of investor names",
                "last_round_amount": "number or null (in USD)",
                "last_round_date": "string YYYY-MM or null",
                "reporting_currency": "string: original currency code (USD/GBP/EUR/etc) before conversion, or null",
            },
            extraction_instructions="Focus on the most recent funding round. Convert all amounts to USD but preserve the original reporting currency in reporting_currency field.",
        )

    async def _tool_search_company_product(self, inputs: dict) -> dict:
        """Search for product, business model, pricing, target market. 1 Tavily search."""
        return await self._granular_search_and_extract(
            company_name=inputs.get("company_name", ""),
            search_suffix="product description business model pricing customers",
            extract_fields={
                "description": "string: what they actually do (factual, no buzzwords)",
                "business_model": "string: SaaS/Marketplace/Hardware/Services/etc",
                "pricing_model": "string: per-seat/usage-based/enterprise/freemium/etc",
                "target_market": "string: who they sell to",
                "category": "string: industry sector",
            },
            extraction_instructions="Describe EXACTLY what the company does. NO buzzwords like 'AI-powered platform'.",
        )

    async def _tool_search_company_team(self, inputs: dict) -> dict:
        """Search for founders, leadership, team size. 1 Tavily search."""
        return await self._granular_search_and_extract(
            company_name=inputs.get("company_name", ""),
            search_suffix="founder CEO CTO team size employees headcount",
            extract_fields={
                "team_size": "number or null (total employees)",
                "founders": "list of founder names",
                "ceo": "string or null",
                "cto": "string or null",
                "founded_year": "number YYYY or null",
                "hq_location": "string: City, Country",
                "hq_country": "string: 2-letter ISO country code (US/GB/DE/FR/IL/SG/etc) or null",
            },
            extraction_instructions="Look for LinkedIn data, Crunchbase profiles, About pages. Extract the country as a 2-letter ISO code.",
        )

    async def _tool_search_company_market(self, inputs: dict) -> dict:
        """Search for competitors, TAM, market position. 1 Tavily search."""
        return await self._granular_search_and_extract(
            company_name=inputs.get("company_name", ""),
            search_suffix="competitors market size TAM alternatives industry",
            extract_fields={
                "competitors": "list of competitor company names",
                "tam_estimate": "number or null (total addressable market in USD)",
                "market_position": "string: leader/challenger/niche/emerging",
                "category": "string: industry sector",
            },
            extraction_instructions="Focus on direct competitors and market sizing. Use real numbers.",
        )

    # ------------------------------------------------------------------
    # Financial analysis services (gross margin, burn, growth projections)
    # ------------------------------------------------------------------

    async def _tool_analyze_financials(self, inputs: dict) -> dict:
        """Analyze/infer financial metrics for a company using stage benchmarks + available data.

        Computes: gross margin estimate, burn rate, runway, growth projection, unit economics.
        Returns grid_commands for any fields it can fill.
        """
        company_name = (inputs.get("company_name") or inputs.get("company", "")).strip().lstrip("@")
        if not company_name:
            return {"error": "company_name is required"}

        # Gather all available data from shared_data + grid
        company_data: Dict[str, Any] = {"name": company_name}

        # Check cached company data
        for c in self.shared_data.get("companies", []):
            c_name = (c.get("name") or c.get("company") or "").lower()
            if company_name.lower() in c_name or c_name in company_name.lower():
                company_data.update(c)
                break

        # Check grid context
        matrix_ctx = self.shared_data.get("matrix_context") or {}
        grid_snapshot = matrix_ctx.get("gridSnapshot") or {}
        grid_rows = grid_snapshot.get("rows", []) if isinstance(grid_snapshot, dict) else (
            grid_snapshot if isinstance(grid_snapshot, list) else []
        )
        for row in grid_rows:
            row_name = (row.get("companyName") or row.get("company_name") or "").lower()
            if company_name.lower() in row_name or row_name in company_name.lower():
                cells = row.get("cells") or row.get("cellValues") or {}
                for k, v in cells.items():
                    if v is not None and v != "" and k not in company_data:
                        company_data[k] = v
                break

        # Stage benchmarks for inference
        stage = (company_data.get("funding_stage") or company_data.get("stage") or "").lower()
        STAGE_BENCHMARKS = {
            "seed": {"gross_margin": 0.60, "burn_monthly": 150_000, "growth_yoy": 3.0, "team_size": 12, "revenue_multiple": 50},
            "series a": {"gross_margin": 0.65, "burn_monthly": 400_000, "growth_yoy": 2.5, "team_size": 40, "revenue_multiple": 30},
            "series b": {"gross_margin": 0.70, "burn_monthly": 1_200_000, "growth_yoy": 1.5, "team_size": 150, "revenue_multiple": 15},
            "series c": {"gross_margin": 0.72, "burn_monthly": 3_000_000, "growth_yoy": 0.8, "team_size": 400, "revenue_multiple": 10},
            "series d": {"gross_margin": 0.75, "burn_monthly": 5_000_000, "growth_yoy": 0.5, "team_size": 800, "revenue_multiple": 8},
        }
        benchmarks = STAGE_BENCHMARKS.get(stage, STAGE_BENCHMARKS.get("series a", {}))

        # Business model adjustments
        bm = (company_data.get("business_model") or "").lower()
        if "saas" in bm:
            benchmarks["gross_margin"] = min(benchmarks.get("gross_margin", 0.65) + 0.05, 0.85)
        elif "marketplace" in bm:
            benchmarks["gross_margin"] = min(benchmarks.get("gross_margin", 0.65) + 0.10, 0.90)
        elif "hardware" in bm or "manufacturing" in bm:
            benchmarks["gross_margin"] = max(benchmarks.get("gross_margin", 0.65) - 0.25, 0.20)
        elif "services" in bm or "consulting" in bm:
            benchmarks["gross_margin"] = max(benchmarks.get("gross_margin", 0.65) - 0.15, 0.35)

        # Compute metrics
        revenue = company_data.get("arr") or company_data.get("revenue") or company_data.get("inferred_revenue") or 0
        total_funding = company_data.get("total_funding") or 0
        team_size = company_data.get("team_size") or benchmarks.get("team_size", 40)

        # Gross margin: use actual if available, otherwise benchmark
        gross_margin = company_data.get("gross_margin")
        gm_source = "actual"
        if gross_margin is None:
            gross_margin = benchmarks.get("gross_margin", 0.65)
            gm_source = f"benchmark ({stage or 'Series A'} + {bm or 'general'})"

        # Burn rate: estimate from team size + overhead
        avg_salary = 120_000  # USD annual avg for tech companies
        if company_data.get("hq_location", "").lower() in ("san francisco", "new york", "london"):
            avg_salary = 150_000
        annual_people_cost = team_size * avg_salary
        burn_monthly = company_data.get("burn_rate") or (annual_people_cost / 12 * 1.4)  # 1.4x for non-people costs
        burn_source = "actual" if company_data.get("burn_rate") else "estimated (team × avg salary × 1.4)"

        # Runway
        cash_on_hand = total_funding * 0.5 if total_funding else 0  # assume ~50% deployed
        runway_months = int(cash_on_hand / burn_monthly) if burn_monthly > 0 else 0

        # Growth projection: 3-year forecast
        growth_yoy = company_data.get("revenue_growth") or benchmarks.get("growth_yoy", 1.5)
        if isinstance(growth_yoy, (int, float)) and growth_yoy > 10:
            growth_yoy = growth_yoy / 100  # convert percentage to decimal
        projections = []
        proj_rev = revenue if revenue > 0 else (burn_monthly * 12 * 0.3)  # assume 30% of burn if no revenue data
        for year in range(1, 4):
            proj_rev = proj_rev * (1 + growth_yoy)
            projections.append({"year": year, "projected_revenue": round(proj_rev)})

        # Rule of 40 (SaaS metric)
        rule_of_40 = None
        if revenue > 0 and gross_margin:
            implied_margin = (revenue * gross_margin - burn_monthly * 12) / revenue if revenue else 0
            rule_of_40 = round((growth_yoy * 100) + (implied_margin * 100), 1)

        # Capital efficiency
        capital_efficiency = revenue / total_funding if total_funding > 0 and revenue > 0 else None

        analysis = {
            "company": company_name,
            "stage": stage or "unknown",
            "business_model": bm or "unknown",
            "gross_margin": round(gross_margin, 3),
            "gross_margin_source": gm_source,
            "burn_rate_monthly": round(burn_monthly),
            "burn_source": burn_source,
            "runway_months": runway_months,
            "revenue": revenue,
            "growth_yoy": round(growth_yoy, 3),
            "projections": projections,
            "rule_of_40": rule_of_40,
            "capital_efficiency": round(capital_efficiency, 3) if capital_efficiency else None,
            "team_size": team_size,
        }

        # Auto-suggest grid edits for computed fields
        suggest_data = {
            "name": company_name,
            "gross_margin": gross_margin,
            "revenue_growth": growth_yoy,
        }
        if not company_data.get("burn_rate"):
            suggest_data["burn_rate"] = round(burn_monthly)
        grid_commands = self._auto_suggest_grid_edits(suggest_data)

        # Build memo sections
        memo_items = [
            f"**Gross Margin**: {gross_margin * 100:.0f}% ({gm_source})",
            f"**Monthly Burn**: ${burn_monthly / 1000:,.0f}K ({burn_source})",
            f"**Runway**: ~{runway_months} months",
            f"**Growth**: {growth_yoy * 100:.0f}% YoY",
        ]
        if rule_of_40 is not None:
            memo_items.append(f"**Rule of 40**: {rule_of_40}")
        if capital_efficiency is not None:
            memo_items.append(f"**Capital Efficiency**: {capital_efficiency:.2f}x")
        for p in projections:
            memo_items.append(f"**Year {p['year']} Revenue**: ${p['projected_revenue'] / 1e6:,.1f}M")

        memo_sections = [
            {"type": "heading2", "content": f"{company_name} — Financial Analysis"},
            {"type": "list", "items": memo_items},
        ]

        out: Dict[str, Any] = {
            "analysis": analysis,
            "memo_sections": memo_sections,
        }
        if grid_commands:
            out["grid_commands"] = grid_commands
            out["auto_suggestions_count"] = len(grid_commands)

        return out

    async def _tool_fpa(self, inputs: dict) -> dict:
        """Run FP&A analysis via NL→parse→classify→build→execute pipeline."""
        try:
            from app.services.nl_fpa_parser import NLFPAParser
            from app.services.fpa_query_classifier import FPAQueryClassifier
            from app.services.fpa_workflow_builder import FPAWorkflowBuilder
            from app.services.fpa_executor import FPAExecutor, ExecutorContext

            parser = NLFPAParser()
            classifier = FPAQueryClassifier()
            builder = FPAWorkflowBuilder()
            executor = FPAExecutor()

            query = inputs.get("query", "")
            parsed = parser.parse(query)
            handler_key = classifier.route(parsed)
            workflow = builder.build(parsed, handler_key)
            ctx = ExecutorContext(
                fund_id=self.shared_data.get("fund_context", {}).get("fundId"),
                portfolio_snapshot=self.shared_data.get("matrix_context", {}).get("gridSnapshot"),
            )
            result = await executor.execute(workflow, ctx)
            fpa_result = result if isinstance(result, dict) else {"result": str(result)}
            # Auto-generate memo sections from FPA model structure
            model_structure = fpa_result.get("model_structure")
            if model_structure and isinstance(model_structure, dict):
                assumptions = model_structure.get("assumptions", {})
                if assumptions:
                    fpa_result["memo_sections"] = [
                        {"type": "heading2", "content": f"FP&A: {query[:50]}"},
                        {
                            "type": "table",
                            "table": {
                                "headers": ["Assumption", "Value"],
                                "rows": [[str(k), str(v)] for k, v in assumptions.items()],
                                "caption": "Editable assumptions",
                            },
                        },
                    ]

            # Store in shared_data so chart dispatch and memo service can reach it
            async with self.shared_data_lock:
                self.shared_data["fpa_result"] = fpa_result

            return fpa_result
        except Exception as e:
            logger.warning(f"[TOOL] fpa failed: {e}")
            return {"error": str(e)}

    async def _tool_parse_accounts(self, inputs: dict) -> dict:
        """Parse raw financial accounts into structured data."""
        text = inputs.get("text", "")
        doc_id = inputs.get("document_id")

        if doc_id:
            try:
                from app.services.document_query_service import DocumentQueryService
                dqs = DocumentQueryService()
                doc = await dqs.analyze_document(doc_id)
                text = doc.get("content", "") or doc.get("summary", "")
            except Exception as e:
                logger.warning(f"[TOOL] parse_accounts doc fetch failed: {e}")

        if not text:
            return {"error": "No accounts data provided"}

        extraction_prompt = f"""Extract financial data from these accounts into JSON:
{text[:4000]}

Return: {{"periods": ["Q1 2025", ...], "line_items": [{{"name": "Revenue", "values": [1000000, ...], "type": "revenue|expense|asset|liability|equity"}}]}}"""

        try:
            response = await self.model_router.get_completion(
                prompt=extraction_prompt,
                system_prompt="Extract financial data into structured JSON. Numbers in raw form (no formatting).",
                capability=ModelCapability.STRUCTURED,
                max_tokens=1500,
                json_mode=True,
                caller_context="parse_accounts",
            )
            content = response.get("response", "") if isinstance(response, dict) else str(response)
            try:
                accounts = json.loads(content)
            except (json.JSONDecodeError, TypeError):
                return {"error": "Failed to parse LLM response as JSON", "raw": content[:500]}
            return {"accounts": accounts, "row_count": len(accounts.get("line_items", []))}
        except Exception as e:
            logger.warning(f"[TOOL] parse_accounts extraction failed: {e}")
            return {"error": f"Failed to parse accounts: {e}"}

    async def _tool_fx_check(self, inputs: dict) -> dict:
        """Check FX rates and compute currency impact on portfolio companies."""
        try:
            from app.services.fx_intelligence_service import fx_intelligence_service
            base = inputs.get("base_currency", "USD")
            grid = self.shared_data.get("matrix_context", {}).get("gridSnapshot", {})
            companies = []
            for row in grid.get("rows", []):
                cells = row.get("cells", {})
                name = cells.get("name", {}).get("value") or cells.get("company_name", {}).get("value") or ""
                rev = cells.get("revenue", {}).get("value") or cells.get("arr", {}).get("value")
                mix = cells.get("currency_mix", {}).get("value")
                if name:
                    companies.append({"name": name, "revenue_usd": rev, "currency_mix": mix})
            if not companies:
                rates = await fx_intelligence_service._fetch_rates()
                top_rates = {k: rates[k] for k in ["EUR", "GBP", "JPY", "CHF", "CAD"] if k in rates}
                return {"rates": top_rates, "note": "No portfolio companies with currency mix data. Showing major rates."}
            result = await fx_intelligence_service.get_portfolio_fx_summary(companies, base)
            return result
        except Exception as e:
            logger.warning(f"[TOOL] fx_check failed: {e}")
            return {"error": str(e)}

    # ------------------------------------------------------------------
    # Output generation tools — allow agent loop to produce decks, memos, run skills
    # ------------------------------------------------------------------

    async def _tool_generate_deck(self, inputs: dict) -> dict:
        """Invoke deck-storytelling skill from agent loop.

        Requires companies in shared_data (from fetch_company_data or portfolio).
        Returns full deck with slides, theme, charts.
        """
        try:
            # Prerequisites (companies, cap_table_history, etc.) are auto-resolved
            # by the tool wiring layer in _execute_tool before this handler runs.
            result = await self._execute_deck_generation(inputs)
            return result
        except Exception as e:
            logger.warning(f"[TOOL] generate_deck failed: {e}")
            return {"error": str(e), "format": "deck", "slides": []}

    async def _tool_generate_memo(self, inputs: dict) -> dict:
        """Invoke lightweight memo pipeline from agent loop.

        Supports all memo types: ic_memo, followon, lp_report, gp_strategy,
        comparison, bespoke_lp, fund_analysis, ownership_analysis, plan_memo.
        Returns structured document sections for MemoEditor.

        Prerequisites are auto-resolved by TOOL_WIRING before this handler
        runs.  The wiring layer calls run_valuation, cap_table_evolution, and
        run_scenario in parallel to populate shared_data with real analysis.
        _hydrate_shared_data_from_companies provides lightweight fallbacks
        for any keys the real tools fail to produce.  The agent never needs
        to manually chain these — one generate_memo call does everything.
        """
        try:
            memo_type = inputs.get("memo_type", "investment")

            # Log what the wiring layer pre-populated so debugging is easy
            _prereq_sources = {
                k: ("real" if self.shared_data.get(k, {}).get("_source") != "hydrated_from_companies" else "hydrated")
                for k in ("valuations", "cap_table_history", "scenario_analysis")
                if self.shared_data.get(k)
            }
            if _prereq_sources:
                logger.info(f"[MEMO] Prerequisite data sources: {_prereq_sources}")

            memo_inputs = {
                "memo_type": memo_type,
                "prompt": inputs.get("prompt", self.shared_data.get("original_prompt", "")),
            }
            result = await self._execute_memo_generation(memo_inputs)
            # Also return memo sections as side effects for the streaming pipeline
            if result.get("sections"):
                result["memo_sections"] = result["sections"]
            return result
        except Exception as e:
            logger.warning(f"[TOOL] generate_memo failed: {e}")
            return {"error": str(e), "format": "docs", "sections": []}

    async def _tool_run_skill(self, inputs: dict) -> dict:
        """Run any registered analysis skill by name.

        Dispatches to the skill registry (self.skills). Results are merged into
        shared_data so downstream tools (e.g., generate_deck) can use them.
        """
        skill_name = inputs.get("skill", "")
        skill_inputs = inputs.get("inputs") or {}
        skill_inputs["use_shared_data"] = True

        if not hasattr(self, "skills") or not self.skills:
            return {"error": "Skill registry not initialized"}

        skill_entry = self.skills.get(skill_name)
        if not skill_entry:
            available = list(self.skills.keys())[:20]
            return {"error": f"Unknown skill '{skill_name}'. Available: {available}"}

        handler = skill_entry.get("handler") if isinstance(skill_entry, dict) else skill_entry
        if not handler:
            return {"error": f"Skill '{skill_name}' has no handler"}

        try:
            result = await handler(skill_inputs)
            # Merge skill output into shared_data so subsequent tools can use it
            if isinstance(result, dict):
                async with self.shared_data_lock:
                    for k, v in result.items():
                        if k == "companies":
                            existing = self.shared_data.get("companies", [])
                            existing_names = {c.get("company", c.get("name", "")).lower() for c in existing}
                            for c in v:
                                cname = c.get("company", c.get("name", "")).lower()
                                if cname not in existing_names:
                                    existing.append(c)
                            self.shared_data["companies"] = existing
                        elif k not in ("error",):
                            self.shared_data[k] = v
            return result
        except Exception as e:
            logger.warning(f"[TOOL] run_skill({skill_name}) failed: {e}")
            return {"error": f"Skill '{skill_name}' failed: {e}"}

    # ------------------------------------------------------------------
    # Dedicated tool handlers — thin adapters to _execute_* methods
    # ------------------------------------------------------------------

    async def _tool_portfolio_health(self, inputs: dict) -> dict:
        """Portfolio health dashboard: delegates to _execute_company_health_dashboard."""
        return await self._execute_company_health_dashboard({
            "fund_id": inputs.get("fund_id") or self.shared_data.get("fund_context", {}).get("fund_id"),
            "context": inputs,
        })

    async def _tool_followon(self, inputs: dict) -> dict:
        """Follow-on strategy: delegates to _execute_followon_strategy."""
        return await self._execute_followon_strategy({
            "company": inputs.get("company"),
            "company_name": inputs.get("company"),
            "fund_id": inputs.get("fund_id") or self.shared_data.get("fund_context", {}).get("fund_id"),
            "context": inputs,
        })

    async def _tool_round_modeling(self, inputs: dict) -> dict:
        """Round modeling: delegates to _execute_round_modeling."""
        return await self._execute_round_modeling({
            "company": inputs.get("company"),
            "company_name": inputs.get("company"),
            "round_type": inputs.get("round_type"),
            "raise_amount": inputs.get("raise_amount"),
            "context": inputs,
        })

    async def _tool_exit_modeling(self, inputs: dict) -> dict:
        """Exit modeling: delegates to _execute_exit_modeling."""
        return await self._execute_exit_modeling({
            "company": inputs.get("company"),
            "company_name": inputs.get("company"),
            "exit_values": inputs.get("exit_values"),
            "context": inputs,
        })

    async def _tool_regression(self, inputs: dict) -> dict:
        """Dispatches to regression / monte carlo / sensitivity / time-series by type."""
        analysis_type = (inputs.get("type") or "linear").lower()
        base_inputs = {
            "company": inputs.get("company"),
            "metric": inputs.get("metric"),
            "context": inputs,
            **(inputs.get("inputs") or {}),
        }
        dispatch = {
            "linear": self._execute_regression_analysis,
            "regression": self._execute_regression_analysis,
            "monte_carlo": self._execute_monte_carlo,
            "montecarlo": self._execute_monte_carlo,
            "sensitivity": self._execute_sensitivity_analysis,
            "time_series": self._execute_time_series_forecast,
            "forecast": self._execute_time_series_forecast,
            "growth_decay": self._execute_growth_decay_forecast,
        }
        handler = dispatch.get(analysis_type, self._execute_regression_analysis)
        return await handler(base_inputs)

    async def _tool_report(self, inputs: dict) -> dict:
        """Report generation: delegates to _execute_report_generation."""
        return await self._execute_report_generation({
            "type": inputs.get("type", "lp_report"),
            "report_type": inputs.get("type", "lp_report"),
            "fund_id": inputs.get("fund_id") or self.shared_data.get("fund_context", {}).get("fund_id"),
            "company": inputs.get("company"),
            "context": inputs,
        })

    # ------------------------------------------------------------------
    # Phase 3: New tool handlers — wiring existing services
    # ------------------------------------------------------------------

    # --- Cap Table & Ownership ---

    async def _ensure_funding_rounds(self, company_data: dict) -> bool:
        """Ensure company_data has funding_rounds, reconstructing if needed.

        Tries gap_filler.generate_stage_based_funding_rounds first (uses stage
        benchmarks), then falls back to micro_skills.reconstruct_funding_history
        which can leverage last_round_amount + total_funding for scaling.

        Mutates company_data in-place. Returns True if rounds exist after call.
        """
        existing = company_data.get("funding_rounds") or company_data.get("funding_history") or []
        if existing:
            return True

        name = company_data.get("name", "unknown")
        stage = company_data.get("stage", "")

        # Attempt 1: gap_filler's stage-based synthesis (sync, fast)
        try:
            synthetic = self.gap_filler.generate_stage_based_funding_rounds(company_data)
            if synthetic:
                company_data["funding_rounds"] = synthetic
                company_data["funding_data_source"] = "stage_inferred"
                logger.info(f"[CAP_TABLE_FALLBACK] gap_filler generated {len(synthetic)} rounds for {name} (stage={stage})")
                return True
        except Exception as e:
            logger.warning(f"[CAP_TABLE_FALLBACK] gap_filler failed for {name}: {e}")

        # Attempt 2: micro-skill reconstruct_funding_history (async, uses last_round_amount)
        try:
            from app.services.micro_skills.benchmark_skills import reconstruct_funding_history
            result = await reconstruct_funding_history(company_data)
            rounds = result.field_updates.get("funding_rounds", []) if result and result.field_updates else []
            if rounds:
                company_data["funding_rounds"] = rounds
                company_data["funding_data_source"] = "reconstructed"
                logger.info(f"[CAP_TABLE_FALLBACK] reconstruct_funding_history built {len(rounds)} rounds for {name} (stage={stage}, confidence={result.confidence})")
                return True
        except Exception as e:
            logger.warning(f"[CAP_TABLE_FALLBACK] reconstruct_funding_history failed for {name}: {e}")

        return False

    def _persist_cap_table(self, portfolio_id: str, company_id: str, company_name: str, result: dict, funding_data_source: str = "synthetic"):
        """Upsert cap table result to Supabase for persistence across sessions."""
        try:
            from app.core.database import get_supabase_service
            sb = get_supabase_service()
            if not sb:
                return
            client = sb.get_client() if hasattr(sb, 'get_client') else sb
            if not client:
                return
            row = {
                "portfolio_id": portfolio_id,
                "company_id": company_id,
                "company_name": company_name,
                "cap_table_json": result.get("current_cap_table", {}),
                "sankey_data": result.get("sankey_data"),
                "waterfall_data": result.get("waterfall_data"),
                "ownership_summary": result.get("ownership_summary"),
                "founder_ownership": result.get("founder_ownership"),
                "total_raised": result.get("total_raised"),
                "num_rounds": result.get("num_rounds"),
                "source": funding_data_source,
                "funding_data_source": funding_data_source,
                "updated_at": datetime.utcnow().isoformat(),
            }
            client.table("company_cap_tables").upsert(row, on_conflict="portfolio_id,company_id").execute()
            logger.info(f"[CAP_TABLE_PERSIST] Upserted cap table for {company_name} (source={funding_data_source})")
        except Exception as e:
            logger.warning(f"[CAP_TABLE_PERSIST] Failed for {company_name}: {e}")

    def _load_cached_cap_table(self, portfolio_id: str, company_id: str) -> dict | None:
        """Load persisted cap table from Supabase if it exists."""
        try:
            from app.core.database import get_supabase_service
            sb = get_supabase_service()
            if not sb:
                return None
            client = sb.get_client() if hasattr(sb, 'get_client') else sb
            if not client:
                return None
            result = client.table("company_cap_tables").select("*").eq("portfolio_id", portfolio_id).eq("company_id", company_id).limit(1).execute()
            if result.data:
                row = result.data[0]
                return {
                    "current_cap_table": row.get("cap_table_json", {}),
                    "sankey_data": row.get("sankey_data"),
                    "waterfall_data": row.get("waterfall_data"),
                    "ownership_summary": row.get("ownership_summary"),
                    "founder_ownership": row.get("founder_ownership"),
                    "total_raised": row.get("total_raised"),
                    "num_rounds": row.get("num_rounds"),
                    "source": row.get("source"),
                    "company": row.get("company_name", ""),
                    "cached": True,
                }
        except Exception as e:
            logger.warning(f"[CAP_TABLE_CACHE] Load failed: {e}")
        return None

    async def _tool_cap_table_evolution(self, inputs: dict) -> dict:
        """Track dilution through all funding rounds with Sankey visualization."""
        try:
            company_name = inputs.get("company", "")
            force_recalc = inputs.get("force_recalculate", False)
            company_data = await self._resolve_company(company_name)
            if not company_data:
                return {"error": f"Could not resolve company: {company_name}"}

            portfolio_id = company_data.get("portfolio_id") or inputs.get("portfolio_id") or ""
            company_id = company_data.get("company_id") or company_data.get("id") or ""

            # Check cache first (skip if force recalculate)
            if not force_recalc and portfolio_id and company_id:
                cached = self._load_cached_cap_table(portfolio_id, company_id)
                if cached:
                    logger.info(f"[TOOL] cap_table_evolution: using cached result for {company_name}")
                    # Still store in shared_data for downstream use
                    async with self.shared_data_lock:
                        cap_hist = self.shared_data.get("cap_table_history")
                        if not isinstance(cap_hist, dict):
                            cap_hist = {}
                        cap_hist[cached.get("company", company_name)] = cached
                        self.shared_data["cap_table_history"] = cap_hist
                    return cached

            # Ensure funding rounds exist — reconstruct from stage/benchmarks if needed
            await self._ensure_funding_rounds(company_data)
            funding_rounds = company_data.get("funding_rounds") or company_data.get("funding_history") or []
            if not funding_rounds:
                return {"error": f"No funding round data available for {company_name}. Need at least a 'stage' field to reconstruct."}

            # Pass the full company_data dict (method accepts Dict or List)
            result = self.cap_table_service.calculate_full_cap_table_history(company_data)

            result["company"] = company_data.get("name") or company_name

            # Persist to shared_data so downstream memo/chart generation can use it
            company_key = result["company"]
            async with self.shared_data_lock:
                cap_hist = self.shared_data.get("cap_table_history")
                if not isinstance(cap_hist, dict):
                    cap_hist = {}
                cap_hist[company_key] = result
                self.shared_data["cap_table_history"] = cap_hist
            logger.info(f"[TOOL] cap_table_evolution: stored cap_table_history for {company_key}")

            # Persist to Supabase for cross-session caching
            funding_source = company_data.get("funding_data_source", "extracted")
            if portfolio_id and company_id:
                self._persist_cap_table(portfolio_id, company_id, company_key, result, funding_source)

                # Emit founderOwnership as a pending suggestion so it shows in the grid
                founder_own = result.get("founder_ownership")
                if founder_own is not None:
                    self._write_pending_suggestion(
                        fund_id=portfolio_id,
                        company_id=company_id,
                        column_id="founderOwnership",
                        suggested_value=founder_own,
                        source_service="cap_table_evolution",
                        reasoning=f"Founder ownership after {result.get('num_rounds', '?')} rounds: {founder_own*100:.1f}%",
                    )

            return result
        except Exception as e:
            logger.error(f"[TOOL] cap_table_evolution error: {e}")
            return {"error": str(e)}

    async def _tool_liquidation_waterfall(self, inputs: dict) -> dict:
        """Model liquidation waterfall at specific exit values."""
        try:
            company_name = inputs.get("company", "")
            exit_value = float(inputs.get("exit_value", 0))
            exit_type_str = inputs.get("exit_type", "strategic_ma")

            company_data = await self._resolve_company(company_name)
            if not company_data:
                return {"error": f"Could not resolve company: {company_name}"}

            if AdvancedWaterfallCalculator is None:
                return {"error": "AdvancedWaterfallCalculator not available"}

            # Ensure funding rounds exist — reconstruct if needed
            await self._ensure_funding_rounds(company_data)

            # Build investor terms from funding data
            from app.services.waterfall_advanced import LiquidationTerms, ExitType
            funding_rounds = company_data.get("funding_rounds") or company_data.get("funding_history") or []
            investors = []
            for rnd in funding_rounds:
                investors.append(LiquidationTerms(
                    investor_name=rnd.get("lead_investor") or rnd.get("investor") or f"Series {rnd.get('round', 'Unknown')} Investor",
                    amount_invested=Decimal(str(rnd.get("amount") or rnd.get("funding_amount") or 0)),
                    series=rnd.get("round") or rnd.get("series") or "Unknown",
                ))

            if not investors:
                return {"error": f"No investor data to model waterfall for {company_name}"}

            calc = AdvancedWaterfallCalculator(investors=investors)
            et = ExitType.STRATEGIC_MA
            try:
                et = ExitType[exit_type_str.upper()]
            except (KeyError, AttributeError):
                pass

            result = calc.calculate_waterfall(
                exit_value=Decimal(str(exit_value)),
                exit_type=et,
            )
            result["company"] = company_data.get("name") or company_name
            result["exit_value_input"] = exit_value
            return result
        except Exception as e:
            logger.error(f"[TOOL] liquidation_waterfall error: {e}")
            return {"error": str(e)}

    async def _tool_anti_dilution(self, inputs: dict) -> dict:
        """Model anti-dilution scenarios on cap table."""
        try:
            company_name = inputs.get("company", "")
            mechanism = inputs.get("mechanism", "weighted_average")
            new_round_price = inputs.get("new_round_price")

            company_data = await self._resolve_company(company_name)
            if not company_data:
                return {"error": f"Could not resolve company: {company_name}"}

            if not self.advanced_cap_table:
                return {"error": "AdvancedCapTable service not available"}

            # Ensure funding rounds exist — reconstruct if needed
            await self._ensure_funding_rounds(company_data)

            funding_rounds = company_data.get("funding_rounds") or company_data.get("funding_history") or []
            result = self.advanced_cap_table.apply_anti_dilution(
                funding_rounds=funding_rounds,
                new_round_price=new_round_price,
                anti_dilution_type=mechanism,
                company_name=company_data.get("name") or company_name,
            )
            result["company"] = company_data.get("name") or company_name
            return result if isinstance(result, dict) else {"result": result}
        except Exception as e:
            logger.error(f"[TOOL] anti_dilution error: {e}")
            return {"error": str(e)}

    async def _tool_debt_conversion(self, inputs: dict) -> dict:
        """Model SAFEs, convertible notes, and debt conversion triggers."""
        try:
            company_name = inputs.get("company", "")
            trigger_valuation = inputs.get("trigger_valuation")

            company_data = await self._resolve_company(company_name)
            if not company_data:
                return {"error": f"Could not resolve company: {company_name}"}

            if AdvancedDebtStructures is None:
                return {"error": "AdvancedDebtStructures service not available"}

            debt_service = AdvancedDebtStructures()
            debt_structure = debt_service.analyze_debt_structure(company_data)

            result = {"company": company_data.get("name") or company_name, "debt_structure": {}}
            if hasattr(debt_structure, "__dict__"):
                result["debt_structure"] = {k: v for k, v in debt_structure.__dict__.items() if not k.startswith("_")}
            elif isinstance(debt_structure, dict):
                result["debt_structure"] = debt_structure

            if trigger_valuation:
                conversion = debt_service.calculate_conversion_scenarios(
                    debt_structure=debt_structure,
                    exit_valuation=float(trigger_valuation),
                )
                result["conversion_scenarios"] = conversion if isinstance(conversion, dict) else {"result": conversion}

            return result
        except Exception as e:
            logger.error(f"[TOOL] debt_conversion error: {e}")
            return {"error": str(e)}

    # --- Scenario & Stress Testing ---

    async def _tool_stress_test_portfolio(self, inputs: dict) -> dict:
        """Portfolio-wide shock modeling across all holdings."""
        try:
            from app.services.scenario_analyzer import ScenarioAnalyzer, ScenarioType
            sa = ScenarioAnalyzer()

            shock_type = inputs.get("shock_type", "revenue_decline")
            magnitude = float(inputs.get("magnitude", 0.3))
            affected = inputs.get("affected_companies")

            # Get portfolio companies
            companies = self.shared_data.get("companies", [])
            if not companies:
                portfolio_result = await self._execute_tool("query_portfolio", {"query": "all companies"})
                companies = portfolio_result.get("companies", []) if isinstance(portfolio_result, dict) else []

            if affected:
                affected_lower = [a.lower().strip().lstrip("@") for a in affected]
                companies = [c for c in companies if (c.get("name") or c.get("company_name") or "").lower() in affected_lower]

            results = []
            for company in companies[:20]:  # Cap at 20
                c_name = company.get("name") or company.get("company_name") or "Unknown"
                try:
                    scenario = await sa.create_scenario(
                        model_id=company.get("id", c_name),
                        scenario_name=f"{shock_type} stress: {magnitude:.0%}",
                        scenario_type=ScenarioType.STRESS,
                        description=f"Apply {shock_type} shock of {magnitude:.0%} to {c_name}",
                    )
                    if scenario and scenario.get("id"):
                        exec_result = await sa.execute_scenario(scenario["id"])
                        results.append({"company": c_name, "impact": exec_result if isinstance(exec_result, dict) else {"analysis": str(exec_result)}})
                    else:
                        results.append({"company": c_name, "impact": {"note": "Scenario created but not executed"}})
                except Exception as ce:
                    results.append({"company": c_name, "error": str(ce)})

            return {
                "shock_type": shock_type,
                "magnitude": magnitude,
                "companies_tested": len(results),
                "results": results,
            }
        except Exception as e:
            logger.error(f"[TOOL] stress_test_portfolio error: {e}")
            return {"error": str(e)}

    async def _tool_world_model_scenario(self, inputs: dict) -> dict:
        """Multi-factor scenario with propagated effects."""
        try:
            company_name = inputs.get("company", "")
            factor_changes = inputs.get("factor_changes", {})

            company_data = await self._resolve_company(company_name)
            if not company_data:
                return {"error": f"Could not resolve company: {company_name}"}

            from app.services.world_model_builder import WorldModelBuilder
            wm = WorldModelBuilder()

            # Build world model for the company
            model = await wm.build_company_world_model(
                company_data=company_data,
                model_name=f"{company_data.get('name', company_name)} scenario",
            )

            result = {"company": company_data.get("name") or company_name, "world_model": model}

            # Execute model with factor changes if provided
            if model and model.get("model_id") and factor_changes:
                exec_result = await wm.execute_model(model["model_id"])
                result["execution"] = exec_result if isinstance(exec_result, dict) else {"result": str(exec_result)}

            return result
        except Exception as e:
            logger.error(f"[TOOL] world_model_scenario error: {e}")
            return {"error": str(e)}

    async def _tool_monte_carlo_portfolio(self, inputs: dict) -> dict:
        """Monte Carlo simulation across entire portfolio."""
        try:
            if FPARegressionService is None:
                return {"error": "FPARegressionService not available"}

            fpa = FPARegressionService()
            iterations = int(inputs.get("iterations", 1000))
            variables = inputs.get("variables", {})

            companies = self.shared_data.get("companies", [])
            if not companies:
                portfolio_result = await self._execute_tool("query_portfolio", {"query": "all companies"})
                companies = portfolio_result.get("companies", []) if isinstance(portfolio_result, dict) else []

            results = []
            for company in companies[:15]:  # Cap at 15
                c_name = company.get("name") or company.get("company_name") or "Unknown"
                revenue = ensure_numeric(company.get("revenue") or company.get("arr") or company.get("inferred_revenue"), 0)
                growth = ensure_numeric(company.get("growth_rate") or company.get("revenue_growth"), 0.5)
                try:
                    mc_result = await fpa.monte_carlo_simulation(
                        base_case={"revenue": revenue, "growth_rate": growth, **variables},
                        variable_distributions={"revenue_growth": {"type": "normal", "mean": growth, "std": growth * 0.3}},
                        iterations=iterations,
                    )
                    results.append({"company": c_name, "simulation": mc_result if isinstance(mc_result, dict) else {"result": mc_result}})
                except Exception as ce:
                    results.append({"company": c_name, "error": str(ce)})

            mc_portfolio_result = {"iterations": iterations, "companies_simulated": len(results), "results": results}

            # Store in shared_data so chart dispatch and memo service can reach it
            async with self.shared_data_lock:
                self.shared_data["monte_carlo_result"] = mc_portfolio_result

            return mc_portfolio_result
        except Exception as e:
            logger.error(f"[TOOL] monte_carlo_portfolio error: {e}")
            return {"error": str(e)}

    async def _tool_sensitivity_matrix(self, inputs: dict) -> dict:
        """2D sensitivity analysis on two variables."""
        try:
            if FPARegressionService is None:
                return {"error": "FPARegressionService not available"}

            fpa = FPARegressionService()
            company_name = inputs.get("company", "")
            var_x = inputs.get("var_x", "revenue_growth")
            var_y = inputs.get("var_y", "exit_multiple")
            range_x = inputs.get("range_x") or [-0.3, -0.15, 0, 0.15, 0.3]
            range_y = inputs.get("range_y") or [5, 8, 10, 15, 20]

            company_data = await self._resolve_company(company_name) if company_name else {}
            base_case = {
                "revenue": ensure_numeric((company_data or {}).get("revenue") or (company_data or {}).get("arr"), 10_000_000),
                "growth_rate": ensure_numeric((company_data or {}).get("growth_rate"), 0.5),
                "exit_multiple": 10,
            }

            result = await fpa.sensitivity_analysis(
                base_case=base_case,
                sensitive_variables=[var_x, var_y],
                sensitivity_ranges={var_x: (min(range_x), max(range_x)), var_y: (min(range_y), max(range_y))},
            )
            return {
                "company": (company_data or {}).get("name") or company_name,
                "var_x": var_x, "var_y": var_y,
                "sensitivity": result if isinstance(result, dict) else {"result": result},
            }
        except Exception as e:
            logger.error(f"[TOOL] sensitivity_matrix error: {e}")
            return {"error": str(e)}

    AgentTool(
        name="run_bull_bear_base",
        description="Build bull/bear/base scenario analysis for one or more companies. 3-path projection with probability-weighted outcomes.",
        handler="_tool_bull_bear_base",
        input_schema={"companies": "list[str]", "years": "int?"},
        cost_tier="expensive",
        timeout_ms=60_000,
    ),
    AgentTool(
        name="apply_macro_shock",
        description="Apply a macro event (recession, rate hike, regulation, tariff, pandemic, ai_winter) to an existing scenario tree and see impact on portfolio NAV/DPI.",
        handler="_tool_macro_shock",
        input_schema={"shock_type": "str", "magnitude": "float?", "start_year": "int?"},
        cost_tier="cheap",
        timeout_ms=30_000,
    ),
    AgentTool(
        name="portfolio_snapshot",
        description="Get probability-weighted portfolio state (NAV, DPI, TVPI, per-company revenue) at a specific future year from scenario analysis.",
        handler="_tool_portfolio_snapshot",
        input_schema={"year": "int"},
        cost_tier="cheap",
        timeout_ms=15_000,
    ),
    AgentTool(
        name="three_scenario_cash_flow",
        description="Build bull/base/bear P&L models side by side for a company.",
        handler="_tool_three_scenario_cash_flow",
        input_schema={"company": "str", "years": "int?"},
        cost_tier="cheap",
        timeout_ms=30_000,
    ),

    # --- Portfolio Operations ---

    async def _tool_add_company(self, inputs: dict) -> dict:
        """Fetch, validate, and suggest adding a company to portfolio."""
        try:
            company_name = inputs.get("company_name", "")
            stage = inputs.get("stage")
            check_size = inputs.get("check_size")

            # Fetch company data from web
            fetch_result = await self._execute_tool("fetch_company_data", {"companies": [company_name]})
            companies = self.shared_data.get("companies", [])
            fetched = None
            for c in companies:
                if (c.get("name") or c.get("company_name") or "").lower() == company_name.lower().strip().lstrip("@"):
                    fetched = c
                    break

            if not fetched:
                return {"error": f"Could not fetch data for {company_name}"}

            # Return as suggestion requiring approval
            return {
                "action": "suggest_add_company",
                "requires_approval": True,
                "company": fetched,
                "suggested_stage": stage or fetched.get("stage"),
                "suggested_check_size": check_size,
                "summary": f"Add {fetched.get('name', company_name)} to portfolio — {fetched.get('description', 'No description')}",
            }
        except Exception as e:
            logger.error(f"[TOOL] add_company error: {e}")
            return {"error": str(e)}

    async def _tool_bulk_operation(self, inputs: dict) -> dict:
        """Batch valuations, health checks, or data refreshes."""
        try:
            operation = inputs.get("operation", "health_check")
            company_names = inputs.get("companies")

            companies = self.shared_data.get("companies", [])
            if company_names:
                names_lower = [n.lower().strip().lstrip("@") for n in company_names]
                companies = [c for c in companies if (c.get("name") or c.get("company_name") or "").lower() in names_lower]

            if not companies:
                portfolio_result = await self._execute_tool("query_portfolio", {"query": "all companies"})
                companies = portfolio_result.get("companies", []) if isinstance(portfolio_result, dict) else []

            op_map = {
                "health_check": "_execute_company_health_dashboard",
                "valuation": "_execute_valuation",
                "data_refresh": "_tool_fetch_company",
            }
            handler_name = op_map.get(operation, "_execute_company_health_dashboard")
            handler = getattr(self, handler_name, None)

            results = []
            for company in companies[:20]:
                c_name = company.get("name") or company.get("company_name") or "Unknown"
                try:
                    r = await handler({"company": c_name, "company_name": c_name, "context": company})
                    results.append({"company": c_name, "result": r if isinstance(r, dict) else {"data": r}})
                except Exception as ce:
                    results.append({"company": c_name, "error": str(ce)})

            return {"operation": operation, "companies_processed": len(results), "results": results}
        except Exception as e:
            logger.error(f"[TOOL] bulk_operation error: {e}")
            return {"error": str(e)}

    async def _tool_portfolio_comparison(self, inputs: dict) -> dict:
        """Side-by-side comparison of N companies on specified metrics."""
        try:
            company_names = inputs.get("companies", [])
            metrics = inputs.get("metrics") or ["revenue", "valuation", "growth_rate", "stage", "burn_rate"]

            comparisons = []
            for name in company_names[:10]:
                company_data = await self._resolve_company(name)
                if company_data:
                    row = {"company": company_data.get("name") or name}
                    for m in metrics:
                        val = company_data.get(m)
                        if val is None:
                            val = company_data.get(f"inferred_{m}")
                        if hasattr(val, "value"):
                            val = val.value
                        row[m] = val
                    comparisons.append(row)
                else:
                    comparisons.append({"company": name, "error": "not_found"})

            return {"metrics": metrics, "companies": comparisons}
        except Exception as e:
            logger.error(f"[TOOL] portfolio_comparison error: {e}")
            return {"error": str(e)}

    async def _tool_graduation_rates(self, inputs: dict) -> dict:
        """Stage progression analysis across portfolio."""
        try:
            companies = self.shared_data.get("companies", [])
            if not companies:
                portfolio_result = await self._execute_tool("query_portfolio", {"query": "all companies"})
                companies = portfolio_result.get("companies", []) if isinstance(portfolio_result, dict) else []

            stage_counts = {}
            progressions = []
            for c in companies:
                stage = c.get("stage") or c.get("funding_stage") or "Unknown"
                stage_counts[stage] = stage_counts.get(stage, 0) + 1
                rounds = c.get("funding_rounds") or c.get("funding_history") or []
                if len(rounds) > 1:
                    progressions.append({
                        "company": c.get("name") or c.get("company_name") or "Unknown",
                        "stages": [r.get("round") or r.get("series") for r in rounds],
                        "total_rounds": len(rounds),
                    })

            return {
                "stage_distribution": stage_counts,
                "total_companies": len(companies),
                "companies_with_progression": len(progressions),
                "progressions": progressions[:20],
            }
        except Exception as e:
            logger.error(f"[TOOL] graduation_rates error: {e}")
            return {"error": str(e)}

    # --- Market & Intelligence ---

    async def _tool_market_landscape(self, inputs: dict) -> dict:
        """Competitive landscape mapping via MarketIntelligenceService."""
        try:
            if not self.market_intelligence:
                return {"error": "MarketIntelligenceService not available"}

            sector = inputs.get("sector", "")
            geography = inputs.get("geography")
            stage = inputs.get("stage")

            landscape = await self.market_intelligence.generate_sector_landscape(sector=sector, geography=geography)
            result = landscape.__dict__ if hasattr(landscape, "__dict__") else landscape if isinstance(landscape, dict) else {"landscape": str(landscape)}

            # Also find hot companies
            try:
                hot = await self.market_intelligence.find_companies_by_geography_sector(
                    geography=geography or "US", sector=sector, stage=stage,
                )
                result["hot_companies"] = [
                    h.__dict__ if hasattr(h, "__dict__") else h for h in (hot or [])[:10]
                ]
            except Exception:
                pass

            return result
        except Exception as e:
            logger.error(f"[TOOL] market_landscape error: {e}")
            return {"error": str(e)}

    async def _tool_market_timing(self, inputs: dict) -> dict:
        """Assess market timing for entry/exit."""
        try:
            if not self.market_intelligence:
                return {"error": "MarketIntelligenceService not available"}

            sector = inputs.get("sector", "")
            geography = inputs.get("geography")

            timing = await self.market_intelligence.analyze_market_timing(sector=sector, geography=geography)
            return timing if isinstance(timing, dict) else {"timing": timing}
        except Exception as e:
            logger.error(f"[TOOL] market_timing error: {e}")
            return {"error": str(e)}

    async def _tool_company_history(self, inputs: dict) -> dict:
        """Full company history analysis with DPI Sankey."""
        try:
            company_name = inputs.get("company", "")
            fund_id = inputs.get("fund_id") or self.shared_data.get("fund_context", {}).get("fund_id")

            company_data = await self._resolve_company(company_name)
            if not company_data:
                return {"error": f"Could not resolve company: {company_name}"}

            if self.company_history_service:
                company_id = company_data.get("id") or company_data.get("company_id")
                if company_id:
                    result = await self.company_history_service.analyze_full_history(
                        company_id=company_id, fund_id=fund_id,
                    )
                    return result if isinstance(result, dict) else {"history": result}

            # Fallback: return what we have
            return {
                "company": company_data.get("name") or company_name,
                "funding_rounds": company_data.get("funding_rounds") or company_data.get("funding_history") or [],
                "stage": company_data.get("stage"),
                "source": "cached_data",
            }
        except Exception as e:
            logger.error(f"[TOOL] company_history error: {e}")
            return {"error": str(e)}

    # --- Financial Modeling ---

    async def _tool_revenue_projection(self, inputs: dict) -> dict:
        """Project revenue with quality-adjusted decay curves."""
        try:
            if RevenueProjectionService is None:
                return {"error": "RevenueProjectionService not available"}

            company_name = inputs.get("company", "")
            years = int(inputs.get("years", 5))
            growth_override = inputs.get("growth_rate")

            company_data = await self._resolve_company(company_name) if company_name else {}
            company_data = company_data or {}

            revenue = ensure_numeric(
                company_data.get("revenue") or company_data.get("arr") or company_data.get("inferred_revenue"), 1_000_000
            )
            growth = float(growth_override) if growth_override else ensure_numeric(company_data.get("growth_rate"), 0.5)

            projections = RevenueProjectionService.project_revenue_with_decay(
                base_revenue=revenue,
                initial_growth=growth,
                years=years,
                quality_score=ensure_numeric(company_data.get("quality_score"), 1.0),
                stage=company_data.get("stage"),
                investor_quality=company_data.get("investor_quality"),
                geography=company_data.get("geography") or company_data.get("location"),
                sector=company_data.get("business_model") or company_data.get("category") or company_data.get("sector"),
                company_age_years=company_data.get("company_age_years"),
                market_size_tam=ensure_numeric(company_data.get("market_size_tam") or company_data.get("tam"), None),
                return_projections=True,
            )
            result = {
                "company": company_data.get("name") or company_name,
                "base_revenue": revenue,
                "initial_growth": growth,
                "years": years,
                "projections": projections,
            }

            # Persist to shared_data so downstream memo/chart generation can use it
            company_key = result["company"]
            async with self.shared_data_lock:
                rev_proj = self.shared_data.get("revenue_projections")
                if not isinstance(rev_proj, dict):
                    rev_proj = {}
                rev_proj[company_key] = result
                self.shared_data["revenue_projections"] = rev_proj
            logger.info(f"[TOOL] revenue_projection: stored revenue_projections for {company_key}")

            return result
        except Exception as e:
            logger.error(f"[TOOL] revenue_projection error: {e}")
            return {"error": str(e)}

    async def _tool_fund_deployment(self, inputs: dict) -> dict:
        """Model fund J-curve, pacing, reserve allocation."""
        try:
            if not self.fund_modeling:
                return {"error": "FundModelingService not available"}

            fund_id = inputs.get("fund_id") or self.shared_data.get("fund_context", {}).get("fund_id")
            scenarios = inputs.get("scenarios") or [{"name": "base_case"}]

            result = await self.fund_modeling.model_fund_scenarios(
                fund_data={"fund_id": fund_id, **self.shared_data.get("fund_context", {})},
                scenarios=scenarios,
            ) if hasattr(self.fund_modeling, "model_fund_scenarios") else await self.fund_modeling.model_fund(
                fund_id=fund_id, scenarios=scenarios,
            )
            return result if isinstance(result, dict) else {"result": result}
        except Exception as e:
            logger.error(f"[TOOL] fund_deployment error: {e}")
            return {"error": str(e)}

    async def _tool_financial_calc(self, inputs: dict) -> dict:
        """On-demand NPV, IRR, PMT, PV, FV calculations."""
        try:
            if not self.financial_calculator:
                return {"error": "FinancialCalculator not available"}

            calc = self.financial_calculator
            func_name = inputs.get("function", "npv").lower()
            calc_inputs = inputs.get("inputs", {})

            dispatch = {
                "npv": lambda: calc.npv(rate=float(calc_inputs.get("rate", 0.1)), cash_flows=calc_inputs.get("cash_flows", [])),
                "irr": lambda: calc.irr(cash_flows=calc_inputs.get("cash_flows", [])),
                "pv": lambda: calc.pv(rate=float(calc_inputs.get("rate", 0.1)), nper=int(calc_inputs.get("nper", 10)), pmt=float(calc_inputs.get("pmt", 0)), fv=float(calc_inputs.get("fv", 0))),
                "fv": lambda: calc.fv(rate=float(calc_inputs.get("rate", 0.1)), nper=int(calc_inputs.get("nper", 10)), pmt=float(calc_inputs.get("pmt", 0)), pv=float(calc_inputs.get("pv", 0))),
                "pmt": lambda: calc.pmt(rate=float(calc_inputs.get("rate", 0.1)), nper=int(calc_inputs.get("nper", 10)), pv=float(calc_inputs.get("pv", 0))),
            }

            handler = dispatch.get(func_name)
            if not handler:
                return {"error": f"Unknown function: {func_name}. Available: {list(dispatch.keys())}"}

            result = handler()
            return {"function": func_name, "inputs": calc_inputs, "result": result}
        except Exception as e:
            logger.error(f"[TOOL] financial_calc error: {e}")
            return {"error": str(e)}

    async def _tool_fx_portfolio_impact(self, inputs: dict) -> dict:
        """Full portfolio FX exposure analysis across all non-USD holdings."""
        try:
            base_currency = inputs.get("base_currency", "USD")
            shock_pct = float(inputs.get("shock_pct", 0.1))

            companies = self.shared_data.get("companies", [])
            if not companies:
                portfolio_result = await self._execute_tool("query_portfolio", {"query": "all companies"})
                companies = portfolio_result.get("companies", []) if isinstance(portfolio_result, dict) else []

            fx_exposures = []
            for c in companies:
                currency = c.get("currency") or c.get("reporting_currency") or "USD"
                if currency.upper() != base_currency.upper():
                    revenue = ensure_numeric(c.get("revenue") or c.get("arr") or c.get("inferred_revenue"), 0)
                    fx_exposures.append({
                        "company": c.get("name") or c.get("company_name") or "Unknown",
                        "currency": currency,
                        "revenue": revenue,
                        "impact_at_shock": revenue * shock_pct,
                    })

            total_exposure = sum(e["revenue"] for e in fx_exposures)
            total_impact = sum(e["impact_at_shock"] for e in fx_exposures)

            return {
                "base_currency": base_currency,
                "shock_pct": shock_pct,
                "total_fx_exposure": total_exposure,
                "total_impact": total_impact,
                "companies_exposed": len(fx_exposures),
                "exposures": fx_exposures,
            }
        except Exception as e:
            logger.error(f"[TOOL] fx_portfolio_impact error: {e}")
            return {"error": str(e)}

    # --- Compliance & Reporting ---

    async def _tool_compliance_check(self, inputs: dict) -> dict:
        """Check filing requirements, generate compliance documents."""
        try:
            if not self.compliance_service:
                return {"error": "EnhancedComplianceService not available"}

            check_type = inputs.get("check_type", "filing_requirements")
            advisor_info = inputs.get("advisor_info") or self.shared_data.get("fund_context", {})

            if check_type == "form_adv":
                result = self.compliance_service.generate_form_adv(firm_info=advisor_info)
            elif check_type == "regulatory_calendar":
                result = self.compliance_service.generate_regulatory_calendar(fund_info=advisor_info)
            else:
                fund_id = advisor_info.get("fund_id") or advisor_info.get("fundId") or "default"
                result = self.compliance_service.check_compliance_status(fund_id=fund_id)

            return result if isinstance(result, dict) else {"result": result}
        except Exception as e:
            logger.error(f"[TOOL] compliance_check error: {e}")
            return {"error": str(e)}

    async def _tool_ma_workflow(self, inputs: dict) -> dict:
        """M&A deal structure analysis: synergies, integration risk, returns."""
        try:
            if not self.ma_service:
                return {"error": "MAWorkflowService not available"}

            acquirer = inputs.get("acquirer", "")
            target = inputs.get("target", "")
            deal_rationale = inputs.get("deal_type")

            result = await self.ma_service.model_acquisition(
                acquirer=acquirer, target=target, deal_rationale=deal_rationale,
            )

            # Also get synergy estimate
            synergies = {}
            try:
                synergies = await self.ma_service.calculate_synergy_value(acquirer=acquirer, target=target)
            except Exception:
                pass

            output = result.__dict__ if hasattr(result, "__dict__") else result if isinstance(result, dict) else {"valuation": result}
            if synergies:
                output["synergies"] = synergies if isinstance(synergies, dict) else {"value": synergies}
            return output
        except Exception as e:
            logger.error(f"[TOOL] ma_workflow error: {e}")
            return {"error": str(e)}

    # --- Generation (varying types) ---

    async def _tool_generate_ic_memo(self, inputs: dict) -> dict:
        """Generate investment committee memo."""
        return await self._execute_memo_generation({
            "memo_type": "ic_memo",
            "prompt": inputs.get("prompt", "Generate IC memo"),
        })

    async def _tool_generate_followon_memo(self, inputs: dict) -> dict:
        """Generate follow-on investment analysis."""
        return await self._execute_memo_generation({
            "memo_type": "followon",
            "prompt": inputs.get("prompt", "Generate follow-on memo"),
        })

    async def _tool_generate_lp_report(self, inputs: dict) -> dict:
        """Generate quarterly LP report."""
        return await self._execute_memo_generation({
            "memo_type": "lp_report",
            "prompt": inputs.get("prompt", "Generate LP quarterly report"),
        })

    async def _tool_generate_gp_update(self, inputs: dict) -> dict:
        """Generate GP strategy update."""
        return await self._execute_memo_generation({
            "memo_type": "gp_strategy",
            "prompt": inputs.get("prompt", "Generate GP strategy update"),
        })

    async def _tool_generate_comparison_report(self, inputs: dict) -> dict:
        """Generate side-by-side investment comparison report."""
        company_names = inputs.get("companies", [])

        # Resolve all companies first
        for name in company_names:
            await self._resolve_company(name)

        return await self._execute_memo_generation({
            "memo_type": "comparison",
            "prompt": inputs.get("prompt", f"Compare {', '.join(company_names)}"),
        })

    async def _tool_generate_plan_memo(self, inputs: dict) -> dict:
        """Generate a plan memo — resumable context document for cross-session use.

        Plan memos are both human-readable AND machine-resumable: they contain
        narrative sections alongside a structured context snapshot (JSON) that
        a future session can hydrate shared_data from.
        """
        return await self._execute_memo_generation({
            "memo_type": "plan_memo",
            "prompt": inputs.get("prompt", self.shared_data.get("original_prompt", "Generate plan")),
        })

    # ------------------------------------------------------------------
    # Feedback loop — read back corrections
    # ------------------------------------------------------------------

    async def _get_recent_corrections(self, prompt: str, company: Optional[str] = None) -> List[str]:
        """Fetch recent user corrections from Supabase for context injection."""
        try:
            supabase_url = settings.SUPABASE_URL
            supabase_key = settings.SUPABASE_SERVICE_ROLE_KEY or settings.SUPABASE_ANON_KEY
            if not supabase_url or not supabase_key:
                return []
            from supabase import create_client
            sb = create_client(supabase_url, supabase_key)
            query = sb.table("agent_corrections").select("correction, model_type, created_at").order("created_at", desc=True).limit(5)
            if company:
                query = query.eq("company", company)
            result = query.execute()
            if result.data:
                return [f"[{r.get('model_type', '')}] {r['correction']}" for r in result.data]
        except Exception as e:
            logger.warning(f"[CORRECTIONS] Failed to fetch: {e}")
        return []

    # ------------------------------------------------------------------
    # Query intent classification — maps open-ended prompts to thinking
    # chains so the agent knows which tools to use and in what order.
    # ------------------------------------------------------------------

    QUERY_INTENTS = [
        # (intent_name, trigger_patterns, thinking_chain, description)
        (
            "portfolio_overview",
            ["tell me about my portfolio", "portfolio overview", "how is my portfolio",
             "portfolio summary", "show my portfolio", "portfolio status",
             "how are my companies doing", "what's in my portfolio"],
            "query_portfolio → run_portfolio_health → calculate_fund_metrics → generate_chart(type=bar, title='Portfolio Overview') → synthesize",
            "Broad portfolio overview: pulls companies, runs health check, computes fund metrics, visualizes.",
        ),
        (
            "company_deep_dive",
            ["tell me about @", "deep dive on", "analyze @", "what do we know about",
             "how is @ doing", "status of @", "update on @"],
            "query_portfolio(company=@X) → run_portfolio_health → run_valuation → run_exit_modeling → generate_chart → synthesize",
            "Deep analysis of a specific portfolio company: health, valuation, exit scenarios.",
        ),
        (
            "comparable_tracking",
            ["comparable", "comps for", "peers of", "similar companies to",
             "benchmarks for", "how does @ compare", "compare @ to"],
            "query_portfolio(company=@X) → web_search(query='@X competitors funding revenue') → fetch_company_data(comps) → run_skill(deal-comparer) → generate_chart → synthesize",
            "Find and track comparables for a portfolio company.",
        ),
        (
            "fund_metrics",
            ["fund performance", "fund metrics", "dpi", "tvpi", "irr",
             "fund returns", "how is the fund", "fund status", "deployment pace",
             "capital deployed", "dry powder"],
            "calculate_fund_metrics → query_portfolio → generate_chart(type=bar) → synthesize",
            "Fund-level metrics: DPI, TVPI, IRR, deployment pacing.",
        ),
        (
            "followon_decision",
            ["follow on", "follow-on", "pro rata", "should we extend",
             "bridge for", "extend @", "double down on", "increase position"],
            "query_portfolio(company=@X) → run_followon_strategy → run_round_modeling → generate_chart → synthesize",
            "Follow-on investment decision: pro-rata, ownership impact, dilution modeling.",
        ),
        (
            "exit_analysis",
            ["exit scenario", "what if @ exits", "ipo scenario", "m&a scenario",
             "secondary", "exit at", "return on @", "what would we make"],
            "query_portfolio(company=@X) → run_exit_modeling → run_skill(pwerm-calculator) → generate_chart(type=probability_cloud) → synthesize",
            "Exit modeling: returns at various exit values, PWERM probability weighting.",
        ),
        (
            "scenario_stress",
            ["what if", "scenario", "stress test", "if rates rise",
             "if market crashes", "downside case", "worst case", "monte carlo",
             "sensitivity analysis", "forecast"],
            "query_portfolio → run_scenario → run_regression(type=sensitivity) → generate_chart → synthesize",
            "Scenario/stress test: run what-if across portfolio or specific companies.",
        ),
        (
            "deck_generation",
            ["generate a deck", "investment deck", "pitch deck", "presentation",
             "make slides", "build a deck", "create a deck"],
            "fetch_company_data → run_valuation → run_skill(cap-table-generator) → run_skill(pwerm-calculator) → generate_deck → synthesize",
            "Full investment deck generation.",
        ),
        (
            "memo_writing",
            ["write a memo", "investment memo", "lp report", "quarterly report",
             "gp strategy", "follow-on memo", "due diligence report",
             "write a report", "draft a memo"],
            "query_portfolio → calculate_fund_metrics → run_portfolio_health → run_report → synthesize",
            "Document/memo generation with real data.",
        ),
        (
            "company_research",
            ["research @", "look up @", "find @", "what is @",
             "search for @", "fetch @", "get data on @"],
            "fetch_company_data → run_valuation → generate_chart → synthesize",
            "Research a new company from the web.",
        ),
        (
            "round_modeling",
            ["model series", "model round", "model the next round", "what if they raise",
             "dilution if", "new round for", "series b for", "series c for"],
            "query_portfolio(company=@X) → run_round_modeling → run_skill(cap-table-generator) → generate_chart → synthesize",
            "Model a future funding round with dilution and waterfall.",
        ),
        (
            "portfolio_construction",
            ["portfolio construction", "allocation", "how should we deploy",
             "remaining capital", "pacing", "stage allocation", "check size"],
            "calculate_fund_metrics → query_portfolio → run_portfolio_health → run_skill(fund-analyzer) → generate_chart → synthesize",
            "Portfolio construction and capital deployment analysis.",
        ),
        (
            "portfolio_enrichment_valuation",
            ["enrich", "run numbers", "value the portfolio", "value all companies",
             "run capm", "run dcf", "portfolio valuation", "fill in the data",
             "fill gaps", "enrich portfolio", "run valuations", "bulk valuation",
             "missing data", "calculate valuations", "fundraising analysis",
             "run some numbers", "value my portfolio", "enrich all",
             "pwerm", "capm", "run some", "valuations", "value these",
             "run valuation", "compute valuation", "calculate valuation"],
            "query_portfolio → resolve_data_gaps → call_tools(run_valuation × ALL portfolio companies IN PARALLEL, batches of 5) → generate_chart(type=bar, title='Portfolio Valuations') → generate_memo → synthesize",
            "Enrich all portfolio companies with gap-filled data, then run valuations for EVERY company. CRITICAL: resolve_data_gaps first to populate missing fields, then run_valuation for each company — do NOT skip companies. Process all 23+ companies, not just 1.",
        ),
        ("lp_query", ["what's our dpi", "lp update", "quarterly update", "what is our tvpi", "what is our irr", "distributions"], "calculate_fund_metrics → run_skill(lp-query-response) → synthesize", "LP questions."),
        ("team_comparison", ["compare teams", "team comparison", "founding team", "compare founders"], "fetch_company_data(all) → run_skill(team-comparison) → synthesize", "Team comparison."),
        ("competitive_landscape", ["competitive landscape", "competitors", "competitive analysis", "market map", "landscape"], "fetch_company_data → web_search(competitors) → run_skill(competitive-landscape-memo) → synthesize", "Competitive landscape."),
        ("followon_deep_dive", ["follow-on deep dive", "cap table evolution", "breakpoint analysis", "follow-on analysis"], "fetch_company_data → run_skill(followon-deep-dive) → synthesize", "Follow-on deep dive."),
        ("comparable_search", ["find comparables", "comparable companies", "similar companies", "comps", "find peers", "comp set"], "fetch_company_data → web_search(comps) → synthesize", "Find comparables."),
        ("sourcing", [
            "show me", "rank my", "score my", "sort my", "filter my",
            "list my", "find series", "series a companies", "series b companies", "series c companies",
            "top companies", "best companies", "capital efficiency", "source companies",
            "sourcing list", "pipeline by", "rank by", "scored by", "sorted by",
            "companies above", "companies with", "companies in the database",
            "what do we have", "what companies", "show companies",
        ], "source_companies(query) → synthesize", "Fast DB sourcing: query + score + rank from existing data."),
        ("company_list_build", ["build a list", "company list", "deal flow list", "find new companies", "startups in", "market map of", "discover companies"], "web_search(sector) → fetch_company_data(multiple) → enrich_portfolio → synthesize", "Build company list via web search."),
        ("bulk_enrichment", ["enrich all", "bulk enrich", "enrich the grid", "fill in everything", "enrich my companies", "refresh all data"], "enrich_portfolio → run_skill(multi-enrich) → synthesize", "Bulk enrich."),
    ]

    def _classify_query_intent(self, prompt: str) -> Optional[Dict[str, Any]]:
        """Classify the user's prompt into a known intent with a thinking chain.

        Returns None if no strong match — the agent loop will use generic reasoning.
        When matched, returns {"intent", "chain", "description"} to guide routing.
        """
        lower = prompt.lower()
        for intent_name, triggers, chain, description in self.QUERY_INTENTS:
            for trigger in triggers:
                # Handle @-mention patterns
                if "@" in trigger:
                    pattern = trigger.replace("@", "")
                    if pattern in lower and "@" in prompt:
                        return {"intent": intent_name, "chain": chain, "description": description}
                elif trigger in lower:
                    return {"intent": intent_name, "chain": chain, "description": description}
        return None

    # ------------------------------------------------------------------
    # Session plan lifecycle (Phase 7)
    # ------------------------------------------------------------------

    def _create_session_plan(
        self, prompt: str, steps: List[Dict[str, Any]], intent: str = "", source: str = "auto",
    ) -> SessionPlan:
        """Build a SessionPlan from raw step dicts (e.g. from _generate_cheap_plan)."""
        import uuid
        plan_steps = [
            PlanStep(
                id=s.get("id", str(i)),
                label=s.get("label", ""),
                tool=s.get("tool", ""),
                inputs=s.get("input", s.get("inputs", {})),
                depends_on=s.get("depends_on", []),
                status="pending",
            )
            for i, s in enumerate(steps)
        ]
        plan = SessionPlan(
            plan_id=str(uuid.uuid4())[:8],
            intent=intent,
            prompt=prompt,
            steps=plan_steps,
            source=source,
        )
        self.shared_data["session_plan"] = plan
        logger.info(f"[SESSION_PLAN] Created plan {plan.plan_id} with {len(plan_steps)} steps")
        return plan

    def _get_session_plan(self) -> Optional[SessionPlan]:
        return self.shared_data.get("session_plan")

    def _skip_plan_step(self, step_id: str) -> bool:
        """Skip a plan step (e.g. user drags to reorder/skip in frontend)."""
        plan = self._get_session_plan()
        if not plan:
            return False
        plan.mark_skipped(step_id)
        logger.info(f"[SESSION_PLAN] Skipped step {step_id}")
        return True

    def _get_or_create_plan_context(self, session_id: str = "") -> PlanContext:
        """Get the current plan context or create a new one."""
        existing = self.shared_data.get("plan_context")
        if isinstance(existing, PlanContext):
            return existing
        ctx = PlanContext(session_id=session_id or str(id(self)))
        self.shared_data["plan_context"] = ctx
        return ctx

    async def _save_plan_context_to_documents(self, plan_context: PlanContext, fund_id: str = "") -> bool:
        """Persist plan context to the documents table for cross-session resume."""
        try:
            from app.core.database import get_supabase_service
            client = get_supabase_service().get_client()
            if not client:
                logger.warning("[PLAN_CONTEXT] Supabase unavailable — context not persisted")
                return False

            plan = self._get_session_plan()
            if plan:
                plan_context.plan = plan.to_dict()

            doc_data = {
                "title": f"Session Context — {plan_context.session_id[:8]}",
                "document_type": "session_context",
                "content": json.dumps(plan_context.to_dict()),
                "metadata": {
                    "session_id": plan_context.session_id,
                    "updated_at": plan_context.updated_at,
                    "has_plan": plan is not None,
                    "entity_count": len(plan_context.entity_refs),
                },
            }
            if fund_id:
                doc_data["fund_id"] = fund_id

            existing = client.table("documents").select("id").eq(
                "document_type", "session_context"
            ).eq("metadata->>session_id", plan_context.session_id).limit(1).execute()

            if existing.data:
                client.table("documents").update(doc_data).eq("id", existing.data[0]["id"]).execute()
                logger.info(f"[PLAN_CONTEXT] Updated context doc for session {plan_context.session_id[:8]}")
            else:
                client.table("documents").insert(doc_data).execute()
                logger.info(f"[PLAN_CONTEXT] Saved new context doc for session {plan_context.session_id[:8]}")
            return True
        except Exception as e:
            logger.warning(f"[PLAN_CONTEXT] Failed to persist context: {e}")
            return False

    async def _load_plan_context_from_documents(self, session_id: str = "") -> Optional[PlanContext]:
        """Load the most recent plan context from the documents table."""
        try:
            from app.core.database import get_supabase_service
            client = get_supabase_service().get_client()
            if not client:
                return None

            query = client.table("documents").select("content, metadata").eq(
                "document_type", "session_context"
            )
            if session_id:
                query = query.eq("metadata->>session_id", session_id)
            result = query.order("updated_at", desc=True).limit(1).execute()

            if not result.data:
                return None

            content = result.data[0].get("content", "{}")
            ctx_dict = json.loads(content) if isinstance(content, str) else content
            ctx = PlanContext.from_dict(ctx_dict)
            self.shared_data["plan_context"] = ctx
            logger.info(f"[PLAN_CONTEXT] Loaded context for session {ctx.session_id[:8]} "
                        f"({len(ctx.findings)} findings, {len(ctx.reasoning)} reasoning steps)")

            if ctx.plan:
                restored_steps = ctx.plan.get("steps", [])
                plan = self._create_session_plan(
                    prompt=ctx.plan.get("prompt", ""),
                    steps=restored_steps,
                    intent=ctx.plan.get("intent", ""),
                    source="restored",
                )
                for s in plan.steps:
                    saved_status = ctx.plan.get("results", {}).get(s.id)
                    if saved_status:
                        plan.results[s.id] = saved_status
                logger.info(f"[PLAN_CONTEXT] Restored plan {plan.plan_id} with {len(plan.steps)} steps")

            return ctx
        except Exception as e:
            logger.warning(f"[PLAN_CONTEXT] Failed to load context: {e}")
            return None

    # ------------------------------------------------------------------
    # Parallel tool execution (Phase 8)
    # ------------------------------------------------------------------

    async def _execute_tools_parallel(
        self, steps: List[PlanStep], plan: Optional[SessionPlan] = None,
    ) -> List[Dict[str, Any]]:
        """Execute multiple independent PlanSteps in parallel via asyncio.gather."""
        if not steps:
            return []

        async def _run_one(step: PlanStep) -> Dict[str, Any]:
            if plan:
                plan.mark_running(step.id)
            try:
                result = await self._execute_tool(step.tool, step.inputs)
                if plan:
                    if "error" in result:
                        plan.mark_failed(step.id, result["error"])
                    else:
                        plan.mark_done(step.id, result)
                return {"tool": step.tool, "input": step.inputs, "output": result, "step_id": step.id}
            except Exception as e:
                if plan:
                    plan.mark_failed(step.id, str(e))
                return {"tool": step.tool, "input": step.inputs, "output": {"error": str(e)}, "step_id": step.id}

        results = await asyncio.gather(*[_run_one(s) for s in steps], return_exceptions=False)
        return list(results)

    # ------------------------------------------------------------------
    # Lightweight plan mode (cheap model)
    # ------------------------------------------------------------------

    def _needs_plan(self, prompt: str) -> bool:
        """Rule-based: does this need user approval before executing?"""
        lower = prompt.lower()
        expensive_triggers = [
            "stress test all", "revalue entire", "lp report",
            "full analysis of", "revalue portfolio",
            "generate report", "generate deck", "generate memo",
            "compare all", "analyze portfolio", "portfolio health",
            "portfolio scenarios", "what if", "forecast",
            "monte carlo", "sensitivity analysis", "model round",
            "model series", "build world model", "cap table scenario",
        ]
        if any(w in lower for w in expensive_triggers):
            return True
        # 2+ company references with a complex verb → needs plan
        at_mentions = re.findall(r"@\w+", prompt)
        # Also count grid company names mentioned without @
        matrix_ctx = self.shared_data.get("matrix_context") or {}
        grid_names = matrix_ctx.get("companyNames") or matrix_ctx.get("company_names") or []
        grid_count = sum(1 for gn in grid_names if gn and len(gn) >= 3 and re.search(r'\b' + re.escape(gn) + r'\b', prompt, re.IGNORECASE))
        total_company_refs = len(at_mentions) + grid_count
        complex_verbs = ["compare", "analyze", "deck", "memo", "report", "forecast", "stress"]
        if total_company_refs >= 2 and any(v in lower for v in complex_verbs):
            return True
        return False

    async def _generate_cheap_plan(
        self, prompt: str, classification: Optional[QueryClassification] = None,
    ) -> List[Dict[str, Any]]:
        """Generate an execution plan via cheap model — the agent's inner monologue.

        The plan reads like reasoning: "I need to do X, which involves Y, then Z."
        Each step maps to a concrete tool call so the plan flows directly into action.

        Uses QueryClassification (Phase 2) when available; falls back to legacy
        intent classifier.
        """
        _plan_intent = classification.intent if classification else "general"
        _plan_tools = get_tools_for_intent(_plan_intent)
        tool_descriptions = "\n".join(f"- {t.name}: {t.description}" for t in _plan_tools)

        # Build intent guidance from classification or legacy
        intent_hint = ""
        if classification and classification.suggested_chain:
            chain_str = " → ".join(classification.suggested_chain)
            intent_hint = (
                f"\nClassified intent: {classification.intent}\n"
                f"Suggested chain: {chain_str}\n"
                f"Needs portfolio data: {classification.needs_portfolio}\n"
                f"Needs external lookup: {classification.needs_external}\n"
                f"Adapt this chain to the specific request.\n"
            )
        else:
            intent = self._classify_query_intent(prompt)
            if intent:
                intent_hint = (
                    f"\nDetected intent: {intent['intent']}\n"
                    f"Suggested chain: {intent['chain']}\n"
                    f"Adapt this chain to the specific request.\n"
                )

        plan_prompt = f"""Think step-by-step about what's needed, then produce 3-6 action steps.

Task: {prompt}

Available tools:
{tool_descriptions}
{intent_hint}
Return JSON array where each step is a clear action:
[{{"id":"1","label":"what this step does and why","tool":"tool_name","input":{{...}},"depends_on":[]}}]

Rules:
- Each label should explain WHAT and WHY (e.g. "Fetch Ramp data from web — need financials for valuation")
- depends_on lists step IDs that must complete first (empty = can run immediately)
- Steps with no dependencies on each other CAN run in parallel
- If the task mentions companies not in portfolio, include a fetch_company_data step
- Keep it practical — 3-6 steps max"""

        try:
            plan_response = await self.model_router.get_completion(
                prompt=plan_prompt,
                system_prompt="You are a portfolio analysis agent planning your work. Return a JSON array of execution steps. Each step is a clear action with a tool call. Be specific about inputs.",
                capability=ModelCapability.FAST,
                max_tokens=500,
                temperature=0.0,
                json_mode=True,
                caller_context="plan_generation",
            )
            content = plan_response.get("response", "[]") if isinstance(plan_response, dict) else str(plan_response)
            parsed = json.loads(content)
            # Handle both array and object responses (Anthropic json_mode
            # prefills '{' which forces the model to return an object)
            if isinstance(parsed, dict):
                # Extract the array from common wrapper keys
                steps = parsed.get("steps") or parsed.get("plan") or parsed.get("actions") or []
                if not isinstance(steps, list):
                    steps = []
            elif isinstance(parsed, list):
                steps = parsed
            else:
                steps = []
            return [
                {
                    "id": s.get("id", str(i)),
                    "label": s.get("label", ""),
                    "status": "pending",
                    "tool": s.get("tool", ""),
                    "input": s.get("input", {}),
                    "depends_on": s.get("depends_on", []),
                }
                for i, s in enumerate(steps)
                if isinstance(s, dict)
            ]
        except Exception as e:
            logger.warning(f"[PLAN] Cheap plan generation failed: {e}")
            # Fallback: build plan from classification template if available
            if classification and classification.suggested_chain:
                return [
                    {
                        "id": str(i),
                        "label": tool_name,
                        "status": "pending",
                        "tool": tool_name,
                        "input": {},
                        "depends_on": [str(i - 1)] if i > 0 else [],
                    }
                    for i, tool_name in enumerate(classification.suggested_chain)
                ]
            return []

    # ------------------------------------------------------------------
    # ReAct Agent Loop: reason → act → reflect → synthesize
    # ------------------------------------------------------------------

    def _truncate(self, text: str, max_chars: int) -> str:
        """Truncate text for LLM context compression."""
        return text[:max_chars] + "..." if len(text) > max_chars else text

    def _summarize_tool_results(self, tool_results: list[dict]) -> str:
        """Structured metric extraction from tool results.

        Instead of blind JSON truncation that loses data for bulk operations,
        extract the key metrics the agent needs: company name, which fields
        were populated, key values, and error status. Keeps ALL results
        (not just last 6) in compact form.
        """
        _DATA_TOOLS = {
            "fetch_company_data", "search_company_funding", "search_company_product",
            "search_company_team", "search_company_market", "run_valuation",
            "resolve_data_gaps", "analyze_financials", "lightweight_diligence",
            "build_company_list", "query_portfolio",
        }

        summaries = []
        for r in tool_results:
            tool = r.get("tool", "unknown")
            output = r.get("output", {})
            if not isinstance(output, dict):
                output = {"raw": str(output)[:200]}

            has_error = "error" in output
            entry: dict = {"tool": tool, "ok": not has_error}

            if has_error:
                entry["error"] = str(output.get("error", ""))[:200]
                summaries.append(entry)
                continue

            # --- Structured extraction per tool type ---
            if tool in ("fetch_company_data", "lightweight_diligence"):
                companies = output.get("companies", [])
                if companies:
                    comp_summaries = []
                    for c in companies[:10]:
                        name = c.get("company") or c.get("name") or "?"
                        cs: dict = {"name": name}
                        # Key financial metrics
                        for fk in ("revenue", "arr", "valuation", "total_funding", "growth_rate", "team_size"):
                            v = c.get(fk) or c.get(f"inferred_{fk}")
                            if v and v != 0:
                                cs[fk] = v
                        # Quality signals
                        for qk in ("stage", "sector", "category", "reporting_currency",
                                    "hq_location", "hq_country", "business_model"):
                            v = c.get(qk)
                            if v and v not in ("Unknown", "N/A", ""):
                                cs[qk] = v
                        # Inferred field list
                        inferred = [k.replace("inferred_", "") for k in c
                                    if k.startswith("inferred_") and c[k] and c[k] != 0]
                        if inferred:
                            cs["inferred_fields"] = inferred
                        comp_summaries.append(cs)
                    entry["companies"] = comp_summaries
                    entry["count"] = len(companies)
                else:
                    entry["summary"] = self._truncate(json.dumps(output), 500)

            elif tool == "run_valuation":
                for vk in ("company", "company_name", "valuation", "methodology",
                            "revenue_multiple", "implied_valuation", "confidence"):
                    v = output.get(vk)
                    if v is not None:
                        entry[vk] = v
                # Extract scenario results compactly
                scenarios = output.get("scenarios") or output.get("pwerm_scenarios")
                if scenarios and isinstance(scenarios, list):
                    entry["scenarios"] = len(scenarios)

            elif tool in ("search_company_funding", "search_company_product",
                           "search_company_team", "search_company_market"):
                # Granular search results — extract just the structured fields
                for k, v in output.items():
                    if k in ("raw_search_results", "search_results", "sources"):
                        continue  # Skip raw search payloads
                    if v and v not in (None, "", "N/A", "Unknown", [], {}):
                        if isinstance(v, str) and len(v) > 150:
                            entry[k] = v[:150] + "..."
                        else:
                            entry[k] = v

            elif tool in ("resolve_data_gaps", "build_company_list"):
                # Bulk operations — summarize per company
                companies = output.get("companies", [])
                if companies:
                    entry["count"] = len(companies)
                    entry["names"] = [c.get("company") or c.get("name") or "?" for c in companies[:20]]
                    # Count fields filled
                    filled = sum(1 for c in companies
                                 for k in ("revenue", "valuation", "stage", "sector")
                                 if c.get(k) or c.get(f"inferred_{k}"))
                    entry["fields_filled"] = filled
                else:
                    entry["summary"] = self._truncate(json.dumps(output), 400)

            elif tool in _DATA_TOOLS:
                entry["summary"] = self._truncate(json.dumps(output), 1500)

            else:
                # Non-data tools — compact summary
                entry["summary"] = self._truncate(json.dumps(output), 600)

            summaries.append(entry)

        # For very large result sets, keep all but compress older entries
        if len(summaries) > 12:
            # Keep full detail for last 8, compress earlier ones to one-liners
            compressed = []
            for s in summaries[:-8]:
                compressed.append({"tool": s["tool"], "ok": s.get("ok", True),
                                    "count": s.get("count", 1)})
            return json.dumps(compressed + summaries[-8:])

        return json.dumps(summaries)

    def _extract_citations_from_results(self, tool_results: List[dict]) -> List[dict]:
        """Extract citations from tool results for the response."""
        citations = []
        for r in tool_results:
            output = r.get("output", {})
            # Web search results become source citations
            if r.get("tool") == "web_search":
                for sr in output.get("results", []):
                    if sr.get("url"):
                        citations.append({
                            "type": "source",
                            "title": sr.get("title", sr["url"]),
                            "url": sr["url"],
                        })
            # Document queries become document citations
            if r.get("tool") == "query_documents":
                for doc in output.get("documents", []):
                    if doc.get("id"):
                        citations.append({
                            "type": "document",
                            "title": doc.get("title", "Document"),
                            "document_id": doc["id"],
                        })
        return citations[:10]  # Cap at 10

    async def _run_agent_loop(
        self, prompt: str, context: Optional[Dict[str, Any]] = None, memo_text: str = "",
        max_iterations: int = 10, approved_plan: bool = False, entities: Optional[Dict[str, Any]] = None,
        classification: Optional[QueryClassification] = None,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """ReAct loop: reason → act → reflect. Cheap model for routing, full model for synthesis.

        When a QueryClassification is provided, its intent and suggested_chain
        guide the REASON step so the agent can produce clear, actionable plans.
        """

        # Plan mode: opt-in only. User must explicitly request a plan (e.g. "plan: ...")
        # before we gate execution behind approval. Never auto-trigger on keywords.
        if approved_plan is False and context and context.get("plan_mode"):
            plan = await self._generate_cheap_plan(prompt, classification=classification)
            if plan:
                plan_artifact = Artifact(
                    type="plan", action="suggest", target="chat",
                    data={"steps": plan, "prompt": prompt}, requires_approval=True,
                )
                yield {
                    "type": "complete",
                    "result": {
                        "content": "",
                        "format": "analysis",
                        "plan_steps": plan,
                        "awaiting_approval": True,
                        "artifacts": [{"type": plan_artifact.type, "action": plan_artifact.action,
                                       "data": plan_artifact.data, "target": plan_artifact.target,
                                       "requires_approval": plan_artifact.requires_approval}],
                    },
                }
                return

        tool_results: List[dict] = []
        memo_sections: List[dict] = []
        _streamed_memo_ids: set = set()  # Track IDs already yielded to frontend
        grid_commands: List[dict] = []
        suggestions: List[dict] = []
        todo_items: List[dict] = []
        charts: List[dict] = []
        plan_steps: List[dict] = []
        failed_tools: set = set()

        # SessionState: read-only lens on shared_data (Phase 1)
        state = SessionState(self.shared_data) if SessionState else None

        # SessionMemo: compressed findings for context between LLM calls
        session_memo = SessionMemo() if SessionMemo else None

        # Recover plan steps from approved plan (sent back by frontend)
        plan_steps_from_approval: List[dict] = []
        session_plan: Optional[SessionPlan] = None
        plan_ctx: PlanContext = self._get_or_create_plan_context()
        if approved_plan and context:
            plan_steps_from_approval = context.get("plan_steps", []) or []
            if plan_steps_from_approval:
                # Build a SessionPlan so we get lifecycle tracking + parallel execution
                session_plan = self._create_session_plan(
                    prompt=prompt,
                    steps=plan_steps_from_approval,
                    intent=classification.intent if classification else "approved_plan",
                    source="approved",
                )
                max_iterations = min(max_iterations, len(plan_steps_from_approval) + 4)
                logger.info(f"[AGENT_LOOP] Executing approved plan {session_plan.plan_id} "
                            f"with {len(plan_steps_from_approval)} steps")

        # Restore working memory from previous turns so follow-up queries have context
        prior_memory = self.shared_data.get("agent_context", {}).get("working_memory", [])
        if prior_memory:
            for mem in prior_memory:
                tool_results.append({"tool": mem.get("tool", "prior"), "input": {}, "output": mem.get("summary", "")})
            logger.info(f"[AGENT_LOOP] Restored {len(prior_memory)} prior tool results from working_memory")

        # ── Sourcing pre-fetch: load top companies into shared_data so the
        # agent has DB context from the first reasoning step (not only after
        # a tool happens to populate it).  Runs only when companies are not
        # already present (e.g. from an @-mention or a previous turn).
        if not self.shared_data.get("companies"):
            try:
                from app.services.sourcing_service import query_companies
                _fund_id = (self.shared_data.get("fund_context") or {}).get("fundId") or getattr(self, "fund_id", None)
                _prefetch = await query_companies(filters={}, sort_by="name", sort_desc=False, limit=100, fund_id=_fund_id)
                _prefetch_companies = _prefetch.get("companies", [])
                if _prefetch_companies:
                    async with self.shared_data_lock:
                        self.shared_data["companies"] = _prefetch_companies
                    logger.info(f"[AGENT_LOOP] Pre-fetched {len(_prefetch_companies)} companies into shared_data for context")
            except Exception as _pf_err:
                logger.warning(f"[AGENT_LOOP] Sourcing pre-fetch failed (non-fatal): {_pf_err}")

        # Build intent-scoped tool catalog — only relevant categories
        _intent = classification.intent if classification else "general"
        _scoped_tools = get_tools_for_intent(_intent)
        tool_catalog = "\n".join(f"- {t.name}: {t.description}" for t in _scoped_tools)

        ROUTE_MAX_TOKENS = 500
        REFLECT_MAX_TOKENS = 150
        SYNTH_MAX_TOKENS = 4000

        # Track correction count at loop start for mid-loop interruption detection
        _correction_count_at_start = len(self.shared_data.get("session_corrections", []))

        # ── Extract structured goals from user request (LLM-driven, no keywords) ──
        _company_names_json = json.dumps((entities or {}).get('companies', []))
        _goal_extraction_prompt = (
            f"User request: {prompt}\n"
            f"Companies mentioned: {_company_names_json}\n\n"
            "Extract the concrete goals the user wants achieved. Each goal should map to a tool outcome.\n"
            "Each goal: {\"id\": \"short_slug\", \"description\": \"what must be true when done\", \"check\": \"scoreboard check\"}\n\n"
            "MEMO RULE — when to include memo_count > 0:\n"
            "ALWAYS include a memo goal when the request involves:\n"
            "- Analysis, comparison, review, assessment, or evaluation of companies/portfolio\n"
            "- Questions about performance, winners/losers, financing needs, runway, health\n"
            "- Any multi-company or portfolio-level question that requires synthesis\n"
            "- Anything that takes more than a single data lookup to answer\n"
            "- Explicit memo/report/deck/write-up requests\n\n"
            "Only SKIP memo goals for:\n"
            "- Single data lookups (\"what is X's revenue?\")\n"
            "- Simple enrichment without analysis (\"fill gaps\", \"enrich\")\n\n"
            "DEFAULT: If in doubt, INCLUDE the memo goal. Users expect written deliverables, not chat dumps.\n\n"
            "Examples:\n"
            '- "portfolio performance and financing needs" -> [{"id":"enrich","description":"Fetch data for all portfolio companies","check":"fetch_count > 0"},{"id":"analyze","description":"Run valuations and metrics","check":"valuation_count > 0"},{"id":"write_memo","description":"Write analysis memo with findings","check":"memo_count > 0"}]\n'
            '- "analyze @Ramp" -> [{"id":"enrich_ramp","description":"Fetch and enrich Ramp data","check":"fetch_count > 0"},{"id":"valuate_ramp","description":"Run valuation on Ramp","check":"valuation_count > 0"},{"id":"write_memo","description":"Generate investment memo for Ramp","check":"memo_count > 0"}]\n'
            '- "who are the winners and losers" -> [{"id":"enrich","description":"Fetch portfolio data","check":"fetch_count > 0"},{"id":"write_memo","description":"Write winner/loser analysis","check":"memo_count > 0"}]\n'
            '- "what is Mercury\'s revenue?" -> [{"id":"quick_look","description":"Look up Mercury revenue","check":"fetch_count > 0"}]\n\n'
            "RESPOND WITH JUST THE JSON ARRAY. NO MARKDOWN. NO EXPLANATION. NO PROSE. 1-5 goals max.\n"
            "JUST THE JSON ARRAY:"
        )

        try:
            _goal_response = await self.model_router.get_completion(
                prompt=_goal_extraction_prompt,
                system_prompt="You are a JSON extraction machine. Return ONLY a raw JSON array. No markdown fences. No ```json. No explanation. Just [{...}]. NOTHING ELSE.",
                capability=ModelCapability.FAST,
                max_tokens=300,
                temperature=0.0,
                json_mode=True,
                caller_context="agent_loop_goal_extraction",
            )
            _goal_text = _goal_response.get("response", "[]") if isinstance(_goal_response, dict) else str(_goal_response)
            # Use robust JSON parser that handles fences, prose, partial arrays
            from app.services.micro_skills.search_skills import _parse_llm_json
            _parsed = _parse_llm_json(_goal_text, "goal_extraction", "goals")
            if isinstance(_parsed, list):
                _extracted_goals = _parsed
            elif isinstance(_parsed, dict):
                _extracted_goals = _parsed.get("goals", []) if isinstance(_parsed.get("goals"), list) else [_parsed]
            else:
                _extracted_goals = []
        except Exception as e:
            logger.warning(f"[AGENT_LOOP] Goal extraction failed: {e}")
            _extracted_goals = []

        _goals_text = "\n".join(f"  - [{g.get('id', '?')}] {g.get('description', '?')}" for g in _extracted_goals)
        logger.info(f"[AGENT_LOOP] Extracted {len(_extracted_goals)} goals:\n{_goals_text}")

        # ── TaskLedger: built from TaskPlanner output (not keywords) ────────
        from app.services.session_state import TaskLedger
        # Placeholder — populated from plan tasks after TaskPlanner runs below
        ledger = TaskLedger([])

        # ── TaskPlanner: LLM-powered task decomposition (Phase 2) ────────
        # Primary: async LLM decomposition with full tool catalog + fingerprint.
        # Fallback: classification-based chain → entity-based defaults.
        _deterministic_plan: Optional[List] = None
        _completion_criteria: list[dict] = []

        if TaskPlanner and not session_plan and not approved_plan:
            # First try: async LLM decomposition (one call → full task DAG)
            try:
                _decomp_result = await TaskPlanner.plan_async(
                    prompt=prompt,
                    state=state,
                    entities=entities,
                    goals=_extracted_goals,
                    model_router=self.model_router if hasattr(self, 'model_router') else None,
                )
                if _decomp_result:
                    _deterministic_plan, _completion_criteria = _decomp_result
                    logger.info(
                        f"[AGENT_LOOP] LLM decomposition produced {len(_deterministic_plan)} tasks "
                        f"with {len(_completion_criteria)} completion criteria"
                    )
                    # Merge LLM completion criteria into extracted goals
                    if _completion_criteria:
                        _existing_checks = {g.get("check") for g in _extracted_goals}
                        for cc in _completion_criteria:
                            if cc.get("check") and cc["check"] not in _existing_checks:
                                _extracted_goals.append(cc)
            except Exception as e:
                logger.warning(f"[AGENT_LOOP] LLM decomposition failed: {e}")

            # Second try: classification chain fallback
            if not _deterministic_plan:
                try:
                    _deterministic_plan = TaskPlanner.plan(
                        prompt=prompt,
                        state=state,
                        entities=entities,
                        classification=classification,
                        goals=_extracted_goals,
                    )
                except Exception as e:
                    logger.warning(f"[AGENT_LOOP] TaskPlanner fallback failed: {e}")
                    _deterministic_plan = None

        if _deterministic_plan:
            _source = "llm_decomposition" if _completion_criteria else "classification_chain"
            logger.info(f"[AGENT_LOOP] TaskPlanner ({_source}) produced {len(_deterministic_plan)} tasks — skipping LLM REASON")
            # Convert to SessionPlan for lifecycle tracking
            _det_steps = [
                {"id": t.id, "label": t.label, "tool": t.tool,
                 "input": t.inputs, "depends_on": t.depends_on}
                for t in _deterministic_plan
            ]
            session_plan = self._create_session_plan(
                prompt=prompt, steps=_det_steps,
                intent=classification.intent if classification else _source,
                source=_source,
            )
            max_iterations = min(max_iterations, len(_deterministic_plan) + 4)

            # ── Build TaskLedger from the ACTUAL plan (not keywords) ──
            ledger = TaskLedger.from_plan_tasks(_deterministic_plan)
            logger.info(f"[AGENT_LOOP] TaskLedger from plan: {ledger.pending_summary()}")

        for i in range(max_iterations):
            # Check for new corrections (chat interruptions) since loop started
            current_corrections = self.shared_data.get("session_corrections", [])
            if len(current_corrections) > _correction_count_at_start:
                new_corrections = current_corrections[_correction_count_at_start:]
                logger.info(f"[AGENT_LOOP] {len(new_corrections)} new correction(s) received mid-loop — adapting")
                _correction_count_at_start = len(current_corrections)
                # The corrections are already injected into route_prompt via correction_guidance

            # Budget enforcement
            if hasattr(self, 'model_router') and hasattr(self.model_router, 'budget') and self.model_router.budget:
                budget = self.model_router.budget
                if budget.exhausted:
                    logger.warning(f"[AGENT_LOOP] Budget exhausted after {i} iterations, stopping")
                    break
                warning = budget.warn_if_expensive(f"agent_loop_iter_{i}")
                if warning:
                    logger.info(f"[AGENT_LOOP] {warning}")

            # --- REASON (cheap model) ---
            # Structured metric extraction from tool results — instead of blind
            # JSON truncation, pull out the key fields the agent needs to reason
            # about. This prevents earlier results from being lost and gives
            # quality signals (actual vs inferred, which fields were populated).
            results_summary = self._summarize_tool_results(tool_results) if tool_results else "[]"

            # Exclude tools that previously failed in this loop
            active_catalog = "\n".join(
                f"- {t.name}: {t.description}" for t in _scoped_tools if t.name not in failed_tools
            ) if failed_tools else tool_catalog

            # Build dense portfolio state — per-company field checklist with
            # jurisdiction map, sector concentrations, and analysis manifest.
            # The fingerprint merges grid rows + enriched companies into one view
            # so the agent can see its own progress and what's still missing.
            if state:
                sd_summary = state.fingerprint()
                # Append stale analysis markers so agent knows what to re-fetch
                stale_keys = [k.replace("_stale_", "") for k in self.shared_data if k.startswith("_stale_")]
                if stale_keys:
                    sd_summary += f"\nSTALE (from prior request, needs refresh): {', '.join(stale_keys)}"
            else:
                portfolio_intel = self._build_portfolio_intelligence()
                sd_summary = portfolio_intel if portfolio_intel else "STATE: empty — no companies in grid or shared_data"

            # Build intent guidance from classification — persist through ALL iterations
            # so the LLM always knows what it should be doing + grid state
            intent_guidance = ""
            if True:  # No iteration gate — always inject intent + fingerprint
                if classification and classification.suggested_chain:
                    chain_str = " → ".join(classification.suggested_chain)
                    intent_guidance = (
                        f"\nINTENT: {classification.intent} (confidence: {classification.confidence:.0%})\n"
                        f"SUGGESTED CHAIN: {chain_str}\n"
                        f"NEEDS PORTFOLIO DATA: {classification.needs_portfolio}\n"
                        f"NEEDS EXTERNAL LOOKUP: {classification.needs_external}\n"
                        f"Use this as guidance — adapt based on results.\n"
                    )
                    # Inject sourcing-specific guidance when intent is sourcing-related
                    _sourcing_intents = {"sourcing", "dealflow", "market", "company_search", "list_building"}
                    if classification.intent in _sourcing_intents:
                        intent_guidance += (
                            "\nSOURCING WORKFLOW: "
                            "1) Call generate_rubric(thesis=<user request>) to get intent-aware weights + filters. "
                            "2) Call source_companies(filters=rubric.filters, custom_weights=rubric.weights, "
                            "discover_web=true, thesis=<user request>, min_web_threshold=5) to search DB + web. "
                            "The rubric auto-classifies intent (acquirer/gtm_leads/lp_investor/dealflow/etc.) "
                            "and adapts queries, extraction, and scoring. No need to call build_company_list separately.\n"
                        )
                else:
                    # Legacy fallback
                    intent = self._classify_query_intent(prompt)
                    if intent:
                        intent_guidance = (
                            f"\nDETECTED INTENT: {intent['intent']}\n"
                            f"SUGGESTED CHAIN: {intent['chain']}\n"
                            f"CONTEXT: {intent['description']}\n"
                            f"Follow this chain unless the results so far indicate a different path.\n"
                        )

            # If we have a SessionPlan, use dependency-aware guidance
            plan_guidance = ""
            if session_plan and not session_plan.is_complete():
                runnable = session_plan.runnable_steps()
                if runnable:
                    if len(runnable) > 1:
                        # Multiple independent steps — guide LLM to call them in parallel
                        tools_desc = ", ".join(
                            f"{s.tool}({json.dumps(s.inputs)})" for s in runnable
                        )
                        plan_guidance = (
                            f"\nAPPROVED PLAN — {len(runnable)} independent steps ready:\n"
                            f"{tools_desc}\n"
                            f"Use call_tools to execute them in parallel.\n"
                        )
                    else:
                        s = runnable[0]
                        plan_guidance = (
                            f"\nAPPROVED PLAN — Execute next step: {s.label}\n"
                            f"Tool: {s.tool}, Input: {json.dumps(s.inputs)}\n"
                            f"Execute this step now.\n"
                        )
            elif approved_plan and plan_steps_from_approval and i < len(plan_steps_from_approval):
                # Fallback: raw plan_steps without SessionPlan
                step = plan_steps_from_approval[i]
                plan_guidance = (
                    f"\nAPPROVED PLAN — Execute step {i+1}: {step.get('label', '')}\n"
                    f"Tool: {step.get('tool', '')}, Input: {json.dumps(step.get('input', {}))}\n"
                    f"Execute this step now.\n"
                )

            # Inject session corrections (chat feedback) into reasoning
            session_corrections = self.shared_data.get("session_corrections", [])
            correction_guidance = ""
            if session_corrections:
                recent = session_corrections[-3:]  # Last 3 corrections
                correction_texts = [c["correction"] for c in recent]
                correction_guidance = (
                    f"\nUSER CORRECTIONS (adjust your approach accordingly):\n"
                    + "\n".join(f"- {ct}" for ct in correction_texts)
                    + "\n"
                )

            # Company context — directives, not hints
            auto_enrich_guidance = ""
            _enrich_parts: list = []

            if self.shared_data.get("needs_auto_enrich"):
                sparse_fields = self.shared_data.get("auto_enrich_fields", [])
                _enrich_parts.append(
                    f"MISSING DATA: {', '.join(sparse_fields)} fields are sparse — call resolve_data_gaps NOW before answering."
                )

            new_mentions = self.shared_data.get("mentioned_new_companies", [])
            if new_mentions:
                _enrich_parts.append(
                    f"UNKNOWN COMPANIES: {', '.join(new_mentions)} are not in the grid — call fetch_company_data NOW."
                )

            sparse_mentions = self.shared_data.get("mentioned_sparse_companies", [])
            if sparse_mentions:
                _enrich_parts.append(
                    f"SPARSE COMPANIES: {', '.join(sparse_mentions)} have thin grid data — call resolve_data_gaps NOW."
                )

            if _enrich_parts:
                auto_enrich_guidance = (
                    "\nDATA DIRECTIVES (act on these before anything else):\n"
                    + "\n".join(f"- {p}" for p in _enrich_parts) + "\n"
                )

            # Freshness-aware skip guidance — tell the LLM which companies already have data
            skip_guidance = ""
            if state:
                extracted_names = (entities or {}).get("companies", []) or list(state.grid_company_names)
                if extracted_names:
                    fresh = [n for n in extracted_names if not state.company_needs(n).get("fetch")]
                    if fresh:
                        skip_guidance = (
                            f"\nFRESH DATA (skip fetch, go straight to valuation/analysis):\n"
                            f"  {', '.join(fresh)}\n"
                        )

            # Build goals status for this iteration — use TaskLedger for per-task tracking
            _goals_status = f"\nTASK PROGRESS:\n{ledger.pending_summary()}\n"

            # Inject compressed memo instead of raw results when available
            _memo_context = session_memo.context(max_tokens=800) if session_memo and session_memo.entries else ""
            _results_display = _memo_context if _memo_context else f"Results so far: {results_summary}"

            # Task tracker + memo canvas state for agent awareness
            _task_state = ""
            if AgentTaskTracker:
                _tracker = AgentTaskTracker(self.shared_data)
                if _tracker.tasks:
                    _task_state = f"\nTASK STATE: {_tracker.summary_for_context()}\n"
            _memo_canvas_state = ""
            _n_memo = len(memo_sections) + len(self.shared_data.get("memo_sections", []))
            if _n_memo:
                _memo_canvas_state = f"MEMO CANVAS: {_n_memo} sections already written — do NOT call write_to_memo for sections tools already emitted\n"

            # Build tool call history for dedup awareness
            _tool_history = ""
            if tool_results:
                _tool_calls_so_far = [f"  - {r['tool']}({json.dumps(r.get('input', {}))[:120]})" for r in tool_results]
                _tool_history = "\nTOOLS ALREADY CALLED (do NOT repeat these):\n" + "\n".join(_tool_calls_so_far) + "\n"

            route_prompt = f"""Task: {prompt}

Available tools:
{active_catalog}

State:
{sd_summary}{auto_enrich_guidance}{skip_guidance}
{_task_state}{_memo_canvas_state}
{_results_display}
{_goals_status}{intent_guidance}{plan_guidance}{correction_guidance}{_tool_history}
RULES:
1. Look at State — what data exists? What's missing for the user's request?
2. Pick tool(s) from the list above to close the gap. Prerequisites auto-resolve.
3. Run independent calls in parallel. Never say "no data" — use tools to get it.
4. NEVER repeat a tool call that already ran — check TOOLS ALREADY CALLED above.
5. After data tools finish, ALWAYS write results somewhere persistent:
   - Use generate_memo or write_to_memo to write analysis to the memo (context bridge for future turns)
   - Use nl-matrix-controller to push extracted values (ARR, valuation, runway) back into the grid
   - The memo is NOT optional — it preserves context across agent turns
6. Charts go in the memo, not the chat. Generate charts as part of memo/analysis output.
7. You are done when: data is fetched AND written to memo AND grid values updated.
   Do NOT say "done" if you only searched/fetched but never wrote to memo or grid.

For a SINGLE tool: {{"action":"call_tool","tool":"name","input":{{...}},"reasoning":"1 sentence"}}
For PARALLEL tools: {{"action":"call_tools","tools":[{{"tool":"name","input":{{...}}}},{{"tool":"name2","input":{{...}}}}],"reasoning":"1 sentence"}}
When done: {{"action":"done"}}"""

            route_response = await self.model_router.get_completion(
                prompt=route_prompt,
                system_prompt=self._build_system_prompt("Pick the next tool(s). Return valid JSON only."),
                capability=ModelCapability.ANALYSIS,
                max_tokens=ROUTE_MAX_TOKENS,
                temperature=0.0,
                json_mode=True,
                caller_context="agent_loop_reason",
            )
            route_text = route_response.get("response", "{}") if isinstance(route_response, dict) else str(route_response)

            try:
                action = json.loads(route_text)
            except json.JSONDecodeError:
                match = re.search(r'\{.*\}', route_text, re.DOTALL)
                if match:
                    try:
                        action = json.loads(match.group())
                    except json.JSONDecodeError:
                        logger.warning(f"[AGENT] Reason JSON parse failed (iter {i}), ending loop.\nRaw text: {route_text[:500]}")
                        action = {"action": "done"}
                else:
                    logger.warning(f"[AGENT] Reason returned non-JSON (iter {i}), ending loop.\nRaw text: {route_text[:500]}")
                    action = {"action": "done"}

            if action.get("action") == "done":
                # ── STRUCTURAL: Can't be "done" if you haven't started ──
                # Fall back to portfolio company names when no @mentions extracted
                extracted_names = (entities or {}).get("companies", [])
                if not extracted_names:
                    grid_company_names = (
                        self.shared_data.get("matrix_context", {}).get("companyNames")
                        or self.shared_data.get("matrix_context", {}).get("company_names")
                        or []
                    )
                    if grid_company_names:
                        extracted_names = list(grid_company_names)

                # Safety: can't quit on iteration 0 without calling a single tool.
                # Use TaskLedger to dispatch ALL pending tasks in parallel.
                if i == 0 and not tool_results:
                    # ── Memo short-circuit: if we already have company data and
                    # the request is analytical, jump straight to generate_memo
                    # instead of re-running the whole data-gathering chain. ──
                    _chain = classification.suggested_chain if classification and classification.suggested_chain else []
                    _memo_intents = {
                        "memo_writing", "memo_generation", "report_writing",
                        "portfolio_analysis", "company_analysis", "performance_review",
                        "financing_analysis", "deal_analysis", "comparison",
                    }
                    _has_companies = bool(self.shared_data.get("companies"))
                    _intent = classification.intent if classification else ""
                    # Also check if the prompt is analytical (multi-company, portfolio-level)
                    _lower_prompt = prompt.lower()
                    _is_analytical = any(kw in _lower_prompt for kw in [
                        "analyz", "performance", "winner", "loser", "financ",
                        "runway", "burn", "health", "review", "assess", "evaluat",
                        "compare", "rank", "score", "bridge", "follow-on", "debt",
                    ])
                    if (
                        _has_companies
                        and (_intent in _memo_intents or _is_analytical or "generate_memo" in (_chain or []))
                    ):
                        _memo_type = None
                        _lower = prompt.lower()
                        if "followon" in _lower or "follow-on" in _lower or "pro rata" in _lower:
                            _memo_type = "followon"
                        elif "lp report" in _lower or "quarterly" in _lower:
                            _memo_type = "lp_report"
                        elif "gp" in _lower and ("strategy" in _lower or "update" in _lower):
                            _memo_type = "gp_strategy"
                        elif "comparison" in _lower or "compare" in _lower:
                            _memo_type = "comparison"
                        _memo_input: Dict[str, Any] = {"prompt": prompt}
                        if _memo_type:
                            _memo_input["memo_type"] = _memo_type
                        action = {
                            "action": "call_tool",
                            "tool": "generate_memo",
                            "input": _memo_input,
                            "reasoning": f"Shared data already has {len(self.shared_data['companies'])} companies — generating memo directly (intent: {_intent})",
                        }
                        logger.info(
                            f"[AGENT_LOOP] Memo short-circuit: jumping to generate_memo with "
                            f"{len(self.shared_data['companies'])} companies"
                        )
                    else:
                        # Use TaskLedger to fire ALL pending goals in parallel
                        pending_actions = ledger.next_actions()
                        if len(pending_actions) > 1:
                            action = {
                                "action": "call_tools",
                                "tools": pending_actions,
                                "reasoning": f"Executing {len(pending_actions)} pending tasks in parallel",
                            }
                            logger.info(f"[AGENT_LOOP] Iter 0 quit blocked — dispatching {len(pending_actions)} ledger tasks in parallel")
                        elif len(pending_actions) == 1:
                            action = {
                                "action": "call_tool",
                                **pending_actions[0],
                                "reasoning": "Executing pending task",
                            }
                            logger.info(f"[AGENT_LOOP] Iter 0 quit blocked — dispatching 1 ledger task: {pending_actions[0]['tool']}")
                        else:
                            action = {
                                "action": "call_tool",
                                "tool": "query_portfolio",
                                "input": {"query": prompt},
                                "reasoning": "Fallback — no pending ledger tasks",
                            }
                            logger.info("[AGENT_LOOP] Iter 0 quit blocked — fallback to query_portfolio")
                else:
                    break

            # --- Determine tool(s) to execute ---
            reasoning = action.get("reasoning", "")

            # Support both single tool and parallel tools
            if action.get("action") == "call_tools" and action.get("tools"):
                # Parallel: multiple independent tools
                raw_tools = action["tools"]
                tool_calls = [
                    {"tool": t.get("tool", ""), "input": t.get("input", {})}
                    for t in raw_tools if t.get("tool")
                ]
            else:
                # Single tool (existing behavior)
                tool_calls = [{"tool": action.get("tool", ""), "input": action.get("input", {})}]

            # Build PlanSteps for parallel execution (or single)
            iter_steps: List[PlanStep] = []
            for j, tc in enumerate(tool_calls):
                step_id = f"step-{i}-{j}" if len(tool_calls) > 1 else f"step-{i}"
                ps = PlanStep(
                    id=step_id,
                    label=f"{tc['tool']}: {reasoning}",
                    tool=tc["tool"],
                    inputs=tc["input"],
                    status="pending",
                )
                iter_steps.append(ps)
                plan_steps.append({"id": step_id, "label": ps.label, "status": "running"})

            parallel_label = " + ".join(tc["tool"] for tc in tool_calls)
            yield {
                "type": "progress",
                "stage": "agent_step",
                "message": f"{parallel_label}: {reasoning}" if len(tool_calls) > 1 else reasoning,
                "plan_steps": plan_steps,
            }

            # --- ACT (Python service call(s), no LLM) ---
            if len(iter_steps) > 1:
                # Parallel execution via asyncio.gather
                iter_results = await self._execute_tools_parallel(iter_steps, plan=session_plan)
            else:
                # Single tool — direct call (preserves existing behaviour)
                single_step = iter_steps[0]
                result = await self._execute_tool(single_step.tool, single_step.inputs)
                if session_plan:
                    if "error" in result:
                        session_plan.mark_failed(single_step.id, result.get("error", ""))
                    else:
                        session_plan.mark_done(single_step.id, result)
                iter_results = [{"tool": single_step.tool, "input": single_step.inputs,
                                 "output": result, "step_id": single_step.id}]

            # Invalidate fingerprint cache after tool execution
            if state:
                state.mark_dirty()

            # Process results from all tool(s) in this iteration
            last_tool_name = ""
            last_result: Dict[str, Any] = {}
            for tr in iter_results:
                tool_results.append(tr)
                t_name = tr["tool"]
                t_result = tr.get("output", {})
                last_tool_name = t_name
                last_result = t_result

                # Collect side effects and stream memo sections to frontend
                if isinstance(t_result, dict):
                    if "memo_sections" in t_result:
                        for ms in t_result["memo_sections"]:
                            # Normalize: ensure id exists
                            if "id" not in ms:
                                ms["id"] = f"auto_{len(memo_sections)}"
                            memo_sections.append(ms)
                            # Stream each section to frontend in real-time
                            _streamed_memo_ids.add(ms.get("id", id(ms)))
                            yield {"type": "memo_section", "section": ms}
                    if "chart_data" in t_result and isinstance(t_result["chart_data"], list):
                        for cd in t_result["chart_data"]:
                            charts.append(cd)
                            yield {"type": "chart_data", "chart": cd}
                    if "grid_command" in t_result:
                        grid_commands.append(t_result["grid_command"])
                    if "grid_commands" in t_result and isinstance(t_result["grid_commands"], list):
                        grid_commands.extend(t_result["grid_commands"])
                    if "chart_config" in t_result:
                        charts.append(t_result["chart_config"])
                    if "suggestion" in t_result:
                        suggestions.append(t_result["suggestion"])
                    if "todo" in t_result:
                        todo_items.append(t_result["todo"])

                # NOTE: Do NOT persist grid_commands to DB here — the frontend handles
                # persistence via handleGridCommandsFromBackend → addServiceSuggestion().
                # The suggest_grid_edit tool already persists its own suggestions directly.
                # Writing here too caused duplicate suggestions in pending_suggestions.

                # Propagate analysis outputs from tool results to shared_data
                # so downstream tools (memo, synthesis) can find them
                if isinstance(t_result, dict):
                    _analysis_keys = [
                        "valuation", "cap_tables", "scenario_analysis",
                        "exit_modeling", "followon_strategy", "reserve_forecast",
                        "fund_metrics", "portfolio_health", "fund_scenarios",
                        "regression_analysis", "forecast_results",
                    ]
                    for _ak in _analysis_keys:
                        if _ak in t_result and t_result[_ak]:
                            self.shared_data[_ak] = t_result[_ak]

                if isinstance(t_result, dict) and "error" in t_result:
                    failed_tools.add(t_name)
                    logger.warning(f"[AGENT] Tool {t_name} failed, excluding from future iterations: {t_result['error']}")

                # Record into PlanContext
                plan_ctx.record_finding(f"{t_name}_{tr.get('step_id', i)}", self._truncate(json.dumps(t_result), 1500))

                # Compress tool result into SessionMemo (context anti-rot)
                if session_memo:
                    session_memo.add(
                        tool=t_name,
                        inputs=tr.get("input", {}),
                        result=t_result if isinstance(t_result, dict) else {"result": str(t_result)[:200]},
                    )

            # Update TaskLedger with this iteration's results
            ledger.update(iter_results)
            logger.info(f"[AGENT_LOOP] Ledger after iter {i}: {ledger.pending_summary()}")

            # Update plan_steps statuses for frontend
            for ps_dict in plan_steps:
                for tr in iter_results:
                    if ps_dict["id"] == tr.get("step_id"):
                        out = tr.get("output", {})
                        ps_dict["status"] = "failed" if (isinstance(out, dict) and "error" in out) else "done"

            yield {
                "type": "progress",
                "stage": "agent_step",
                "message": f"{parallel_label} complete" if len(tool_calls) > 1 else f"{last_tool_name} complete",
                "plan_steps": plan_steps,
            }

            # --- Check if SessionPlan is fully resolved (skip REASON/REFLECT) ---
            if session_plan and session_plan.is_complete():
                logger.info(f"[AGENT_LOOP] SessionPlan {session_plan.plan_id} fully resolved")
                # Persist completed plan to DB
                fund_id = self.shared_data.get("fund_context", {}).get("fundId")
                session_plan.persist_to_db(fund_id=fund_id)
                break

            # --- REFLECT (Python-first, LLM fallback) ---
            # Hard rule: if no tools have run yet, skip reflection and force another iteration
            if not tool_results:
                logger.info(f"[AGENT_LOOP] No tools called yet (iter {i}), skipping reflect — forcing tool execution")
                continue

            # Build scoreboard from tool_results (pure Python)
            if Scoreboard:
                _scoreboard = Scoreboard.from_tool_results(tool_results)
                _portfolio_size = state.portfolio_size if state else len(
                    self.shared_data.get("matrix_context", {}).get("companyNames")
                    or self.shared_data.get("matrix_context", {}).get("company_names")
                    or []
                )
            else:
                _scoreboard = None
                _portfolio_size = 0

            # Phase 2: TaskLedger completion check (deterministic, zero LLM tokens)
            # Only gate on ledger if it was populated from an actual plan
            if ledger.goals and ledger.is_complete():
                logger.info(f"[REFLECT_PY] All {len(ledger.goals)} plan tasks complete — breaking loop")
                break
            if ledger.goals:
                pending = ledger.pending_goals()
                ready = ledger.next_actions()
                logger.info(
                    f"[REFLECT_PY] iter={i} — {len(pending)} pending, "
                    f"{len(ready)} ready: {[g.id for g in pending]}"
                )
                continue  # ledger is tracking — skip LLM reflection unless stale

            # ── Fallback: LLM-based reflection (only when CompletionChecker unavailable) ──
            # Prefer the pre-built Scoreboard which counts side-effect
            # memo_sections from tool outputs.  Also take max with the live
            # memo_sections list to capture sections from non-tool paths.
            if _scoreboard:
                fetch_count = _scoreboard.fetch_count
                valuation_count = _scoreboard.valuation_count
                chart_count = _scoreboard.chart_count
                suggest_count = _scoreboard.suggest_count
                auto_suggest_count = _scoreboard.auto_suggest_count
                memo_section_count = max(_scoreboard.memo_section_count, len(memo_sections))
                memo_count = _scoreboard.memo_count
                if memo_section_count >= 2 and memo_count == 0:
                    memo_count = 1
                portfolio_size = _portfolio_size
            else:
                suggest_count = sum(1 for r in tool_results if r["tool"] in ("suggest_grid_edit", "suggest_action"))
                auto_suggest_count = sum(
                    r.get("output", {}).get("auto_suggestions_count", 0)
                    for r in tool_results
                    if isinstance(r.get("output"), dict)
                )
                valuation_count = sum(1 for r in tool_results if r["tool"] == "run_valuation")
                fetch_count = sum(1 for r in tool_results if r["tool"] in ("fetch_company_data", "resolve_data_gaps"))
                memo_section_count = len(memo_sections)
                memo_tool_count = sum(1 for r in tool_results if r["tool"] in ("generate_memo", "run_report", "write_to_memo"))
                memo_count = memo_tool_count if memo_tool_count else (1 if memo_section_count >= 2 else 0)
                chart_count = sum(1 for r in tool_results if r["tool"] == "generate_chart")
                grid_company_names = (
                    self.shared_data.get("matrix_context", {}).get("companyNames")
                    or self.shared_data.get("matrix_context", {}).get("company_names")
                    or []
                )
                portfolio_size = len(grid_company_names)

            _goals_checklist = ""
            if _extracted_goals:
                _goals_checklist = "GOALS TO ACHIEVE:\n" + "\n".join(
                    f"  - [{g.get('id')}] {g.get('description')} (check: {g.get('check', 'N/A')})"
                    for g in _extracted_goals
                ) + "\n\n"

            # Detect redundant tool calls
            _tool_call_counts: Dict[str, int] = {}
            for r in tool_results:
                _tool_call_counts[r["tool"]] = _tool_call_counts.get(r["tool"], 0) + 1
            _repeated_tools = [f"{t}(x{c})" for t, c in _tool_call_counts.items() if c >= 2]
            _dedup_warning = ""
            if _repeated_tools:
                _dedup_warning = (
                    f"\nWARNING: These tools were called multiple times: {', '.join(_repeated_tools)}. "
                    "STOP repeating them. Either use generate_memo to write findings, or say done.\n"
                )

            # Check if we have data but no memo — force escalation
            _has_data_no_memo = fetch_count > 0 and memo_count == 0 and memo_section_count == 0
            _escalation_hint = ""
            if _has_data_no_memo:
                _escalation_hint = (
                    "\nIMPORTANT: Data has been fetched but NO memo/analysis has been written yet. "
                    "The memo is the context bridge — call generate_memo or write_to_memo NOW. "
                    "Do NOT mark as sufficient without writing to the memo.\n"
                )

            reflect_prompt = f"""Task: {prompt}
Tools called so far: {', '.join(r['tool'] for r in tool_results)}
Latest tool: {last_tool_name}
Result summary: {self._truncate(json.dumps(last_result), 1500)}
{_dedup_warning}{_escalation_hint}
{_goals_checklist}SCOREBOARD:
- Portfolio companies: {portfolio_size}
- Companies fetched/enriched: {fetch_count}
- Valuations run: {valuation_count}
- Memos generated: {memo_count}
- Memo sections (auto-emitted from tools): {memo_section_count}
- Charts generated: {chart_count}
- Suggestions emitted: {suggest_count + auto_suggest_count}

Look at the user's request and the scoreboard. Is the request fully satisfied?
- CRITICAL: If fetch_count > 0 but memo_count is 0 and memo_section_count is 0, it is NOT sufficient.
  The user expects a written deliverable in the memo, not a chat dump.
- If memo sections > 0, the memo already has content from tools — do NOT loop to write more sections.
- If the user asked for valuations but valuation_count is 0, that's not satisfied.
- If the user asked about the whole portfolio but only some companies were processed, that's not satisfied.
- resolve_data_gaps fills gaps but does NOT count as a valuation — you still need run_valuation.
- NEVER mark as sufficient if the same tool was called 3+ times — you're stuck in a loop.
  Call generate_memo to write what you have, then say done.

{{"sufficient":true|false,"reason":"1 sentence","missing":"what tool to call next if not sufficient"}}"""

            reflect_response = await self.model_router.get_completion(
                prompt=reflect_prompt,
                system_prompt="You are a completion checker. Look at the user's request and the scoreboard numbers. Is the request fully satisfied? Return JSON only.",
                capability=ModelCapability.FAST,
                max_tokens=REFLECT_MAX_TOKENS,
                temperature=0.0,
                json_mode=True,
                caller_context="agent_loop_reflect",
            )
            reflect_text = reflect_response.get("response", "{}") if isinstance(reflect_response, dict) else str(reflect_response)

            try:
                reflection = json.loads(reflect_text)
            except json.JSONDecodeError as e:
                logger.warning(f"[AGENT] Reflect JSON parse failed (iter {i}): {e}\nRaw text: {reflect_text[:500]}")
                reflection = {"sufficient": False, "reason": "JSON parse failed, continuing"}

            logger.info(f"[REFLECT] iter={i} sufficient={reflection.get('sufficient')} reason={reflection.get('reason', '')}")

            # ── Anti-loop: force memo generation if stuck ──
            # If we've done 3+ iterations with data but no memo, force generate_memo
            _any_repeated = any(c >= 3 for c in _tool_call_counts.values())
            if i >= 2 and fetch_count > 0 and memo_count == 0 and memo_section_count == 0:
                logger.warning(f"[REFLECT] iter={i} — data fetched but no memo after {i+1} iterations, forcing generate_memo")
                action = {
                    "action": "call_tool",
                    "tool": "generate_memo",
                    "input": {"prompt": prompt},
                    "reasoning": f"Forced memo generation — {fetch_count} fetches but 0 memo sections after {i+1} iterations",
                }
                # Don't break — let the tool execute in the next iteration
                continue
            elif _any_repeated and i >= 2:
                logger.warning(f"[REFLECT] iter={i} — tool loop detected ({_repeated_tools}), forcing done")
                break

            if reflection.get("sufficient"):
                break

        # --- SYNTHESIZE (full model, one call) ---
        # When grid_commands are the primary output, enrich synth context with
        # a human-readable summary so the synthesizer can produce meaningful prose
        # alongside the grid mutations (composite response).
        grid_summary_ctx = ""
        if grid_commands:
            edit_cmds = [c for c in grid_commands if c.get("action") == "edit"]
            run_cmds = [c for c in grid_commands if c.get("action") == "run"]
            parts = []
            if edit_cmds:
                parts.append(f"Set {len(edit_cmds)} cell value(s) in the grid")
            if run_cmds:
                action_ids = list({c.get("actionId", "unknown") for c in run_cmds})
                parts.append(f"Queued {len(run_cmds)} service run(s): {', '.join(action_ids)}")
            grid_summary_ctx = f'\nGrid operations performed: {"; ".join(parts)}.'

        synth_context = json.dumps([
            {
                "tool": r["tool"],
                "output": self._truncate(json.dumps(r.get("output", {})), 3000),
            }
            for r in tool_results
        ]) + grid_summary_ctx

        # Inject corrections from feedback loop — fall back to grid companies when no @mentions
        entity_companies = (entities or {}).get("companies", [])
        if not entity_companies:
            entity_companies = list(
                self.shared_data.get("matrix_context", {}).get("companyNames")
                or self.shared_data.get("matrix_context", {}).get("company_names")
                or []
            )
        first_company = entity_companies[0] if entity_companies else None
        corrections = await self._get_recent_corrections(prompt, first_company)
        correction_ctx = ""
        if corrections:
            correction_ctx = f"\n\nUser has previously corrected: {'; '.join(corrections[:3])}\nAdjust your response accordingly."

        memo_context = f"\nWorking memo context:\n{memo_text[:2000]}\n" if memo_text else ""

        # Inject portfolio intelligence context so synthesis sees actual portfolio data
        portfolio_text = self._build_portfolio_intelligence()
        portfolio_context = f"\n\nPortfolio Context:\n{portfolio_text}\n" if portfolio_text.strip() else ""

        # Determine if we should use lightweight memo (structured docs) vs raw text
        has_company_data = bool(self.shared_data.get("companies"))
        has_portfolio_data = bool(self.shared_data.get("portfolio_enrichment") or self.shared_data.get("fund_metrics"))
        use_memo_format = has_company_data or has_portfolio_data

        if use_memo_format:
            # Structured markdown synthesis — headings ensure parseMarkdownToSections works
            synth_prompt = f"""User asked: {prompt}{memo_context}{portfolio_context}
Tool results:
{synth_context}{correction_ctx}

Write a structured investment analysis using markdown headings and paragraphs.

FORMAT:
## Executive Summary
1-2 paragraphs with the key finding and investment narrative.

## Key Metrics
A markdown table of the most important numbers. Use | Col | Col | format.

## Analysis
1-2 paragraphs of detailed analysis with specific numbers.

## Risks & Next Steps
1 paragraph on risks, 1-2 concrete next steps.

Rules:
- Every section MUST have a ## heading
- Start each section with the most important finding
- Only cite numbers from tool results — do not invent figures
- Bold key metrics: **$150M ARR**, **2.5x revenue multiple**
- If data is estimated, say "[Est]" with confidence
- NO filler: never start with "Based on my analysis", "Let me break this down", "Here's what I found"

ABSOLUTE RULES:
- NEVER write "cannot be completed", "no data available", "data retrieval returned no results"
- If you have estimates/benchmarks, present them with confidence levels
- If you have partial data, show what you have and flag what's estimated
- Present ranges (low/mid/high) rather than saying "unknown"
- Always show SOMETHING — even stage benchmarks are more useful than 'no data'"""

            synthesis_response = await self.model_router.get_completion(
                prompt=synth_prompt,
                system_prompt="You are a portfolio CFO. Write a structured analysis with ## markdown headings for each section. Use **bold** for key numbers. Use markdown tables for comparisons. Start with the key finding. NEVER say 'I don't have data'. If estimated, say '[Est: confidence%]'.",
                capability=ModelCapability.ANALYSIS,
                max_tokens=2000,
                temperature=0.3,
                caller_context="agent_loop_synthesize_brief",
            )
            synthesis = synthesis_response.get("response", "") if isinstance(synthesis_response, dict) else str(synthesis_response)

            # Memo strategy: always run LightweightMemoService for template-driven
            # narratives + charts. Progressive tool sections are supplementary data.
            tool_emitted_sections = list(memo_sections)  # save tool-emitted sections
            _memo_artifact_appended = False  # track whether we already added to memo_artifacts
            try:
                from app.services.lightweight_memo_service import LightweightMemoService
                memo_svc = LightweightMemoService(model_router=self.model_router, shared_data=self.shared_data)
                yield {
                    "type": "progress",
                    "stage": "memo_generation",
                    "message": "Generating investment memo narratives...",
                }
                memo_result = await asyncio.wait_for(
                    memo_svc.generate(prompt=prompt),
                    timeout=120,  # 2 minutes max for full memo generation
                )
                if memo_result and memo_result.get("sections"):
                    # Use LightweightMemoService output as the primary memo.
                    # The 3-shot pipeline already produces complete output —
                    # do NOT append raw tool_emitted_sections or synthesis
                    # as they dump JSON noise into the memo.
                    memo_sections.clear()
                    memo_sections.extend(memo_result["sections"])
                    # Extract chart sections from memo into top-level charts array
                    for sec in memo_result["sections"]:
                        if sec.get("type") == "chart" and sec.get("chart"):
                            charts.append(sec["chart"])
                    if charts:
                        logger.info(f"[AGENT_LOOP] Extracted {len(charts)} charts from memo sections")
                    async with self.shared_data_lock:
                        artifacts = self.shared_data.get("memo_artifacts", [])
                        artifacts.append(memo_result)
                        self.shared_data["memo_artifacts"] = artifacts
                    _memo_artifact_appended = True
                    logger.info(f"[AGENT_LOOP] Full memo via LightweightMemoService: {len(memo_sections)} sections")
                elif tool_emitted_sections:
                    # LightweightMemoService returned nothing — fall back to tool sections
                    memo_sections.append({"type": "heading2", "content": "Summary"})
                    memo_sections.append({"type": "paragraph", "content": synthesis})
                    logger.info(f"[AGENT_LOOP] Fallback to tool-accumulated memo: {len(memo_sections)} sections")
            except asyncio.TimeoutError:
                logger.warning("[AGENT_LOOP] Lightweight memo generation timed out after 120s — falling back to tool sections")
                if tool_emitted_sections:
                    memo_sections.append({"type": "heading2", "content": "Summary"})
                    memo_sections.append({"type": "paragraph", "content": synthesis})
            except Exception as e:
                logger.warning(f"[AGENT_LOOP] Lightweight memo generation failed: {e}")
                if tool_emitted_sections:
                    memo_sections.append({"type": "heading2", "content": "Summary"})
                    memo_sections.append({"type": "paragraph", "content": synthesis})
            # Only append a second artifact if LightweightMemoService didn't
            # already write one (avoids double-appending the same memo).
            if not _memo_artifact_appended:
                async with self.shared_data_lock:
                    artifacts = self.shared_data.get("memo_artifacts", [])
                    if memo_sections:
                        artifacts.append({
                            "format": "docs",
                            "title": f"Analysis: {prompt[:60]}",
                            "sections": memo_sections,
                            "memo_type": "full",
                            "metadata": {
                                "section_count": len(memo_sections),
                                "generated_at": datetime.now().isoformat(),
                                "source": "tool_accumulated",
                            },
                        })
                        self.shared_data["memo_artifacts"] = artifacts
        else:
            synth_prompt = f"""User asked: {prompt}{memo_context}{portfolio_context}
Tool results:
{synth_context}{correction_ctx}

Write a thorough analysis as flowing prose. Structure:
1. Direct answer (1 paragraph) — lead with the key number or finding
2. Analysis (1-2 paragraphs) — weave metrics into narrative sentences
3. Risks & outlook (1 paragraph) — specific risks with quantified impact where possible
Rules:
- NO markdown tables, bullet lists, or metric dumps — the frontend renders those separately
- NO filler phrases: never start with "Based on", "Let me", "Here's what", "I'd be happy to"
- Only cite numbers that appear in the tool results — do not invent or round aggressively
- When exact data is missing, say "estimated at [value] (confidence: X%)" with your reasoning
- Write like a professional investment memo, not a chatbot response

ABSOLUTE RULES:
- NEVER write "cannot be completed", "no data available", "data retrieval returned no results"
- If you have estimates/benchmarks, present them with confidence levels
- Present ranges (low/mid/high) rather than saying "unknown"
- Always show SOMETHING — stage benchmarks > "no data"."""

            synthesis_response = await self.model_router.get_completion(
                prompt=synth_prompt,
                system_prompt="You are an investment analyst. Write professional analysis as flowing prose. Only use numbers from tool results — never invent figures. If data is estimated, state confidence level. The frontend renders tables, charts, and metric cards separately — do not duplicate them in prose. Start with the most important finding. NEVER say 'I don't have data', 'cannot be completed', or 'no data available' — present estimates with confidence ranges instead.",
                capability=ModelCapability.ANALYSIS,
                max_tokens=SYNTH_MAX_TOKENS,
                temperature=0.3,
                caller_context="agent_loop_synthesize",
            )
            synthesis = synthesis_response.get("response", "") if isinstance(synthesis_response, dict) else str(synthesis_response)

        # Build condensed working memory for session continuity
        # Use SessionMemo entries (compressed) instead of raw JSON (bloated)
        if session_memo and session_memo.entries:
            working_memory = [
                {"tool": "session_memo", "summary": entry}
                for entry in session_memo.entries
            ]
        else:
            working_memory = [
                {"tool": r["tool"], "summary": self._truncate(json.dumps(r.get("output", {})), 1000)}
                for r in tool_results
            ]

        # Detect output format from tool results — deck/memo take priority over analysis
        detected_format = "analysis"
        extra_result_fields: Dict[str, Any] = {}
        for r in tool_results:
            out = r.get("output", {})
            if isinstance(out, dict):
                if out.get("format") == "deck" and out.get("slides"):
                    detected_format = "deck"
                    # Forward deck fields: slides, theme, metadata, companies
                    for dk in ("slides", "theme", "metadata", "companies", "deck"):
                        if dk in out:
                            extra_result_fields[dk] = out[dk]
                elif out.get("format") == "docs" and out.get("sections"):
                    detected_format = "docs"
                    extra_result_fields["sections"] = out["sections"]
                    if out.get("title"):
                        extra_result_fields["title"] = out["title"]
                    if out.get("memo_type"):
                        extra_result_fields["memo_type"] = out["memo_type"]
                    if out.get("is_resumable"):
                        extra_result_fields["is_resumable"] = True

        # If memo_sections were generated (lightweight memo), promote to docs format
        if memo_sections and detected_format == "analysis":
            detected_format = "docs"
            extra_result_fields["sections"] = memo_sections
            # Extract title from first heading section
            for ms in memo_sections:
                if ms.get("type") == "heading1":
                    extra_result_fields["title"] = ms.get("content", "Analysis")
                    break

        # Collect memo artifacts accumulated during this request
        memo_artifacts = self.shared_data.get("memo_artifacts", [])

        # Finalize PlanContext with reasoning + working memory
        plan_ctx.record_reasoning(f"synthesized {len(tool_results)} tool results")
        plan_ctx_dict = plan_ctx.to_dict() if plan_ctx else None

        # Build serialized SessionPlan state for frontend
        session_plan_dict = session_plan.to_dict() if session_plan else None

        # Build artifacts list for frontend rendering
        result_artifacts: List[Dict[str, Any]] = []
        chart_target = "memo" if detected_format in ("docs", "document") else "chat"
        # When sections are already carried in the docs-format result, the
        # frontend will render them directly — don't duplicate as individual
        # memo_section artifacts (they were either already streamed in
        # real-time or are part of the docs payload).
        _sections_in_result = "sections" in extra_result_fields
        # Convert charts to memo-compatible sections for fallback memo_updates
        charts_as_memo = [{"type": "chart", "chart": c, "id": f"chart_{i}"} for i, c in enumerate(charts)]
        for chart in charts:
            result_artifacts.append({"type": "chart", "action": "append", "data": chart, "target": chart_target})
        for gc in grid_commands:
            result_artifacts.append({"type": "grid_command", "action": "replace", "data": gc, "target": "grid"})
        for sg in suggestions:
            result_artifacts.append({"type": "suggestion", "action": "suggest", "data": sg, "target": "chat"})
        if not _sections_in_result:
            for ms in memo_sections:
                ms_id = ms.get("id", id(ms))
                if ms_id not in _streamed_memo_ids:
                    result_artifacts.append({"type": "memo_section", "action": "append", "data": ms, "target": "memo"})
        for td in todo_items:
            result_artifacts.append({"type": "todo", "action": "suggest", "data": td, "target": "chat"})

        # For docs format, strip internal planner noise that pollutes
        # the frontend when section extraction fails.
        # Also strip from extra_result_fields to prevent leaking via **spread
        if detected_format == "docs":
            for noise_key in ("plan_steps", "working_memory", "session_plan", "plan_context"):
                extra_result_fields.pop(noise_key, None)

        yield {
            "type": "complete",
            "result": {
                "content": synthesis,
                "format": detected_format,
                **extra_result_fields,
                "grid_commands": grid_commands,
                "suggestions": suggestions,
                "todos": todo_items if todo_items else None,
                # Always send memo_updates so frontend writes to memo, not just chat.
                # When sections are in result, send them as replace; otherwise append non-streamed ones.
                "memo_updates": (
                    {"action": "replace", "sections": memo_sections} if _sections_in_result and memo_sections
                    else ({"action": "append", "sections": [s for s in memo_sections if s.get("id", id(s)) not in _streamed_memo_ids]}
                          if memo_sections and any(s.get("id", id(s)) not in _streamed_memo_ids for s in memo_sections)
                          else ({"action": "append", "sections": charts_as_memo} if charts else None))
                ),
                "plan_steps": None if detected_format == "docs" else plan_steps,
                "charts": charts,
                "citations": self._extract_citations_from_results(tool_results),
                "working_memory": None if detected_format == "docs" else working_memory,
                "memo_artifacts": memo_artifacts if memo_artifacts else None,
                "artifacts": result_artifacts if result_artifacts else None,
                "session_plan": None if detected_format == "docs" else session_plan_dict,
                "plan_context": None if detected_format == "docs" else plan_ctx_dict,
            },
        }

    async def process_request_stream(
        self,
        prompt: str,
        output_format: str = "analysis",
        context: Optional[Dict[str, Any]] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Process a request with direct execution (streaming disabled)
        """
        try:
            # Initialize session
            if not self.session:
                self.session = aiohttp.ClientSession()
            
            # Clear all caches at the start of each request
            # CRITICAL: Preserve companies if they were already added to shared_data
            companies_to_preserve = self.shared_data.get('companies', [])
            
            self._tavily_cache.clear()
            self._company_cache.clear()
            self.shared_data.clear()
            
            # Restore companies if they existed
            if companies_to_preserve:
                self.shared_data['companies'] = companies_to_preserve
                logger.info(f"[REQUEST_START] Cleared caches but preserved {len(companies_to_preserve)} companies in shared_data")
            else:
                logger.info(f"[REQUEST_START] Cleared all caches and shared data for new request")

            # Start per-request budget tracking
            budget = self.model_router.start_budget(max_cost=2.0, max_tokens=500_000)
            
            # Store context in shared_data if provided
            if context:
                async with self.shared_data_lock:
                    self.shared_data['fund_context'] = dict(context)
                    # Control centre: matrix context for grid-aware skills (rowIds, companyNames, columns)
                    matrix_ctx = context.get('matrix_context') or context.get('matrixContext')
                    if matrix_ctx:
                        self.shared_data['matrix_context'] = matrix_ctx
                        logger.info(f"[MATRIX_CONTEXT] Stored {len(matrix_ctx.get('rowIds', []) or matrix_ctx.get('companyNames', []))} rows")
                    # Agent context for conversation continuity
                    agent_ctx = context.get('agent_context')
                    if agent_ctx:
                        self.shared_data['agent_context'] = agent_ctx
                        logger.info(f"[AGENT_CONTEXT] Stored agent context: {list(agent_ctx.keys())}")
                    # Restore analysis manifest from prior request — marks which derived
                    # data was computed so the agent knows what to re-fetch vs skip
                    analysis_manifest = context.get('analysis_manifest')
                    if analysis_manifest and isinstance(analysis_manifest, dict):
                        from app.services.session_state import SessionState as _SS
                        _ss = _SS(self.shared_data)
                        _ss.restore_manifest(analysis_manifest)
                        logger.info(f"[MANIFEST] Restored analysis manifest: {list(analysis_manifest.keys())}")
                logger.info(f"[CONTEXT] Stored fund context with keys: {list(context.keys())}")
                # Log key fund parameters if available
                if 'fund_size' in context:
                    logger.info(f"[CONTEXT] Fund size: ${context['fund_size']/1e6:.0f}M")
                if 'remaining_capital' in context:
                    logger.info(f"[CONTEXT] Remaining capital: ${context['remaining_capital']/1e6:.0f}M")
                
                # If fund_size missing, try to extract from prompt
                if 'fund_size' not in context and 'portfolio_contribution' not in context:
                    fund_params = self._extract_fund_params_from_prompt(prompt)
                    if fund_params:
                        self.shared_data['fund_context'].update(fund_params)
                        logger.info(f"[CONTEXT] Extracted fund params from prompt: {fund_params}")
            
            # Extract entities from prompt
            yield {
                "type": "progress",
                "stage": "initialization",
                "message": "Analyzing request and extracting entities"
            }
            
            entities = await self._extract_entities(prompt)
            
            # Phase 1: Merge context.companies (all @mentions from frontend) into entities
            if context and context.get("companies"):
                ctx_companies = context["companies"]
                if isinstance(ctx_companies, list) and ctx_companies:
                    existing = entities.get("companies") or []
                    merged = list(dict.fromkeys(ctx_companies + [c for c in existing if c not in ctx_companies]))
                    entities["companies"] = merged
                    logger.info(f"[ENTITY_EXTRACTION] Merged context.companies: {merged}")
            
            # NEW: Merge extracted fund context into shared_data
            if entities:
                fund_keys = ['fund_size', 'remaining_capital', 'deployed_capital', 'fund_year', 
                             'fund_quarter', 'portfolio_count', 'dpi', 'tvpi', 'target_tvpi']
                extracted_fund_context = {k: v for k, v in entities.items() if k in fund_keys and v is not None}
                
                if extracted_fund_context:
                    async with self.shared_data_lock:
                        if 'fund_context' not in self.shared_data:
                            self.shared_data['fund_context'] = {}
                        self.shared_data['fund_context'].update(extracted_fund_context)
                        logger.info(f"[ENTITY_EXTRACTION] Updated fund_context: {extracted_fund_context}")
                else:
                    logger.info(f"[ENTITY_EXTRACTION] No fund context extracted from entities")
            
            # ---- Plan resumption: detect "resume plan" intent and hydrate shared_data ----
            plan_resume_signals = [
                "resume plan", "continue the plan", "do this plan",
                "execute the plan", "pick up where", "resume where",
            ]
            if any(s in prompt.lower() for s in plan_resume_signals):
                await self._try_load_and_hydrate_plan()

            # ---- Entity context: classify mentioned companies for agent awareness ----
            # NOTE: No auto-triggering — the agent decides when/whether to fetch.
            # We just categorize companies as new vs in-grid for agent context.
            extracted_companies = entities.get("companies", [])
            matrix_ctx = self.shared_data.get("matrix_context") or {}
            grid_company_names = [n.lower() for n in (matrix_ctx.get("companyNames") or [])]
            fund_id = (context or {}).get("fundId") or (context or {}).get("fund_id")

            if extracted_companies:
                new_companies = []
                sparse_companies = []
                for comp_name in extracted_companies:
                    clean_name = comp_name.replace("@", "").strip().lower()
                    if clean_name and clean_name not in grid_company_names:
                        new_companies.append(comp_name.replace("@", "").strip())
                    elif clean_name in grid_company_names:
                        grid_snapshot = matrix_ctx.get("gridSnapshot") or {}
                        grid_rows = grid_snapshot.get("rows", []) if isinstance(grid_snapshot, dict) else []
                        for row in grid_rows:
                            row_name = (row.get("companyName") or row.get("company_name") or "").lower()
                            if clean_name in row_name or row_name in clean_name:
                                cells = row.get("cells") or row.get("cellValues") or {}
                                empty_count = sum(1 for v in cells.values()
                                                  if (v.get("value") if isinstance(v, dict) else v) in (None, "", "N/A"))
                                if empty_count >= 3:
                                    sparse_companies.append(comp_name.replace("@", "").strip())
                                break

                # Store as context hints, not forced actions
                if new_companies:
                    logger.info(f"[ENTITY_CONTEXT] New companies (not in grid): {new_companies}")
                    async with self.shared_data_lock:
                        self.shared_data["mentioned_new_companies"] = new_companies
                if sparse_companies:
                    logger.info(f"[ENTITY_CONTEXT] Sparse grid data for: {sparse_companies}")
                    async with self.shared_data_lock:
                        self.shared_data["mentioned_sparse_companies"] = sparse_companies

            # ---- Complexity gate: route to agent loop, direct dispatch, or existing pipeline ----
            # Also read memo context from agent_context for augmentation
            memo_ctx = self.shared_data.get("agent_context", {}).get("memo_sections", [])
            if memo_ctx:
                memo_text = self._serialize_memo_sections(memo_ctx, limit=15)
                if memo_text:
                    logger.info(f"[MEMO_CONTEXT] Injected {len(memo_ctx)} memo sections ({len(memo_text)} chars) into context")

            # --- Build grid fingerprint BEFORE classification ---
            # SessionState provides a dense ~400 token map of what's filled/missing/inferred
            _session_state = SessionState(self.shared_data) if SessionState else None
            grid_fingerprint = _session_state.fingerprint() if _session_state else ""
            if grid_fingerprint:
                logger.info(f"[FINGERPRINT] Built grid fingerprint ({len(grid_fingerprint)} chars) for classifier")

            # --- Phase 2: LLM-based intent classification (with grid context) ---
            classification = await self._classify_intent(
                prompt, entities=entities, context=context, grid_fingerprint=grid_fingerprint
            )
            logger.info(
                f"[ORCHESTRATOR] Classification: complexity={classification.complexity}, "
                f"intent={classification.intent}, needs_portfolio={classification.needs_portfolio}, "
                f"needs_external={classification.needs_external}, confidence={classification.confidence}"
            )
            # Store classification for downstream use
            async with self.shared_data_lock:
                self.shared_data["classification"] = {
                    "complexity": classification.complexity,
                    "intent": classification.intent,
                    "suggested_chain": classification.suggested_chain,
                    "needs_portfolio": classification.needs_portfolio,
                    "needs_external": classification.needs_external,
                    "confidence": classification.confidence,
                }

            # --- Fast path: memo polish (1 cheap LLM call) ---
            if classification.intent == "memo_polish":
                result = await self._handle_memo_polish(prompt)
                budget_summary = self.model_router.end_budget() or {}
                yield {
                    "type": "complete",
                    "result": result,
                    "success": True,
                    "metadata": {"budget": budget_summary, "complexity": "simple", "intent": "memo_polish"},
                }
                return

            # --- Fast path: save memo to documents ---
            if classification.intent == "memo_save":
                result = await self._handle_memo_save()
                budget_summary = self.model_router.end_budget() or {}
                yield {
                    "type": "complete",
                    "result": result,
                    "success": True,
                    "metadata": {"budget": budget_summary, "complexity": "simple", "intent": "memo_save"},
                }
                return

            if classification.complexity == "simple":
                # Try direct dispatch — returns None if it can't handle it
                result = await self._direct_dispatch(prompt, context)
                if result is not None:
                    budget_summary = self.model_router.end_budget() or {}
                    yield {
                        "type": "complete",
                        "result": result,
                        "success": True,
                        "metadata": {"budget": budget_summary, "complexity": "simple", "intent": classification.intent},
                    }
                    return
                # Direct dispatch couldn't handle it → upgrade to complex
                logger.info("[ORCHESTRATOR] Upgrading simple→complex: direct dispatch returned None")
                classification.complexity = "complex"

            if classification.complexity == "complex":
                memo_ctx = self.shared_data.get("agent_context", {}).get("memo_sections", [])
                memo_text = self._serialize_memo_sections(memo_ctx) if memo_ctx else ""
                approved_plan = bool(context.get("approved_plan")) if context else False
                async for event in self._run_agent_loop(
                    prompt, context, memo_text=memo_text, approved_plan=approved_plan,
                    entities=entities, classification=classification,
                ):
                    yield event
                self.model_router.end_budget()
                return

            # Fallback: any remaining classification (e.g. legacy "dealflow") → skill chain pipeline
            # This should rarely be reached since the LLM classifier now returns "simple" or "complex"
            logger.info(f"[ORCHESTRATOR] Fallthrough to skill chain pipeline: complexity={classification.complexity}")

            # Phase 2: Planning for complex prompts
            planning_triggers = [
                "all", "full", "complete", "do everything", "all 5", "full analysis",
                "step by step", "comprehensive", "detailed analysis",
                # Fund-wide / multi-company valuation triggers
                "value the fund", "value all", "value every", "value entire",
                "fill in everything", "fill in every", "fill everything",
                "run valuation on all", "run valuation for all",
                "run pwerm on all", "run pwerm for all",
                "value the portfolio", "value entire portfolio",
            ]
            grid_action_triggers = [
                "run valuation", "value @", "value for", "run pwerm", "pwerm for",
                "run dcf", "dcf for", "value acme", "value mercury",
            ]
            lower_prompt = prompt.lower()
            matrix_ctx = context.get("matrix_context") or context.get("matrixContext") if context else {}
            has_matrix = bool(matrix_ctx and (matrix_ctx.get("rowIds") or matrix_ctx.get("row_ids")))
            needs_planning = (
                any(t in lower_prompt for t in planning_triggers)
                or len(entities.get("companies", [])) >= 3
                or (has_matrix and any(t in lower_prompt for t in grid_action_triggers))
            )
            
            if needs_planning:
                yield {
                    "type": "progress",
                    "stage": "planning",
                    "message": "Creating multi-step execution plan"
                }
                plan_steps = await self._execute_planning(prompt, output_format, entities)
                if plan_steps:
                    async with self.shared_data_lock:
                        self.shared_data["plan_steps"] = plan_steps
                    logger.info(f"[PLANNING] Created {len(plan_steps)} plan steps")
            
            # Build skill chain based on prompt (or plan when present)
            yield {
                "type": "progress",
                "stage": "planning",
                "message": "Building execution plan",
                "plan_steps": self.shared_data.get("plan_steps", []),
            }
            
            skill_chain = await self.build_skill_chain(prompt, output_format, entities=entities)
            
            # FORCE deck-storytelling when output_format is "deck"
            if output_format == "deck":
                logger.critical(f"[FORCE_DECK] 🟡🟡🟡 Checking deck-storytelling, chain has {len(skill_chain)} skills 🟡🟡🟡")
                
                deck_storytelling_exists = any(node.skill == "deck-storytelling" for node in skill_chain)
                logger.info(f"[FORCE_DECK] 🔒 deck-storytelling exists: {deck_storytelling_exists}")
                
                
                if not deck_storytelling_exists:
                    logger.warning(f"[FORCE_DECK] ⚠️ deck-storytelling NOT in chain! Force-adding it now...")
                    skill_chain.append(SkillChainNode(
                        skill="deck-storytelling",
                        purpose="Generate presentation (FORCED)",
                        inputs={"use_shared_data": True},
                        parallel_group=3
                    ))
                    logger.info(f"[FORCE_DECK] ✅ Force-added deck-storytelling. Chain length now: {len(skill_chain)}")
                    
                else:
                    logger.info(f"[FORCE_DECK] ✅ deck-storytelling already in chain")
            
            # Execute skill chain with real-time plan step updates
            plan_steps_snapshot = self.shared_data.get("plan_steps", [])
            yield {
                "type": "progress",
                "stage": "execution",
                "message": f"Executing {len(skill_chain)} skills",
                "plan_steps": plan_steps_snapshot,
            }

            if plan_steps_snapshot:
                # Use queue to relay real-time plan step updates during execution
                _progress_queue: asyncio.Queue = asyncio.Queue()

                async def _plan_progress_cb(steps, message=""):
                    await _progress_queue.put((steps, message))

                exec_task = asyncio.create_task(
                    self._execute_skill_chain(skill_chain, progress_callback=_plan_progress_cb)
                )

                while not exec_task.done():
                    try:
                        steps, msg = await asyncio.wait_for(_progress_queue.get(), timeout=1.0)
                        yield {
                            "type": "progress",
                            "stage": "execution",
                            "message": msg or "Executing plan steps",
                            "plan_steps": steps,
                        }
                    except asyncio.TimeoutError:
                        continue

                # Drain any remaining queued updates
                while not _progress_queue.empty():
                    steps, msg = _progress_queue.get_nowait()
                    yield {
                        "type": "progress",
                        "stage": "execution",
                        "message": msg or "Executing plan steps",
                        "plan_steps": steps,
                    }

                results = exec_task.result()
            else:
                results = await self._execute_skill_chain(skill_chain)
            
            # Format output based on requested format
            yield {
                "type": "progress",
                "stage": "formatting",
                "message": f"Formatting output as {output_format}",
                "plan_steps": self.shared_data.get("plan_steps", []),
            }
            
            formatted_result = await self._format_output(results, output_format, prompt)

            # Propagate skill chain warnings into the formatted result
            skill_warnings = results.get("warnings", [])
            if skill_warnings and isinstance(formatted_result, dict):
                formatted_result["warnings"] = skill_warnings

            # Add detailed logging for the complete result being yielded
            logger.info(f"[STREAM] About to yield complete result")
            logger.info(f"[STREAM] formatted_result type: {type(formatted_result)}")
            logger.info(f"[STREAM] formatted_result keys: {list(formatted_result.keys()) if isinstance(formatted_result, dict) else 'not_dict'}")
            if isinstance(formatted_result, dict):
                logger.info(f"[STREAM] formatted_result format: {formatted_result.get('format')}")
                logger.info(f"[STREAM] formatted_result slides count: {len(formatted_result.get('slides') or [])}")
                slides_data = formatted_result.get('slides') or []
                if slides_data:
                    logger.info(f"[STREAM] Slide IDs being yielded: {[s.get('id') for s in slides_data[:3]]}")
            
            # End budget tracking and capture summary
            budget_summary = self.model_router.end_budget() or {}

            # Build analysis manifest for state persistence across requests
            from app.services.session_state import SessionState as _SS
            _manifest = _SS(self.shared_data).analysis_manifest()

            # Yield single complete result
            yield {
                "type": "complete",
                "result": formatted_result,
                "success": True,
                "analysis_manifest": _manifest,
                "metadata": {
                    "streaming_disabled": True,
                    "format": output_format,
                    "skills_executed": len(skill_chain),
                    "tools_used": [node.skill for node in skill_chain],
                    "budget": budget_summary,
                }
            }
            
        except Exception as e:
            logger.error(f"Error processing request: {e}")
            self.model_router.end_budget()  # Clean up budget on error
            yield {
                "type": "error",
                "error": str(e)
            }

    async def process_request_complete(
        self,
        prompt: str,
        output_format: str = "analysis",
        context: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Buffer all streaming chunks and return a single atomic payload.

        Wraps ``process_request_stream()`` so the frontend receives ONE
        response with all artifacts (suggestions, charts, grid_commands,
        memo_sections) rather than partial streaming events.

        Progress events are condensed into a ``status_log`` list.
        """
        final_result: Dict[str, Any] = {}
        status_log: List[str] = []
        all_suggestions: List[Dict[str, Any]] = []
        all_grid_commands: List[Dict[str, Any]] = []
        all_charts: List[Dict[str, Any]] = []

        async for event in self.process_request_stream(prompt, output_format, context):
            event_type = event.get("type", "")
            if event_type == "progress":
                status_log.append(event.get("message", ""))
            elif event_type == "complete":
                final_result = event
            elif event_type == "error":
                final_result = event

        # Enrich the final payload with any accumulated artifacts
        if final_result.get("type") == "complete":
            result = final_result.get("result", {})
            if isinstance(result, dict):
                result.setdefault("status_log", status_log)
            final_result["result"] = result

        return final_result

    async def _execute_planning(
        self, prompt: str, output_format: str, entities: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Decompose complex prompt into structured plan steps using LLM."""
        available_skills = list(self.skills.keys())
        companies = entities.get("companies", [])
        grid_snapshot = self.shared_data.get("matrix_context", {}).get("gridSnapshot")
        
        planning_prompt = f"""Decompose this investment prompt into a multi-step execution plan.

<prompt>
{prompt}
</prompt>

<available_skills>
{json.dumps(available_skills[:25])}
</available_skills>

<companies_mentioned>
{json.dumps(companies)}
</companies_mentioned>

Return a JSON array of steps. Each step:
{{
  "id": "step-1",
  "label": "Short human-readable label",
  "action": "skill_name",
  "detail": "Brief description",
  "tool_to_use": "company-data-fetcher|valuation-engine|cap-table-generator|deal-comparer|exit-modeler|deck-storytelling|portfolio-analyzer|fund-metrics-calculator|followon-strategy|round-modeler|report-generator|scenario-analyzer|memo-generator|portfolio-scenario-modeler|company-health-dashboard|grid-run-valuation|grid-run-pwerm|grid-run-document-extract",
  "companies": ["CompanyA"],
  "explanation": "Why this step"
}}

Map tool_to_use to one of: company-data-fetcher, valuation-engine, cap-table-generator, deal-comparer, exit-modeler, deck-storytelling, portfolio-analyzer, fund-metrics-calculator, followon-strategy, round-modeler, report-generator, scenario-analyzer, memo-generator, excel-generator, portfolio-scenario-modeler, company-health-dashboard, grid-run-valuation, grid-run-pwerm, grid-run-document-extract.
CRITICAL: When valuing the whole fund or all companies ("value the fund", "fill in everything", "value all"):
1. FIRST enrich each company with "company-data-fetcher" (fetches real data: revenue, gross margin, growth, burn, headcount, etc.)
2. THEN run "grid-run-valuation" for all companies (uses enriched data, not guesses)
This is the SAME enrichment pipeline that runs when a user @mentions a company. Always enrich before valuing.

When the user asks to run valuation, value a company, run PWERM, or run pwerm for specific companies, use tool_to_use "grid-run-valuation" or "grid-run-pwerm" with the companies list in the "companies" field.
When the user asks to extract a document for a company (e.g. "extract document for @Acme"), use tool_to_use "grid-run-document-extract" with the company in the "companies" field.
When the user asks about follow-on, pro-rata, dilution, or whether to follow on, use "followon-strategy".
When the user asks to model a next round (Series D, etc.), use "round-modeler".
When the user asks about cap table or ownership, use "cap-table-generator".
When the user asks to generate a report (LP quarterly, follow-on memo, GP deck), use "report-generator".
When the user asks "what if" or scenario questions, use "scenario-analyzer".
When the user asks to generate a memo or document, use "memo-generator".
When the user asks about fund return scenarios, portfolio-level what-ifs, or how different company outcomes affect fund returns, use "portfolio-scenario-modeler".
When the user asks about portfolio health, company growth/burn/runway, company signals, or wants a health dashboard, use "company-health-dashboard".
Output format requested: {output_format}
Return ONLY the JSON array, no other text."""

        try:
            result = await self.model_router.get_completion(
                prompt=planning_prompt,
                capability=ModelCapability.STRUCTURED,
                max_tokens=1500,
                temperature=0,
                json_mode=True,
                fallback_enabled=True
            )
            content = result.get("response", "[]")
            import re
            match = re.search(r'\[.*\]', content, re.DOTALL)
            if match:
                steps = json.loads(match.group(0))
                for i, s in enumerate(steps):
                    s.setdefault("id", f"step-{i+1}")
                    s.setdefault("status", "pending")
                    s.setdefault("label", s.get("action", f"Step {i+1}"))
                return steps
        except Exception as e:
            logger.warning(f"[PLANNING] LLM planning failed: {e}")
        return []
    
    async def build_skill_chain(
        self, prompt: str, output_format: str, entities: Optional[Dict[str, Any]] = None
    ) -> List[SkillChainNode]:
        """
        Use Claude to analyze prompt and build optimal skill chain.
        When plan_steps exists in shared_data, build chain from plan instead of keyword matching.
        """
        logger.critical(f"[SKILL_BUILDER] 🟠🟠🟠 build_skill_chain CALLED: prompt='{prompt[:100]}...', format={output_format} 🟠🟠🟠")
        
        # Phase 2: Plan-driven skill chain when plan_steps exists
        plan_steps = self.shared_data.get("plan_steps", [])
        if plan_steps:
            logger.info(f"[SKILL_BUILDER] 📋 Using plan-driven chain with {len(plan_steps)} steps")
            chain = []
            tool_to_skill = {
                "company-data-fetcher": "company-data-fetcher",
                "valuation-engine": "valuation-engine",
                "valuation_engine": "valuation-engine",
                "cap-table-generator": "cap-table-generator",
                "deal-comparer": "deal-comparer",
                "exit-modeler": "exit-modeler",
                "deck-storytelling": "deck-storytelling",
                "portfolio-analyzer": "portfolio-analyzer",
                "fund-metrics-calculator": "fund-metrics-calculator",
                "followon-strategy": "followon-strategy",
                "round-modeler": "round-modeler",
                "report-generator": "report-generator",
                "scenario-analyzer": "scenario-generator",
                "memo-generator": "memo-writer",
                "excel-generator": "excel-generator",
                "portfolio-scenario-modeler": "portfolio-scenario-modeler",
                "company-health-dashboard": "company-health-dashboard",
                **{k: k for k in GRID_ACTION_MAP},  # All grid-run-* skills map to themselves
            }
            for i, step in enumerate(plan_steps):
                tool = step.get("tool_to_use") or step.get("action", "")
                skill = tool_to_skill.get(tool, tool) if isinstance(tool, str) else "company-data-fetcher"
                if skill not in self.skills:
                    skill = "company-data-fetcher"  # Fallback
                companies = step.get("companies", [])
                group = min(i, 2)
                if skill == "company-data-fetcher" and len(companies) > 1:
                    for c in companies:
                        chain.append(SkillChainNode(
                            skill=skill,
                            purpose=f"Fetch data for {c}",
                            inputs={"company": c, "prompt_handle": c, "_plan_step_index": i},
                            parallel_group=group,
                            depends_on=[]
                        ))
                else:
                    inputs = {"use_shared_data": True} if not companies else {"company": companies[0], "prompt_handle": companies[0]}
                    if len(companies) > 1 and skill != "company-data-fetcher":
                        inputs = {"companies": companies, "use_shared_data": True}
                    inputs["_plan_step_index"] = i
                    chain.append(SkillChainNode(
                        skill=skill,
                        purpose=step.get("label", step.get("detail", str(skill))),
                        inputs=inputs,
                        parallel_group=group,
                        depends_on=[]
                    ))
            return chain
        
        # Fallback: keyword-based skill chain
        if entities is None:
            logger.info(f"[SKILL_BUILDER] 🔍 Extracting entities from prompt...")
            entities = await self._extract_entities(prompt)
        else:
            logger.info(f"[SKILL_BUILDER] 🔍 Using provided entities: {entities}")
        logger.info(f"[SKILL_BUILDER] 🔍 Extracted entities: {entities}")
        
        chain = []
        
        # Phase 0: Data Gathering (parallel)
        logger.info(f"[SKILL_BUILDER] 📊 Phase 0: Data Gathering")
        if entities.get("companies"):
            logger.info(f"[SKILL_BUILDER] 📊 Found {len(entities['companies'])} companies: {entities['companies']}")
            for company in entities["companies"]:
                logger.info(f"[SKILL_BUILDER] ✅ Adding company-data-fetcher skill for '{company}'")
                chain.append(SkillChainNode(
                    skill="company-data-fetcher",
                    purpose=f"Fetch data for {company}",
                    inputs={"company": company, "prompt_handle": company},
                    parallel_group=0
                ))
        else:
            logger.warning(f"[SKILL_BUILDER] ⚠️  No companies found in entities - company-data-fetcher will NOT be added")
            # FORCE-ADD company-data-fetcher if deck/docs format is requested and we can extract company names from prompt
            if output_format in ("deck", "docs") or "deck" in prompt.lower():
                logger.info(f"[SKILL_BUILDER] 📊 Deck format requested but no companies in entities - attempting to extract from prompt")
                # Try to extract @mentions from prompt as fallback
                import re
                at_mentions = re.findall(r'@(\w+)', prompt)
                # Also match against known grid company names (no @ required)
                matrix_ctx = self.shared_data.get("matrix_context") or {}
                grid_names = matrix_ctx.get("companyNames") or matrix_ctx.get("company_names") or []
                grid_matches = []
                at_set = {m.lower() for m in at_mentions}
                for gname in grid_names:
                    if gname and len(gname) >= 3 and gname.lower() not in at_set:
                        if re.search(r'\b' + re.escape(gname) + r'\b', prompt, re.IGNORECASE):
                            grid_matches.append(gname)
                all_matches = at_mentions + grid_matches
                if all_matches:
                    logger.info(f"[SKILL_BUILDER] 📊 Found companies in prompt: @mentions={at_mentions}, grid_matches={grid_matches}")
                    for company in all_matches:
                        src = "@mention" if company in at_mentions else "grid match"
                        logger.info(f"[SKILL_BUILDER] ✅ Force-adding company-data-fetcher for '{company}' ({src})")
                        chain.append(SkillChainNode(
                            skill="company-data-fetcher",
                            purpose=f"Fetch data for {company} ({src})",
                            inputs={"company": company, "prompt_handle": company},
                            parallel_group=0
                        ))
                    entities["companies"] = all_matches  # Update entities for later use
                else:
                    logger.warning(f"[SKILL_BUILDER] ⚠️  No @mentions or grid matches found - company-data-fetcher will NOT be added")
        
        # Phase 1: Analysis (parallel where possible)
        logger.info(f"[SKILL_BUILDER] 🔬 Phase 1: Analysis")
        
        if "valuation" in prompt.lower() or "value" in prompt.lower():
            logger.info(f"[SKILL_BUILDER] 🔬 Adding valuation-engine (prompt contains 'valuation' or 'value')")
            chain.append(SkillChainNode(
                skill="valuation-engine",
                purpose="Calculate valuations",
                inputs={"use_shared_data": True},
                parallel_group=1
            ))
        
        if "compare" in prompt.lower() and len(entities.get("companies", [])) > 1:
            logger.info(f"[SKILL_BUILDER] 🔬 Adding deal-comparer (prompt contains 'compare' and multiple companies)")
            chain.append(SkillChainNode(
                skill="deal-comparer",
                purpose="Compare companies",
                inputs={"companies": entities["companies"]},
                parallel_group=1
            ))
        
        # ALWAYS generate cap tables for any company analysis
        companies_count = len(entities.get("companies", []))
        if companies_count > 0:
            logger.info(f"[SKILL_BUILDER] 🔬 Adding cap-table-generator ({companies_count} companies found)")
            chain.append(SkillChainNode(
                skill="cap-table-generator",
                purpose="Generate cap tables with ownership evolution",
                inputs={"use_shared_data": True},
                parallel_group=1
            ))
        else:
            logger.warning(f"[SKILL_BUILDER] ⚠️  No companies in entities - skipping cap-table-generator")
        
        # ALWAYS do valuations for investment decisions (skip if already added by keyword match above)
        companies_count = len(entities.get("companies", []))
        already_has_valuation = any(n.skill == "valuation-engine" for n in chain)
        if companies_count > 0 and not already_has_valuation:
            logger.info(f"[SKILL_BUILDER] 🔬 Adding valuation-engine ({companies_count} companies found)")
            chain.append(SkillChainNode(
                skill="valuation-engine",
                purpose="Calculate valuations (bull/bear/base scenarios)",
                inputs={"use_shared_data": True},
                parallel_group=1
            ))
        elif already_has_valuation:
            logger.info(f"[SKILL_BUILDER] ⏭️  valuation-engine already in chain, skipping duplicate")
        else:
            logger.warning(f"[SKILL_BUILDER] ⚠️  No companies in entities - skipping valuation-engine")
        
        # Fund portfolio analysis - ALWAYS if fund context mentioned
        if "fund" in prompt.lower() or "portfolio" in prompt.lower() or "deploy" in prompt.lower():
            logger.info(f"[SKILL_BUILDER] 🔬 Adding portfolio-analyzer (fund/portfolio/deploy mentioned)")
            chain.append(SkillChainNode(
                skill="portfolio-analyzer",
                purpose="Analyze portfolio",
                inputs={"context": entities},
                parallel_group=1
            ))
        
        # Fund metrics if DPI or deployment mentioned (deck generation handled separately below)
        if "dpi" in prompt.lower() or "deploy" in prompt.lower() or "tvpi" in prompt.lower():
            logger.info(f"[SKILL_BUILDER] 🔬 Adding fund-metrics-calculator (DPI/deploy/TVPI mentioned)")
            chain.append(SkillChainNode(
                skill="fund-metrics-calculator",
                purpose="Calculate fund metrics",
                inputs={"context": entities},
                parallel_group=1
            ))
        
        # Multi-stage analysis
        if ("seed" in prompt.lower() and "series" in prompt.lower()) or "stage" in prompt.lower():
            logger.info(f"[SKILL_BUILDER] 🔬 Adding stage-analyzer (multi-stage mentioned)")
            chain.append(SkillChainNode(
                skill="stage-analyzer",
                purpose="Analyze investment stages",
                inputs={"stages": ["seed", "series_a", "series_b"]},
                parallel_group=1
            ))
        
        # ALWAYS model exit scenarios for investment decisions
        if len(entities.get("companies", [])) > 0:
            logger.info(f"[SKILL_BUILDER] 🔬 Adding exit-modeler (companies found)")
            chain.append(SkillChainNode(
                skill="exit-modeler",
                purpose="Model exit scenarios (win/lose/base cases)",
                inputs={"use_shared_data": True},
                parallel_group=1
            ))

        # Follow-on strategy
        lower = prompt.lower()
        if any(kw in lower for kw in ["follow on", "follow-on", "followon", "pro rata", "pro-rata", "should we follow", "extension"]):
            logger.info(f"[SKILL_BUILDER] 🔬 Adding followon-strategy")
            chain.append(SkillChainNode(
                skill="followon-strategy",
                purpose="Analyze follow-on / extension / sell decision",
                inputs={"use_shared_data": True},
                parallel_group=1
            ))

        # Round modeling
        if any(kw in lower for kw in ["next round", "model round", "series d", "series c", "series b"]):
            logger.info(f"[SKILL_BUILDER] 🔬 Adding round-modeler")
            chain.append(SkillChainNode(
                skill="round-modeler",
                purpose="Model next funding round with dilution & waterfall",
                inputs={"use_shared_data": True},
                parallel_group=1
            ))

        # NL scenario analysis
        if any(kw in lower for kw in ["what if", "what happens", "stress test", "scenario"]):
            logger.info(f"[SKILL_BUILDER] 🔬 Adding scenario-generator")
            chain.append(SkillChainNode(
                skill="scenario-generator",
                purpose="Run scenario / what-if analysis",
                inputs={"use_shared_data": True},
                parallel_group=1
            ))

        # FPA: Regression analysis
        if any(kw in lower for kw in ["regression", "correlat", "r-squared", "r squared", "fit line", "trend line"]):
            logger.info(f"[SKILL_BUILDER] 🔬 Adding regression-analyzer")
            chain.append(SkillChainNode(
                skill="regression-analyzer",
                purpose="Run regression / correlation analysis",
                inputs={"use_shared_data": True},
                parallel_group=1
            ))

        # FPA: Time series forecast
        if any(kw in lower for kw in ["forecast", "project revenue", "predict", "time series"]):
            logger.info(f"[SKILL_BUILDER] 🔬 Adding time-series-forecaster")
            chain.append(SkillChainNode(
                skill="time-series-forecaster",
                purpose="Forecast time series with confidence intervals",
                inputs={"use_shared_data": True},
                parallel_group=1
            ))

        # FPA: Growth/decay modeling
        if any(kw in lower for kw in ["growth rate", "decay", "half life", "half-life", "exponential growth", "exponential decay"]):
            logger.info(f"[SKILL_BUILDER] 🔬 Adding growth-decay-forecaster")
            chain.append(SkillChainNode(
                skill="growth-decay-forecaster",
                purpose="Model exponential growth/decay",
                inputs={"use_shared_data": True},
                parallel_group=1
            ))

        # FPA: Monte Carlo
        if any(kw in lower for kw in ["monte carlo", "simulation", "probability distribution", "variance"]):
            logger.info(f"[SKILL_BUILDER] 🔬 Adding monte-carlo-simulator")
            chain.append(SkillChainNode(
                skill="monte-carlo-simulator",
                purpose="Run Monte Carlo simulation",
                inputs={"use_shared_data": True},
                parallel_group=1
            ))

        # FPA: Sensitivity / tornado
        if any(kw in lower for kw in ["sensitivity", "tornado", "what drives", "key driver"]):
            logger.info(f"[SKILL_BUILDER] 🔬 Adding sensitivity-analyzer")
            chain.append(SkillChainNode(
                skill="sensitivity-analyzer",
                purpose="Sensitivity / tornado analysis",
                inputs={"use_shared_data": True},
                parallel_group=1
            ))

        # Fund-level analysis (comprehensive)
        if any(kw in lower for kw in ["analyze fund", "analyse fund", "fund analysis", "fund strategy", "fund performance"]):
            logger.info(f"[SKILL_BUILDER] 🔬 Adding fund-analyzer")
            chain.append(SkillChainNode(
                skill="fund-analyzer",
                purpose="Comprehensive fund analysis with follow-on strategy",
                inputs={"use_shared_data": True},
                parallel_group=1
            ))

        # Portfolio scenario modeling (fund-level what-if)
        if any(kw in lower for kw in ["fund return scenario", "portfolio scenario", "what if company", "fund impact"]):
            logger.info(f"[SKILL_BUILDER] 🔬 Adding portfolio-scenario-modeler")
            chain.append(SkillChainNode(
                skill="portfolio-scenario-modeler",
                purpose="Model fund return scenarios across portfolio",
                inputs={"use_shared_data": True},
                parallel_group=1
            ))

        # Company health dashboard (portfolio-wide analytics)
        if any(kw in lower for kw in ["portfolio health", "company health", "health dashboard", "runway analysis", "growth decay"]):
            logger.info(f"[SKILL_BUILDER] 🔬 Adding company-health-dashboard")
            chain.append(SkillChainNode(
                skill="company-health-dashboard",
                purpose="Portfolio health: growth, burn, runway, signals",
                inputs={"use_shared_data": True},
                parallel_group=1
            ))

        # Memo / report generation
        if any(kw in lower for kw in ["memo", "generate report", "lp report", "quarterly report", "gp deck", "follow-on memo"]):
            logger.info(f"[SKILL_BUILDER] 🔬 Adding report-generator")
            chain.append(SkillChainNode(
                skill="report-generator",
                purpose="Generate report / memo with charts",
                inputs={"use_shared_data": True},
                parallel_group=2,
                required=False
            ))

        # Phase 1.5: Grid skills (Phase 6) - when matrix_context present and keywords match
        matrix_ctx = self.shared_data.get("matrix_context") or {}
        has_matrix = bool(matrix_ctx.get("rowIds") or matrix_ctx.get("row_ids"))
        if has_matrix:
            lower = prompt.lower()
            for skill_name, triggers in GRID_TRIGGER_MAP.items():
                if any(t in lower for t in triggers):
                    action_id = GRID_ACTION_MAP.get(skill_name, ("", "value"))[0]
                    chain.append(SkillChainNode(
                        skill=skill_name,
                        purpose=f"Run {action_id} on grid",
                        inputs={"use_shared_data": True},
                        parallel_group=1
                    ))
        
        # Phase 2: Generation/Formatting
        logger.info(f"[SKILL_BUILDER] 🎨 Phase 2: Generation/Formatting")
        logger.info(f"[SKILL_BUILDER] 🎨 output_format value: '{output_format}'")
        logger.info(f"[SKILL_BUILDER] 🎨 output_format == 'deck': {output_format == 'deck'}")
        logger.info(f"[SKILL_BUILDER] 🎨 output_format == 'spreadsheet': {output_format == 'spreadsheet'}")
        
        if output_format == "spreadsheet":
            logger.info(f"[SKILL_BUILDER] 🎨 Adding excel-generator (output_format=spreadsheet)")
            chain.append(SkillChainNode(
                skill="excel-generator",
                purpose="Generate spreadsheet",
                inputs={"format": "comparison_matrix"},
                parallel_group=2
            ))
        elif output_format == "deck":
            logger.info(f"[SKILL_BUILDER] 🎨 Adding deck-storytelling (output_format=deck)")
            logger.info(f"[SKILL_BUILDER] 🎨 Creating SkillChainNode for deck-storytelling")
            chain.append(SkillChainNode(
                skill="deck-storytelling",  # This is the actual registered skill name
                purpose="Generate presentation",
                inputs={"use_shared_data": True},
                parallel_group=3  # CRITICAL FIX: Move to group 3 to ensure companies are available
            ))
            logger.info(f"[SKILL_BUILDER] 🎨 ✅ Added deck-storytelling to chain. Chain length now: {len(chain)}")
        elif output_format == "docs":
            logger.info(f"[SKILL_BUILDER] 📝 Adding memo upstream services + memo-writer (output_format=docs)")

            # Fix 2: Ensure the skills that produce scenario_analysis, cap_table_history,
            # and revenue_projections are in the chain (at group 2) even when they weren't
            # added by Phase 1 keyword matching.  These mirror what deck uses at group 1.
            existing_skills = {n.skill for n in chain}
            memo_upstream = [
                ("valuation-engine",    "Calculate valuations + PWERM scenarios for memo"),
                ("cap-table-generator", "Build cap table history for memo sankey"),
                ("exit-modeler",        "Model exit scenarios for memo probability cloud"),
            ]
            for skill_name, purpose in memo_upstream:
                if skill_name not in existing_skills:
                    chain.append(SkillChainNode(
                        skill=skill_name,
                        purpose=purpose,
                        inputs={"use_shared_data": True},
                        parallel_group=2,
                    ))
                    logger.info(f"[SKILL_BUILDER] 📝   Added {skill_name} at group 2 for memo")

            chain.append(SkillChainNode(
                skill="memo-writer",
                purpose="Generate investment memo with charts",
                inputs={"use_shared_data": True},
                parallel_group=3  # After data fetching/valuation
            ))
            logger.info(f"[SKILL_BUILDER] 📝 ✅ Added memo-writer to chain. Chain length now: {len(chain)}")
        else:
            logger.warning(f"[SKILL_BUILDER] ⚠️ Unknown output_format: '{output_format}'")
        
        logger.info(f"[SKILL_BUILDER] ✅ Built skill chain with {len(chain)} skills")
        logger.info(f"[SKILL_BUILDER] 📋 Final skill chain:")
        for i, node in enumerate(chain):
            logger.info(f"[SKILL_BUILDER]   {i+1}. Group {node.parallel_group}: {node.skill} - {node.purpose}")
            logger.info(f"[SKILL_BUILDER]      Inputs: {node.inputs}")
        
        return chain
    
    def _safe_multiply(self, *args: Any) -> float:
        """Safe multiplication handling None, InferenceResult, and Decimal
        
        Multiplies all arguments together, handling various types safely
        """
        result = 1.0
        for value in args:
            safe_val = safe_get_value(value, 0)
            if safe_val == 0:
                return 0
            result *= float(safe_val)
        return result
    
    async def _execute_skill_chain(self, chain: List[SkillChainNode], progress_callback=None) -> Dict[str, Any]:
        """Execute skill chain with parallel group support.

        Args:
            progress_callback: Optional async callable(steps, message) invoked
                whenever a plan step status changes, enabling real-time streaming.
        """
        logger.critical(f"[SKILL_CHAIN] 🟣🟣🟣 _execute_skill_chain CALLED with {len(chain)} skills 🟣🟣🟣")
        
        results = {}
        
        logger.info(f"[SKILL_CHAIN] 🚀 Starting skill chain execution with {len(chain)} skills")
        logger.info(f"[SKILL_CHAIN] Chain overview:")
        for i, node in enumerate(chain):
            logger.info(f"[SKILL_CHAIN]   {i+1}. {node.skill} (group {node.parallel_group}) - {node.purpose}")
        
        # Group skills by parallel group
        groups = {}
        for node in chain:
            if node.parallel_group not in groups:
                groups[node.parallel_group] = []
            groups[node.parallel_group].append(node)
        
        logger.info(f"[SKILL_CHAIN] 📊 Grouped into {len(groups)} parallel groups: {sorted(groups.keys())}")
        
        # Execute groups in order
        logger.info(f"[SKILL_CHAIN] 🎯 About to execute {len(groups)} groups: {sorted(groups.keys())}")
        for group_num in sorted(groups.keys()):
            group_skills = groups[group_num]
            logger.info(f"[SKILL_CHAIN] 🔄 Executing group {group_num} with {len(group_skills)} skills: {[s.skill for s in group_skills]}")
            
            # Snapshot shared_data under lock so parallel skills in this group
            # read a consistent view even while the prior group's writes land.
            async with self.shared_data_lock:
                group_snapshot = dict(self.shared_data)

            # Pre-execution validation for critical groups
            if group_num == 3:  # Deck generation group
                companies_count = len(group_snapshot.get("companies", []))
                if companies_count == 0:
                    logger.warning(f"[SKILL_CHAIN] ⚠️ Group 3 (deck generation) has no companies - will attempt to generate anyway")
                    logger.warning(f"[SKILL_CHAIN] ⚠️ Available shared_data keys: {list(group_snapshot.keys())}")
                    # Don't raise - let deck generation handle empty companies gracefully
                else:
                    logger.info(f"[SKILL_CHAIN] ✅ Group 3 validation passed: {companies_count} companies available")

            # Log shared_data state before group execution (from snapshot)
            logger.info(f"[SKILL_CHAIN] 📋 Shared data before group {group_num}:")
            logger.info(f"[SKILL_CHAIN]   Keys: {list(group_snapshot.keys())}")
            if 'companies' in group_snapshot:
                companies = group_snapshot['companies']
                logger.info(f"[SKILL_CHAIN]   Companies count: {len(companies)}")
                for i, company in enumerate(companies):
                    logger.info(f"[SKILL_CHAIN]     Company {i}: {company.get('company', 'NO_COMPANY_FIELD')} (keys: {list(company.keys())})")
            else:
                logger.info(f"[SKILL_CHAIN]   No 'companies' key in shared_data")
            
            # Phase 5: Dependency validation — skip skills whose prerequisites failed
            validated_skills = []
            for node in group_skills:
                if node.depends_on:
                    missing_deps = [
                        dep for dep in node.depends_on
                        if dep not in results or (isinstance(results.get(dep), dict) and results[dep].get("error"))
                    ]
                    if missing_deps:
                        logger.warning(f"[SKILL_CHAIN] ⏭️ Skipping '{node.skill}' — missing dependencies: {missing_deps}")
                        node.status = "skipped"
                        node.result = {"skipped": True, "reason": f"Missing dependencies: {missing_deps}"}
                        results[node.skill] = node.result
                        # Update plan step status
                        plan_steps = self.shared_data.get("plan_steps", [])
                        if plan_steps:
                            step_idx = node.inputs.get("_plan_step_index")
                            if step_idx is not None and step_idx < len(plan_steps):
                                plan_steps[step_idx]["status"] = "skipped"
                                plan_steps[step_idx]["detail"] = f"Skipped — missing: {', '.join(missing_deps)}"
                                if progress_callback:
                                    await progress_callback(
                                        [dict(s) for s in plan_steps],
                                        f"Skipped {node.skill}",
                                    )
                        continue
                validated_skills.append(node)

            # Execute all skills in group in parallel with timeout protection
            tasks = []
            for node in validated_skills:
                skill_info = self.skills.get(node.skill, {})
                handler = skill_info.get("handler")
                logger.info(f"[SKILL_CHAIN] 🎯 Preparing skill '{node.skill}' with inputs: {node.inputs}")
                if handler:
                    logger.info(f"[SKILL_CHAIN] ✅ Handler found for '{node.skill}', adding to tasks")
                    # Wrap handler in timeout protection (5 minutes max per skill) and error handling
                    async def safe_handler_wrapper(skill_name, handler_func, inputs):
                        try:
                            return await asyncio.wait_for(handler_func(inputs), timeout=300.0)
                        except asyncio.TimeoutError:
                            logger.error(f"[SKILL_CHAIN] ⏱️ Skill '{skill_name}' timed out after 5 minutes")
                            return TimeoutError(f"Skill '{skill_name}' execution timed out")
                        except Exception as e:
                            logger.error(f"[SKILL_CHAIN] ❌ Skill '{skill_name}' raised exception: {type(e).__name__}: {e}")
                            import traceback
                            logger.error(f"[SKILL_CHAIN] ❌ Traceback: {traceback.format_exc()}")
                            return e
                    
                    # Capture node.skill and handler in closure
                    skill_name = node.skill
                    tasks.append(safe_handler_wrapper(skill_name, handler, node.inputs))
                else:
                    logger.error(f"[SKILL_CHAIN] ❌ No handler found for skill '{node.skill}'")
                    # Store error result for missing handler - always use valid list structures
                    error_result = {
                        "error": f"No handler found for skill '{node.skill}'",
                        "error_type": "MissingHandler",
                        "format": "deck" if node.skill == "deck-storytelling" else None,
                        "slides": [],  # Always a list, never None
                        "theme": "professional",
                        "metadata": {"error": True, "error_type": "MissingHandler"},
                        "citations": [],
                        "charts": [],
                        "companies": []
                    }
                    node.status = "failed"
                    node.result = error_result
                    results[node.skill] = error_result
            
            if tasks:
                logger.info(f"[SKILL_CHAIN] 🔍 CHECKPOINT 4: About to execute {len(tasks)} tasks in parallel for group {group_num}")
                logger.info(f"[SKILL_CHAIN] 🔍 CHECKPOINT 4: Task skills: {[node.skill for node in validated_skills]}")
                logger.info(f"[SKILL_CHAIN] 🔍 CHECKPOINT 4: Task inputs preview: {[str(node.inputs)[:200] + '...' if len(str(node.inputs)) > 200 else str(node.inputs) for node in validated_skills]}")
                logger.info(f"[SKILL_CHAIN] 🏃 Executing {len(tasks)} tasks in parallel for group {group_num}")
                # Use return_exceptions=True to prevent one failure from breaking the entire chain
                # This ensures partial results are always returned even if some skills fail
                group_results = await asyncio.gather(*tasks, return_exceptions=True)
                logger.info(f"[SKILL_CHAIN] 🔍 CHECKPOINT 4: Group {group_num} execution completed, processing {len(group_results)} results")
                logger.info(f"[SKILL_CHAIN] 🔍 CHECKPOINT 4: Result types: {[type(result).__name__ for result in group_results]}")
                logger.info(f"[SKILL_CHAIN] 🔍 CHECKPOINT 4: Exception results: {[result for result in group_results if isinstance(result, Exception)]}")
                logger.info(f"[SKILL_CHAIN] ✅ Group {group_num} execution completed, processing {len(group_results)} results")
                
                # Store results
                for node, result in zip(validated_skills, group_results):
                    if node.skill == "deck-storytelling":
                        logger.critical(f"[SKILL_CHAIN] 🟢🟢🟢 deck-storytelling result: type={type(result)}, is_exception={isinstance(result, Exception)} 🟢🟢🟢")
                        
                    
                    logger.info(f"[SKILL_CHAIN] 🔍 Processing result for skill '{node.skill}'")
                    
                    # Phase 2: Update plan_steps status when present
                    plan_steps = self.shared_data.get("plan_steps", [])
                    if plan_steps:
                        try:
                            step_idx = node.inputs.get("_plan_step_index")
                            if step_idx is not None and step_idx < len(plan_steps):
                                plan_steps[step_idx]["status"] = "failed" if isinstance(result, Exception) else "done"
                                plan_steps[step_idx]["detail"] = str(result)[:200] if isinstance(result, Exception) else (plan_steps[step_idx].get("explanation") or f"Completed {node.skill}")
                            elif step_idx is None and node in chain:
                                step_idx = chain.index(node)
                                if step_idx < len(plan_steps):
                                    plan_steps[step_idx]["status"] = "failed" if isinstance(result, Exception) else "done"
                                    plan_steps[step_idx]["detail"] = str(result)[:200] if isinstance(result, Exception) else (plan_steps[step_idx].get("explanation") or f"Completed {node.skill}")
                        except (ValueError, IndexError, TypeError):
                            pass
                        # Notify caller of plan step status change
                        if progress_callback:
                            await progress_callback(
                                [dict(s) for s in plan_steps],
                                f"{'Failed' if isinstance(result, Exception) else 'Completed'} {node.skill}",
                            )

                    if isinstance(result, Exception):
                        logger.error(f"[SKILL_CHAIN] ❌ Skill '{node.skill}' failed with exception: {result}")
                        logger.error(f"[SKILL_CHAIN] ❌ Exception type: {type(result).__name__}")
                        import traceback
                        if hasattr(result, '__traceback__'):
                            logger.error(f"[SKILL_CHAIN] ❌ Traceback: {''.join(traceback.format_tb(result.__traceback__))}")

                        # Phase 5: Graceful degradation — optional skills don't break the chain
                        if not node.required:
                            logger.info(f"[SKILL_CHAIN] ⚠️ Optional skill '{node.skill}' failed — continuing chain")
                            node.status = "degraded"
                            node.result = {"degraded": True, "error": str(result), "note": f"{node.skill} was unavailable — showing data only"}
                            results[node.skill] = node.result
                            continue

                        # Record failure in error handler for circuit breaker
                        if self.error_handler:
                            self.error_handler.record_failure(node.skill)

                        node.status = "failed"
                        # CRITICAL FIX: Store error result so _format_deck knows deck-storytelling was attempted
                        # Always use valid list structures, never None
                        error_result = {
                            "error": str(result),
                            "error_type": type(result).__name__,
                            "format": "deck" if node.skill == "deck-storytelling" else None,
                            "slides": [],  # Always a list, never None
                            "theme": "professional",
                            "metadata": {"error": True, "error_type": type(result).__name__},
                            "citations": [],
                            "charts": [],
                            "companies": []
                        }
                        node.result = error_result
                        results[node.skill] = error_result
                        logger.warning(f"[SKILL_CHAIN] ⚠️ Stored error result for skill '{node.skill}' so format_deck can handle it")
                        continue
                    
                    node.result = result
                    results[node.skill] = result
                    # Phase 6: Collect grid_commands from grid-run-* skills for frontend
                    if isinstance(result, dict) and result.get("grid_commands"):
                        async with self.shared_data_lock:
                            self.shared_data.setdefault("grid_commands", []).extend(result["grid_commands"])
                        logger.info(f"[SKILL_CHAIN] 📋 Appended {len(result['grid_commands'])} grid_commands from '{node.skill}'")
                    logger.info(f"[SKILL_CHAIN] ✅ Stored result for skill '{node.skill}', type: {type(result)}")
                    
                    
                    # Deep logging for specific skills
                    if isinstance(result, dict):
                        logger.info(f"[SKILL_CHAIN] 📊 {node.skill} result keys: {list(result.keys())}")
                        
                        if node.skill == "deck-storytelling":
                            logger.info(f"[SKILL_CHAIN] 🎨 deck-storytelling result keys: {list(result.keys())}")
                            logger.info(f"[SKILL_CHAIN] 🎨 deck-storytelling has {len(result.get('slides', []))} slides")
                            if result.get('slides'):
                                logger.info(f"[SKILL_CHAIN] 🎨 First slide preview: {result['slides'][0] if result['slides'] else 'No slides'}")
                            
                            # CHANGED: Don't raise exception, just log warning
                            slides = result.get('slides') or []
                            if not isinstance(slides, list):
                                slides = []
                            if not slides or len(slides) == 0:
                                logger.warning(f"[SKILL_CHAIN] ⚠️ deck-storytelling returned EMPTY slides!")
                                logger.warning(f"[SKILL_CHAIN] ⚠️ Companies available: {len(self.shared_data.get('companies', []))}")
                                logger.warning(f"[SKILL_CHAIN] ⚠️ Will use fallback deck generation in _format_deck")
                                # Don't raise - let _format_deck handle fallback
                            else:
                                logger.info(f"[SKILL_CHAIN] ✅ deck-storytelling validation passed: {len(slides)} slides generated")
                        
                        if "companies" in result:
                            companies = result["companies"]
                            logger.info(f"[SKILL_CHAIN] 🏢 {node.skill} returned {len(companies)} companies")
                            for i, company in enumerate(companies):
                                if isinstance(company, dict):
                                    logger.info(f"[SKILL_CHAIN] 🏢   Company {i}: {company.get('company', 'NO_COMPANY_FIELD')} (keys: {list(company.keys())})")
                                else:
                                    logger.warning(f"[SKILL_CHAIN] 🏢   Company {i}: Invalid type {type(company)} - {company}")
                        
                        if "error" in result:
                            logger.error(f"[SKILL_CHAIN] ⚠️ {node.skill} returned error: {result['error']}")
                    
                    else:
                        logger.warning(f"[SKILL_CHAIN] ⚠️ {node.skill} returned non-dict result: {type(result)} - {result}")
                        
                    # Update shared data
                    if isinstance(result, dict):
                        logger.info(f"[SKILL_CHAIN] 🔄 Updating shared_data for skill '{node.skill}'")
                        
                        # Special handling for companies data
                        if "companies" in result:
                            logger.info(f"[SKILL_CHAIN] 🏢 {node.skill} returned companies data with {len(result['companies'])} items")
                            
                            # Log raw companies data
                            logger.info(f"[SKILL_CHAIN] 🏢 Raw companies from {node.skill}:")
                            for i, company in enumerate(result['companies']):
                                if isinstance(company, dict):
                                    logger.info(f"[SKILL_CHAIN] 🏢   Raw {i}: {company.get('company', 'NO_COMPANY_FIELD')} (keys: {list(company.keys())})")
                                else:
                                    logger.warning(f"[SKILL_CHAIN] 🏢   Raw {i}: Invalid type {type(company)} - {company}")
                            
                            # Filter out None values and ensure valid company data
                            valid_companies = [c for c in result["companies"] if c and isinstance(c, dict) and c.get('company')]
                            logger.info(f"[SKILL_CHAIN] 🏢 {node.skill} has {len(valid_companies)} valid companies after filtering")
                            
                            if valid_companies:
                                # Log existing companies in shared_data
                                existing_companies = self.shared_data.get("companies", [])
                                logger.info(f"[SKILL_CHAIN] 🏢 Existing companies in shared_data: {len(existing_companies)}")
                                for i, company in enumerate(existing_companies):
                                    logger.info(f"[SKILL_CHAIN] 🏢   Existing {i}: {company.get('company', 'NO_COMPANY_FIELD')}")
                                
                                existing_handles = {
                                    (company.get('prompt_handle')
                                     or company.get('company_handle')
                                     or company.get('requested_company')
                                     or company.get('company', "")
                                     or "").lower()
                                    for company in existing_companies
                                    if isinstance(company, dict)
                                }
                                logger.info(f"[SKILL_CHAIN] 🏢 {node.skill} existing handles: {existing_handles}")
                                
                                new_companies = []
                                for company in valid_companies:
                                    if not isinstance(company, dict):
                                        logger.warning(f"[SKILL_CHAIN] 🏢 Skipping non-dict company: {type(company)}")
                                        continue
                                    
                                    handle = (company.get('prompt_handle')
                                              or company.get('company_handle')
                                              or company.get('requested_company')
                                              or company.get('company', "")
                                              or "").lower()
                                    
                                    logger.info(f"[SKILL_CHAIN] 🏢 Processing company: {company.get('company', 'NO_COMPANY_FIELD')} (handle: '{handle}')")
                                    
                                    if handle and handle in existing_handles:
                                        logger.info(f"[SKILL_CHAIN] 🏢 Skipping duplicate company handle '{handle}'")
                                        continue
                                    
                                    if handle:
                                        existing_handles.add(handle)
                                        logger.info(f"[SKILL_CHAIN] 🏢 Adding handle '{handle}' to existing_handles")
                                    
                                    new_companies.append(company)
                                    logger.info(f"[SKILL_CHAIN] 🏢 Added company: {company.get('company', 'NO_COMPANY_FIELD')}")
                                
                                if new_companies:
                                    logger.info(f"[SKILL_CHAIN] 🏢 Extending shared_data with {len(new_companies)} new companies")
                                    async with self.shared_data_lock:
                                        if "companies" not in self.shared_data:
                                            self.shared_data["companies"] = []
                                        self.shared_data["companies"].extend(new_companies)
                                        # CRITICAL: Also store the skill result for direct lookup
                                        self.shared_data[node.skill] = result
                                    logger.info(f"[SKILL_CHAIN] 🏢 ✅ Added {len(new_companies)} companies to shared_data")
                                    logger.info(f"[SKILL_CHAIN] 🏢 ✅ Stored {node.skill} result in shared_data for direct lookup")
                                else:
                                    logger.warning(f"[SKILL_CHAIN] 🏢 No new companies to add after deduplication")
                            else:
                                logger.error(f"[SKILL_CHAIN] 🏢 No valid companies found! Raw companies: {result['companies']}")
                                # CRITICAL: Even if validation fails, store the raw result so deck generation can try to use it
                                async with self.shared_data_lock:
                                    self.shared_data[node.skill] = result
                                logger.warning(f"[SKILL_CHAIN] ⚠️ Stored raw {node.skill} result despite validation failure")
                            
                            logger.info(f"[SKILL_CHAIN] 🏢 Total companies in shared_data after {node.skill}: {len(self.shared_data.get('companies', []))}")
                        else:
                            logger.info(f"[SKILL_CHAIN] 🔄 {node.skill} updating shared_data with non-companies data: {list(result.keys())}")
                            async with self.shared_data_lock:
                                self.shared_data.update(result)
                            logger.info(f"[SKILL_CHAIN] 🔄 Updated shared_data keys: {list(self.shared_data.keys())}")
                    
                    # Log shared_data state after each skill
                    logger.info(f"[SKILL_CHAIN] 📋 Shared data after {node.skill}:")
                    logger.info(f"[SKILL_CHAIN]   Keys: {list(self.shared_data.keys())}")
                    if 'companies' in self.shared_data:
                        companies = self.shared_data['companies']
                        logger.info(f"[SKILL_CHAIN]   Companies count: {len(companies)}")
                        for i, company in enumerate(companies):
                            logger.info(f"[SKILL_CHAIN]     Company {i}: {company.get('company', 'NO_COMPANY_FIELD')}")
                    else:
                        logger.info(f"[SKILL_CHAIN]   No 'companies' key in shared_data")
            
            # Log shared_data state after each group
            logger.info(f"[SKILL_CHAIN] 📋 Shared data after group {group_num}:")
            logger.info(f"[SKILL_CHAIN]   Keys: {list(self.shared_data.keys())}")
            if 'companies' in self.shared_data:
                companies = self.shared_data['companies']
                logger.info(f"[SKILL_CHAIN]   Companies count: {len(companies)}")
                for i, company in enumerate(companies):
                    logger.info(f"[SKILL_CHAIN]     Company {i}: {company.get('company', 'NO_COMPANY_FIELD')}")
            else:
                logger.info(f"[SKILL_CHAIN]   No 'companies' key in shared_data")
        
        logger.info(f"[SKILL_CHAIN] 🎉 Skill chain execution completed!")
        logger.info(f"[SKILL_CHAIN] Final results keys: {list(results.keys())}")
        logger.info(f"[SKILL_CHAIN] Final shared_data keys: {list(self.shared_data.keys())}")
        if 'companies' in self.shared_data:
            logger.info(f"[SKILL_CHAIN] Final companies count: {len(self.shared_data['companies'])}")
        
        # Collect warnings from failed/degraded/skipped skills so the frontend can display them
        skill_warnings = []
        for node in chain:
            if node.status == "failed" and isinstance(node.result, dict) and node.result.get("error"):
                skill_warnings.append(f"Skill '{node.skill}' failed: {node.result['error']}")
            elif node.status == "degraded":
                skill_warnings.append(f"Skill '{node.skill}' degraded: {node.result.get('error', 'unavailable')}")
            elif node.status == "skipped":
                skill_warnings.append(f"Skill '{node.skill}' skipped: {node.result.get('reason', 'missing dependencies')}")

        # CRITICAL FIX: Include shared_data in results to ensure data flows to _format_output
        enhanced_results = {
            **results,
            "companies": self.shared_data.get("companies", []),
            "citations": self.citation_manager.get_all_citations(),
            "warnings": skill_warnings,
            "shared_data": self.shared_data  # Include full shared_data for debugging
        }
        
        logger.info(f"[SKILL_CHAIN] Enhanced results keys: {list(enhanced_results.keys())}")
        logger.info(f"[SKILL_CHAIN] Enhanced companies count: {len(enhanced_results.get('companies', []))}")
        
        return enhanced_results
    
    def _extract_fund_params_from_prompt(self, prompt: str) -> Dict[str, Any]:
        """Extract fund parameters from prompt using regex patterns"""
        import re
        
        fund_params = {}
        
        # Portfolio contribution (millions)
        port_contrib_match = re.search(r'portfolio contribution.*?(\d+(?:\.\d+)?)\s*m', prompt, re.IGNORECASE)
        if port_contrib_match:
            fund_params['portfolio_contribution'] = float(port_contrib_match.group(1)) * 1_000_000
            fund_params['fund_size'] = fund_params['portfolio_contribution']  # Use as fund_size
        
        # Fund size
        fund_size_match = re.search(r'(\$?\d+(?:\.\d+)?)\s*m(?:illion)?\s+fund', prompt, re.IGNORECASE)
        if fund_size_match:
            fund_params['fund_size'] = float(fund_size_match.group(1).replace('$', '')) * 1_000_000
        
        # DPI
        dpi_match = re.search(r'(\d+(?:\.\d+)?)\s+dpi', prompt, re.IGNORECASE)
        if dpi_match:
            fund_params['dpi'] = float(dpi_match.group(1))
        
        # TVPI
        tvpi_match = re.search(r'(\d+(?:\.\d+)?)\s+tvpi', prompt, re.IGNORECASE)
        if tvpi_match:
            fund_params['tvpi'] = float(tvpi_match.group(1))
        
        return fund_params

    async def _resolve_company(self, ref: str, allow_external: bool = True) -> Optional[Dict[str, Any]]:
        """Resolve a company reference by name, @mention, or ID.

        Resolution order:
        1. shared_data (already fetched this session)
        2. Grid context (frontend matrix rows)
        3. Portfolio DB by name (fuzzy match via portfolio_service)
        4. Lightweight fetch (1 search, cheap — default for non-@ mentions)
        5. Full external web fetch (4 searches — only for @ mentions or explicit deep dive)

        Any company name should be resolvable — portfolio is checked first,
        then lightweight, then full fetch. No gatekeeping.
        """
        from difflib import SequenceMatcher

        ref_clean = ref.strip().lstrip("@").lower()
        if not ref_clean:
            return None

        is_at_mention = ref.strip().startswith("@")

        # 1. Check shared_data companies (already fetched this session)
        for c in self.shared_data.get("companies", []):
            c_name = (c.get("name") or c.get("companyName") or "").lower()
            if ref_clean == c_name or ref_clean in c_name:
                return c

        # 2. Check grid context rows
        matrix_ctx = self.shared_data.get("matrix_context") or {}
        grid_snapshot = matrix_ctx.get("gridSnapshot") or {}
        grid_rows = grid_snapshot.get("rows", []) if isinstance(grid_snapshot, dict) else grid_snapshot if isinstance(grid_snapshot, list) else []
        best_grid_match = None
        best_grid_score = 0.0
        for row in grid_rows:
            row_name = (row.get("companyName") or row.get("company_name") or "").lower()
            if ref_clean == row_name:
                return {"name": row.get("companyName") or row.get("company_name"), "source": "grid", **row.get("cells", {})}
            score = SequenceMatcher(None, ref_clean, row_name).ratio()
            if ref_clean in row_name or row_name in ref_clean:
                score = max(score, 0.85)
            if score > best_grid_score:
                best_grid_score = score
                best_grid_match = row

        if best_grid_match and best_grid_score > 0.6:
            return {"name": best_grid_match.get("companyName") or best_grid_match.get("company_name"), "source": "grid", **best_grid_match.get("cells", {})}

        # 3. Check portfolio DB
        try:
            from app.services.portfolio_service import portfolio_service
            fund_id = self.shared_data.get("fund_context", {}).get("fundId")
            result = await portfolio_service.get_company_by_name(ref_clean, fund_id=fund_id)
            if result:
                return {**result, "source": "portfolio_db"}
        except Exception as e:
            logger.debug(f"_resolve_company DB fallback failed: {e}")

        # 4. Lightweight fetch first (1 search, cached automatically)
        if allow_external and not is_at_mention:
            try:
                logger.info(f"[RESOLVE_COMPANY] '{ref_clean}' not in portfolio — trying lightweight fetch")
                result = await self._execute_lightweight_diligence({"company_name": ref_clean})
                if result and not result.get("error") and result.get("company"):
                    return {**result["company"], "source": "lightweight"}
            except Exception as e:
                logger.debug(f"_resolve_company lightweight fetch failed: {e}")

        # 5. Full external web fetch — for @ mentions or when lightweight failed
        if allow_external:
            try:
                logger.info(f"[RESOLVE_COMPANY] '{ref_clean}' — full web fetch (@ mention or lightweight failed)")
                result = await self._execute_tool("fetch_company_data", {"companies": [ref_clean]})
                if result and not result.get("error"):
                    fetched = result.get("companies", [])
                    if fetched:
                        async with self.shared_data_lock:
                            existing = self.shared_data.get("companies", [])
                            existing.extend(fetched)
                            self.shared_data["companies"] = existing
                        return {**fetched[0], "source": "external_fetch"}
            except Exception as e:
                logger.debug(f"_resolve_company external fetch failed: {e}")

        return None

    async def _extract_entities(self, prompt: str) -> Dict[str, Any]:
        """Extract companies, funds, and other entities from prompt using LLM"""
        import re
        
        # Quick regex for @mentions — supports multi-word names like @Safe Intelligence
        company_pattern = r'@([\w][\w\s]*[\w]|[\w]+)'
        at_mentions = list(dict.fromkeys(m.strip() for m in re.findall(company_pattern, prompt)))
        
        # Use Claude to semantically extract entities
        extraction_prompt = f"""Extract the following information from this investment prompt:

<prompt>
{prompt}
</prompt>

Return a JSON object with:
{{
  "companies": ["company1", "company2"],  // Company names mentioned (use @ handles if present, otherwise company names)
  "fund_size": 150000000,  // Fund size in USD
  "remaining_capital": 100000000,  // Remaining to deploy in USD
  "dpi": 0.5,  // Current DPI if mentioned
  "tvpi": 2.5,  // Current TVPI if mentioned
  "portfolio_size": 16,  // Number of portfolio companies
  "exits": 2,  // Number of exits
  "fund_year": 3,  // What year of the fund
  "fund_quarter": 2,  // What quarter
  "deployed_capital": 50000000,  // How much deployed
  "check_size_range": [5000000, 15000000],  // Check size range if mentioned
  "target_ownership": 0.12,  // Target ownership % as decimal
  "is_lead": false,  // Whether they lead rounds
  "stage_focus": ["Series B"],  // Fund stage focus
  "metrics_requested": ["irr", "dpi"],  // Specific metrics the user wants (irr, dpi, nav, tvpi, burn, runway, arr, mrr, revenue, valuation, multiple)
  "time_period": "Q3 2024",  // Time reference ("last quarter", "Q3 2024", "since Series B", "YTD", "trailing 12 months")
  "scenario_params": {{"rate_change_bps": 200, "revenue_change_pct": -0.30}},  // Scenario parameters if a what-if query
  "report_type": "ic_memo",  // If generation requested: ic_memo, followon_memo, lp_report, gp_update, comparison, deck
  "exit_value": 5000000000  // Target exit value if mentioned (e.g., "exit at $5B")
}}

RULES:
1. ALL monetary values must be converted to raw USD (no strings like "150M")
2. ALL percentages as decimals (0.12 for 12%)
3. If a value is not mentioned, use null (not 0)
4. Be flexible - "456m fund with 276m left" means fund_size=456000000, remaining_capital=276000000
5. Extract company names even without @ symbols — natural language names count
6. Return ONLY the JSON, no explanation.
7. metrics_requested: only include metrics explicitly asked for or strongly implied
8. scenario_params: only populate for what-if / stress test / scenario queries
9. report_type: only populate when user explicitly asks for a document/memo/report/deck

CRITICAL COMPANY DISAMBIGUATION RULES (PROMPT-ONLY, NO KEYWORD HEURISTICS):
• If a handle like "@Dex" is ambiguous, you MUST pick the single most likely company based on the request context and the fund profile (stage focus, typical check size, sector thesis, geography). Do NOT list multiple options; select one and proceed.
• LinkedIn company/organization pages are a very strong disambiguation signal. Prefer entities with matching LinkedIn org pages. Deprioritize OS features (e.g., Samsung DeX) or crypto exchanges unless the context clearly indicates them.
• If confidence < 0.6, ask ONE brief clarification question; otherwise continue silently.
• Only extract and return information for the selected company. Ignore similarly named products or platforms.
"""

        try:
            result = await self.model_router.get_completion(
                prompt=extraction_prompt,
                capability=ModelCapability.STRUCTURED,
                max_tokens=1000,
                temperature=0,
                json_mode=True,
                fallback_enabled=True
            )
            content = result.get('response', '{}')
            # Extract JSON from response
            import json
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                entities = json.loads(json_match.group(0))
                
                # Ensure companies is a list
                if not entities.get("companies") and at_mentions:
                    entities["companies"] = at_mentions
                
                # Map fund_year to deployment_year for compatibility
                if entities.get("fund_year"):
                    entities["deployment_year"] = entities["fund_year"]
                if entities.get("fund_quarter"):
                    entities["deployment_quarter"] = entities["fund_quarter"]
                
                # Add company_handles for compatibility
                entities["company_handles"] = entities.get("companies", [])
                
                # Remove null values
                entities = {k: v for k, v in entities.items() if v is not None}
                
                logger.info(f"[ENTITY_EXTRACTION] LLM extracted: companies={entities.get('companies')}, fund_size={entities.get('fund_size')}, remaining={entities.get('remaining_capital')}, stage_focus={entities.get('stage_focus')}")
                return entities
            else:
                raise ValueError("No JSON found in Claude response")
                
        except Exception as e:
            logger.warning(f"[ENTITY_EXTRACTION] LLM extraction failed: {e}, falling back to @mentions")
            # Fallback to basic extraction
            return {
                "companies": at_mentions,
                "company_handles": at_mentions
            }
    
    def _clean_company_name_for_search(self, company_name: str) -> str:
        """Clean and normalize company name for search queries
        
        Only splits camelCase and removes @ - preserves original capitalization.
        Converts:
        - @gradientlabs → gradientlabs (then splits if needed)
        - GradientLabs → Gradient Labs
        - OpenAI → OpenAI (preserves acronyms)
        - gradientlabs ai → gradientlabs ai (preserves case)
        """
        import re
        
        # Remove @ symbol
        cleaned = company_name.replace('@', '').strip()
        
        if not cleaned:
            return company_name  # Return original if empty after cleaning
        
        # Convert camelCase to spaced (ArtificialSocieties → Artificial Societies)
        # This preserves capitalization - only adds spaces
        cleaned = re.sub(r'([a-z])([A-Z])', r'\1 \2', cleaned)
        
        # Handle multiple caps (GradientLabs → Gradient Labs, but OpenAI stays OpenAI)
        cleaned = re.sub(r'([A-Z]+)([A-Z][a-z])', r'\1 \2', cleaned)
        
        # Handle common suffixes (preserve case of suffix)
        cleaned = re.sub(r'AI$', ' AI', cleaned)
        cleaned = re.sub(r'AI\s', ' AI ', cleaned)
        
        # Remove extra spaces
        cleaned = re.sub(r'  +', ' ', cleaned).strip()
        
        # Don't force title case - preserve the original capitalization pattern
        # Search engines handle case-insensitive matching anyway
        
        return cleaned
    
    async def _execute_company_fetch(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Fetch company data using Tavily"""
        logger.critical(f"[COMPANY_FETCH] 🔴🔴🔴 _execute_company_fetch CALLED with inputs: {inputs} 🔴🔴🔴")
        logger.info(f"[COMPANY_FETCH] 🔍 STARTING extraction for inputs: {inputs}")
        company = inputs.get("company", "")
        prompt_handle = inputs.get("prompt_handle") or company
        # Fix: Handle None prompt_handle gracefully
        if not prompt_handle:
            prompt_handle = company or "unknown"
        cache_key = str(prompt_handle).lower() if prompt_handle else "unknown"
        
        logger.info(f"[COMPANY_FETCH] 🔍 Processing company='{company}', prompt_handle='{prompt_handle}', cache_key='{cache_key}'")
        
        if not company:
            logger.error(f"[COMPANY_FETCH] ❌ No company name provided! Returning minimal structure.")
            return {
                "companies": [{
                    "company": prompt_handle or "Unknown",
                    "prompt_handle": prompt_handle or "Unknown",
                    "requested_company": prompt_handle or "Unknown",
                    "extraction_failed": True,
                    "error": "No company name provided",
                    "error_type": "MISSING_INPUT",
                    "funding_rounds": [],
                    "team_size": 0,
                    "revenue": 0,
                    "valuation": 0
                }]
            }
        
        # Check if Tavily API key is configured
        if not self.tavily_api_key:
            logger.error(f"[COMPANY_FETCH] ❌ Tavily API key not configured! Cannot fetch company data.")
            return {
                "companies": [{
                    "company": company,
                    "prompt_handle": prompt_handle,
                    "requested_company": prompt_handle,
                    "extraction_failed": True,
                    "error": "Tavily API key not configured",
                    "error_type": "API_KEY_MISSING",
                    "funding_rounds": [],
                    "team_size": 0,
                    "revenue": 0,
                    "valuation": 0
                }]
            }
        
        # Check cache
        if cache_key in self._company_cache:
            cache_entry = self._company_cache[cache_key]
            if datetime.now() - cache_entry["timestamp"] < timedelta(minutes=5):
                cached_company = deepcopy(cache_entry["data"])
                cached_company["prompt_handle"] = prompt_handle
                cached_company.setdefault("requested_company", prompt_handle)
                logger.info(
                    f"[COMPANY_CACHE] Returning cached data for handle '{prompt_handle}' → "
                    f"canonical '{cached_company.get('company', 'UNKNOWN')}'"
                )
                return {"companies": [cached_company]}
        
        # Clean company name for search
        search_name = self._clean_company_name_for_search(company)
        logger.info(f"[COMPANY_FETCH] Cleaned '{company}' → '{search_name}' for search")

        try:
            # Phase 4A: Dynamic search depth based on existing data gaps
            existing_data = inputs.get('existing_data', {})
            gaps = self._identify_data_gaps(existing_data) if existing_data else []
            if gaps:
                search_queries = self._build_gap_specific_queries(search_name, gaps)
                logger.info(f"[COMPANY_FETCH] Dynamic search: {len(gaps)} gaps → {len(search_queries)} queries: {gaps}")
            else:
                # FOCUSED SEARCH QUERIES - Only essential company info
                search_queries = [
                    f'"{search_name}" startup company funding valuation revenue',  # Use quotes for exact match
                    f'"{search_name}" company business model team founders',
                    f'"{search_name}" Series A B C funding round investors',
                    f'"{search_name}" founder CEO CTO LinkedIn profile background',
                    f'"{search_name}" pricing plans cost per user enterprise',
                    f'"{search_name}" competitors alternatives versus vs comparison'
                ]
            
            tasks = [self._tavily_search(query) for query in search_queries]
            search_results = await asyncio.gather(*tasks)
            
            # Log total results count across all queries
            total_results = sum(len(r.get('results', [])) for r in search_results if r)
            logger.info(f"[SEARCH_SUMMARY][{company}] Tavily returned {total_results} total results across {len(search_queries)} queries")
            
            # FALLBACK: If we got zero results, try alternative search strategies
            if total_results == 0:
                logger.warning(f"[SEARCH_FALLBACK][{company}] No results with quoted search, trying alternative strategies")
                
                # Strategy 1: Try without quotes (broader match)
                fallback_queries = [
                    f'{search_name} startup funding',
                    f'{search_name} company',
                    f'{search_name} raised seed series',
                    f'{search_name} founder CEO'
                ]
                logger.info(f"[SEARCH_FALLBACK][{company}] Trying {len(fallback_queries)} unquoted queries")
                fallback_tasks = [self._tavily_search(query) for query in fallback_queries]
                fallback_results = await asyncio.gather(*fallback_tasks)
                
                fallback_total = sum(len(r.get('results', [])) for r in fallback_results if r)
                logger.info(f"[SEARCH_FALLBACK][{company}] Fallback searches returned {fallback_total} results")
                
                if fallback_total > 0:
                    # Merge fallback results into main results
                    search_results.extend(fallback_results)
                    total_results = fallback_total
                else:
                    # Strategy 2: Try even simpler queries - just the company name
                    logger.warning(f"[SEARCH_FALLBACK][{company}] Still no results, trying simple name-only queries")
                    simple_queries = [
                        search_name,
                        search_name.lower(),
                        search_name.replace(' ', ''),
                        f'{search_name} tech',
                        f'{search_name} AI'
                    ]
                    simple_tasks = [self._tavily_search(query) for query in simple_queries]
                    simple_results = await asyncio.gather(*simple_tasks)
                    
                    simple_total = sum(len(r.get('results', [])) for r in simple_results if r)
                    logger.info(f"[SEARCH_FALLBACK][{company}] Simple queries returned {simple_total} results")
                    
                    if simple_total > 0:
                        search_results.extend(simple_results)
                        total_results = simple_total
            
            # Log all results (handle extended search_results from fallbacks)
            all_queries = search_queries.copy()
            if total_results == 0 and len(search_results) > len(search_queries):
                # Add fallback query labels for logging
                all_queries.extend([f"FALLBACK_{i}" for i in range(len(search_results) - len(search_queries))])
            
            for i, result in enumerate(search_results):
                query_label = all_queries[i] if i < len(all_queries) else f"Query_{i}"
                if not result or not result.get("results"):
                    logger.warning(f"[SEARCH][{company}] No results returned for query: {query_label}")
                    continue
                
                results_list = result["results"]
                logger.info(f"[SEARCH][{company}] Query returned {len(results_list)} results: {query_label}")
                
                # Log ALL result sources (not just top 2)
                for idx, hit in enumerate(results_list, start=1):
                    title = hit.get("title", "Untitled")
                    url = hit.get("url", "No URL")
                    published = hit.get("published_date", "No date")
                    snippet = (hit.get("content", "") or "")[:200].replace("\n", " ").strip()
                    source_type = "TechCrunch" if "techcrunch.com" in url else ("News" if any(x in url for x in ["bloomberg", "reuters", "wsj"]) else "Other")
                    logger.info(f"[SEARCH][{company}] Result#{idx} [{source_type}] {title}")
                    logger.info(f"[SEARCH][{company}]   URL: {url} | Published: {published}")
                    logger.info(f"[SEARCH][{company}]   Snippet: {snippet}")
            
            # CRITICAL FIX: Validate search results BEFORE extraction
            # Filter out empty results
            valid_search_results = [r for r in search_results if r and r.get("results")]
            total_valid_results = sum(len(r.get('results', [])) for r in valid_search_results)
            
            if total_valid_results == 0:
                logger.error(f"[EXTRACTION_SKIP][{company}] ⚠️ ZERO search results after all fallbacks! Skipping extraction to avoid empty data.")
                logger.error(f"[EXTRACTION_SKIP][{company}] This means Tavily API returned no results for any query variant.")
                logger.error(f"[EXTRACTION_SKIP][{company}] Possible causes: API key issue, rate limit, or company name not found in web.")
                return {
                    "companies": [{
                        "company": company,
                        "prompt_handle": prompt_handle,
                        "requested_company": prompt_handle,
                        "extraction_failed": True,
                        "error": "No search results found - Tavily API returned zero results for all query variants",
                        "error_type": "SEARCH_FAILURE",
                        "funding_rounds": [],
                        "team_size": 0,
                        "revenue": 0,
                        "valuation": 0
                    }]
                }
            
            logger.info(f"[COMPANY_FETCH][{company}] ✅ Validated {total_valid_results} search results across {len(valid_search_results)} queries - proceeding with extraction")
            
            # Extract citations from search results
            for search_result in valid_search_results:
                if search_result and "results" in search_result:
                    for result in search_result["results"]:
                        if result.get("url") and result.get("title"):
                            self.citation_manager.add_citation(
                                result["url"],
                                result["title"],
                                result.get("content", "")[:200]  # First 200 chars as snippet
                            )
            
            # Extract comprehensive profile using Claude with comprehensive error handling
            try:
                logger.info(f"[COMPANY_FETCH][{company}] 📞 Calling _extract_comprehensive_profile with {len(valid_search_results)} valid search result sets...")
                extracted_data = await self._extract_comprehensive_profile(
                    company_name=company,
                    search_results=valid_search_results
                )
                if not isinstance(extracted_data, dict):
                    logger.warning(f"[COMPANY_FETCH][{company}] ⚠️  _extract_comprehensive_profile returned non-dict, using empty dict")
                    extracted_data = {}
                else:
                    logger.info(f"[COMPANY_FETCH][{company}] ✅ Profile extraction completed with {len(extracted_data)} fields")
                    
            except Exception as e:
                logger.error(f"[COMPANY_FETCH][{company}] ❌ _extract_comprehensive_profile FAILED: {type(e).__name__}: {str(e)}")
                import traceback
                logger.error(f"[COMPANY_FETCH][{company}] 🔴 Stack trace:\n{traceback.format_exc()}")
                # Return partial data instead of completely empty
                extracted_data = {
                    "company": company,
                    "extraction_error": str(e),
                    "extraction_failed": True
                }
            
            # Extract competitors from search results (use valid_search_results)
            competitors_raw = await self._extract_competitors(company, valid_search_results)
            
            # Separate competitors and incumbents
            competitors = [c for c in competitors_raw if c.get('category') == 'direct']
            incumbents = [c for c in competitors_raw if c.get('category') == 'incumbent']
            open_source = [c for c in competitors_raw if c.get('category') == 'open_source']
            
            extracted_data['competitors'] = competitors
            extracted_data['incumbents'] = incumbents
            extracted_data['open_source_alternatives'] = open_source

            extracted_data["prompt_handle"] = prompt_handle
            extracted_data.setdefault("requested_company", prompt_handle)
            extracted_data.setdefault("company_handle", prompt_handle)
            if not extracted_data.get("company") and extracted_data.get("company_name"):
                extracted_data["company"] = extracted_data["company_name"]
            extracted_data.setdefault("display_name", extracted_data.get("company") or prompt_handle)
            
            # CRITICAL: Validate category extraction immediately after extraction
            category = extracted_data.get('category')
            if not category or (isinstance(category, str) and category.strip() == '') or (isinstance(category, str) and category.lower() == 'unknown'):
                logger.error(f"[CATEGORY_EXTRACTION_FAILED] {company}: Claude returned empty/invalid category: '{category}'")
                # Try to infer from business_model
                business_model = str(extracted_data.get('business_model', '')).lower()
                if 'ai' in business_model or 'machine learning' in business_model or 'llm' in business_model:
                    extracted_data['category'] = 'ai_first'
                    logger.info(f"[CATEGORY_INFERRED] {company}: Inferred 'ai_first' from business_model")
                elif 'saas' in business_model or 'software' in business_model:
                    extracted_data['category'] = 'saas'
                    logger.info(f"[CATEGORY_INFERRED] {company}: Inferred 'saas' from business_model")
                else:
                    extracted_data['category'] = 'saas'  # Default fallback
                    logger.warning(f"[CATEGORY_DEFAULT] {company}: Using default 'saas' category")
            
            # Validate vertical extraction
            vertical = extracted_data.get('vertical')
            if not vertical or (isinstance(vertical, str) and vertical.strip() == '') or (isinstance(vertical, str) and vertical.lower() == 'unknown'):
                logger.error(f"[VERTICAL_EXTRACTION_FAILED] {company}: Claude returned empty/invalid vertical: '{vertical}'")
                # Don't default to 'Technology' - it's too generic for TAM search
                extracted_data['vertical'] = ''  # Leave empty to force category extraction
            
            # TAM extraction removed - skip entirely
            extracted_data['market_size'] = {
                'tam': 0,
                'sam': 0,
                'som': 0,
                'status': 'disabled',
                'notes': 'TAM analysis disabled'
            }

            # Calculate next round predictions
            try:
                next_round_data = self.gap_filler.predict_next_round(extracted_data)
                extracted_data["next_round"] = next_round_data
                
                # Log key predictions
                logger.info(f"[NEXT_ROUND] {company} predictions:")
                logger.info(f"  - Timing: {next_round_data['next_round_timing']:.0f} months ({next_round_data['next_round_timing_label']})")
                logger.info(f"  - Stage: {next_round_data['next_round_stage']}")
                logger.info(f"  - Size: ${next_round_data['next_round_size']/1e6:.1f}M")
                logger.info(f"  - Valuation: ${next_round_data['next_round_valuation_pre']/1e6:.0f}M pre")
                logger.info(f"  - Down round risk: {next_round_data['down_round_risk']} ({next_round_data['down_round_probability']*100:.0f}%)")
                logger.info(f"  - Our pro-rata: ${next_round_data['our_prorata_amount']/1e6:.1f}M")
            except Exception as e:
                logger.warning(f"Failed to predict next round for {company}: {e}")
                extracted_data["next_round"] = {}
            
            # Calculate fund fit scoring with actual fund context
            try:
                # Use actual context from API request, with intelligent inference
                stored_context = self.shared_data.get('fund_context', {})
                logger.info(f"[FUND_FIT] Raw stored fund context for {company}: {stored_context}")
                
                # Start with provided context
                fund_context = dict(stored_context)  # Copy to avoid modifying original
                
                # Intelligently infer missing values based on what we know
                # Use fund size from extraction if provided
                fund_size = fund_context.get('fund_size')
                if not fund_size:
                    logger.info("[FUND_FIT] No fund size provided - skipping fund fit analysis")
                    # Skip fund fit scoring entirely
                    fund_fit_result = None
                else:
                    # If we have fund_size but not deployed/remaining, infer based on typical deployment
                    if 'deployed_capital' not in fund_context and 'remaining_capital' not in fund_context:
                        # Assume 40% deployed by default (typical for year 3)
                        fund_context['deployed_capital'] = fund_size * 0.4
                        fund_context['remaining_capital'] = fund_size * 0.6
                    elif 'deployed_capital' in fund_context and 'remaining_capital' not in fund_context:
                        fund_context['remaining_capital'] = fund_size - fund_context['deployed_capital']
                    elif 'remaining_capital' in fund_context and 'deployed_capital' not in fund_context:
                        fund_context['deployed_capital'] = fund_size - fund_context['remaining_capital']
                    
                    # Infer portfolio size based on deployed capital if not provided
                    if 'portfolio_size' not in fund_context and 'portfolio_count' not in fund_context:
                        deployed = fund_context.get('deployed_capital', fund_size * 0.4)
                        # Assume average check size is 2-3% of fund
                        avg_check = fund_size * 0.025
                        fund_context['portfolio_size'] = max(5, int(self._safe_divide(deployed, avg_check, 1)))
                    
                    # Set portfolio_count as alias
                    fund_context['portfolio_count'] = fund_context.get('portfolio_size', 
                                                                       fund_context.get('portfolio_count', 10))
                    fund_context['portfolio_size'] = fund_context['portfolio_count']
                    
                    # Infer fund year based on deployment percentage if not provided
                    if 'fund_year' not in fund_context:
                        # FIXED: Safe division to prevent None/zero errors
                        deployed = fund_context.get('deployed_capital', fund_size * 0.4) if fund_size else 0
                        deployment_pct = self._safe_divide(deployed, fund_size, 0.4)
                        if deployment_pct < 0.2:
                            fund_context['fund_year'] = 1
                        elif deployment_pct < 0.45:
                            fund_context['fund_year'] = 3
                        elif deployment_pct < 0.7:
                            fund_context['fund_year'] = 5
                        else:
                            fund_context['fund_year'] = 7
                    
                    # Set sensible defaults for any still-missing fields
                    defaults = {
                        "fund_size": fund_size,
                        "deployed_capital": fund_size * 0.4,
                        "remaining_capital": fund_size * 0.6,
                        "portfolio_size": 10,
                        "portfolio_count": 10,
                        "current_dpi": 0.4,
                        "target_dpi": 3.0,
                        "dpi": 0.4,  # Alias
                        "target_tvpi": 3.0,  # Alias
                        "tvpi": 0.4,  # Current TVPI
                        "unicorns": 0,
                        "fund_year": 3,
                        "fund_quarter": 2,
                        "is_lead": False,
                        "lead_investor": False,  # Alias
                        "check_size_range": (fund_size * 0.01, fund_size * 0.05),  # 1-5% of fund
                        "target_ownership": 0.10 if fund_size < 100_000_000 else 0.08,  # Smaller funds need more ownership
                        "stage_focus": ["Series A", "Series B"]
                    }
                    
                    # Apply defaults for missing fields
                    for key, default_value in defaults.items():
                        if key not in fund_context or fund_context[key] is None:
                            fund_context[key] = default_value
                    
                    # Parse stage_focus from prompt if not extracted
                    if 'stage_focus' not in fund_context or not fund_context.get('stage_focus'):
                        # Try to parse from original prompt
                        prompt_lower = self.shared_data.get('prompt', '').lower() if hasattr(self, 'shared_data') and self.shared_data else ''
                        if 'seed fund' in prompt_lower or 'pre-seed fund' in prompt_lower:
                            fund_context['stage_focus'] = ['Seed', 'Pre-seed']
                        elif 'series b fund' in prompt_lower or 'series-b fund' in prompt_lower:
                            fund_context['stage_focus'] = ['Series B']
                        elif 'series a fund' in prompt_lower or 'series-a fund' in prompt_lower:
                            fund_context['stage_focus'] = ['Series A']
                        elif 'series a-b fund' in prompt_lower or 'series a/b fund' in prompt_lower or 'series a-b' in prompt_lower:
                            fund_context['stage_focus'] = ['Series A', 'Series B']
                        elif 'series c fund' in prompt_lower or 'series-c fund' in prompt_lower:
                            fund_context['stage_focus'] = ['Series C']
                        else:
                            # Default based on fund size if not explicitly mentioned
                            if fund_size and fund_size < 50_000_000:
                                fund_context['stage_focus'] = ['Seed']
                            elif fund_size and fund_size < 150_000_000:
                                fund_context['stage_focus'] = ['Series A', 'Series B']
                            else:
                                fund_context['stage_focus'] = ['Series B', 'Series C']
                    
                    # CRITICAL FIX: Handle None values in format strings
                    remaining_capital = fund_context.get('remaining_capital') or 0
                    logger.info(f"[FUND_FIT] Using fund context: size=${fund_context['fund_size']/1e6:.0f}M, remaining=${remaining_capital/1e6:.0f}M, portfolio={fund_context['portfolio_size']}, year={fund_context['fund_year']}, stage_focus={fund_context.get('stage_focus')}")
                    
                    # Score fund fit using the intelligent gap filler
                    fund_fit_result = self.gap_filler.score_fund_fit(
                        extracted_data,
                        {},  # Empty inferences dict since we already applied them
                        fund_context
                    )
                
                # Add fund fit data to extracted_data
                if fund_fit_result:
                    extracted_data["fund_fit_score"] = fund_fit_result.get("overall_score", 0)
                    extracted_data["fund_fit_recommendation"] = fund_fit_result.get("recommendation", "")
                    extracted_data["fund_fit_action"] = fund_fit_result.get("action", "")
                    extracted_data["fund_fit_reasons"] = fund_fit_result.get("reasons", [])
                    extracted_data["fund_fit_confidence"] = fund_fit_result.get("confidence", 0)
                    extracted_data["fund_economics_score"] = fund_fit_result.get("fund_economics_score", 0)
                    extracted_data["optimal_check_size"] = fund_fit_result.get("selected_check", 0)
                    extracted_data["target_ownership_pct"] = fund_fit_result.get("target_ownership", 0)
                    extracted_data["actual_ownership_pct"] = fund_fit_result.get("selected_ownership", 0)
                    extracted_data["total_capital_required"] = fund_fit_result.get("required_check_for_target", 0)
                    extracted_data["exit_ownership_pct"] = fund_fit_result.get("exit_ownership", 0)
                    extracted_data["exit_proceeds"] = fund_fit_result.get("expected_proceeds", 0)
                    extracted_data["expected_irr"] = fund_fit_result.get("expected_irr", 0)
                    extracted_data["fund_fit_details"] = fund_fit_result.get("specific_recommendations", {})
                    
                    # BACKTEST OWNERSHIP using PrePostCapTable
                    optimal_check = fund_fit_result.get("selected_check", 0)
                    if optimal_check > 0 and self.cap_table_service:
                        try:
                            # Calculate our ownership at next round entry
                            # Try all possible valuation sources
                            current_val = self._get_field_safe(extracted_data, 'valuation') or 0
                            if current_val == 0:
                                current_val = (extracted_data.get('inferred_valuation') or 
                                             extracted_data.get('latest_valuation') or 
                                             extracted_data.get('valuation') or 0)
                            
                            # If still no valuation, calculate from funding round
                            if current_val is None or current_val == 0:
                                funding = (extracted_data.get('total_funding') or 
                                          (extracted_data.get('funding_rounds', [{}])[-1].get('amount', 0) if extracted_data.get('funding_rounds') else 0) or 0)
                                if funding > 0:
                                    # Seed round: valuation typically 3-4x funding amount
                                    # Series A: 2-3x, Series B: 1.5-2x
                                    stage = extracted_data.get('stage', 'Series A').lower()
                                    if 'seed' in stage or 'pre-seed' in stage:
                                        current_val = funding * 3.5
                                    elif 'series a' in stage:
                                        current_val = funding * 2.5
                                    elif 'series b' in stage:
                                        current_val = funding * 2.0
                                    else:
                                        current_val = funding * 2.0
                                    logger.info(f"Calculated valuation from funding for {company}: ${funding/1e6:.1f}M funding → ${current_val/1e6:.1f}M valuation")
                            
                            # Skip if still no valuation
                            if current_val is None or current_val == 0:
                                logger.warning(f"Cannot calculate ownership backtesting for {company}: no valuation available. Sources tried: extracted={extracted_data.get('valuation')}, inferred={extracted_data.get('inferred_valuation')}, latest={extracted_data.get('latest_valuation')}, funding={extracted_data.get('total_funding')}")
                                extracted_data["entry_ownership"] = 0
                                extracted_data["exit_ownership_without_prorata"] = 0
                            else:
                                # Check if company is raising next round now (e.g., Seed company raising Series A)
                                # If next_round timing is imminent (0-6 months), use that valuation directly
                                next_round_data = extracted_data.get('next_round', {})
                                next_round_timing = next_round_data.get('next_round_timing', 999)
                                next_round_valuation_pre = next_round_data.get('next_round_valuation_pre', 0)
                                
                                # If raising next round soon (within 6 months), use that round's valuation
                                if next_round_timing <= 6 and next_round_valuation_pre > 0:
                                    # Company is raising the next round now - use that valuation
                                    entry_valuation = next_round_valuation_pre
                                    logger.info(f"Using next round valuation for {company}: ${entry_valuation/1e6:.0f}M pre (raising {next_round_data.get('next_round_stage', 'next round')} in {next_round_timing:.0f} months)")
                                else:
                                    # Calculate next round valuation from current valuation
                                    stage = extracted_data.get('stage', 'Series A')
                                    if 'seed' in stage.lower() or 'pre-seed' in stage.lower():
                                        next_round_step_up = 3.0  # Seed to A typically 3x
                                    elif 'series a' in stage.lower():
                                        next_round_step_up = 2.5  # A to B typically 2.5x
                                    elif 'series b' in stage.lower():
                                        next_round_step_up = 2.0  # B to C typically 2x
                                    else:
                                        next_round_step_up = 1.5  # Later stages
                                    
                                    entry_valuation = current_val * next_round_step_up
                                    logger.info(f"Projected next round valuation for {company}: ${entry_valuation/1e6:.0f}M (from ${current_val/1e6:.0f}M current at {next_round_step_up}x step-up)")
                                
                                # Calculate ownership at entry (using pre-money valuation)
                                our_ownership = self._safe_divide(optimal_check, (entry_valuation + optimal_check), 0.1)
                                
                                # For future dilution modeling, use the next round after entry
                                next_round_valuation = entry_valuation * 2.5  # Typical step-up after our entry
                                
                                # Use PrePostCapTable to model dilution through to exit
                                # Assume 2 more rounds after our investment
                                future_rounds_size = optimal_check * 3  # Next round typically 3x larger
                                future_pre_money = next_round_valuation  # Already calculated above
                                
                                cap_table_projection = self.cap_table_service.calculate_pro_rata_investment(
                                    current_ownership=our_ownership,
                                    new_money_raised=future_rounds_size,
                                    pre_money_valuation=future_pre_money
                                )
                                
                                # Store backtested ownership data
                                extracted_data["entry_ownership"] = our_ownership * 100
                                extracted_data["exit_ownership_without_prorata"] = cap_table_projection["ownership_without_pro_rata"]
                                extracted_data["dilution_to_exit"] = cap_table_projection["dilution_if_no_pro_rata"]
                                extracted_data["pro_rata_needed"] = cap_table_projection["pro_rata_investment_needed"]
                                extracted_data["entry_valuation"] = entry_valuation
                                extracted_data["next_round_valuation"] = next_round_valuation
                                
                                logger.info(f"Ownership backtesting for {company}: Entry={our_ownership*100:.1f}%, Exit={cap_table_projection['ownership_without_pro_rata']:.1f}%, Pro-rata needed=${cap_table_projection['pro_rata_investment_needed']/1e6:.1f}M")
                        except Exception as e:
                            logger.warning(
                                f"Ownership backtesting failed for {company}: {e}. "
                                f"Valuation sources tried: extracted={extracted_data.get('valuation')}, "
                                f"inferred={extracted_data.get('inferred_valuation')}, "
                                f"latest={extracted_data.get('latest_valuation')}, "
                                f"funding={extracted_data.get('total_funding')}"
                            )
                    
                    logger.info(f"Fund fit for {company}: Score={fund_fit_result.get('overall_score', 0):.1f}, Check=${fund_fit_result.get('selected_check', 0)/1e6:.1f}M, Ownership={fund_fit_result.get('selected_ownership', 0)*100:.1f}%")
            except Exception as e:
                logger.error(f"Fund fit scoring failed for {company}: {e}")
                # Set default values if scoring fails
                extracted_data["fund_fit_score"] = 50
                extracted_data["fund_fit_recommendation"] = "Unable to calculate"
            
            # If extraction missed funding data, synthesize rounds from stage benchmarks
            if not extracted_data.get("funding_rounds"):
                synthetic_rounds = self.gap_filler.generate_stage_based_funding_rounds(extracted_data)
                if synthetic_rounds:
                    extracted_data["funding_rounds"] = synthetic_rounds
                    synthetic_total = sum((round_info.get("amount", 0) or 0) for round_info in synthetic_rounds)
                    if synthetic_total > 0:
                        if not extracted_data.get("total_funding"):
                            extracted_data["total_funding"] = synthetic_total
                        if not extracted_data.get("total_raised"):
                            extracted_data["total_raised"] = synthetic_total
                    extracted_data["funding_data_source"] = "stage_inferred"
                    logger.info(f"[FUNDING_FALLBACK] Generated {len(synthetic_rounds)} stage-based funding rounds for {company}")
            
            # Calculate PWERM scenarios and ownership evolution right after fund fit
            try:
                # Map funding stage to Stage enum
                stage_map = {
                    "Pre-Seed": Stage.PRE_SEED,
                    "Pre Seed": Stage.PRE_SEED,
                    "Seed": Stage.SEED,
                    "Series A": Stage.SERIES_A,
                    "Series B": Stage.SERIES_B,
                    "Series C": Stage.SERIES_C,
                    "Series D": Stage.GROWTH,
                    "Series E": Stage.GROWTH,
                    "Growth": Stage.GROWTH,
                    "Late": Stage.LATE,
                    "Late Stage Private": Stage.LATE,
                    "Late Stage": Stage.LATE
                }
                company_stage = stage_map.get(extracted_data.get("stage", "Series A"), Stage.SERIES_A)
                
                # Create valuation request with guaranteed values
                valuation_for_pwerm = (
                    extracted_data.get("valuation") or 
                    extracted_data.get("inferred_valuation") or
                    (extracted_data.get("total_funding", 0) * 3 if extracted_data.get("total_funding") else 0) or
                    100_000_000  # Ultimate fallback
                )
                
                # Log what we're sending to PWERM
                logger.info(f"PWERM valuation for {company}: valuation={valuation_for_pwerm}, "
                          f"extracted={extracted_data.get('valuation')}, "
                          f"inferred={extracted_data.get('inferred_valuation')}, "
                          f"funding={extracted_data.get('total_funding')}")
                
                # Use inferred_revenue if revenue is None - CRITICAL FIX
                revenue = ensure_numeric(extracted_data.get("revenue"), 0)
                if revenue == 0:
                    revenue = ensure_numeric(extracted_data.get("inferred_revenue"), 0)
                    if revenue == 0:
                        revenue = ensure_numeric(extracted_data.get("arr"), 0)
                        if revenue == 0:
                            revenue = ensure_numeric(extracted_data.get("inferred_arr"), 1_000_000)
                
                # Use inferred_growth_rate if growth_rate is None
                growth_rate = ensure_numeric(extracted_data.get("growth_rate"), 0)
                if growth_rate == 0:
                    growth_rate = ensure_numeric(extracted_data.get("inferred_growth_rate"), 1.5)
                
                # Extract inferred_valuation if available
                inferred_val = ensure_numeric(extracted_data.get("inferred_valuation"), None) if extracted_data.get("inferred_valuation") is not None else None
                val_request = ValuationRequest(
                    company_name=company,
                    stage=company_stage,
                    revenue=revenue,
                    growth_rate=growth_rate,
                    last_round_valuation=valuation_for_pwerm if valuation_for_pwerm and valuation_for_pwerm > 0 else None,
                    inferred_valuation=inferred_val,
                    total_raised=self._get_field_safe(extracted_data, "total_funding")
                )
                
                # Calculate PWERM scenarios (includes return metrics)
                pwerm_result = await self.valuation_engine._calculate_pwerm(val_request)
                full_scenarios = pwerm_result.scenarios
                
                # Model cap table evolution for each scenario
                check_size = safe_get_value(extracted_data.get("optimal_check_size"), 0)
                valuation = self._get_field_with_fallback(extracted_data, 'valuation', 0)
                post_money = valuation + check_size
                our_investment = {
                    'amount': check_size,
                    'ownership': self._safe_divide(check_size, post_money, 0.08)
                }
                
                # Add cap table evolution to each scenario
                for scenario in full_scenarios:
                    self.valuation_engine.model_cap_table_evolution(
                        scenario,
                        extracted_data,  # Contains geography, funding history, etc.
                        our_investment
                    )
                
                # Generate return curves for each scenario
                self.valuation_engine.generate_return_curves(full_scenarios, our_investment)
                
                # Calculate breakpoint probability distributions
                breakpoint_distributions = self.valuation_engine.calculate_breakpoint_distributions(full_scenarios)
                
                # Add PWERM data to extracted_data
                extracted_data["pwerm_scenarios"] = full_scenarios
                extracted_data["pwerm_valuation"] = pwerm_result.fair_value
                extracted_data["breakpoint_distributions"] = breakpoint_distributions
                
                # Use PrePostCapTable service for ownership evolution
                check_size = safe_get_value(extracted_data.get("optimal_check_size"), 5_000_000)  # Default $5M check
                # CRITICAL: Use inferred_valuation which should always exist
                valuation = extracted_data.get("inferred_valuation", 0)
                if not valuation or valuation == 0:
                    # Emergency fallback - calculate from funding
                    valuation = extracted_data.get("total_raised", 10_000_000) * 3  # 3x raised as fallback
                    logger.warning(f"No valuation for {company}, using fallback: ${valuation:,.0f}")
                
                # Proper post-money calculation (no hardcoded 1.2x)
                post_money = valuation + check_size
                entry_ownership = self._safe_divide(check_size, post_money, 0)
                
                # Always use intelligent gap filler for dynamic dilution calculations
                stage = extracted_data.get("stage", "Series A")
                
                # Map stage to expected rounds to exit
                stage_to_rounds = {
                    "Seed": 4,  # Seed -> A -> B -> C -> Exit
                    "Series A": 3,  # A -> B -> C -> Exit
                    "Series B": 2,  # B -> C -> Exit
                    "Series C": 1,  # C -> Exit
                    "Series D": 1,  # D -> Exit
                    "Growth": 1,   # Growth -> Exit
                }
                rounds_to_exit = stage_to_rounds.get(stage, 2)
                
                dilution_calc = self.gap_filler.calculate_exit_dilution_scenarios(
                    initial_ownership=entry_ownership,
                    rounds_to_exit=rounds_to_exit,
                    company_data=extracted_data  # Pass full context for quality adjustments
                )
                
                # Use actual calculated dilution rates from the service
                # Now properly uses growth rates, investor quality, geography
                exit_ownership_with_followon = dilution_calc.get("with_pro_rata", entry_ownership * 0.8)  # Fallback
                exit_ownership_no_followon = dilution_calc.get("without_pro_rata", entry_ownership * 0.5)  # Fallback
                
                # Use calculated values if available
                if dilution_calc.get("with_pro_rata"):
                    exit_ownership_with_followon = dilution_calc.get("with_pro_rata")
                if dilution_calc.get("without_pro_rata"):
                    exit_ownership_no_followon = dilution_calc.get("without_pro_rata")
                
                # Calculate check size constraints based on fund context
                fund_context = self.shared_data.get('fund_context', {})
                fund_size = fund_context.get('fund_size')
                remaining = fund_context.get('remaining_capital')
                
                # Apply fund constraints
                concentration_limit = fund_size * 0.10  # Max 10% of fund per company
                target_ownership = 0.08  # 8% minimum ownership target
                
                # Calculate optimal check based on constraints
                desired_check = valuation * target_ownership  # For 8% ownership
                max_check = min(concentration_limit, remaining * 0.25)  # Max 25% of remaining
                actual_check = min(desired_check, max_check, check_size)
                
                # Update check size if constrained
                if actual_check != check_size:
                    logger.info(f"Check size constrained from ${check_size/1e6:.1f}M to ${actual_check/1e6:.1f}M due to fund limits")
                    check_size = actual_check
                    # Recalculate ownership with constrained check
                    entry_ownership = self._safe_divide(check_size, post_money, 0)
                
                # Calculate follow-on capital needed based on stage
                funding_rounds = extracted_data.get("funding_rounds", [])
                
                if funding_rounds:
                    # Calculate full cap table history for more accurate projections
                    try:
                        cap_table_history = self.cap_table_service.calculate_full_cap_table_history(extracted_data)
                        if not cap_table_history:
                            cap_table_history = {"history": [], "ownership_evolution": {}}
                    except Exception as e:
                        logger.warning(f"Cap table calculation failed: {e}")
                        cap_table_history = {"history": [], "ownership_evolution": {}}
                    
                    # Extract ownership evolution if available
                    if cap_table_history and "history" in cap_table_history:
                        history = cap_table_history["history"]
                        
                        # Calculate actual follow-on requirements from historical data
                        if history and len(history) > 1:
                            # Sum up funding amounts from future rounds
                            future_funding = sum(
                                h.get("amount", 0) for h in history[1:]
                            )
                            # Pro-rata would be our share of future funding
                            total_followon = future_funding * entry_ownership if future_funding > 0 else check_size * 2
                        else:
                            # Use stage-based reserve ratio
                            reserve_ratios = {"Seed": 3, "Series A": 2.5, "Series B": 2, "Series C": 1.5}
                            total_followon = check_size * reserve_ratios.get(stage, 2)
                    else:
                        # Use stage-based reserve ratio
                        reserve_ratios = {"Seed": 3, "Series A": 2.5, "Series B": 2, "Series C": 1.5}
                        total_followon = check_size * reserve_ratios.get(stage, 2)
                else:
                    # No funding history - use stage-based defaults
                    reserve_ratios = {"Seed": 3, "Series A": 2.5, "Series B": 2, "Series C": 1.5}
                    total_followon = check_size * reserve_ratios.get(stage, 2)
                
                # Get exit multiple from ValuationEngineService (not hardcoded 5.0)
                # Use inferred_revenue if revenue is None - CRITICAL FIX
                revenue = ensure_numeric(extracted_data.get("revenue"), 0)
                if revenue == 0:
                    revenue = ensure_numeric(extracted_data.get("inferred_revenue"), 0)
                    if revenue == 0:
                        revenue = ensure_numeric(extracted_data.get("arr"), 0)
                        if revenue == 0:
                            revenue = ensure_numeric(extracted_data.get("inferred_arr"), 1_000_000)
                
                # Use inferred_growth_rate if growth_rate is None
                growth_rate = ensure_numeric(extracted_data.get("growth_rate"), 0)
                if growth_rate == 0:
                    growth_rate = ensure_numeric(extracted_data.get("inferred_growth_rate"), 1.0)
                
                # Use inferred_valuation if valuation is None - CRITICAL FIX
                valuation = ensure_numeric(extracted_data.get("valuation"), 0)
                if valuation == 0:
                    valuation = ensure_numeric(extracted_data.get("inferred_valuation"), 0)
                    if valuation == 0:
                        # Calculate from total_raised as fallback
                        valuation = ensure_numeric(extracted_data.get("total_funding"), 0) * 3
                
                # Extract inferred_valuation if available
                inferred_val = ensure_numeric(extracted_data.get("inferred_valuation"), None) if extracted_data.get("inferred_valuation") is not None else None
                valuation_request = ValuationRequest(
                    company_name=company,
                    stage=self._get_stage_enum(extracted_data.get("stage", "Series A")),
                    revenue=revenue,
                    growth_rate=growth_rate,
                    last_round_valuation=valuation if valuation and valuation > 0 else None,
                    inferred_valuation=inferred_val,
                    total_raised=self._get_field_safe(extracted_data, "total_funding"),
                    category=extracted_data.get("category", "SaaS"),
                    ai_component_percentage=extracted_data.get("ai_percentage", 0)
                )
                valuation_result = await self.valuation_engine.calculate_valuation(valuation_request)
                # Get exit multiple from PWERM scenarios or use default
                exit_multiple = 5.0
                if valuation_result.scenarios:
                    # Calculate weighted average exit multiple from PWERM scenarios
                    weighted_exit_value = sum(s.exit_value * s.probability for s in valuation_result.scenarios)
                    revenue = ensure_numeric(extracted_data.get("revenue"), 0)
                    if revenue > 0:
                        exit_multiple = safe_divide(safe_divide(weighted_exit_value, revenue, 10), 1000000, 10)  # Convert to multiple
                    else:
                        exit_multiple = 10  # Default multiple if no revenue
                
                # Build follow-on scenarios with service data
                followon_scenarios = {
                    "no_followon": {
                        "capital_deployed": check_size,
                        "final_ownership": exit_ownership_no_followon,
                        "exit_value": self._safe_multiply(valuation, exit_multiple, exit_ownership_no_followon),
                        "moic": self._safe_divide(self._safe_multiply(valuation, exit_multiple, exit_ownership_no_followon), check_size, 0)
                    },
                    "with_followon": {
                        "capital_deployed": check_size + total_followon,
                        "final_ownership": exit_ownership_with_followon,
                        "exit_value": self._safe_multiply(valuation, exit_multiple, exit_ownership_with_followon),
                        "moic": self._safe_divide(self._safe_multiply(valuation, exit_multiple, exit_ownership_with_followon), (check_size + total_followon), 0)
                    }
                }
                
                # Add ownership evolution data
                extracted_data["ownership_evolution"] = {
                    "entry_ownership": entry_ownership,
                    "exit_ownership_no_followon": followon_scenarios["no_followon"]["final_ownership"],
                    "exit_ownership_with_followon": followon_scenarios["with_followon"]["final_ownership"],
                    "followon_capital_required": followon_scenarios["with_followon"]["capital_deployed"] - check_size,
                    "followon_scenarios": followon_scenarios
                }
                
                logger.info(f"Added PWERM scenarios for {company}: {len(full_scenarios)} scenarios, PWERM value: ${pwerm_result.fair_value:,.0f}")
                
            except Exception as e:
                logger.error(f"Failed to calculate PWERM scenarios for {company}: {e}")
                # Continue without PWERM data but ALWAYS set ownership_evolution
                
                # Ensure ownership_evolution is ALWAYS set (even if PWERM fails)
                if "ownership_evolution" not in extracted_data:
                    # Recalculate basic ownership data
                    check_size = safe_get_value(extracted_data.get("optimal_check_size"), 0)
                    valuation = self._get_field_with_fallback(extracted_data, 'valuation', 0)
                    post_money = valuation + check_size
                    entry_ownership = self._safe_divide(check_size, post_money, 0.10)
                    
                    # Get stage for rounds to exit
                    stage = extracted_data.get("stage", "Series A")
                    stage_to_rounds = {
                        "Seed": 4, "Series A": 3, "Series B": 2, 
                        "Series C": 1, "Series D": 1, "Growth": 1
                    }
                    rounds_to_exit = stage_to_rounds.get(stage, 2)
                    
                    # Calculate dilution with company context
                    dilution_calc = self.gap_filler.calculate_exit_dilution_scenarios(
                        initial_ownership=entry_ownership,
                        rounds_to_exit=rounds_to_exit,
                        company_data=extracted_data
                    )
                    
                    # Get exit ownership from dilution calc
                    exit_no_followon = dilution_calc.get("without_pro_rata", entry_ownership * 0.5)
                    exit_with_followon = dilution_calc.get("with_pro_rata", entry_ownership * 0.8)
                    
                    # Calculate follow-on capital required
                    reserve_ratios = {"Seed": 3, "Series A": 2.5, "Series B": 2, "Series C": 1.5}
                    followon_capital = check_size * reserve_ratios.get(stage, 2)
                    
                    # Set ownership evolution so charts always have data
                    extracted_data["ownership_evolution"] = {
                        "entry_ownership": entry_ownership,
                        "exit_ownership_no_followon": exit_no_followon,
                        "exit_ownership_with_followon": exit_with_followon,
                        "followon_capital_required": followon_capital,
                        "followon_scenarios": {
                            "no_followon": {
                                "capital_deployed": check_size,
                                "final_ownership": exit_no_followon
                            },
                            "with_followon": {
                                "capital_deployed": check_size + followon_capital,
                                "final_ownership": exit_with_followon
                            }
                        }
                    }
                    logger.info(f"Set fallback ownership evolution for {company}: Entry={entry_ownership:.1%}, Exit={exit_no_followon:.1%}")
            
            # Cache the result
            self._company_cache[cache_key] = {
                "timestamp": datetime.now(),
                "data": deepcopy(extracted_data)
            }

            # Persist enriched data to companies table (non-blocking best-effort)
            try:
                await self._persist_company_to_db(
                    extracted_data,
                    fund_id=inputs.get("fund_id"),
                )
            except Exception as persist_err:
                logger.warning(f"[COMPANY_FETCH] DB persist failed (non-blocking): {persist_err}")

            # CRITICAL FIX: Initialize key_metrics dict to prevent downstream errors
            if extracted_data and isinstance(extracted_data, dict):
                if "key_metrics" not in extracted_data or extracted_data["key_metrics"] is None:
                    extracted_data["key_metrics"] = {
                        "gross_margin": extracted_data.get("gross_margin", 0.7),
                        "burn_rate": extracted_data.get("burn_rate"),
                        "runway_months": extracted_data.get("runway_months"),
                        "ltv_cac_ratio": extracted_data.get("ltv_cac_ratio", 3.0)
                    }
                    logger.info(f"[KEY_METRICS] Initialized key_metrics for {company}")

            # Return with companies list format - ensure extracted_data is valid
            # CRITICAL: Ensure 'company' field exists
            if not extracted_data.get('company'):
                extracted_data['company'] = company
            
            if extracted_data and isinstance(extracted_data, dict) and extracted_data.get('company'):
                # Stamp fetch time for freshness gating
                extracted_data["_fetched_at"] = datetime.now().astimezone().isoformat()
                # Validate data before returning to prevent None errors downstream
                validated_data = validate_company_data(deepcopy(extracted_data))
                logger.info(
                    f"[COMPANY_FETCH] ✅ Handle '{prompt_handle}' resolved to '{validated_data.get('company')}' - data validated"
                )
                logger.info(f"[COMPANY_FETCH] ✅ Returning {len([validated_data])} company with keys: {list(validated_data.keys())[:10]}")
                
                logger.critical(f"[COMPANY_FETCH] 🔴🔴🔴 RETURNING {len([validated_data])} company: {validated_data.get('company')} 🔴🔴🔴")
                
                return {"companies": [validated_data]}
            else:
                logger.error(f"Invalid extracted data for {company}: {extracted_data}")
                # Return minimal valid structure instead of error
                minimal_data = {
                    "company": company,
                    "business_model": "Unknown",
                    "stage": "Unknown",
                    "total_funding": 0,
                    "funding_rounds": [],
                    "team_size": 0,
                    "revenue": 0,
                    "error": "Extraction failed"
                }
                return {"companies": [minimal_data]}
            
        except Exception as e:
            # COMPREHENSIVE ERROR LOGGING: Catch and log ALL exceptions
            logger.error(f"[COMPANY_FETCH][{company}] ❌ COMPLETE FAILURE in _execute_company_fetch")
            logger.error(f"[COMPANY_FETCH][{company}] 🔴 Error type: {type(e).__name__}")
            logger.error(f"[COMPANY_FETCH][{company}] 🔴 Error message: {str(e)}")
            import traceback
            logger.error(f"[COMPANY_FETCH][{company}] 🔴 Full stack trace:\n{traceback.format_exc()}")
            
            # Return minimal valid structure with error info
            return {
                "companies": [{
                    "company": company,
                    "prompt_handle": prompt_handle,
                    "requested_company": prompt_handle,
                    "extraction_failed": True,
                    "error": str(e),
                    "error_type": type(e).__name__,
                    "funding_rounds": [],
                    "team_size": 0,
                    "revenue": 0,
                    "valuation": 0
                }],
                "error": str(e)
            }
    
    async def _is_session_healthy(self) -> bool:
        """Check if the current session is healthy and usable"""
        if not self.session:
            return False
        try:
            # Check if session is closed
            if self.session.closed:
                return False
            # Check if connector is closed
            if hasattr(self.session, '_connector') and self.session._connector:
                if hasattr(self.session._connector, 'closed') and self.session._connector.closed:
                    return False
            return True
        except Exception as e:
            logger.warning(f"[TAVILY] Session health check failed: {e}")
            return False
    
    async def _create_tavily_session(self, use_ssl_verification: bool = True) -> None:
        """Create aiohttp session with proper SSL handling
        
        Args:
            use_ssl_verification: If True, try to use SSL verification. If False, disable it.
        """
        import ssl
        import aiohttp
        
        # Close existing session if it exists
        if self.session:
            try:
                await self.session.close()
            except Exception as close_error:
                logger.warning(f"[TAVILY] Error closing existing session: {close_error}")
            self.session = None
        
        if not use_ssl_verification:
            # Create session without SSL verification (fallback)
            connector = aiohttp.TCPConnector(ssl=False)
            self.session = aiohttp.ClientSession(connector=connector)
            logger.warning("[TAVILY] Created session WITHOUT SSL verification (fallback mode)")
            return
        
        # Try to use certifi for SSL certs
        try:
            import certifi
            ssl_context = ssl.create_default_context(cafile=certifi.where())
            connector = aiohttp.TCPConnector(ssl=ssl_context)
            self.session = aiohttp.ClientSession(connector=connector)
            logger.info("[TAVILY] Created session with SSL verification (certifi)")
        except ImportError:
            logger.warning("[TAVILY] certifi not installed, trying system certs")
            try:
                ssl_context = ssl.create_default_context()
                connector = aiohttp.TCPConnector(ssl=ssl_context)
                self.session = aiohttp.ClientSession(connector=connector)
                logger.info("[TAVILY] Created session with system SSL certs")
            except Exception as ssl_error:
                logger.warning(f"[TAVILY] SSL context creation failed: {ssl_error}, disabling verification")
                connector = aiohttp.TCPConnector(ssl=False)
                self.session = aiohttp.ClientSession(connector=connector)
                logger.warning("[TAVILY] Using session WITHOUT SSL verification (dev mode)")
        except Exception as ssl_error:
            logger.warning(f"[TAVILY] SSL context creation failed: {ssl_error}, disabling verification")
            connector = aiohttp.TCPConnector(ssl=False)
            self.session = aiohttp.ClientSession(connector=connector)
            logger.warning("[TAVILY] Using session WITHOUT SSL verification (dev mode)")
    
    async def _tavily_search(self, query: str) -> Dict[str, Any]:
        """Execute Tavily search with caching"""
        # Check cache
        if query in self._tavily_cache:
            return self._tavily_cache[query]
        
        # Check if API key exists FIRST
        if not self.tavily_api_key:
            logger.error("🔴 [TAVILY] API key not found! Cannot search.")
            logger.error("🔴 [TAVILY] Set TAVILY_API_KEY environment variable!")
            return {"results": [], "error": "API key missing"}
        
        try:
            # Initialize or recreate session if needed
            if not self.session or not await self._is_session_healthy():
                if self.session:
                    logger.info("[TAVILY] Session unhealthy, recreating")
                else:
                    logger.info("[TAVILY] Creating new session (nonexistent)")
                await self._create_tavily_session(use_ssl_verification=True)
            
            # Use query as-is - Tavily handles this better
            # Don't add country exclusions to search query
            url = "https://api.tavily.com/search"
            headers = {
                "Content-Type": "application/json"
            }
                
            payload = {
                "api_key": self.tavily_api_key,
                "query": query,
                "search_depth": "advanced",
                "max_results": 5
            }
            
            # Log the actual query being sent
            logger.info(f"[TAVILY] Searching: {query[:100]}")
            logger.info(f"[TAVILY] API key present: {bool(self.tavily_api_key)}, starts with: {self.tavily_api_key[:10] if self.tavily_api_key else 'None'}")
            
            # Attempt request with SSL error handling and retry logic
            max_retries = 2
            for attempt in range(max_retries):
                try:
                    # Check session health before each attempt
                    if not await self._is_session_healthy():
                        logger.warning(f"[TAVILY] Session unhealthy before attempt {attempt + 1}, recreating")
                        await self._create_tavily_session(use_ssl_verification=(attempt == 0))
                    
                    async with self.session.post(url, json=payload, headers=headers) as response:
                        if response.status == 200:
                            result = await response.json()
                            # Validate that we actually got results
                            results_count = len(result.get("results", []))
                            if results_count == 0:
                                logger.warning(f"[TAVILY] Search returned 200 OK but zero results for query: {query[:50]}")
                            else:
                                logger.info(f"[TAVILY] Search successful: {results_count} results for query: {query[:50]}")
                            self._tavily_cache[query] = result
                            return result
                        else:
                            error_text = await response.text()
                            logger.error(f"Tavily search failed: {response.status}, Query: {query[:50]}, Error: {error_text[:200]}")
                            return {"results": []}
                            
                except (aiohttp.ClientConnectorError, aiohttp.ClientSSLError, aiohttp.ClientError, RuntimeError) as conn_error:
                    # Connection/SSL error during request - recreate session and retry
                    error_msg = str(conn_error)
                    logger.warning(f"[TAVILY] Session/connector error (attempt {attempt + 1}/{max_retries}): {conn_error}")
                    
                    if attempt < max_retries - 1:
                        # Recreate session - try without SSL on second attempt
                        use_ssl = attempt == 0
                        logger.info(f"[TAVILY] Recreating session (SSL verification: {use_ssl})")
                        await self._create_tavily_session(use_ssl_verification=use_ssl)
                        continue
                    else:
                        # Final attempt failed
                        logger.error(f"[TAVILY] Request failed after {max_retries} attempts: {conn_error}")
                        return {"results": []}
                        
        except Exception as e:
            logger.error(f"Tavily search error: {e}")
            import traceback
            logger.error(f"Tavily error traceback: {traceback.format_exc()}")
            return {"results": []}
    
    async def batch_search_companies(self, company_names: List[str]) -> Dict[str, Any]:
        """Search multiple companies via Tavily in parallel"""
        import asyncio
        
        async def search_company(name: str) -> tuple[str, Dict[str, Any]]:
            """Search for a single company"""
            try:
                search_query = f"{name} funding revenue valuation ARR"
                result = await self._tavily_search(search_query)
                
                # Extract key information from Tavily results
                extracted_data: Dict[str, Any] = {
                    "name": name,
                    "sector": None,
                    "arr": None,
                    "revenue": None,
                    "valuation": None,
                    "latestValuation": None,
                    "industry": None,
                }
                
                # Parse results to extract structured data
                if result.get("results"):
                    # Try to extract information from search results
                    for res in result.get("results", [])[:3]:  # Use top 3 results
                        content = res.get("content", "").lower()
                        title = res.get("title", "").lower()
                        combined = f"{title} {content}"
                        
                        # Extract ARR/Revenue
                        if not extracted_data.get("arr") and not extracted_data.get("revenue"):
                            # Look for ARR patterns
                            import re
                            arr_match = re.search(r'arr[:\s]*\$?([\d.]+)\s*(?:m|million|b|billion)', combined, re.IGNORECASE)
                            if arr_match:
                                value = float(arr_match.group(1))
                                if 'b' in combined[arr_match.start():arr_match.end()]:
                                    value *= 1000
                                extracted_data["arr"] = value * 1000000
                                extracted_data["revenue"] = value * 1000000
                        
                        # Extract valuation
                        if not extracted_data.get("valuation"):
                            val_match = re.search(r'valuation[:\s]*\$?([\d.]+)\s*(?:m|million|b|billion)', combined, re.IGNORECASE)
                            if val_match:
                                value = float(val_match.group(1))
                                if 'b' in combined[val_match.start():val_match.end()]:
                                    value *= 1000
                                extracted_data["valuation"] = value * 1000000
                                extracted_data["latestValuation"] = value * 1000000
                        
                        # Extract sector/industry
                        if not extracted_data.get("sector"):
                            sectors = ["saas", "fintech", "healthcare", "e-commerce", "enterprise", "b2b", "b2c"]
                            for sector in sectors:
                                if sector in combined:
                                    extracted_data["sector"] = sector.capitalize()
                                    extracted_data["industry"] = sector.capitalize()
                                    break
                
                return (name, extracted_data)
            except Exception as e:
                logger.error(f"Error searching for {name}: {e}")
                return (name, {"error": str(e), "name": name})
        
        # Execute all searches in parallel
        tasks = [search_company(name) for name in company_names]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Build result dictionary
        result_dict: Dict[str, Any] = {}
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"Batch search exception: {result}")
                continue
            name, data = result
            result_dict[name] = data
        
        return result_dict
    
    async def _execute_valuation(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Execute comprehensive valuation analysis using ValuationEngineService"""
        try:
            companies = self.shared_data.get("companies", [])
            if not companies:
                return {"error": "No companies to value"}
            
            valuation_results = {}
            
            for company_data in companies:
                company_name = company_data.get("company", "Unknown")
                
                # Create valuation request using the proper API
                # Map funding stage to Stage enum
                stage_mapping = {
                    "Pre-Seed": Stage.PRE_SEED,
                    "Pre Seed": Stage.PRE_SEED,
                    "Seed": Stage.SEED,
                    "Series A": Stage.SERIES_A,
                    "Series B": Stage.SERIES_B,
                    "Series C": Stage.SERIES_C,
                    "Series D": Stage.GROWTH,
                    "Series E": Stage.GROWTH,
                    "Series F": Stage.LATE,
                    "Growth": Stage.GROWTH,
                    "Late": Stage.LATE
                }
                funding_stage = company_data.get("funding_stage", "Series B")
                stage = stage_mapping.get(funding_stage, Stage.SERIES_B)
                
                # Use inferred_revenue if revenue is None - CRITICAL FIX
                revenue = ensure_numeric(company_data.get("revenue"), 0)
                if revenue == 0:
                    # Try inferred revenue
                    revenue = ensure_numeric(company_data.get("inferred_revenue"), 0)
                    if revenue == 0:
                        # Try ARR
                        revenue = ensure_numeric(company_data.get("arr"), 0)
                        if revenue == 0:
                            # Try inferred ARR
                            revenue = ensure_numeric(company_data.get("inferred_arr"), 1_000_000)
                
                # Use inferred_growth_rate if revenue_growth is None
                growth_rate = ensure_numeric(company_data.get("revenue_growth"), 0)
                if growth_rate == 0:
                    growth_rate = ensure_numeric(company_data.get("inferred_growth_rate"), 0)
                    if growth_rate == 0:
                        growth_rate = ensure_numeric(company_data.get("growth_rate"), 0.5)
                
                # Use inferred_valuation if valuation is None - CRITICAL FIX
                valuation = ensure_numeric(company_data.get("valuation"), 0)
                # Check if inferred_valuation exists in the data (even if 0, it might be a valid value)
                inferred_val_raw = company_data.get("inferred_valuation")
                inferred_val = ensure_numeric(inferred_val_raw, None) if inferred_val_raw is not None else None
                
                # Only use fallback if both are truly missing (None or 0)
                if (valuation == 0 or valuation is None) and (inferred_val == 0 or inferred_val is None):
                    # Calculate from total_raised as fallback
                    valuation = ensure_numeric(company_data.get("total_raised"), 0) * 3
                    if valuation == 0:
                        valuation = 100_000_000  # Final fallback
                
                valuation_request = ValuationRequest(
                    company_name=company_name,
                    stage=stage,
                    revenue=revenue,
                    growth_rate=growth_rate,
                    last_round_valuation=valuation if valuation and valuation > 0 else None,
                    inferred_valuation=inferred_val,  # Pass it even if 0 - let valuation engine decide
                    last_round_date=company_data.get("last_funding_date"),
                    total_raised=self._get_field_safe(company_data, "total_raised"),
                    business_model=company_data.get("business_model"),
                    industry=company_data.get("sector", "Technology"),
                    method=ValuationMethod.AUTO
                )
                
                # Run valuation with proper method
                valuation_result = await self.valuation_engine.calculate_valuation(valuation_request)
                
                # Valuation should never return None, but add defensive check
                if valuation_result is None:
                    logger.error(f"Valuation returned None for {company_name} - this should never happen")
                    # Create error result
                    from app.services.valuation_engine_service import ValuationResult
                    valuation_result = ValuationResult(
                        method_used="error",
                        fair_value=valuation or inferred_val or 0,
                        explanation="Valuation calculation returned None",
                        confidence=0
                    )
                
                # Store valuation results
                valuation_results[company_name] = {
                    "method": valuation_result.method_used,
                    "fair_value": valuation_result.fair_value,
                    "common_stock_value": valuation_result.common_stock_value,
                    "preferred_value": valuation_result.preferred_value,
                    "confidence": valuation_result.confidence,
                    "explanation": valuation_result.explanation,
                    "assumptions": valuation_result.assumptions,
                    "current_valuation": ensure_numeric(company_data.get("valuation"), 0),
                    "upside_potential": safe_divide(
                                       (valuation_result.fair_value - ensure_numeric(company_data.get("valuation"), valuation_result.fair_value)),
                                       max(ensure_numeric(company_data.get("valuation"), valuation_result.fair_value), 1),
                                       0)
                }

                # Persist valuation to pending_suggestions so it appears in the grid
                fund_id = self.shared_data.get("fund_context", {}).get("fundId")
                if fund_id and valuation_result.fair_value:
                    # Resolve company UUID from grid snapshot
                    matrix_ctx = self.shared_data.get("matrix_context") or {}
                    grid_snapshot = matrix_ctx.get("gridSnapshot") or {}
                    grid_rows = grid_snapshot.get("rows", []) if isinstance(grid_snapshot, dict) else (
                        grid_snapshot if isinstance(grid_snapshot, list) else []
                    )
                    company_uuid = None
                    for row in grid_rows:
                        row_name = (row.get("companyName") or row.get("company_name") or "").lower()
                        if company_name.lower() in row_name or row_name in company_name.lower():
                            company_uuid = row.get("companyId") or row.get("rowId") or row.get("row_id")
                            break
                    # Also check company_data for a stored id
                    if not company_uuid:
                        company_uuid = company_data.get("company_id") or company_data.get("id")
                    if company_uuid:
                        try:
                            supabase_url = settings.SUPABASE_URL
                            supabase_key = settings.SUPABASE_SERVICE_ROLE_KEY or settings.SUPABASE_ANON_KEY
                            if supabase_url and supabase_key:
                                from supabase import create_client
                                sb = create_client(supabase_url, supabase_key)
                                method_used = valuation_result.method_used or "auto"
                                sb.table("pending_suggestions").upsert({
                                    "fund_id": fund_id,
                                    "company_id": company_uuid,
                                    "column_id": "valuation",
                                    "suggested_value": valuation_result.fair_value,
                                    "source_service": f"valuation_engine.{method_used}",
                                    "reasoning": f"Valuation via {method_used}: {valuation_result.explanation or ''}",
                                    "metadata": {"tool": "execute_valuation", "method": method_used, "confidence": valuation_result.confidence},
                                }, on_conflict="fund_id,company_id,column_id").execute()
                                # Also persist inferred ARR if revenue was calculated
                                if revenue and revenue > 0:
                                    sb.table("pending_suggestions").upsert({
                                        "fund_id": fund_id,
                                        "company_id": company_uuid,
                                        "column_id": "arr",
                                        "suggested_value": revenue,
                                        "source_service": f"valuation_engine.{method_used}",
                                        "reasoning": f"ARR inferred during {method_used} valuation (confidence: {valuation_result.confidence:.0%})",
                                        "metadata": {"tool": "execute_valuation", "method": method_used, "confidence": valuation_result.confidence},
                                    }, on_conflict="fund_id,company_id,column_id").execute()
                                logger.info(f"[VALUATION_PERSIST] Persisted valuation suggestion for {company_name} ({company_uuid})")
                        except Exception as e:
                            logger.warning(f"[VALUATION_PERSIST] Failed to persist valuation for {company_name}: {e}")

            # Write valuation results to shared_data so downstream consumers
            # (memo service, synthesis) can access them without re-running
            self.shared_data["valuation_results"] = valuation_results
            # Also enrich company objects with their valuation data
            for company_data in self.shared_data.get("companies", []):
                cname = company_data.get("company", "Unknown")
                val_data = valuation_results.get(cname)
                if val_data:
                    company_data["computed_valuation"] = val_data.get("fair_value")
                    company_data["valuation_method"] = val_data.get("method")
                    company_data["valuation_confidence"] = val_data.get("confidence")
                    company_data["valuation_explanation"] = val_data.get("explanation")

            return {"valuation": valuation_results, "success": True}
            
        except Exception as e:
            logger.error(f"Valuation error: {e}")
            return {"error": str(e), "success": False}
    
    async def _execute_pwerm(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Execute PWERM valuation using sophisticated stage-specific scenarios"""
        try:
            companies = self.shared_data.get("companies", [])
            if not companies:
                return {"error": "No companies to value"}
            
            pwerm_results = {}
            
            for company in companies:
                company_name = company.get("company", "Unknown")
                
                # Check if we already calculated PWERM scenarios in process_request
                if company.get("pwerm_scenarios"):
                    # Use the sophisticated scenarios we already calculated
                    scenarios = company["pwerm_scenarios"]
                    pwerm_valuation = company.get("pwerm_valuation")
                    
                    # Convert to dict format for return
                    pwerm_calc = {
                        "weighted_valuation": pwerm_valuation,
                        "scenarios": [
                            {
                                "name": s.scenario,
                                "probability": s.probability,
                                "exit_value": s.exit_value,
                                "time_to_exit": s.time_to_exit,
                                "present_value": s.present_value,
                                "moic": s.moic
                            } for s in scenarios
                        ]
                    }
                else:
                    # Fallback: Calculate now with stage-specific scenarios
                    stage_map = {
                        "Pre-Seed": Stage.PRE_SEED,
                        "Pre Seed": Stage.PRE_SEED,
                        "Seed": Stage.SEED,
                        "Series A": Stage.SERIES_A,
                        "Series B": Stage.SERIES_B,
                        "Series C": Stage.SERIES_C,
                        "Series D": Stage.GROWTH,
                        "Growth": Stage.GROWTH,
                        "Late": Stage.LATE,
                        "Late Stage Private": Stage.LATE,
                        "Late Stage": Stage.LATE
                    }
                    
                    company_stage = stage_map.get(company.get("stage", "Series A"), Stage.SERIES_A)
                    
                    # Use inferred_revenue if revenue is None - CRITICAL FIX
                    revenue = ensure_numeric(company.get("revenue"), 0)
                    if revenue == 0:
                        revenue = ensure_numeric(company.get("inferred_revenue"), 0)
                        if revenue == 0:
                            revenue = ensure_numeric(company.get("arr"), 0)
                            if revenue == 0:
                                revenue = ensure_numeric(company.get("inferred_arr"), 1_000_000)
                    
                    # Use inferred_growth_rate if growth_rate is None
                    growth_rate = ensure_numeric(company.get("growth_rate"), 0)
                    if growth_rate == 0:
                        growth_rate = ensure_numeric(company.get("inferred_growth_rate"), 1.5)
                    
                    # Use inferred_valuation if valuation is None - CRITICAL FIX
                    valuation = ensure_numeric(company.get("valuation"), 0)
                    if valuation == 0:
                        valuation = ensure_numeric(company.get("inferred_valuation"), 0)
                        if valuation == 0:
                            # Calculate from total_funding as fallback
                            valuation = ensure_numeric(company.get("total_funding"), 0) * 3
                    
                    # Extract inferred_valuation if available
                    inferred_val = ensure_numeric(company.get("inferred_valuation"), None) if company.get("inferred_valuation") is not None else None
                    val_request = ValuationRequest(
                        company_name=company_name,
                        stage=company_stage,
                        revenue=revenue,
                        growth_rate=growth_rate,
                        last_round_valuation=valuation if valuation and valuation > 0 else None,
                        inferred_valuation=inferred_val,
                        total_raised=self._get_field_safe(company, "total_funding")
                    )
                    
                    # Get sophisticated scenarios with MOIC calculated
                    pwerm_result = await self.valuation_engine._calculate_pwerm(val_request)
                    scenarios = pwerm_result.scenarios
                    
                    pwerm_calc = {
                        "weighted_valuation": pwerm_result.fair_value,
                        "scenarios": [
                            {
                                "name": s.scenario,
                                "probability": s.probability,
                                "exit_value": s.exit_value,
                                "time_to_exit": s.time_to_exit,
                                "present_value": s.present_value,
                                "moic": s.moic
                            } for s in scenarios
                        ]
                    }
                
                # Add PWERM scenarios directly to company data for deck generation
                company["pwerm_scenarios"] = scenarios
                company["pwerm_valuation"] = pwerm_result.fair_value
                
                # Add cap table impact if we have funding data
                if company.get("funding_rounds"):
                    company_data_for_cap_table = {
                        "company": company.get('company', 'Unknown'),
                        "funding_rounds": company.get("funding_rounds", []),
                        "stage": company.get("stage"),
                        "valuation": company.get("valuation"),
                        "is_yc": company.get("is_yc", False),
                        "geography": company.get("geography", "Unknown"),
                        "founders": company.get("founders", [])
                    }
                    try:
                        cap_table = self.cap_table_service.calculate_full_cap_table_history(
                            company_data=company_data_for_cap_table
                        )
                        if not cap_table:
                            cap_table = {"history": [], "ownership_evolution": {}}
                    except Exception as e:
                        logger.warning(f"Cap table calculation failed: {e}")
                        cap_table = {"history": [], "ownership_evolution": {}}
                    pwerm_calc["cap_table"] = cap_table
                
                # Add sophisticated waterfall analysis using AdvancedWaterfallCalculator
                if pwerm_calc.get("weighted_valuation", 0) > 0:
                    from app.services.waterfall_advanced import AdvancedWaterfallCalculator, LiquidationTerms, InvestorStage, InvestorQuality
                    
                    # Convert funding rounds to LiquidationTerms
                    investor_terms = []
                    for round_data in company.get("funding_rounds", []):
                        # Map series to stage
                        stage_mapping = {
                            "Seed": InvestorStage.SEED,
                            "Series A": InvestorStage.SERIES_A,
                            "Series B": InvestorStage.SERIES_B,
                            "Series C": InvestorStage.SERIES_C,
                            "Series D": InvestorStage.SERIES_D,
                            "Series E": InvestorStage.SERIES_E_PLUS
                        }
                        
                        stage = stage_mapping.get(round_data.get("series", "Series A"), InvestorStage.SERIES_A)
                        
                        # Determine investor quality from lead investor
                        lead = round_data.get("lead_investor", "")
                        if any(fund in lead.lower() for fund in ["tiger", "coatue", "dst"]):
                            quality = InvestorQuality.MEGA_FUND
                        elif any(fund in lead.lower() for fund in ["sequoia", "a16z", "benchmark", "andreessen"]):
                            quality = InvestorQuality.TIER_1
                        else:
                            quality = InvestorQuality.TIER_2
                        
                        # Use AdvancedCapTable service for market-standard terms
                        # Get stage-appropriate liquidation terms from service
                        stage_benchmarks = self.advanced_cap_table.BENCHMARKS.get(
                            stage.lower().replace(" ", "_"),
                            self.advanced_cap_table.BENCHMARKS.get("series_a", {})
                        )
                        
                        # Market standard terms based on stage and market conditions
                        # Only use participating for down rounds or bridge rounds
                        is_down_round = round_data.get("is_down_round", False)
                        is_bridge = "bridge" in round_data.get("series", "").lower()
                        
                        # Service-calculated liquidation preference
                        if is_down_round or is_bridge:
                            # Tougher terms for challenging rounds
                            liquidation_multiple = Decimal(str(stage_benchmarks.get("liquidation_preference", 1.5)))
                            participating = True
                            participation_cap = Decimal(str(stage_benchmarks.get("participation_cap", 2.0)))
                        else:
                            # Standard market terms
                            liquidation_multiple = Decimal(str(stage_benchmarks.get("liquidation_preference", 1.0)))
                            participating = False
                            participation_cap = None
                        
                        # Override with actual data if available
                        if "liquidation_preference" in round_data:
                            liquidation_multiple = Decimal(str(round_data["liquidation_preference"]))
                        if "participating" in round_data:
                            participating = round_data["participating"]
                        if participating and "participation_cap" in round_data:
                            participation_cap = Decimal(str(round_data["participation_cap"]))
                        
                        terms = LiquidationTerms(
                            investor_name=lead or f"{round_data.get('series', 'Unknown')} Investors",
                            stage=stage,
                            investment_amount=Decimal(str(round_data.get("amount", 0))),
                            shares_owned=Decimal(str(round_data.get("ownership_pct", 0.1) * 100)),
                            investor_quality=quality,
                            liquidation_multiple=liquidation_multiple,
                            participating=participating,
                            participation_cap=participation_cap
                        )
                        investor_terms.append(terms)
                    
                    # Use AdvancedCapTable for waterfall calculations
                    # Calculate waterfall using AdvancedCapTable service
                    waterfall_scenarios = self.advanced_cap_table.calculate_liquidation_waterfall(
                        exit_value=pwerm_calc["weighted_valuation"],
                        cap_table=company.get("cap_table", {}),
                        liquidation_preferences=company.get("liquidation_preferences", {}),
                        funding_rounds=company.get("funding_rounds", [])
                    )
                    
                    # Also calculate waterfall breakpoints for visualization
                    waterfall_breakpoints = self.advanced_cap_table.calculate_waterfall_breakpoints(
                        base_case_exit=Decimal(str(pwerm_calc["weighted_valuation"])),
                        bull_multiplier=2.0,
                        bear_multiplier=0.5
                    )
                    
                    pwerm_calc["waterfall"] = waterfall_scenarios
                    pwerm_calc["waterfall_breakpoints"] = waterfall_breakpoints
                    
                    # Store waterfall data directly in company for deck generation
                    company["waterfall_data"] = waterfall_scenarios
                    company["waterfall_breakpoints"] = waterfall_breakpoints
                
                pwerm_results[company_name] = pwerm_calc

                # Persist PWERM valuation to pending_suggestions
                fund_id = self.shared_data.get("fund_context", {}).get("fundId")
                weighted_val = pwerm_calc.get("weighted_valuation")
                if fund_id and weighted_val:
                    matrix_ctx = self.shared_data.get("matrix_context") or {}
                    grid_snapshot = matrix_ctx.get("gridSnapshot") or {}
                    grid_rows = grid_snapshot.get("rows", []) if isinstance(grid_snapshot, dict) else (
                        grid_snapshot if isinstance(grid_snapshot, list) else []
                    )
                    company_uuid = None
                    for row in grid_rows:
                        row_name = (row.get("companyName") or row.get("company_name") or "").lower()
                        if company_name.lower() in row_name or row_name in company_name.lower():
                            company_uuid = row.get("companyId") or row.get("rowId") or row.get("row_id")
                            break
                    if not company_uuid:
                        company_uuid = company.get("company_id") or company.get("id")
                    if company_uuid:
                        try:
                            supabase_url = settings.SUPABASE_URL
                            supabase_key = settings.SUPABASE_SERVICE_ROLE_KEY or settings.SUPABASE_ANON_KEY
                            if supabase_url and supabase_key:
                                from supabase import create_client
                                sb = create_client(supabase_url, supabase_key)
                                num_scenarios = len(pwerm_calc.get("scenarios", []))
                                sb.table("pending_suggestions").upsert({
                                    "fund_id": fund_id,
                                    "company_id": company_uuid,
                                    "column_id": "valuation",
                                    "suggested_value": weighted_val,
                                    "source_service": "valuation_engine.pwerm",
                                    "reasoning": f"PWERM analysis with {num_scenarios} scenarios",
                                    "metadata": {"tool": "execute_pwerm", "method": "pwerm", "scenarios": num_scenarios},
                                }, on_conflict="fund_id,company_id,column_id").execute()
                                logger.info(f"[PWERM_PERSIST] Persisted PWERM suggestion for {company_name} ({company_uuid}): {weighted_val}")
                        except Exception as e:
                            logger.warning(f"[PWERM_PERSIST] Failed to persist PWERM for {company_name}: {e}")

            return {"pwerm": pwerm_results, "success": True}

        except Exception as e:
            logger.error(f"PWERM calculation error: {e}")
            return {"error": str(e), "success": False}
    
    async def _execute_funding_aggregation(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Aggregate funding data across companies"""
        try:
            companies = self.shared_data.get("companies", [])
            
            # Use safe getters to avoid None values
            total_funding = sum(self._get_field_safe(c, "total_funding") for c in companies)
            avg_valuation = self._safe_divide(
                sum(self._get_field_safe(c, "valuation") for c in companies),
                max(len(companies), 1),
                0)
            
            funding_by_stage = {}
            for company in companies:
                stage = company.get("stage", "Unknown")
                funding_by_stage[stage] = funding_by_stage.get(stage, 0) + self._get_field_safe(company, "total_funding")
            
            return {
                "funding_aggregation": {
                    "total_funding": total_funding,
                    "average_valuation": avg_valuation,
                    "by_stage": funding_by_stage,
                    "company_count": len(companies)
                }
            }
            
        except Exception as e:
            logger.error(f"Funding aggregation error: {e}")
            return {"error": str(e)}
    
    async def _execute_market_research(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Return placeholder market research data now that TAM is disabled."""
        try:
            companies = self.shared_data.get("companies", [])
            if not companies:
                logger.info("[MARKET_RESEARCH] No companies available; TAM disabled placeholder returned.")
                return {
                    "market_research": {},
                    "status": "tam_disabled"
                }
            
            market_data: Dict[str, Any] = {}
            for company in companies:
                if not isinstance(company, dict):
                    continue
                name = company.get("company") or company.get("display_name") or "Unknown"
                market_data[name] = {
                    "sector": company.get("sector", "Technology"),
                    "market_category": company.get("sector", "Technology"),
                    "market_subcategory": "",
                    "market_maturity": "unknown",
                    "tam": 0,
                    "sam": 0,
                    "som": 0,
                    "growth_rate": 0,
                    "notes": "TAM processing disabled"
                }
            
            return {
                "market_research": market_data,
                "status": "tam_disabled"
            }
        except Exception as e:
            logger.error(f"Market research error: {e}")
            return {"error": str(e), "status": "tam_disabled"}
    
    async def _execute_competitive_analysis(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Execute competitive analysis"""
        try:
            companies = self.shared_data.get("companies", [])
            
            # Group by sector for competitive analysis
            by_sector = {}
            for company in companies:
                sector = company.get("sector", "Unknown")
                if sector not in by_sector:
                    by_sector[sector] = []
                by_sector[sector].append(company)
            
            competitive_analysis = []
            for sector, sector_companies in by_sector.items():
                # Calculate sector averages - use safe getters
                avg_valuation = sum(self._get_field_safe(c, "valuation") for c in sector_companies) / len(sector_companies)
                avg_revenue = sum(self._get_field_safe(c, "revenue") for c in sector_companies) / len(sector_companies)
                
                # Rank companies within sector
                ranked = sorted(sector_companies, key=lambda x: x.get("valuation", 0), reverse=True)
                
                competitive_analysis.append({
                    "sector": sector,
                    "company_count": len(sector_companies),
                    "market_leader": ranked[0].get("company") if ranked else None,
                    "average_valuation": avg_valuation,
                    "average_revenue": avg_revenue,
                    "companies_ranked": [
                        {
                            "rank": i + 1,
                            "company": c.get("company"),
                            "valuation": c.get("valuation", 0),
                            "market_share": c.get("valuation", 0) / sum(x.get("valuation", 1) for x in sector_companies)
                        }
                        for i, c in enumerate(ranked)
                    ]
                })
            
            return {"competitive_analysis": competitive_analysis}
            
        except Exception as e:
            logger.error(f"Competitive analysis error: {e}")
            return {"error": str(e)}
    
    async def _execute_financial_analysis(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Execute financial analysis with key metrics"""
        try:
            companies = self.shared_data.get("companies", [])
            financial_analysis = []
            
            for company in companies:
                # Use safe getters that check inferred_ versions first
                revenue = self._get_field_safe(company, "revenue")
                valuation = self._get_field_safe(company, "valuation")
                funding = self._get_field_safe(company, "total_funding")
                gross_margin = self._get_field_safe(company.get("key_metrics", {}), "gross_margin", default=0.7)
                
                # Use ValuationEngineService for proper revenue multiple
                category = company.get("category", "SaaS")
                stage = company.get("stage", "Series A")
                
                # Get proper valuation multiples from service
                valuation_request = ValuationRequest(
                    company_name=company.get("company"),
                    stage=self._get_stage_enum(stage) if stage else Stage.SERIES_A,
                    revenue=revenue,
                    growth_rate=self._get_field_safe(company, "growth_rate", 1.0),
                    category=category,
                    ai_component_percentage=company.get("ai_percentage", 0)
                )
                
                try:
                    valuation_result = await self.valuation_engine.calculate_valuation(valuation_request)
                    # Use the service-calculated revenue multiple
                    revenue_multiple = valuation_result.revenue_multiple
                except Exception as e:
                    logger.error(f"Failed to get revenue multiple from service: {e}")
                    # Fallback to simple calculation ONLY if service fails
                    revenue_multiple = self._safe_divide(valuation, revenue, default=0)
                
                # Use safe division to prevent crashes
                capital_efficiency = self._safe_divide(revenue, funding, default=0)
                burn_multiple = self._safe_divide(funding, revenue, default=0)
                
                # Use IntelligentGapFiller for adjusted gross margin and EBITDA
                adjusted_metrics = self.gap_filler.calculate_adjusted_gross_margin(
                    company_data=company,
                    stage=stage,
                    category=category
                )
                
                # Get service-calculated EBITDA margin
                if adjusted_metrics and 'ebitda_margin' in adjusted_metrics:
                    ebitda_margin = adjusted_metrics['ebitda_margin']
                    logger.info(f"Using service-calculated EBITDA margin: {ebitda_margin*100:.1f}% for {company.get('company')}")
                else:
                    # Only as last resort - request with minimal data
                    minimal_data = {
                        'gross_margin': gross_margin,
                        'stage': stage,
                        'category': category
                    }
                    fallback_metrics = self.gap_filler.calculate_adjusted_gross_margin(
                        company_data=minimal_data,
                        stage=stage,
                        category=category
                    )
                    ebitda_margin = fallback_metrics.get('ebitda_margin', gross_margin - 0.3)  # Better fallback
                
                # Rule of 40 (growth rate + profit margin)
                growth_rate = company.get("revenue_growth", 0) * 100
                rule_of_40 = growth_rate + (ebitda_margin * 100)
                
                financial_analysis.append({
                    "company": company.get("company"),
                    "metrics": {
                        "revenue_multiple": round(revenue_multiple, 1),
                        "capital_efficiency": round(capital_efficiency, 2),
                        "burn_multiple": round(burn_multiple, 1) if burn_multiple > 0 else "",
                        "rule_of_40": round(rule_of_40, 1),
                        "gross_margin": round(gross_margin * 100, 1),
                        "ltv_cac": self._get_field_safe(company.get("key_metrics", {}), "ltv_cac_ratio", default=3.0)
                    },
                    "health_score": min(100, max(0, rule_of_40 + (capital_efficiency * 10)))
                })
            
            return {"financial_analysis": financial_analysis}
            
        except Exception as e:
            logger.error(f"Financial analysis error: {e}")
            return {"error": str(e)}
    
    async def _execute_scenario_analysis(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Execute scenario analysis using PWERM from ValuationEngineService.

        Also supports NL "what if" queries via NLScenarioComposer when prompt
        contains 'what if' / 'what happens'.
        """
        try:
            # NL "what if" branch — parse and model cap table impact
            prompt = inputs.get("prompt", "") or self.shared_data.get("original_prompt", "")
            prompt_lower = prompt.lower() if prompt else ""
            if self.nl_scenario_composer and any(kw in prompt_lower for kw in ["what if", "what happens"]):
                logger.info(f"[SCENARIO] NL scenario branch for: {prompt[:80]}")
                composed = await self.nl_scenario_composer.parse_what_if_query(
                    prompt,
                    fund_id=self.shared_data.get("fund_context", {}).get("fund_id")
                )
                # Map parsed events to cap table operations
                nl_results = {
                    "scenario_name": composed.scenario_name,
                    "events": [
                        {
                            "entity": e.entity_name,
                            "type": e.event_type,
                            "description": e.event_description,
                            "timing": e.timing,
                            "parameters": e.parameters,
                            "impact_factors": e.impact_factors
                        }
                        for e in composed.events
                    ],
                    "probability": composed.probability,
                    "cap_table_impacts": {}
                }
                # For each event, compute cap table impact if it's a fundraise
                for event in composed.events:
                    if event.event_type == "funding" and event.parameters.get("amount"):
                        amount = event.parameters["amount"]
                        pre_money = amount * 4  # Rough estimate
                        post_money = pre_money + amount
                        nl_results["cap_table_impacts"][event.entity_name] = {
                            "new_round_size": amount,
                            "estimated_pre_money": pre_money,
                            "new_investor_pct": amount / post_money * 100,
                            "dilution_to_existing": amount / post_money * 100
                        }
                    elif event.event_type == "exit":
                        nl_results["cap_table_impacts"][event.entity_name] = {
                            "event": "exit",
                            "impact": "liquidation waterfall applies"
                        }

                self.shared_data["scenario_analysis"] = nl_results
                # Also run standard PWERM below so we get both

            companies = self.shared_data.get("companies", [])
            
            scenarios = []
            for company in companies:
                # Use safe getter that checks inferred_revenue first
                base_revenue = self._get_field_safe(company, "revenue")
                # Use safe getter that checks inferred_valuation first
                base_valuation = self._get_field_safe(company, "valuation")
                stage = company.get("stage", "Series A")
                category = company.get("category", "SaaS")
                
                # Use ValuationEngineService for proper scenario generation
                valuation_request = ValuationRequest(
                    company_name=company.get("company"),
                    stage=self._get_stage_enum(stage) if stage else Stage.SERIES_A,
                    revenue=base_revenue,
                    growth_rate=self._get_field_safe(company, "growth_rate", 1.0),
                    category=category,
                    ai_component_percentage=company.get("ai_percentage", 0)
                )
                
                # Get PWERM scenarios from service
                valuation_result = await self.valuation_engine.calculate_valuation(valuation_request)
                
                # Use actual PWERM scenarios instead of random simulations
                pwerm_scenarios = []
                if valuation_result.scenarios:
                    for scenario in valuation_result.scenarios:
                        pwerm_scenarios.append({
                            "scenario": scenario.scenario,
                            "probability": scenario.probability,
                            "exit_multiple": scenario.moic if hasattr(scenario, 'moic') else 5.0,
                            "exit_valuation": scenario.exit_value,
                            "irr": scenario.irr if hasattr(scenario, 'irr') else 0,
                            "time_to_exit": scenario.time_to_exit if hasattr(scenario, 'time_to_exit') else 5
                        })
                
                # Calculate weighted expected value
                expected_value = sum(
                    s["exit_valuation"] * s["probability"] 
                    for s in pwerm_scenarios
                )
                
                # Get probability distribution
                valuations = [s["exit_valuation"] for s in pwerm_scenarios]
                probabilities = [s["probability"] for s in pwerm_scenarios]
                
                # Calculate percentiles based on probability-weighted outcomes
                sorted_scenarios = sorted(pwerm_scenarios, key=lambda x: x["exit_valuation"])
                cumulative_prob = 0
                p10_exit = p50_exit = p90_exit = 0
                
                for scenario in sorted_scenarios:
                    cumulative_prob += scenario["probability"]
                    if cumulative_prob >= 0.1 and p10_exit == 0:
                        p10_exit = scenario["exit_valuation"]
                    if cumulative_prob >= 0.5 and p50_exit == 0:
                        p50_exit = scenario["exit_valuation"]
                    if cumulative_prob >= 0.9 and p90_exit == 0:
                        p90_exit = scenario["exit_valuation"]
                
                scenarios.append({
                    "company": company.get("company"),
                    "base_valuation": base_valuation,
                    "expected_exit": expected_value,
                    "median_exit": p50_exit,
                    "p10_exit": p10_exit,
                    "p90_exit": p90_exit,
                    "pwerm_scenarios": pwerm_scenarios,
                    "methodology": "PWERM (Probability-Weighted Expected Return Method)"
                })
                
                # CRITICAL: Attach scenarios to the company object so deck generation can access them
                company["pwerm_scenarios"] = pwerm_scenarios
                company["expected_exit"] = expected_value
                company["scenarios"] = pwerm_scenarios  # Also store as 'scenarios' for memo chart generation

            # Store in shared_data for downstream skills (memo, report)
            self.shared_data["scenario_analysis"] = {
                c.get("company", "Unknown"): s for c, s in zip(companies, scenarios)
            } if companies and scenarios else {}

            return {"scenario_analysis": scenarios}
            
        except Exception as e:
            logger.error(f"Scenario analysis error: {e}")
            return {"error": str(e)}
    
    async def _execute_deal_comparison(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Execute comprehensive deal comparison"""
        try:
            companies = self.shared_data.get("companies", [])
            
            # Get fund context from shared data
            fund_context = self.shared_data.get('fund_context', {})
            
            if len(companies) < 2:
                return {"comparison": "Need at least 2 companies to compare"}
            
            # Create comparison matrix
            comparison = {
                "companies": [],
                "metrics": {},
                "rankings": {},
                "recommendations": []
            }
            
            # Collect all metrics
            metrics_to_compare = [
                "valuation", "revenue", "total_funding", "team_size",
                "revenue_growth", "gross_margin", "burn_rate"
            ]
            
            for metric in metrics_to_compare:
                values = []
                for company in companies:
                    # Skip None companies (shouldn't happen with validation above, but be safe)
                    if not company or not isinstance(company, dict):
                        logger.warning(f"Skipping invalid company in deal comparison: {company}")
                        continue
                        
                    if metric == "gross_margin":
                        value = company.get("key_metrics", {}).get(metric, 0) if company.get("key_metrics") else 0
                    elif metric == "burn_rate":
                        value = company.get("key_metrics", {}).get(metric, 0) if company.get("key_metrics") else 0
                    else:
                        value = company.get(metric, 0)
                    
                    # Handle dict values (convert to numeric)
                    if isinstance(value, dict):
                        # Try to extract a numeric value from dict
                        if 'value' in value:
                            value = value['value']
                        elif 'amount' in value:
                            value = value['amount']
                        else:
                            value = 0
                    
                    # Ensure numeric value
                    try:
                        value = float(value) if value else 0
                    except (TypeError, ValueError):
                        value = 0
                        
                    values.append(value)
                
                # Filter out zeros for calculations
                non_zero_values = [v for v in values if v > 0]
                
                comparison["metrics"][metric] = {
                    "values": values,
                    "average": sum(non_zero_values) / len(non_zero_values) if non_zero_values else 0,
                    "best": max(non_zero_values) if non_zero_values else 0,
                    "worst": min(non_zero_values) if non_zero_values else 0
                }
            
            # Rank companies by key metrics
            for company in companies:
                # Skip None companies
                if not company or not isinstance(company, dict):
                    logger.warning(f"Skipping invalid company in ranking: {company}")
                    continue
                    
                score = 0
                
                # Safely get numeric values - use safe getters that check inferred versions
                valuation = self._get_field_safe(company, "valuation")
                revenue = self._get_field_safe(company, "revenue")
                revenue_growth = self._get_field_safe(company, "revenue_growth")
                key_metrics = company.get("key_metrics", {}) if company else {}
                gross_margin = self._get_field_safe(key_metrics, "gross_margin", default=0)
                
                # Handle dict values
                if isinstance(valuation, dict):
                    valuation = valuation.get('value', 0) or valuation.get('amount', 0) or 0
                if isinstance(revenue, dict):
                    revenue = revenue.get('value', 0) or revenue.get('amount', 0) or 0
                
                # Calculate investment score with proper normalization
                # Use config thresholds instead of hardcoded values
                scoring_thresholds = {}
                if ConfigLoader:
                    try:
                        config_loader = ConfigLoader()
                        scoring_thresholds = config_loader.get_scoring_thresholds()
                    except Exception as e:
                        logger.warning(f"Could not load scoring thresholds from config: {e}")
                
                # Get thresholds from config or use defaults
                valuation_cap = scoring_thresholds.get('valuation_cap', 1_000_000_000)  # Default $1B
                revenue_excellent = scoring_thresholds.get('revenue_excellent', 100_000_000)  # Default $100M
                
                # Normalize valuation (cap at threshold from config)
                val_score = min(float(valuation) / valuation_cap, 1.0) * 100 if valuation else 0
                
                # Normalize revenue (use threshold from config)
                rev_score = min(float(revenue) / revenue_excellent, 1.0) * 100 if revenue else 0
                
                # Normalize growth (cap at 300% = 3.0)
                growth_score = min(float(revenue_growth), 3.0) / 3.0 * 100 if revenue_growth else 0
                
                # Normalize margin (already 0-1 range)
                margin_score = float(gross_margin) * 100 if gross_margin else 0
                
                # Analyze unit economics and ACV (Annual Contract Value)
                unit_econ = company.get("unit_economics", {})
                compute_intensity = unit_econ.get("compute_intensity", "").lower()
                target_segment = unit_econ.get("target_segment", "").lower()
                
                # Estimate ACV based on target segment
                acv_estimate = 0
                if "fortune" in target_segment or "largest" in target_segment:
                    acv_estimate = 500_000  # $500K+ ACV typical
                elif "enterprise" in target_segment:
                    acv_estimate = 100_000  # $100K ACV typical
                elif "mid-market" in target_segment:
                    acv_estimate = 30_000   # $30K ACV typical
                elif "sme" in target_segment:
                    acv_estimate = 5_000    # $5K ACV typical
                elif "prosumer" in target_segment:
                    acv_estimate = 500      # $500 ACV typical
                
                # Calculate burn based on compute intensity vs revenue (it's a spectrum)
                # Every company now has AI costs, but impact varies by revenue base
                # Use stage-appropriate check size from fund context or inference
                base_check = fund_context.get('typical_check_size') if fund_context else None
                if not base_check:
                    # Fallback to stage-based defaults only if no context
                    stage_checks = {
                        "Seed": 2_000_000,
                        "Series A": 10_000_000,
                        "Series B": 20_000_000,
                        "Series C": 40_000_000
                    }
                    company_stage = company.get('stage', 'Series A')
                    base_check = stage_checks.get(company_stage, 10_000_000)
                existing_revenue = float(revenue) if revenue else 0
                
                # Calculate AI spend as % of revenue
                if any(term in compute_intensity for term in ["generates", "50 slides", "video", "image", "code"]):
                    if existing_revenue > 50_000_000:
                        # Large SaaS adding AI features (5-10% of revenue)
                        ai_spend_ratio = 0.08
                        required_check = base_check * 1.1
                        burn_estimate = f"Sustainable ({ai_spend_ratio*100:.0f}% of ${existing_revenue/1_000_000:.0f}M revenue on AI)"
                    elif existing_revenue > 10_000_000:
                        # Mid-size transitioning to AI (10-20% of revenue)
                        ai_spend_ratio = 0.15
                        required_check = base_check * 1.5
                        burn_estimate = f"Moderate ({ai_spend_ratio*100:.0f}% of ${existing_revenue/1_000_000:.0f}M revenue on AI)"
                    elif existing_revenue > 1_000_000:
                        # Small with AI features (20-40% of revenue)
                        ai_spend_ratio = 0.30
                        required_check = base_check * 2.0
                        burn_estimate = f"Heavy ({ai_spend_ratio*100:.0f}% of ${existing_revenue/1_000_000:.1f}M revenue on AI)"
                    else:
                        # AI-first startup (could be 50-80% on compute)
                        ai_spend_ratio = 0.60
                        required_check = base_check * 3.0
                        burn_estimate = "Extreme (60%+ on compute, pre-revenue)"
                else:
                    # Traditional SaaS still adding some AI (2-5%)
                    ai_spend_ratio = 0.03
                    required_check = base_check
                    burn_estimate = f"Low (3% on AI features)"
                
                # Store for later use in recommendations
                company["required_check_size"] = required_check
                company["burn_estimate"] = burn_estimate
                
                # GPU costs impact scoring based on ACV
                # Per CLAUDE.md: High GPU + ACV > $100K = still good (10-15x multiple)
                gpu_penalty = 1.0  # No penalty by default
                
                # Determine if it's an AI-intensive company
                is_ai_company = False
                
                # High compute workloads
                if any(term in compute_intensity for term in ["generates", "50 slides", "video", "image", "code"]):
                    is_ai_company = True
                    if acv_estimate >= 100_000:
                        gpu_penalty = 0.9  # Only 10% penalty - they can pass through costs
                    elif acv_estimate >= 30_000:
                        gpu_penalty = 0.7  # 30% penalty - margins squeezed
                    else:
                        gpu_penalty = 0.3  # 70% penalty - unit economics broken
                
                # Low compute = always good
                elif any(term in compute_intensity for term in ["stores", "queries", "crud", "database"]):
                    gpu_penalty = 1.1  # 10% bonus for low compute costs
                
                # Calculate weighted score
                score = (val_score * 0.25 +     # 25% weight on valuation
                        rev_score * 0.35 +       # 35% weight on revenue
                        growth_score * 0.25 +    # 25% weight on growth
                        margin_score * 0.15      # 15% weight on margins
                        ) * gpu_penalty
                
                # Add detailed scoring breakdown
                comparison["companies"].append({
                    "name": company.get("company"),
                    "score": round(score, 2),
                    "stage": company.get("stage"),
                    "sector": company.get("sector"),
                    "business_model": company.get("business_model"),
                    "valuation": valuation,
                    "revenue": revenue,
                    "growth": revenue_growth,
                    "margin": gross_margin,
                    "gpu_intensive": is_ai_company,
                    "score_breakdown": {
                        "valuation_score": round(val_score, 1),
                        "revenue_score": round(rev_score, 1),
                        "growth_score": round(growth_score, 1),
                        "margin_score": round(margin_score, 1),
                        "gpu_penalty_applied": gpu_penalty < 1.0
                    }
                })
            
            # Sort by score
            comparison["companies"] = sorted(comparison["companies"], key=lambda x: x["score"], reverse=True)
            
            # Generate detailed investment recommendations
            if comparison["companies"]:
                top_company = comparison["companies"][0]
                
                # Analyze entry points
                for company in comparison["companies"]:
                    stage = company.get("stage", "Unknown").lower()
                    
                    # Entry point analysis with burn considerations
                    optimal_check = self._get_optimal_check_size(company, fund_context)
                    burn_estimate = company.get("burn_estimate", "Unknown")
                    
                    # Use OwnershipReturnAnalyzer for detailed entry analysis
                    ownership_scenarios = self.ownership_analyzer.calculate_ownership_scenarios(
                        company_data=company,
                        investment_amount=optimal_check,
                        investment_type=InvestmentType.FOLLOW,  # Default to follow
                        fund_size=fund_context.get('fund_size')  # Use fund size from context if provided
                    )
                    
                    entry_ownership = ownership_scenarios.get('ownership_percentage', 0)
                    expected_return = ownership_scenarios.get('expected_return', 0)
                    exit_scenarios = ownership_scenarios.get('exit_scenarios', [])
                    
                    # Find best case scenario
                    best_case = max(exit_scenarios, key=lambda x: x.get('irr', 0)) if exit_scenarios else None
                    
                    # CRITICAL FIX: Handle None values in valuation
                    valuation = company.get('valuation') or 0
                    entry_analysis = (
                        f"{stage} entry at ${valuation/1e6:.0f}M post. "
                        f"Our ${optimal_check/1e6:.1f}M = {entry_ownership:.1f}% ownership. "
                        f"Expected {expected_return:.1f}x return"
                    )
                    
                    # Add ownership analysis to company data
                    company['ownership_analysis'] = {
                        'entry_ownership': entry_ownership,
                        'expected_return': expected_return,
                        'exit_scenarios': exit_scenarios,
                        'best_case_irr': best_case.get('irr', 0) if best_case else 0,
                        'entry_analysis': entry_analysis
                    }
                    
                    if best_case:
                        entry_analysis += f" (Best: {best_case['irr']:.0f}% IRR)"
                    
                    # Burn rate impact
                    gpu_analysis = ""
                    if company.get("gpu_intensive") or burn_estimate.startswith("High"):
                        gpu_analysis = f" BURN RATE: {burn_estimate}. Needs larger rounds to reach profitability."
                    
                    comparison["recommendations"].append(
                        f"{company['name']} (Score: {company['score']}): {entry_analysis}{gpu_analysis}"
                    )
                
                # Winner recommendation
                comparison["recommendations"].insert(0,
                    f"RECOMMENDED: {top_company['name']} - Better entry point based on score/stage/market combination."
                )
            
            return {"deal_comparison": comparison}
            
        except Exception as e:
            logger.error(f"Deal comparison error: {e}")
            return {"error": str(e)}
    
    def _generate_competitive_insights(self, company: Dict[str, Any]) -> List[str]:
        """Generate analyst-grade competitive and strategic insights - PROFESSIONAL TONE"""
        insights = []
        company_name = company.get('company', 'Unknown')
        
        # MOAT ANALYSIS
        revenue = self._get_field_safe(company, 'revenue')
        customers = company.get('customers', [])
        if isinstance(customers, dict):
            customers = customers.get('customer_names', [])
        
        # Network effects & defensibility
        category = (company.get('category', '') or '').lower()
        if 'marketplace' in category or 'platform' in category:
            insights.append(f"Network effects strengthen with scale - defensibility increases as user base grows")
        
        # Enterprise traction = stickiness
        if isinstance(customers, list) and len(customers) > 0:
            fortune_500 = ['microsoft', 'google', 'amazon', 'apple', 'meta', 'salesforce', 'oracle', 'sap']
            has_enterprise = any(any(f500 in str(c).lower() for f500 in fortune_500) for c in customers[:10])
            if has_enterprise:
                insights.append(f"Fortune 500 customer traction indicates high switching costs and expansion revenue potential")
        
        # COMPETITIVE POSITION
        funding_rounds = company.get('funding_rounds', [])
        investors_str = str(funding_rounds).lower()
        tier1_present = any(vc in investors_str for vc in ['sequoia', 'a16z', 'benchmark', 'accel'])
        
        if tier1_present:
            insights.append(f"Tier 1 VC backing provides competitive advantages in talent acquisition and follow-on capital")
        
        # MARKET TIMING
        sector = (company.get('sector', '') or '').lower()
        if 'ai' in sector or 'artificial intelligence' in category:
            insights.append(f"Positioned in AI secular growth trend with multi-year enterprise budget tailwinds")
        elif 'fintech' in sector or 'payments' in category:
            insights.append(f"Operating in mature fintech category - consolidation opportunities remain")
        
        # EXECUTION VELOCITY
        stage = company.get('stage', '')
        total_funding = company.get('total_funding', 0)
        if revenue > 0 and total_funding > 0:
            capital_efficiency = revenue / total_funding
            if capital_efficiency > 2.0:
                insights.append(f"Strong capital efficiency ({capital_efficiency:.1f}x revenue-to-funding ratio) indicates product-market fit")
            elif capital_efficiency < 0.3:
                insights.append(f"Low capital efficiency ({capital_efficiency:.1f}x) may indicate product-market fit challenges or heavy R&D phase")
        
        # GROWTH SUSTAINABILITY
        growth_rate = company.get('revenue_growth') or company.get('inferred_growth_rate', 0)
        if growth_rate > 1.5:  # 150%+ YoY
            insights.append(f"Exceptional growth velocity ({int(growth_rate*100)}% YoY) - monitor for deceleration as revenue base scales")
        elif 0.3 < growth_rate < 0.5:  # 30-50%
            insights.append(f"Moderate growth rate ({int(growth_rate*100)}% YoY) may limit ability to command premium exit multiples")
        
        # RISK FACTORS
        valuation = self._get_field_safe(company, 'valuation')
        if revenue > 0 and valuation > 0:
            multiple = self._safe_divide(valuation, revenue, default=0)
            if multiple > 30:
                insights.append(f"Trading at {multiple:.0f}x revenue multiple - execution risk elevated, down round risk if growth falters")
        
        # TAM CAPTURE POTENTIAL
        market_size = company.get('market_size', {})
        tam = market_size.get('tam', 0)
        if revenue > 0 and tam > 0:
            penetration = (revenue / tam) * 100
            if penetration < 0.1:
                insights.append(f"Market penetration below 0.1% - significant whitespace opportunity but execution risk remains")
            elif penetration > 5:
                insights.append(f"Market penetration above 5% - category leadership but requires TAM expansion for continued growth")
        
        return insights[:6]  # Cap at 6 most relevant insights
    
    def _generate_transparent_scoring(self, company_data: Dict) -> Dict:
        """Generate investment recommendation with TRANSPARENT scoring methodology"""
        
        # Use new investment thesis generator instead of keyword scoring
        thesis = self._generate_investment_thesis(company_data)
        
        # Convert thesis to scoring format for backward compatibility
        scores = {
            'moat': thesis.get('moat_score', 50),
            'momentum': thesis.get('momentum_score', 50),
            'market': thesis.get('market_score', 50),
            'team': thesis.get('team_score', 60),
            'fund_fit': thesis.get('fund_fit_score', 50)
        }
        
        total_score = thesis.get('overall_score', 50)
        recommendation = thesis.get('recommendation', 'CONSIDER')
        action = thesis.get('action', 'Requires deeper analysis')
        reasoning = thesis.get('thesis_narrative', '')
        
        return {
            'recommendation': recommendation,
            'action': action,
            'total_score': total_score,
            'component_scores': scores,
            'methodology': thesis.get('methodology', ''),
            'reasoning': reasoning
        }
    
    def _generate_investment_thesis(self, company_data: Dict) -> Dict:
        """Generate real investment thesis based on deep analysis, not keywords"""
        
        company_name = company_data.get('company', 'Unknown')
        
        # Get real data points - use inferred values when actual values are None
        revenue = safe_get_value(company_data.get('revenue', 0)) or safe_get_value(company_data.get('inferred_revenue', 0))
        valuation = safe_get_value(company_data.get('valuation', 0)) or safe_get_value(company_data.get('inferred_valuation', 0))
        total_funding = safe_get_value(company_data.get('total_funding', 0))
        team_size = safe_get_value(company_data.get('team_size', 0))
        stage = self._determine_accurate_stage(company_data)
        business_desc = company_data.get('business_description', '')
        
        # Market analysis
        market_size = company_data.get('market_size', {})
        tam = market_size.get('tam', 0)
        sam = market_size.get('sam', 0)
        som = market_size.get('som', 0)
        
        # Initialize scores based on real analysis
        scores = {}
        
        # 1. MOAT ANALYSIS - Based on actual competitive position
        moat_score = 50
        moat_reasons = []
        
        # Market penetration analysis
        if tam > 0 and revenue > 0:
            penetration = (revenue / tam) * 100
            if penetration > 5:
                moat_score += 30
                moat_reasons.append(f"market leader with {penetration:.1f}% TAM penetration")
            elif penetration > 1:
                moat_score += 20
                moat_reasons.append(f"strong position with {penetration:.1f}% TAM penetration")
            elif penetration < 0.1:
                moat_score -= 10
                moat_reasons.append(f"minimal market share ({penetration:.2f}%)")
        
        # Revenue per employee (productivity metric)
        if team_size > 0 and revenue > 0:
            rev_per_employee = revenue / team_size if team_size > 0 else 0
            if rev_per_employee > 500_000:
                moat_score += 15
                moat_reasons.append(f"exceptional efficiency at ${rev_per_employee/1000:.0f}K per employee")
            elif rev_per_employee > 200_000:
                moat_score += 10
                moat_reasons.append(f"solid productivity at ${rev_per_employee/1000:.0f}K per employee")
            elif rev_per_employee < 100_000:
                moat_score -= 10
                moat_reasons.append(f"low productivity at ${rev_per_employee/1000:.0f}K per employee")
        
        scores['moat'] = min(max(moat_score, 0), 100)
        
        # 2. MOMENTUM ANALYSIS - Based on funding velocity and growth
        momentum_score = 50
        momentum_reasons = []
        
        # Capital efficiency analysis
        if total_funding > 0 and revenue > 0:
            capital_efficiency = revenue / total_funding if total_funding > 0 else 0
            if capital_efficiency > 1.0:
                momentum_score += 25
                momentum_reasons.append(f"{capital_efficiency:.2f}x capital efficiency (revenue > funding)")
            elif capital_efficiency > 0.5:
                momentum_score += 15
                momentum_reasons.append(f"{capital_efficiency:.2f}x capital efficiency")
            elif capital_efficiency < 0.2:
                momentum_score -= 15
                momentum_reasons.append(f"low capital efficiency ({capital_efficiency:.2f}x)")
        
        # Valuation multiple analysis
        if revenue > 0 and valuation > 0:
            revenue_multiple = self._safe_divide(valuation, revenue, default=0)
            # Compare to stage benchmarks
            stage_benchmarks = {
                'Seed': 30, 'Series A': 15, 'Series B': 10, 
                'Series C': 7, 'Series D': 5, 'Growth': 4
            }
            benchmark_multiple = stage_benchmarks.get(stage, 10)
            
            if revenue_multiple < benchmark_multiple * 0.7:
                momentum_score += 20
                momentum_reasons.append(f"attractive {revenue_multiple:.1f}x multiple vs {benchmark_multiple}x benchmark")
            elif revenue_multiple > benchmark_multiple * 1.5:
                momentum_score -= 10
                momentum_reasons.append(f"expensive at {revenue_multiple:.1f}x vs {benchmark_multiple}x benchmark")
        
        # Funding momentum
        last_funding_date = company_data.get('last_funding_date')
        if last_funding_date:
            # Check if recent funding (within 12 months)
            from datetime import datetime
            try:
                if isinstance(last_funding_date, str):
                    if 'ago' in last_funding_date.lower():
                        if 'month' in last_funding_date and int(last_funding_date.split()[0]) < 12:
                            momentum_score += 10
                            momentum_reasons.append("recent funding shows investor confidence")
            except:
                pass
        
        scores['momentum'] = min(max(momentum_score, 0), 100)
        
        # 3. MARKET ANALYSIS - TAM quality and expansion potential
        market_score = 50
        market_reasons = []
        
        # TAM quality analysis
        if tam > 0:
            if tam > 100_000_000_000:
                market_score += 25
                market_reasons.append(f"massive {self._format_billions(tam, precision=0)} TAM")
            elif tam > 50_000_000_000:
                market_score += 20
                market_reasons.append(f"large {self._format_billions(tam, precision=0)} TAM")
            elif tam > 10_000_000_000:
                market_score += 15
                market_reasons.append(f"solid {self._format_billions(tam, precision=0)} TAM")
            elif tam < 5_000_000_000:
                market_score -= 10
                market_reasons.append(f"limited {self._format_billions(tam)} TAM")
            
            # SAM/SOM realism check
            if sam > 0 and som > 0:
                sam_ratio = sam / tam if tam > 0 else 0
                som_ratio = som / sam if sam > 0 else 0
                
                if 0.05 <= sam_ratio <= 0.3:
                    market_score += 10
                    market_reasons.append(f"realistic SAM at {sam_ratio*100:.0f}% of TAM")
                elif sam_ratio > 0.5:
                    market_score -= 10
                    market_reasons.append(f"overly optimistic SAM ({sam_ratio*100:.0f}% of TAM)")
                
                if 0.05 <= som_ratio <= 0.2:
                    market_score += 10
                    market_reasons.append(f"achievable SOM targets")
        
        # Labor replacement opportunity
        labor_tam = market_size.get('labor_value_capturable', 0)
        if labor_tam > tam * 0.5:
            market_score += 15
            market_reasons.append(
                f"significant labor replacement opportunity ({self._format_billions(labor_tam)})"
            )
        
        scores['market'] = min(max(market_score, 0), 100)
        
        # 4. TEAM ANALYSIS - Team-market fit and execution ability
        team_score = 60  # Baseline for funded companies
        team_reasons = []
        
        # Team size appropriateness
        if team_size > 0:
            expected_sizes = {
                'Seed': (5, 20), 'Series A': (15, 50), 'Series B': (40, 150),
                'Series C': (100, 400), 'Series D': (200, 800)
            }
            min_size, max_size = expected_sizes.get(stage, (20, 100))
            
            if min_size <= team_size <= max_size:
                team_score += 10
                team_reasons.append(f"right-sized team ({team_size} people for {stage})")
            elif team_size > max_size * 1.5:
                team_score -= 15
                team_reasons.append(f"overstaffed ({team_size} people for {stage})")
            elif team_size < min_size * 0.5:
                team_score -= 10
                team_reasons.append(f"understaffed ({team_size} people for {stage})")
        
        # Founder analysis
        founders = company_data.get('founders', [])
        if founders:
            founder_names = []
            for founder in founders:
                if isinstance(founder, dict):
                    if founder.get('previous_exits'):
                        team_score += 15
                        team_reasons.append("repeat founder with exits")
                    if founder.get('technical'):
                        team_score += 10
                        team_reasons.append("technical founder")
                    name = founder.get('name')
                    if name:
                        founder_names.append(name)
                elif isinstance(founder, str):
                    founder_names.append(founder)
            
            if founder_names:
                team_reasons.append(f"founders: {', '.join(founder_names[:2])}")
        
        # Investor quality signal
        investors = company_data.get('investors', [])
        tier1_investors = ['sequoia', 'a16z', 'benchmark', 'accel', 'greylock', 'kleiner']
        if any(inv for inv in investors if isinstance(inv, str) and any(t1 in inv.lower() for t1 in tier1_investors)):
            team_score += 15
            team_reasons.append("tier 1 investor backing")
        
        scores['team'] = min(max(team_score, 0), 100)
        
        # 5. FUND FIT ANALYSIS - Alignment with fund strategy
        fund_fit_score = 50
        fund_fit_reasons = []
        
        # Stage alignment
        stage_scores = {
            'Series A': 25, 'Series B': 25, 'Seed': 15,
            'Series C': 10, 'Pre-Seed': 5, 'Series D': 0
        }
        stage_bonus = stage_scores.get(stage, 0)
        fund_fit_score += stage_bonus
        if stage_bonus > 0:
            fund_fit_reasons.append(f"{stage} aligns with fund stage focus")
        
        # Ownership analysis
        if valuation > 0:
            # Assume $5-10M initial check based on fund size
            for check_size in [7_000_000, 10_000_000, 5_000_000]:
                # Calculate ownership based on post-money valuation
                implied_ownership = (check_size / (valuation + check_size)) * 100
                if 7 <= implied_ownership <= 15:
                    fund_fit_score += 25
                    fund_fit_reasons.append(f"${check_size/1e6:.0f}M gets {implied_ownership:.1f}% ownership")
                    break
                elif 5 <= implied_ownership <= 20:
                    fund_fit_score += 15
                    fund_fit_reasons.append(f"${check_size/1e6:.0f}M gets {implied_ownership:.1f}% ownership")
                    break
            
            # Expected returns analysis
            if som > 0 and revenue > 0:
                # Calculate potential exit value
                potential_market_cap = som * 0.1  # Assume 10% of SOM at exit
                exit_multiple = 5  # Conservative exit multiple
                potential_exit = min(potential_market_cap, valuation * 10)
                
                if potential_exit > valuation * 10:
                    fund_fit_score += 15
                    fund_fit_reasons.append(f"{(potential_exit/valuation):.0f}x return potential")
                elif potential_exit > valuation * 5:
                    fund_fit_score += 10
                    fund_fit_reasons.append(f"{(potential_exit/valuation):.0f}x return potential")
        
        scores['fund_fit'] = min(max(fund_fit_score, 0), 100)
        
        # CALCULATE WEIGHTED TOTAL
        weights = {
            'moat': 0.25,
            'momentum': 0.25,
            'market': 0.20,
            'team': 0.20,
            'fund_fit': 0.10
        }
        
        total_score = sum(scores[k] * weights[k] for k in scores)
        
        # BUILD INVESTMENT THESIS NARRATIVE
        thesis_parts = []
        
        # Lead with strongest dimension
        best_dimension = max(scores.items(), key=lambda x: x[1])
        if best_dimension[1] >= 70:
            dimension_narratives = {
                'moat': f"Strong competitive position with {', '.join(moat_reasons[:2])}",
                'momentum': f"Excellent momentum with {', '.join(momentum_reasons[:2])}",
                'market': f"Attractive market opportunity - {', '.join(market_reasons[:2])}",
                'team': f"Strong team with {', '.join(team_reasons[:2])}",
                'fund_fit': f"Ideal fund fit - {', '.join(fund_fit_reasons[:2])}"
            }
            thesis_parts.append(dimension_narratives.get(best_dimension[0], ''))
        
        # Add concerns if any
        weak_dimensions = [k for k, v in scores.items() if v < 50]
        if weak_dimensions:
            concern_narratives = {
                'moat': moat_reasons[-1] if moat_reasons else "limited defensibility",
                'momentum': momentum_reasons[-1] if momentum_reasons else "slow growth",
                'market': market_reasons[-1] if market_reasons else "market challenges",
                'team': team_reasons[-1] if team_reasons else "team gaps",
                'fund_fit': "ownership/return challenges"
            }
            concerns = [concern_narratives.get(d) for d in weak_dimensions[:2]]
            thesis_parts.append(f"Key concerns: {', '.join(filter(None, concerns))}")
        
        thesis_narrative = ". ".join(filter(None, thesis_parts))
        
        # INVESTMENT DECISION WITH SPECIFIC RATIONALE
        if total_score >= 75:
            recommendation = "STRONG INVEST"
            action = f"Priority opportunity. {thesis_narrative}. Move to partner meeting immediately."
        elif total_score >= 65:
            recommendation = "INVEST"
            action = f"Solid opportunity. {thesis_narrative}. Proceed with diligence."
        elif total_score >= 55:
            recommendation = "WATCH"
            action = f"Promising but not ready. {thesis_narrative}. Re-evaluate in 6 months."
        elif total_score >= 45:
            recommendation = "CONSIDER"
            action = f"Mixed signals. {thesis_narrative}. Needs deeper analysis on key concerns."
        else:
            recommendation = "PASS"
            # Specific pass reasons
            if scores['market'] < 40:
                action = f"Market not attractive enough. {thesis_narrative}. TAM or timing concerns."
            elif scores['moat'] < 40:
                action = f"Insufficient differentiation. {thesis_narrative}. High commoditization risk."
            elif scores['momentum'] < 40:
                action = f"Poor unit economics. {thesis_narrative}. Capital efficiency concerns."
            elif scores['team'] < 40:
                action = f"Team-market fit issues. {thesis_narrative}. Execution risk too high."
            else:
                action = f"Below fund threshold. {thesis_narrative}. Better opportunities available."
        
        # Build detailed methodology explanation - use inferred values before division
        methodology = (
            f"Analysis based on: Capital efficiency ({self._safe_divide(revenue, total_funding, 0):.2f}x), "
            f"Revenue multiple ({self._safe_divide(valuation, revenue, 0):.1f}x vs {stage} benchmark), "
            f"Market penetration ({(self._safe_divide(revenue, tam, 0) * 100) if tam > 0 else 0:.2f}%), "
            f"Team scaling (${self._safe_divide(revenue, team_size, 0) / 1000 if team_size > 0 else 0:.0f}K/employee)"
        )
        
        return {
            'recommendation': recommendation,
            'action': action,
            'total_score': total_score,
            'overall_score': total_score,
            'component_scores': scores,
            'moat_score': scores['moat'],
            'momentum_score': scores['momentum'],
            'market_score': scores['market'],
            'team_score': scores['team'],
            'fund_fit_score': scores['fund_fit'],
            'moat_reasons': moat_reasons,
            'momentum_reasons': momentum_reasons,
            'market_reasons': market_reasons,
            'team_reasons': team_reasons,
            'fund_fit_reasons': fund_fit_reasons,
            'thesis_narrative': thesis_narrative,
            'methodology': methodology
        }
    
    async def _extract_competitors(self, company: str, search_results: List[Dict]) -> List[Dict]:
        """Extract competitor information from search results"""
        try:
            # Combine all search content
            all_content = ""
            for result in search_results:
                if result and "results" in result:
                    for item in result["results"][:3]:
                        all_content += item.get("content", "") + "\n"
            
            # Use Claude to extract competitors
            extraction_prompt = f"""
            Extract competitors and alternatives to {company} from the search results.
            Look for:
            1. Direct competitors mentioned (e.g., "competes with X", "alternative to Y", "versus", "vs")
            2. Companies {company} is "taking on", "challenging", "disrupting", "replacing"
            3. Incumbent solutions being disrupted (usually large established companies)
            4. Companies in the same category or space
            5. Open source alternatives if applicable
            
            IMPORTANT: Separate incumbents (established companies being challenged) from competitors (similar startups)
            
            Return as JSON with this structure:
            {{
                "direct_competitors": [
                    {{"name": "Company A", "description": "What they do", "funding": "amount if known"}}
                ],
                "incumbents": [
                    {{"name": "Company B", "description": "Legacy solution", "market_share": "% if known"}}
                ],
                "open_source": [
                    {{"name": "Project C", "description": "Open alternative", "adoption": "usage stats if known"}}
                ],
                "competitive_landscape": "Brief analysis of competition intensity and market dynamics"
            }}
            
            Search content:
            {all_content[:4000]}
            """
            
            response = await self.model_router.get_completion(
                prompt=extraction_prompt,
                capability=ModelCapability.STRUCTURED,
                temperature=0.1,
                json_mode=True
            )
            
            # ModelRouter returns dict with 'response' key
            response_text = response.get('response', '{}')
            
            # Handle markdown-wrapped JSON (common LLM output format)
            if '```json' in response_text:
                # Extract JSON from markdown code block
                import re
                json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
                if json_match:
                    response_text = json_match.group(1)
                else:
                    # Try to extract any JSON-like content
                    json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                    if json_match:
                        response_text = json_match.group(0)
            
            try:
                competitors_data = json.loads(response_text)
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse competitors JSON: {e}")
                logger.error(f"Raw response: {response_text[:200]}...")
                return []
            
            # Process into flat list with categories
            competitors = []
            
            for comp in competitors_data.get('direct_competitors', []):
                comp['category'] = 'direct'
                competitors.append(comp)
            
            for comp in competitors_data.get('incumbents', []):
                comp['category'] = 'incumbent'
                competitors.append(comp)
            
            for comp in competitors_data.get('open_source', []):
                comp['category'] = 'open_source'
                competitors.append(comp)
            
            # Add competitive landscape analysis
            if competitors_data.get('competitive_landscape'):
                competitors_data['competitive_landscape'] = competitors_data['competitive_landscape']
            
            logger.info(f"[COMPETITORS] Found {len(competitors)} competitors for {company}")
            return competitors
            
        except Exception as e:
            logger.error(f"Failed to extract competitors: {e}")
            return []
    
    def _generate_scoring_reasoning(self, scores: Dict, total: float) -> str:
        """Generate human-readable reasoning from scores"""
        
        reasoning = []
        
        # Strengths
        strong_areas = [k for k, v in scores.items() if v >= 75]
        if strong_areas:
            reasoning.append(f"**Strengths**: {', '.join(strong_areas)} (scores: {', '.join([f'{scores[k]:.0f}' for k in strong_areas])})")
        
        # Concerns
        weak_areas = [k for k, v in scores.items() if v < 60]
        if weak_areas:
            reasoning.append(f"**Concerns**: {', '.join(weak_areas)} (scores: {', '.join([f'{scores[k]:.0f}' for k in weak_areas])})")
        
        # Overall assessment
        if total >= 75:
            reasoning.append("**Overall**: High-conviction opportunity with multiple positive signals.")
        elif total >= 65:
            reasoning.append("**Overall**: Solid opportunity with manageable risks.")
        else:
            reasoning.append("**Overall**: Significant questions require resolution before investment.")
        
        return " ".join(reasoning)
    
    def _validate_citations(self, citations: List[Dict]) -> List[Dict]:
        """Remove fake/placeholder citations, validate sources"""
        
        # Whitelist of valid source domains
        valid_domains = [
            'crunchbase.com', 'pitchbook.com', 'cbinsights.com',
            'techcrunch.com', 'bloomberg.com', 'wsj.com', 'reuters.com',
            'forbes.com', 'venturebeat.com', 'theinformation.com',
            'linkedin.com', 'sec.gov', 'company website',
            'gartner.com', 'forrester.com', 'idc.com', 'mckinsey.com',
            'bcg.com', 'bain.com', 'statista.com', 'similarweb.com'
        ]
        
        # Blacklist patterns
        fake_patterns = ['example.com', 'placeholder', 'test.com', 'fake', 'lorem', 'ipsum', 'N/A']
        
        valid_citations = []
        
        for citation in citations:
            if not isinstance(citation, dict):
                continue
            
            source = str(citation.get('source', '')).lower()
            url = str(citation.get('url', '')).lower()
            text = str(citation.get('text', ''))
            
            # Skip if no source
            if not source or len(source) < 3:
                continue
            
            # Skip if fake/placeholder
            if any(fake in source for fake in fake_patterns):
                logger.warning(f"[CITATIONS] Skipping fake source: {source}")
                continue
            
            # Check if from valid domain
            is_valid = any(domain in source or domain in url for domain in valid_domains)
            
            # Also accept if has meaningful text
            if not is_valid and text and len(text) > 20:
                # Has content, keep it
                is_valid = True
            
            if is_valid:
                valid_citations.append(citation)
        
        logger.info(f"[CITATIONS] Validated {len(valid_citations)}/{len(citations)} citations")
        return valid_citations
    
    def _generate_revenue_multiple_analysis(self, company_data: Dict, stage: str) -> Dict[str, Any]:
        """Generate quartile-based revenue multiple analysis"""
        revenue = self._get_revenue_safe(company_data)
        valuation = self._get_field_safe(company_data, 'valuation')
        
        if revenue > 0 and valuation > 0:
            current_multiple = self._safe_divide(valuation, revenue, default=0)
            
            # Stage-specific benchmarks from intelligent_gap_filler.py lines 4249-4272
            stage_benchmarks = {
                'seed': {'p25': 8, 'p50': 15, 'p75': 30},
                'series_a': {'p25': 5, 'p50': 10, 'p75': 20},
                'series_b': {'p25': 4, 'p50': 8, 'p75': 15},
                'series_c': {'p25': 3, 'p50': 6, 'p75': 10}
            }
            
            benchmarks = stage_benchmarks.get(stage.lower().replace(' ', '_'), stage_benchmarks['series_a'])
            
            # Determine quartile
            if current_multiple >= benchmarks['p75']:
                quartile = "Top Quartile (>75th percentile)"
                assessment = "Premium valuation - market expects exceptional growth"
            elif current_multiple >= benchmarks['p50']:
                quartile = "Above Median (50-75th percentile)"
                assessment = "Healthy valuation with strong investor confidence"
            elif current_multiple >= benchmarks['p25']:
                quartile = "Below Median (25-50th percentile)"
                assessment = "Fair valuation - reasonable entry point"
            else:
                quartile = "Bottom Quartile (<25th percentile)"
                assessment = "Attractive valuation - potential value opportunity"
            
            return {
                "current_multiple": f"{current_multiple:.1f}x",
                "quartile": quartile,
                "assessment": assessment,
                "benchmarks": {
                    "p25": f"{benchmarks['p25']}x",
                    "median": f"{benchmarks['p50']}x",
                    "p75": f"{benchmarks['p75']}x"
                },
                "narrative": f"{company_data.get('company')} trading at {current_multiple:.1f}x revenue ({quartile}). {assessment}. Stage benchmarks: P25={benchmarks['p25']}x | Median={benchmarks['p50']}x | P75={benchmarks['p75']}x"
            }
        return {}

    async def _generate_competitive_landscape_analysis(self, companies: List[Dict], companies_business_data: Dict) -> Dict[str, Any]:
        """Generate comprehensive competitive landscape with risks, strategy, win/lose scenarios"""
        
        if not companies or len(companies) < 2:
            logger.info("[COMPETITIVE_LANDSCAPE] Less than 2 companies, returning empty dict")
            return {}  # Return empty dict for consistency, not None
            
        company1 = companies[0]
        company2 = companies[1]
        company1_name = company1.get('company', 'Company 1')
        company2_name = company2.get('company', 'Company 2')
        
        # Get market research data for competitive analysis
        market_research = {}
        for company in companies:
            if hasattr(self, 'market_research_cache') and company.get('company') in self.market_research_cache:
                market_research[company.get('company')] = self.market_research_cache[company.get('company')]
        
        # Extract competitive data
        direct_competitors = []
        public_comparables = []
        incumbents = []
        labor_replacement_data = {}
        
        for company_name, research in market_research.items():
            if research:
                # Direct competitors (private startups)
                competitors = research.get('direct_competitors', [])
                for comp in competitors[:5]:  # Top 5 competitors
                    if isinstance(comp, dict):
                        direct_competitors.append({
                            "name": comp.get('name', 'Unknown'),
                            "stage": comp.get('stage', 'Unknown'),
                            "revenue": comp.get('revenue', 'Unknown'),
                            "valuation": comp.get('valuation', 'Unknown'),
                            "differentiation": comp.get('differentiation', 'Competing in same space')
                        })
                
                # Public comparables
                public_comps = research.get('public_comparables', [])
                for comp in public_comps[:5]:  # Top 5 public comps
                    if isinstance(comp, dict):
                        public_comparables.append({
                            "ticker": comp.get('ticker', 'N/A'),
                            "name": comp.get('name', 'Unknown'),
                            "revenue_multiple": f"{comp.get('revenue_multiple', 0):.1f}x",
                            "growth_rate": f"{comp.get('growth_rate', 0)*100:.0f}%",
                            "market_cap": comp.get('market_cap', 'Unknown'),
                            "relevance": comp.get('relevance', 'Similar business model')
                        })
                
                # Incumbents being disrupted
                incumbents_list = research.get('incumbents', [])
                for inc in incumbents_list[:5]:  # Top 5 incumbents
                    if isinstance(inc, dict):
                        incumbents.append({
                            "name": inc.get('name', 'Unknown'),
                            "type": inc.get('type', 'Enterprise'),
                            "market_cap": inc.get('market_cap', 'Unknown'),
                            "weakness": inc.get('weakness', 'Legacy technology'),
                            "our_advantage": inc.get('our_advantage', 'Modern AI-first approach')
                        })
                
                # Labor replacement analysis with safe value extraction
                labor_stats = company.get('labor_statistics', {})
                if labor_stats:
                    labor_replacement_data[company_name] = {
                        "roles_replaced": labor_stats.get('job_titles', []),
                        "total_workers": safe_get_value(labor_stats.get('number_of_workers', 0), 0),
                        "avg_salary": safe_get_value(labor_stats.get('avg_salary_per_role', 0), 0),
                        "total_spend": safe_get_value(labor_stats.get('total_addressable_labor_spend', 0), 0)
                    }
        
        # Market dynamics analysis
        market_dynamics = self._analyze_market_dynamics(companies_business_data)
        
        # Risk analysis
        risk_analysis = self._analyze_competitive_risks(companies, direct_competitors, incumbents)
        
        # Win/Lose scenarios
        win_lose_scenarios = self._generate_win_lose_scenarios(companies, market_dynamics)
        
        # Cambridge Associates benchmarks
        cambridge_benchmarks = self._get_cambridge_associates_context(companies)
        
        return {
            "title": "Competitive Landscape & Risk Analysis",
            "subtitle": f"Market positioning, risks, and win/lose scenarios for {company1_name} & {company2_name}",
            "narrative": "Comprehensive competitive analysis including direct competitors, market dynamics, and strategic risks:",
            
            # Direct Competitors Analysis
            "direct_competitors": {
                "title": "Direct Competitors (Private Startups)",
                "companies": direct_competitors,
                "our_positioning": f"{company1_name} differentiated by {self._get_key_differentiator(company1)}",
                "competitive_intensity": self._assess_competitive_intensity(direct_competitors),
                "funding_validation": f"Competitors raised ${self._calculate_total_competitor_funding(direct_competitors)/1e6:.0f}M, validating market opportunity"
            },
            
            # Public Comparables - TEMPORARILY DISABLED (missing methods)
            # "public_comparables": {
            #     "title": "Public Comparables (Valuation Benchmarks)",
            #     "companies": public_comparables,
            #     "benchmark_analysis": {
            #         "median_multiple": f"{self._calculate_median_public_multiple(public_comparables):.1f}x",
            #         "our_multiple": f"{self._calculate_our_multiple(companies):.1f}x",
            #         "assessment": self._assess_valuation_vs_public_comps(companies, public_comparables)
            #     }
            # },
            
            # Incumbents Being Disrupted
            "incumbents": {
                "title": "Incumbents Being Disrupted",
                "companies": incumbents,
                "disruption_narrative": f"{company1_name} attacking {self._get_incumbent_weakness(incumbents)} with {self._get_our_technology_advantage(company1)}",
                "market_share_at_stake": (
                    f"Incumbents control "
                    f"{self._format_billions(self._calculate_incumbent_revenue(incumbents))} "
                    f"market - ripe for disruption"
                )
            },
            
            # Labor Replacement Analysis
            "labor_replacement": {
                "title": "Labor Replacement Opportunity",
                "analysis": labor_replacement_data,
                "total_labor_tam": self._format_billions(
                    sum((data.get('total_spend') or 0) for data in labor_replacement_data.values())
                ),
                "adoption_thesis": "Companies save 70% vs hiring, driving rapid adoption",
                "citations": ["[2] BLS Occupational Employment Statistics"]
            },
            
            # Market Dynamics
            "market_dynamics": {
                "title": "Market Dynamics & Fragmentation",
                "tam_growth": market_dynamics.get('tam_growth', 'Unknown'),
                "market_fragmentation": market_dynamics.get('fragmentation', 'Unknown'),
                "saturation_level": market_dynamics.get('saturation', 'Unknown'),
                "vc_darling_risk": self._assess_vc_darling_risk(companies)
            },
            
            # Risk Analysis
            "risk_analysis": {
                "title": "Key Competitive Risks",
                "risks": risk_analysis,
                "mitigation_strategies": self._generate_mitigation_strategies(risk_analysis)
            },
            
            # Win/Lose Scenarios
            "win_lose_scenarios": {
                "title": "Win/Lose Scenarios",
                "scenarios": win_lose_scenarios,
                "probability_assessment": self._assess_scenario_probabilities(win_lose_scenarios)
            },
            
            # Cambridge Associates Context
            "institutional_benchmarks": {
                "title": "Cambridge Associates VC Benchmarks",
                "source": "Cambridge Associates US VC Index Q2 2024",
                "stage_benchmarks": cambridge_benchmarks,
                "our_projection": self._calculate_our_cambridge_positioning(companies, cambridge_benchmarks)
            },
            
            # Strategic Insights
            "strategic_insights": [
                f"{company1_name} trading at {self._get_revenue_multiple_quartile(company1)} for growth rate",
                f"Market shows {market_dynamics.get('fragmentation', 'moderate')} fragmentation - {self._interpret_fragmentation(market_dynamics.get('fragmentation'))}",
                f"Competitive intensity: {self._assess_competitive_intensity(direct_competitors)}",
                f"VC darling risk: {self._assess_vc_darling_risk(companies)}",
                f"Expected IRR places in {self._calculate_our_cambridge_positioning(companies, cambridge_benchmarks).get('quartile_positioning', 'median')} of Cambridge Associates benchmark"
            ]
        }

    def _analyze_market_dynamics(self, companies_business_data: Dict) -> Dict[str, str]:
        """Analyze market dynamics including TAM growth, fragmentation, saturation"""
        # Analyze TAM growth trends
        tam_values = [data.get('tam', 0) for data in companies_business_data.values() if data.get('tam', 0) > 0]
        avg_tam = sum(tam_values) / len(tam_values) if tam_values else 0
        
        if not tam_values:
            tam_growth = "TAM analysis disabled"
        elif avg_tam > 50_000_000_000:  # >$50B
            tam_growth = "High growth potential - large addressable market"
        elif avg_tam > 10_000_000_000:  # >$10B
            tam_growth = "Moderate growth - established market with expansion opportunities"
        else:
            tam_growth = "Emerging market - early stage with high uncertainty"
        
        # Assess market fragmentation
        categories = [data.get('category', '') for data in companies_business_data.values()]
        unique_categories = len(set(categories))
        
        if unique_categories > 3:
            fragmentation = "Highly fragmented - multiple sub-markets"
        elif unique_categories == 2:
            fragmentation = "Moderately fragmented - clear market segments"
        else:
            fragmentation = "Concentrated - single dominant market"
        
        # Assess saturation
        customer_counts = [data.get('customer_count', 0) for data in companies_business_data.values()]
        avg_customers = sum(customer_counts) / len(customer_counts) if customer_counts else 0
        
        if avg_customers > 10000:
            saturation = "Mature market - high customer penetration"
        elif avg_customers > 1000:
            saturation = "Growing market - moderate penetration"
        else:
            saturation = "Early market - low penetration, high upside"
        
        return {
            "tam_growth": tam_growth,
            "fragmentation": fragmentation,
            "saturation": saturation
        }

    def _analyze_competitive_risks(self, companies: List[Dict], competitors: List[Dict], incumbents: List[Dict]) -> List[Dict]:
        """Analyze key competitive risks"""
        risks = []
        
        # Risk 1: Competitive intensity
        if len(competitors) > 5:
            risks.append({
                "risk": "High Competitive Intensity",
                "description": f"{len(competitors)}+ direct competitors in market",
                "impact": "High",
                "probability": "High"
            })
        
        # Risk 2: Incumbent response
        if len(incumbents) > 0:
            risks.append({
                "risk": "Incumbent Response",
                "description": f"Large incumbents may respond with competing solutions",
                "impact": "High",
                "probability": "Medium"
            })
        
        # Risk 3: Market saturation
        for company in companies:
            customer_count = company.get('customer_count', 0)
            if customer_count > 50000:
                risks.append({
                    "risk": "Market Saturation",
                    "description": f"{company.get('company')} already has {customer_count:,} customers",
                    "impact": "Medium",
                    "probability": "Medium"
                })
        
        # Risk 4: VC darling effect
        for company in companies:
            valuation = self._get_field_safe(company, 'valuation')
            revenue = self._get_field_safe(company, 'revenue')
            multiple = self._safe_divide(valuation, revenue, default=0)
            if multiple > 50:  # >50x revenue multiple
                risks.append({
                    "risk": "VC Darling Premium",
                    "description": f"{company.get('company')} trading at premium valuation",
                    "impact": "High",
                    "probability": "High"
                })
        
        return risks

    def _generate_mitigation_strategies(self, risk_analysis: List[Dict]) -> List[Dict]:
        """Generate mitigation strategies for identified risks"""
        strategies = []
        
        for risk in risk_analysis:
            risk_name = risk.get('risk', '')
            impact = risk.get('impact', 'Medium')
            probability = risk.get('probability', 'Medium')
            
            # Generate specific strategies based on risk type
            if 'Competitive Intensity' in risk_name:
                strategies.append({
                    "strategy": "Differentiation Focus",
                    "description": "Focus on unique value proposition and technical moats",
                    "implementation": "Invest in R&D, build switching costs, establish partnerships",
                    "risk_addressed": risk_name
                })
                strategies.append({
                    "strategy": "Speed to Market",
                    "description": "Accelerate product development and market penetration",
                    "implementation": "Increase burn rate, hire key talent, expand sales team",
                    "risk_addressed": risk_name
                })
            
            elif 'Incumbent Response' in risk_name:
                strategies.append({
                    "strategy": "Innovation Advantage",
                    "description": "Maintain technology leadership over incumbents",
                    "implementation": "Continuous R&D investment, patent portfolio, talent acquisition",
                    "risk_addressed": risk_name
                })
                strategies.append({
                    "strategy": "Customer Lock-in",
                    "description": "Build strong customer relationships and switching costs",
                    "implementation": "Long-term contracts, integration depth, customer success programs",
                    "risk_addressed": risk_name
                })
            
            elif 'Market Saturation' in risk_name:
                strategies.append({
                    "strategy": "Market Expansion",
                    "description": "Expand into adjacent markets and use cases",
                    "implementation": "Product line extensions, geographic expansion, vertical diversification",
                    "risk_addressed": risk_name
                })
                strategies.append({
                    "strategy": "Efficiency Focus",
                    "description": "Optimize operations and unit economics",
                    "implementation": "Automation, process optimization, cost reduction initiatives",
                    "risk_addressed": risk_name
                })
            
            elif 'VC Darling Premium' in risk_name:
                strategies.append({
                    "strategy": "Revenue Growth Acceleration",
                    "description": "Focus on revenue growth to justify premium valuation",
                    "implementation": "Sales expansion, pricing optimization, new revenue streams",
                    "risk_addressed": risk_name
                })
                strategies.append({
                    "strategy": "Path to Profitability",
                    "description": "Demonstrate clear path to profitability",
                    "implementation": "Unit economics improvement, operational efficiency, strategic partnerships",
                    "risk_addressed": risk_name
                })
            
            # Default strategies for any unhandled risks
            else:
                strategies.append({
                    "strategy": "Risk Monitoring",
                    "description": f"Continuously monitor and assess {risk_name.lower()}",
                    "implementation": "Regular risk assessment, market intelligence, scenario planning",
                    "risk_addressed": risk_name
                })
        
        return strategies

    def _generate_win_lose_scenarios(self, companies: List[Dict], market_dynamics: Dict) -> List[Dict]:
        """Generate win/lose scenarios based on market dynamics"""
        scenarios = []
        
        # Win scenario: Market consolidation
        scenarios.append({
            "scenario": "Market Consolidation Win",
            "description": "One company becomes dominant player through superior execution",
            "probability": "30%",
            "outcome": "Winner takes 60%+ market share",
            "returns": "10x+ for winner, 1-3x for others"
        })
        
        # Lose scenario: Market fragmentation
        scenarios.append({
            "scenario": "Market Fragmentation Loss",
            "description": "Market remains fragmented with no clear winner",
            "probability": "40%",
            "outcome": "Multiple small players, no scale advantages",
            "returns": "1-2x for all players"
        })
        
        # Lose scenario: Incumbent response
        scenarios.append({
            "scenario": "Incumbent Response Loss",
            "description": "Large incumbents build competing solutions",
            "probability": "30%",
            "outcome": "Startups struggle against enterprise sales",
            "returns": "0.5-1x for startups"
        })
        
        return scenarios

    def _get_cambridge_associates_context(self, companies: List[Dict]) -> Dict[str, Any]:
        """Get Cambridge Associates benchmark context"""
        # Get stage from first company
        stage = self._determine_accurate_stage(companies[0])
        
        # Cambridge Associates benchmarks by stage
        stage_benchmarks = {
            'seed': {
                'median_irr': '22%',
                'top_quartile_irr': '45%',
                'median_multiple': '3.2x',
                'top_quartile_multiple': '6.5x'
            },
            'series_a': {
                'median_irr': '25%',
                'top_quartile_irr': '50%',
                'median_multiple': '3.5x',
                'top_quartile_multiple': '7.0x'
            },
            'series_b': {
                'median_irr': '20%',
                'top_quartile_irr': '40%',
                'median_multiple': '3.0x',
                'top_quartile_multiple': '6.0x'
            }
        }
        
        return stage_benchmarks.get(stage.lower().replace(' ', '_'), stage_benchmarks['series_a'])

    def _get_stage_benchmarks(self, stage: str) -> Dict[str, Any]:
        """Get comprehensive stage benchmarks including revenue and growth metrics"""
        stage_benchmarks = {
            'seed': {
                'median_revenue': 500000,  # $500K ARR
                'median_growth_rate': 200,  # 200% YoY
                'median_irr': '22%',
                'top_quartile_irr': '45%',
                'median_multiple': '3.2x',
                'top_quartile_multiple': '6.5x'
            },
            'series_a': {
                'median_revenue': 2000000,  # $2M ARR
                'median_growth_rate': 150,  # 150% YoY
                'median_irr': '25%',
                'top_quartile_irr': '50%',
                'median_multiple': '3.5x',
                'top_quartile_multiple': '7.0x'
            },
            'series_b': {
                'median_revenue': 8000000,  # $8M ARR
                'median_growth_rate': 100,  # 100% YoY
                'median_irr': '20%',
                'top_quartile_irr': '40%',
                'median_multiple': '3.0x',
                'top_quartile_multiple': '6.0x'
            },
            'series_c': {
                'median_revenue': 20000000,  # $20M ARR
                'median_growth_rate': 75,   # 75% YoY
                'median_irr': '18%',
                'top_quartile_irr': '35%',
                'median_multiple': '2.5x',
                'top_quartile_multiple': '5.0x'
            },
            'series_d': {
                'median_revenue': 50000000,  # $50M ARR
                'median_growth_rate': 50,   # 50% YoY
                'median_irr': '15%',
                'top_quartile_irr': '30%',
                'median_multiple': '2.0x',
                'top_quartile_multiple': '4.0x'
            },
            'growth': {
                'median_revenue': 100000000,  # $100M ARR
                'median_growth_rate': 30,   # 30% YoY
                'median_irr': '12%',
                'top_quartile_irr': '25%',
                'median_multiple': '1.5x',
                'top_quartile_multiple': '3.0x'
            }
        }
        
        return stage_benchmarks.get(stage.lower().replace(' ', '_'), stage_benchmarks['series_a'])

    
    def _generate_investment_recommendation(self, company: Dict[str, Any]) -> Dict[str, str]:
        """Generate clear investment recommendation with reasoning"""
        score = self._get_field_safe(company, 'score', 50)
        fund_fit_score = self._get_field_safe(company, 'fund_fit_score', 50)
        
        # Calculate composite score
        composite_score = (score * 0.6 + fund_fit_score * 0.4) if fund_fit_score else score
        
        # Generate recommendation based on score thresholds
        if composite_score >= 75:
            decision = "STRONG BUY"
            action = "Schedule partner meeting immediately"
            reasoning = "Excellent fund fit, strong metrics, clear path to 10x"
            color = "green"
        elif composite_score >= 60:
            decision = "BUY"
            action = "Begin due diligence process"
            reasoning = "Good fund fit, solid fundamentals, attractive valuation"
            color = "blue"
        elif composite_score >= 45:
            decision = "WATCH"
            action = "Monitor for 3-6 months"
            reasoning = "Interesting but needs more traction or better terms"
            color = "yellow"
        else:
            decision = "PASS"
            action = "Decline investment"
            reasoning = "Poor fund fit or weak fundamentals"
            color = "red"
        
        # Add specific reasons based on company data
        reasons = []
        
        # Check revenue multiple
        revenue = self._get_field_safe(company, 'revenue')
        valuation = self._get_field_safe(company, 'valuation')
        if revenue > 0 and valuation > 0:
            multiple = self._safe_divide(valuation, revenue, default=0)
            if multiple < 10:
                reasons.append("Attractive revenue multiple")
            elif multiple > 30:
                reasons.append("High valuation risk")
        
        # Check growth rate
        growth = self._get_field_safe(company, 'growth_rate')
        if growth > 2:
            reasons.append("Strong growth trajectory")
        elif growth < 1.5:
            reasons.append("Slow growth concerns")
        
        # Check team quality
        team_score = self._get_field_safe(company, 'team_quality_score')
        if team_score > 70:
            reasons.append("Exceptional team")
        elif team_score < 40:
            reasons.append("Team execution risk")
        
        if reasons:
            reasoning = f"{reasoning}. {'; '.join(reasons[:2])}"
        
        return {
            "decision": decision,
            "action": action,
            "reasoning": reasoning,
            "score": f"{int(composite_score)}/100",
            "color": color
        }
    
    def _determine_accurate_stage(self, company_data: Dict) -> str:
        """Determine actual stage from funding history, not ambiguous field"""
        
        funding_rounds = company_data.get('funding_rounds', [])
        if not funding_rounds:
            return company_data.get('stage', 'Unknown')
        
        # Get MOST RECENT completed round
        completed_rounds = [r for r in funding_rounds if r.get('announced_date')]
        if not completed_rounds:
            return company_data.get('stage', 'Unknown')
        
        # Sort by date (most recent first)
        completed_rounds.sort(key=lambda x: x.get('announced_date', ''), reverse=True)
        latest = completed_rounds[0]
        
        round_type = latest.get('funding_type', '').lower()
        
        # Map funding type to stage
        stage_map = {
            'pre-seed': 'Pre-Seed',
            'pre seed': 'Pre-Seed',
            'seed': 'Seed',
            'series a': 'Series A',
            'series b': 'Series B',
            'series c': 'Series C',
            'series d': 'Series D'
        }
        
        for key, value in stage_map.items():
            if key in round_type:
                logger.info(f"[STAGE] {company_data.get('company')}: Detected {value} from funding_type '{round_type}'")
                return value
        
        # Fallback to provided stage
        return company_data.get('stage', 'Unknown')
    
    def _compare_portfolio_fit(self, company_a: Dict, company_b: Dict, fund_size: float) -> Dict[str, Any]:
        """Compare portfolio construction fit"""
        def calc_position_size(company, fund_size):
            optimal_check = self._get_optimal_check_size(company, {'fund_size': fund_size})
            # FIXED: Safe division to prevent None/zero errors
            return self._safe_divide(optimal_check, fund_size, 0) * 100
        
        pos_a = calc_position_size(company_a, fund_size)
        pos_b = calc_position_size(company_b, fund_size)
        
        # Determine concentration risk
        def get_concentration_risk(pos_size):
            if pos_size > 5: return "High"
            elif pos_size > 2: return "Medium"
            else: return "Low"
        
        return {
            "company_a": {
                "position_size": f"{pos_a:.1f}% of fund",
                "concentration_risk": get_concentration_risk(pos_a),
                "rationale": f"Diversification play in {company_a.get('sector', 'technology')}"
            },
            "company_b": {
                "position_size": f"{pos_b:.1f}% of fund", 
                "concentration_risk": get_concentration_risk(pos_b),
                "rationale": f"Diversification play in {company_b.get('sector', 'technology')}"
            },
            "winner": "company_a" if pos_a <= pos_b else "company_b",
            "reasoning": "Lower concentration risk for portfolio balance"
        }
    
    def _compare_carry_impact(self, company_a: Dict, company_b: Dict, fund_size: float) -> Dict[str, Any]:
        """Compare carry impact and fund returner potential"""
        def calc_carry_metrics(company, fund_size):
            # Use existing PWERM scenarios if available
            scenarios = company.get('pwerm_scenarios', [])
            if scenarios:
                # Calculate expected DPI contribution
                # PWERMScenario objects have attributes, not dictionary keys
                expected_return = sum(getattr(s, 'present_value', 0) for s in scenarios)
                investment = self._get_optimal_check_size(company, {'fund_size': fund_size})
                dpi_contribution = expected_return / fund_size if fund_size > 0 else 0
                return {
                    "dpi_contribution": f"{dpi_contribution:.2f}x",
                    "fund_returner_potential": "Yes" if dpi_contribution > 0.1 else "No",
                    "years_to_liquidity": getattr(scenarios[0], 'time_to_exit', 5) if scenarios else 5
                }
            else:
                # Fallback calculation
                investment = self._get_optimal_check_size(company, {'fund_size': fund_size})
                return {
                    "dpi_contribution": "0.15x",  # Default assumption
                    "fund_returner_potential": "Yes",
                    "years_to_liquidity": 5
                }
        
        metrics_a = calc_carry_metrics(company_a, fund_size)
        metrics_b = calc_carry_metrics(company_b, fund_size)
        
        return {
            "company_a": metrics_a,
            "company_b": metrics_b,
            "winner": "company_a" if float(metrics_a["dpi_contribution"].replace('x', '')) > float(metrics_b["dpi_contribution"].replace('x', '')) else "company_b",
            "reasoning": "Higher DPI contribution potential"
        }
    
    def _compare_early_late_positioning(self, company_a: Dict, company_b: Dict) -> Dict[str, Any]:
        """Compare early vs late positioning with detailed analysis"""
        def analyze_positioning(company):
            stage = self._determine_accurate_stage(company)
            valuation = safe_get_value(company.get('valuation', 0))
            revenue = self._get_field_with_fallback(company, 'revenue', 0)
            
            # Calculate remaining rounds based on stage
            rounds_remaining = {
                'Seed': 4, 'Series A': 3, 'Series B': 2, 'Series C': 1, 'Series D': 0, 'Growth': 0
            }.get(stage, 2)
            
            # Estimate next round timing and pricing
            next_round_timing = 18 if stage in ['Seed', 'Series A'] else 12 if stage in ['Series B', 'Series C'] else 24
            markup_multiplier = 2.5 if stage in ['Seed', 'Series A'] else 1.75 if stage in ['Series B', 'Series C'] else 1.5
            
            # Calculate revenue stickiness
            nrr = company.get('net_revenue_retention', 120)  # Default 120%
            revenue_stickiness = "High" if nrr > 115 else "Medium" if nrr > 105 else "Low"
            
            # Assess if we're "sucker money"
            investors = str(company.get('investors', '')).lower()
            tier1_investors = ['sequoia', 'andreessen', 'a16z', 'benchmark', 'greylock', 'accel']
            has_tier1 = any(inv in investors for inv in tier1_investors)
            are_we_suckers = "No" if has_tier1 else "Maybe"
            
            return {
                "logos": {
                    "entry_timing": f"{stage}, ${valuation/1e6:.0f}M post-money",
                    "upside_to_exit": f"{5-2 if stage in ['Seed', 'Series A'] else 3-1}x (realistic path to ${valuation*3/1e6:.0f}M-${valuation*5/1e6:.0f}M exit)",
                    "reserve_deployment_window": f"{rounds_remaining} rounds remaining",
                    "option_value": "High" if rounds_remaining >= 2 else "Medium" if rounds_remaining == 1 else "Low",
                    "next_round_timing": f"{next_round_timing} months",
                    "next_round_price": f"${valuation * markup_multiplier/1e6:.0f}M post-money ({markup_multiplier}x markup)",
                    "capital_required_to_exit": f"${self._get_optimal_check_size(company, {})/1e6:.0f}M total (initial + {rounds_remaining}x reserves)",
                    "are_we_suckers": are_we_suckers,
                    "revenue_stickiness": revenue_stickiness,
                    "compounding_potential": "Yes" if company.get('business_model', '') == 'land_and_expand' else "Limited",
                    "dilution_to_exit": f"{rounds_remaining * 20}% ({rounds_remaining} more rounds at 20% dilution each)"
                },
                "pathos": f"We're getting in at the {'inflection point' if stage in ['Series A', 'Series B'] else 'late stage'} - {'proven product-market fit' if stage in ['Series B', 'Series C'] else 'early traction'}, but still {rounds_remaining} rounds from exit. {'Plenty of room' if rounds_remaining >= 2 else 'Limited room'} to build ownership through reserves. {'Not the sucker money' if has_tier1 else 'Risk of being tourist capital'}."
            }
        
        analysis_a = analyze_positioning(company_a)
        analysis_b = analyze_positioning(company_b)
        
        # Determine winner based on upside potential and reserve deployment
        upside_a = analysis_a["logos"]["upside_to_exit"]
        upside_b = analysis_b["logos"]["upside_to_exit"]
        winner = "company_a" if "5-8x" in upside_a or "4-6x" in upside_a else "company_b"
        
        return {
            "company_a": analysis_a,
            "company_b": analysis_b,
            "winner": winner,
            "reasoning": f"Company A offers {'more upside' if winner == 'company_a' else 'less dilution'} and {'more' if winner == 'company_a' else 'fewer'} reserve deployment opportunities"
        }
    
    def _compare_risk_adjusted_returns(self, company_a: Dict, company_b: Dict) -> Dict[str, Any]:
        """Compare risk-adjusted returns using existing scenario analysis"""
        def extract_risk_metrics(company):
            scenarios = company.get('pwerm_scenarios', [])
            if scenarios:
                # Calculate expected IRR and volatility
                # PWERMScenario objects have attributes, not dictionary keys
                irrs = [getattr(s, 'irr', 0) for s in scenarios]
                probabilities = [getattr(s, 'probability', 0.33) for s in scenarios]
                expected_irr = sum(irr * prob for irr, prob in zip(irrs, probabilities))
                
                # Calculate volatility (simplified)
                variance = sum(prob * (irr - expected_irr) ** 2 for irr, prob in zip(irrs, probabilities))
                volatility = variance ** 0.5
                
                # Sharpe ratio (assuming 4.5% risk-free rate)
                sharpe_ratio = (expected_irr - 0.045) / volatility if volatility > 0 else 0
                
                return {
                    "expected_irr": f"{expected_irr*100:.0f}%",
                    "sharpe_ratio": f"{sharpe_ratio:.2f}",
                    "downside_protection": "1x liquidation preference",
                    "volatility": f"{volatility*100:.0f}%"
                }
            else:
                return {
                    "expected_irr": "35%",
                    "sharpe_ratio": "1.2",
                    "downside_protection": "1x liquidation preference",
                    "volatility": "25%"
                }
        
        metrics_a = extract_risk_metrics(company_a)
        metrics_b = extract_risk_metrics(company_b)
        
        sharpe_a = float(metrics_a["sharpe_ratio"])
        sharpe_b = float(metrics_b["sharpe_ratio"])
        
        return {
            "company_a": metrics_a,
            "company_b": metrics_b,
            "winner": "company_a" if sharpe_a > sharpe_b else "company_b",
            "reasoning": "Higher Sharpe ratio indicates better risk-adjusted returns"
        }
    
    def _compare_growth_levers(self, company_a: Dict, company_b: Dict) -> Dict[str, Any]:
        """Compare growth levers and execution confidence"""
        def analyze_growth_levers(company):
            business_model = company.get('business_model', 'SaaS')
            sector = company.get('sector', 'Technology')
            
            # Determine primary and secondary growth levers
            if 'ai' in business_model.lower():
                primary = "AI model improvements + enterprise expansion"
                secondary = "International markets + vertical expansion"
            elif 'marketplace' in business_model.lower():
                primary = "Supply-side expansion"
                secondary = "Geographic expansion"
            else:
                primary = "Enterprise expansion"
                secondary = "Product line expansion"
            
            # Execution confidence based on team and investors
            investors = str(company.get('investors', '')).lower()
            tier1_investors = ['sequoia', 'andreessen', 'a16z', 'benchmark', 'greylock']
            has_tier1 = any(inv in investors for inv in tier1_investors)
            
            return {
                "primary": primary,
                "secondary": secondary,
                "execution_confidence": "High" if has_tier1 else "Medium",
                "growth_stage": "Acceleration" if company.get('stage') in ['Series A', 'Series B'] else "Optimization"
            }
        
        analysis_a = analyze_growth_levers(company_a)
        analysis_b = analyze_growth_levers(company_b)
        
        return {
            "company_a": analysis_a,
            "company_b": analysis_b,
            "winner": "company_a" if analysis_a["execution_confidence"] == "High" else "company_b",
            "reasoning": "Higher execution confidence based on investor quality"
        }
    
    def _compare_execution_quality(self, company_a: Dict, company_b: Dict) -> Dict[str, Any]:
        """Compare execution quality metrics"""
        def analyze_execution(company):
            investors = str(company.get('investors', '')).lower()
            tier1_investors = ['sequoia', 'andreessen', 'a16z', 'benchmark', 'greylock', 'accel']
            has_tier1 = any(inv in investors for inv in tier1_investors)
            
            # Calculate capital efficiency
            revenue = self._get_field_with_fallback(company, 'revenue', 0)
            total_funding = safe_get_value(company.get('total_funding', 0))
            capital_efficiency = revenue / max(total_funding, 1) if total_funding > 0 else 0
            
            return {
                "team_score": 85 if has_tier1 else 70,
                "investor_quality": "Tier 1" if has_tier1 else "Good",
                "capital_efficiency": f"{capital_efficiency:.1f}x revenue/funding",
                "execution_track_record": "Strong" if has_tier1 else "Developing"
            }
        
        analysis_a = analyze_execution(company_a)
        analysis_b = analyze_execution(company_b)
        
        return {
            "company_a": analysis_a,
            "company_b": analysis_b,
            "winner": "company_a" if analysis_a["team_score"] > analysis_b["team_score"] else "company_b",
            "reasoning": "Higher team score indicates better execution quality"
        }
    
    def _compare_market_fragmentation(self, company_a: Dict, company_b: Dict) -> Dict[str, Any]:
        """Compare market fragmentation and competitive position"""
        def analyze_market(company):
            sector = company.get('sector', 'Technology')
            business_model = company.get('business_model', 'SaaS')
            
            # Determine market structure
            if 'ai' in business_model.lower():
                market_structure = "Consolidating - winner-take-all dynamics"
                competitive_position = "Category leader"
                market_share = "15%"
            elif 'marketplace' in business_model.lower():
                market_structure = "Fragmented - consolidation opportunity"
                competitive_position = "Regional leader"
                market_share = "8%"
            else:
                market_structure = "Fragmented - consolidation opportunity"
                competitive_position = "Category leader"
                market_share = "12%"
            
            return {
                "market_structure": market_structure,
                "competitive_position": competitive_position,
                "market_share": market_share,
                "fragmentation_level": "High" if "Fragmented" in market_structure else "Medium"
            }
        
        analysis_a = analyze_market(company_a)
        analysis_b = analyze_market(company_b)
        
        return {
            "company_a": analysis_a,
            "company_b": analysis_b,
            "winner": "company_a" if analysis_a["market_share"] > analysis_b["market_share"] else "company_b",
            "reasoning": "Higher market share indicates stronger competitive position"
        }
    
    def _compare_ai_story(self, company_a: Dict, company_b: Dict) -> Dict[str, Any]:
        """Compare AI story and defensibility against OpenAI/Claude competition"""
        def analyze_ai_story(company):
            business_model = company.get('business_model', '')
            sector = company.get('sector', '')
            description = company.get('description', '').lower()
            
            # Determine AI category and competitive moat
            if 'ai_first' in business_model.lower() or 'ai' in sector.lower():
                ai_category = "AI-First"
                
                # Analyze competitive differentiation against OpenAI/Claude
                if 'proprietary' in description or 'custom model' in description:
                    defensibility = "Proprietary models + domain-specific data"
                    openai_delta = "High - custom models + specialized data OpenAI doesn't have"
                    moat_strength = "Strong"
                elif 'fine-tuned' in description or 'specialized' in description:
                    defensibility = "Fine-tuned models + vertical expertise"
                    openai_delta = "Medium - specialized tuning + domain knowledge"
                    moat_strength = "Moderate"
                else:
                    defensibility = "API wrapper + basic customization"
                    openai_delta = "Low - easily replicable by OpenAI/Claude"
                    moat_strength = "Weak"
                
                compute_costs = "$2M/year (manageable)"
                openai_anthropic_risk = f"{openai_delta}. Risk: OpenAI could release competing product in 6-12 months"
                
            elif 'ai' in business_model.lower():
                ai_category = "AI-Enhanced"
                defensibility = "AI features + traditional moats"
                compute_costs = "$500K/year (low)"
                openai_delta = "Low - AI is feature, not core differentiator"
                moat_strength = "Traditional moats protect"
                openai_anthropic_risk = f"{openai_delta}. Protected by network effects, switching costs"
                
            else:
                ai_category = "Traditional"
                defensibility = "Traditional moats (network effects, switching costs)"
                compute_costs = "$0/year"
                openai_delta = "None - not AI-dependent"
                moat_strength = "Strong traditional moats"
                openai_anthropic_risk = "None - not competing in AI space"
            
            # Assess specific competitive threats
            competitive_threats = []
            if ai_category == "AI-First":
                if moat_strength == "Weak":
                    competitive_threats.append("OpenAI could build competing product")
                    competitive_threats.append("Claude could add similar capabilities")
                elif moat_strength == "Moderate":
                    competitive_threats.append("OpenAI could acquire domain expertise")
                else:
                    competitive_threats.append("Protected by proprietary data/models")
            
            return {
                "ai_category": ai_category,
                "defensibility": defensibility,
                "compute_costs": compute_costs,
                "openai_anthropic_risk": openai_anthropic_risk,
                "openai_delta": openai_delta,
                "moat_strength": moat_strength,
                "competitive_threats": competitive_threats,
                "ai_differentiation": "High" if moat_strength == "Strong" else "Medium" if moat_strength == "Moderate" else "Low"
            }
        
        analysis_a = analyze_ai_story(company_a)
        analysis_b = analyze_ai_story(company_b)
        
        # Score based on moat strength and OpenAI/Claude competitive risk
        def score_ai_company(analysis):
            base_score = 5  # Start neutral
            
            # Adjust for moat strength
            if analysis["moat_strength"] == "Strong":
                base_score += 3
            elif analysis["moat_strength"] == "Moderate":
                base_score += 1
            elif analysis["moat_strength"] == "Weak":
                base_score -= 2
            
            # Adjust for competitive threats
            if "OpenAI could build competing product" in analysis["competitive_threats"]:
                base_score -= 3  # High risk
            elif "OpenAI could acquire domain expertise" in analysis["competitive_threats"]:
                base_score -= 1  # Medium risk
            elif "Protected by proprietary data/models" in analysis["competitive_threats"]:
                base_score += 2  # Low risk
            
            # Cap between 1-10
            return max(1, min(10, base_score))
        
        score_a = score_ai_company(analysis_a)
        score_b = score_ai_company(analysis_b)
        
        return {
            "company_a": analysis_a,
            "company_b": analysis_b,
            "winner": "company_a" if score_a > score_b else "company_b",
            "reasoning": f"Better competitive moat against OpenAI/Claude ({analysis_a['moat_strength']} vs {analysis_b['moat_strength']})"
        }
    
    def _compare_cap_table_quality(self, company_a: Dict, company_b: Dict) -> Dict[str, Any]:
        """Compare cap table quality and structure"""
        def analyze_cap_table(company):
            funding_rounds = company.get('funding_rounds', [])
            investors = str(company.get('investors', '')).lower()
            
            # Estimate founder ownership (simplified)
            rounds_count = len(funding_rounds) if funding_rounds else 2
            estimated_founder_ownership = max(20, 100 - (rounds_count * 20))
            
            # Assess investor quality
            tier1_investors = ['sequoia', 'andreessen', 'a16z', 'benchmark', 'greylock', 'accel']
            has_tier1 = any(inv in investors for inv in tier1_investors)
            
            return {
                "founder_ownership": f"{estimated_founder_ownership}%",
                "investor_quality": "Tier 1" if has_tier1 else "Good",
                "clean_structure": "Yes - standard terms",
                "dilution_history": f"{rounds_count} rounds completed"
            }
        
        analysis_a = analyze_cap_table(company_a)
        analysis_b = analyze_cap_table(company_b)
        
        return {
            "company_a": analysis_a,
            "company_b": analysis_b,
            "winner": "company_a" if analysis_a["investor_quality"] == "Tier 1" else "company_b",
            "reasoning": "Better investor quality indicates cleaner cap table"
        }
    
    def _compare_reserve_strategy(self, company_a: Dict, company_b: Dict, fund_size: float) -> Dict[str, Any]:
        """Compare reserve strategy and total capital allocation"""
        def analyze_reserves(company, fund_size):
            initial_check = self._get_optimal_check_size(company, {'fund_size': fund_size})
            
            # Calculate rounds remaining for reserve deployment
            stage = self._determine_accurate_stage(company)
            rounds_remaining = {
                'Seed': 4, 'Series A': 3, 'Series B': 2, 'Series C': 1, 'Series D': 0, 'Growth': 0
            }.get(stage, 2)
            
            reserved_capital = initial_check * rounds_remaining
            total_exposure = initial_check + reserved_capital
            
            return {
                "initial_check": f"${initial_check/1e6:.0f}M",
                "reserved_capital": f"${reserved_capital/1e6:.0f}M ({rounds_remaining}x)",
                "pro_rata_rights": "Yes",
                "total_exposure": f"${total_exposure/1e6:.0f}M ({(total_exposure/fund_size)*100:.1f}% of fund)"
            }
        
        analysis_a = analyze_reserves(company_a, fund_size)
        analysis_b = analyze_reserves(company_b, fund_size)
        
        return {
            "company_a": analysis_a,
            "company_b": analysis_b,
            "winner": "company_a" if float(analysis_a["total_exposure"].split('(')[1].split('%')[0]) < float(analysis_b["total_exposure"].split('(')[1].split('%')[0]) else "company_b",
            "reasoning": "Lower total fund exposure for better diversification"
        }
    
    def _summarize_pwerm(self, company: Dict) -> Dict[str, Any]:
        """Compact PWERM scenario data for LLM analysis"""
        scenarios = company.get('pwerm_scenarios', [])
        if not scenarios:
            return {
                "scenarios": [],
                "summary": "No PWERM scenarios available"
            }
        
        # Extract key metrics from PWERMScenario objects
        scenario_summaries = []
        for scenario in scenarios:
            scenario_summaries.append({
                "scenario": getattr(scenario, 'scenario_name', 'Unknown'),
                "probability": getattr(scenario, 'probability', 0.33),
                "irr": getattr(scenario, 'irr', 0),
                "present_value": getattr(scenario, 'present_value', 0),
                "time_to_exit": getattr(scenario, 'time_to_exit', 5)
            })
        
        return {
            "scenarios": scenario_summaries,
            "summary": f"{len(scenarios)} scenarios analyzed"
        }
    
    def _clean_llm_json_response(self, response_text: str) -> str:
        """Normalize JSON responses wrapped in markdown fences or with leading text."""
        cleaned = response_text.strip()
        
        # Remove common markdown fences
        if cleaned.lower().startswith('```json'):
            cleaned = cleaned[7:]
        if cleaned.startswith('```'):
            cleaned = cleaned[3:]
        if cleaned.endswith('```'):
            cleaned = cleaned[:-3]
        
        return cleaned.strip()

    def _extract_json_object(self, text: str) -> Optional[str]:
        """
        Extract the first balanced JSON object from the given text.
        Attempts to handle trailing prose by scanning for the first '{'
        and progressively checking closing braces from the end.
        """
        start = text.find('{')
        if start == -1:
            return None

        for end in range(len(text) - 1, start - 1, -1):
            if text[end] != '}':
                continue

            candidate = text[start:end + 1]
            try:
                json.loads(candidate)
                return candidate
            except json.JSONDecodeError:
                continue

        return None

    def _calculate_weighted_scores(self, comparisons: Dict[str, Dict]) -> Dict[str, float]:
        """Calculate weighted scores across all dimensions"""
        weights = {
            "portfolio_fit": 10,
            "carry_impact": 20,
            "early_late_positioning": 25,
            "risk_adjusted_returns": 20,
            "growth_levers": 10,
            "execution_quality": 15,
            "market_fragmentation": 5,
            "ai_story": 10,
            "cap_table_quality": 10,
            "reserve_strategy": 5
        }
        
        scores = {"company_a": 0, "company_b": 0}
        total_weight = sum(weights.values())
        
        for dimension, comparison in comparisons.items():
            weight = weights.get(dimension, 10)
            winner = comparison.get("winner", "company_a")
            
            if winner == "company_a":
                scores["company_a"] += weight
            else:
                scores["company_b"] += weight
        
        # Normalize to 0-10 scale
        scores["company_a"] = (scores["company_a"] / total_weight) * 10
        scores["company_b"] = (scores["company_b"] / total_weight) * 10
        
        return scores
    
    def _generate_recommendation(self, scores: Dict[str, float], company_a: Dict, company_b: Dict) -> str:
        """Generate investment recommendation based on scores"""
        score_a = scores["company_a"]
        score_b = scores["company_b"]
        
        if score_a > score_b:
            winner_name = company_a.get('company', 'Company A')
            loser_name = company_b.get('company', 'Company B')
            score_diff = score_a - score_b
        else:
            winner_name = company_b.get('company', 'Company B')
            loser_name = company_a.get('company', 'Company A')
            score_diff = score_b - score_a
        
        if score_diff > 1.5:
            strength = "strongly"
        elif score_diff > 0.5:
            strength = ""
        else:
            strength = "slightly"
        
        return f"{winner_name} preferred - {strength} better risk-adjusted returns, stronger execution, cleaner cap table"
    
    def _extract_key_differentiators(self, scores: Dict[str, float], company_a: Dict, company_b: Dict) -> List[str]:
        """Extract key differentiators between companies"""
        differentiators = []
        
        # Add investor quality differentiator
        investors_a = str(company_a.get('investors', '')).lower()
        investors_b = str(company_b.get('investors', '')).lower()
        tier1_investors = ['sequoia', 'andreessen', 'a16z', 'benchmark', 'greylock']
        
        has_tier1_a = any(inv in investors_a for inv in tier1_investors)
        has_tier1_b = any(inv in investors_b for inv in tier1_investors)
        
        if has_tier1_a and not has_tier1_b:
            differentiators.append(f"{company_a.get('company', 'Company A')} has Tier 1 investors signaling quality")
        elif has_tier1_b and not has_tier1_a:
            differentiators.append(f"{company_b.get('company', 'Company B')} has Tier 1 investors signaling quality")
        
        # Add capital efficiency differentiator
        revenue_a = self._get_field_with_fallback(company_a, 'revenue', 0)
        revenue_b = self._get_field_with_fallback(company_b, 'revenue', 0)
        funding_a = safe_get_value(company_a.get('total_funding', 0))
        funding_b = safe_get_value(company_b.get('total_funding', 0))
        
        if funding_a > 0 and funding_b > 0:
            efficiency_a = revenue_a / funding_a if funding_a > 0 else 0
            efficiency_b = revenue_b / funding_b if funding_b > 0 else 0
            
            if efficiency_a > efficiency_b * 1.2:
                differentiators.append(f"{company_a.get('company', 'Company A')} shows better capital efficiency ({efficiency_a:.1f}x vs {efficiency_b:.1f}x)")
            elif efficiency_b > efficiency_a * 1.2:
                differentiators.append(f"{company_b.get('company', 'Company B')} shows better capital efficiency ({efficiency_b:.1f}x vs {efficiency_a:.1f}x)")
        
        return differentiators[:3]  # Limit to top 3 differentiators
    
    def _get_key_differentiator(self, company: Dict) -> str:
        """Get a single key differentiator for a company"""
        # Extract key differentiators from company data
        differentiators = []
        
        # Check for AI/technology differentiation
        if company.get('sector', '').lower() in ['ai', 'artificial intelligence', 'machine learning', 'fintech']:
            differentiators.append("AI-first technology")
        
        # Check for revenue growth
        growth_rate = safe_get_value(company.get('growth_rate', 0))
        if growth_rate > 100:
            differentiators.append(f"exceptional {growth_rate:.0f}% YoY growth")
        elif growth_rate > 50:
            differentiators.append(f"strong {growth_rate:.0f}% YoY growth")
        
        # Check for funding stage
        stage = company.get('stage', '').lower()
        if 'series a' in stage:
            differentiators.append("proven product-market fit")
        elif 'seed' in stage:
            differentiators.append("early traction")
        
        # Check for investor quality
        investors = str(company.get('investors', '')).lower()
        tier1_investors = ['sequoia', 'andreessen', 'a16z', 'benchmark', 'greylock']
        if any(inv in investors for inv in tier1_investors):
            differentiators.append("Tier 1 investor validation")
        
        # Return the first differentiator or a default
        return differentiators[0] if differentiators else "innovative technology and strong execution"
    
    def _assess_competitive_intensity(self, competitors: List[Dict]) -> str:
        """Assess competitive intensity based on competitor data"""
        if not competitors:
            return "Low - early market with limited competition"
        
        # Count competitors and assess funding levels
        competitor_count = len(competitors)
        total_funding = sum(safe_get_value(c.get('total_funding', 0)) for c in competitors)
        
        if competitor_count >= 10:
            return "High - crowded market with many well-funded competitors"
        elif competitor_count >= 5:
            return "Moderate - several competitors but market not saturated"
        elif competitor_count >= 2:
            return "Low-Moderate - few competitors, early market opportunity"
        else:
            return "Low - minimal competition, first-mover advantage"
    
    def _get_incumbent_weakness(self, incumbents: List[Dict]) -> str:
        """Get key weakness of incumbent companies being disrupted"""
        if not incumbents:
            return "legacy infrastructure and outdated processes"
        
        # Analyze incumbent weaknesses based on common patterns
        weaknesses = []
        
        for incumbent in incumbents:
            name = incumbent.get('name', '').lower()
            
            # Technology-related weaknesses
            if any(term in name for term in ['bank', 'financial', 'insurance', 'healthcare']):
                weaknesses.append("legacy technology systems")
            elif any(term in name for term in ['retail', 'commerce', 'shopping']):
                weaknesses.append("outdated customer experience")
            elif any(term in name for term in ['media', 'publishing', 'news']):
                weaknesses.append("slow content distribution")
            elif any(term in name for term in ['transportation', 'logistics', 'shipping']):
                weaknesses.append("inefficient operational processes")
            else:
                weaknesses.append("slow innovation cycles")
        
        # Return the most common weakness or a default
        if weaknesses:
            # Count occurrences and return most common
            from collections import Counter
            weakness_counts = Counter(weaknesses)
            return weakness_counts.most_common(1)[0][0]
        
        return "legacy infrastructure and outdated processes"
    
    def _get_our_technology_advantage(self, company: Dict) -> str:
        """Get our technology advantage over incumbents"""
        advantages = []
        
        # Check for AI/ML capabilities
        sector = company.get('sector', '').lower()
        if any(term in sector for term in ['ai', 'artificial intelligence', 'machine learning', 'fintech']):
            advantages.append("AI-powered automation")
        
        # Check for modern tech stack indicators
        description = str(company.get('description', '')).lower()
        if any(term in description for term in ['cloud', 'saas', 'api', 'mobile', 'real-time']):
            advantages.append("modern cloud-native architecture")
        
        # Check for data advantages
        if any(term in description for term in ['data', 'analytics', 'insights', 'intelligence']):
            advantages.append("data-driven insights")
        
        # Check for user experience focus
        if any(term in description for term in ['user experience', 'ux', 'interface', 'design']):
            advantages.append("superior user experience")
        
        # Check for scalability
        if any(term in description for term in ['scalable', 'scale', 'growth', 'expansion']):
            advantages.append("infinite scalability")
        
        # Return the first advantage or a default
        return advantages[0] if advantages else "modern technology and superior execution"
    
    def _calculate_incumbent_revenue(self, incumbents: List[Dict]) -> float:
        """Calculate total revenue of incumbent companies"""
        if not incumbents:
            return 50_000_000_000  # Default $50B market
        
        total_revenue = 0.0
        
        for incumbent in incumbents:
            # Try to get revenue from various fields
            revenue = safe_get_value(incumbent.get('revenue', 0))
            if revenue == 0:
                # Fallback to estimated revenue based on company type
                name = incumbent.get('name', '').lower()
                if any(term in name for term in ['google', 'microsoft', 'apple', 'amazon']):
                    revenue = 100_000_000_000  # $100B+ for mega tech
                elif any(term in name for term in ['salesforce', 'adobe', 'oracle']):
                    revenue = 10_000_000_000   # $10B+ for enterprise software
                elif any(term in name for term in ['bank', 'financial', 'insurance']):
                    revenue = 5_000_000_000    # $5B+ for financial services
                else:
                    revenue = 1_000_000_000    # $1B+ default for large incumbents
            
            total_revenue += revenue
        
        # If no revenue found, return a reasonable default
        return total_revenue if total_revenue > 0 else 50_000_000_000
    
    def _calculate_total_competitor_funding(self, competitors: List[Dict]) -> float:
        """Calculate total funding raised by competitors"""
        if not competitors:
            return 0.0
        
        total = 0.0
        for competitor in competitors:
            funding = safe_get_value(competitor.get('total_funding', 0))
            if funding:
                total += funding
        
        return total
    
    def _get_revenue_multiple_quartile(self, company: Dict) -> str:
        """Get revenue multiple quartile positioning"""
        revenue = safe_get_value(company.get('revenue', 0))
        valuation = safe_get_value(company.get('latest_valuation', 0))
        
        if not revenue or not valuation or revenue <= 0:
            return "unknown multiple"
        
        multiple = self._safe_divide(valuation, revenue, default=0)
        
        if multiple >= 20:
            return "top quartile (20x+)"
        elif multiple >= 10:
            return "upper quartile (10-20x)"
        elif multiple >= 5:
            return "median quartile (5-10x)"
        else:
            return "lower quartile (<5x)"
    
    def _interpret_fragmentation(self, fragmentation: str) -> str:
        """Interpret market fragmentation level"""
        if not fragmentation:
            return "moderate market structure"
        
        fragmentation_lower = fragmentation.lower()
        if 'high' in fragmentation_lower:
            return "opportunity for consolidation"
        elif 'low' in fragmentation_lower:
            return "established market leaders"
        else:
            return "balanced competitive landscape"
    
    def _assess_vc_darling_risk(self, companies: List[Dict]) -> str:
        """Assess VC darling risk based on company metrics"""
        if not companies:
            return "Low - no companies to assess"
        
        # Check for signs of VC darling status
        high_valuations = 0
        high_growth = 0
        
        for company in companies:
            valuation = safe_get_value(company.get('latest_valuation', 0))
            growth = safe_get_value(company.get('revenue_growth', 0))
            
            if valuation > 1_000_000_000:  # $1B+ valuation
                high_valuations += 1
            if growth > 200:  # 200%+ growth
                high_growth += 1
        
        if high_valuations >= len(companies) * 0.5:
            return "High - premium valuations suggest VC darling status"
        elif high_growth >= len(companies) * 0.5:
            return "Moderate - high growth but valuations reasonable"
        else:
            return "Low - valuations and growth in normal ranges"
    
    def _calculate_our_cambridge_positioning(self, companies: List[Dict], benchmarks: Dict) -> Dict:
        """Calculate positioning relative to Cambridge Associates benchmarks"""
        if not companies:
            return {"quartile_positioning": "unknown"}
        
        # Calculate average expected IRR across companies
        total_irr = 0
        count = 0
        
        for company in companies:
            # Try to get IRR from various sources
            irr = (company.get('expected_irr') or 
                   company.get('irr') or 
                   company.get('projected_irr'))
            
            if irr:
                total_irr += safe_get_value(irr)
                count += 1
        
        if count == 0:
            return {"quartile_positioning": "unknown"}
        
        avg_irr = total_irr / count if count > 0 else 0
        
        # Compare to Cambridge benchmarks (typical VC fund returns)
        if avg_irr >= 25:
            return {"quartile_positioning": "top quartile", "irr": avg_irr}
        elif avg_irr >= 15:
            return {"quartile_positioning": "upper quartile", "irr": avg_irr}
        elif avg_irr >= 10:
            return {"quartile_positioning": "median", "irr": avg_irr}
        else:
            return {"quartile_positioning": "lower quartile", "irr": avg_irr}
    
    async def _generate_scoring_matrix(self, companies: List[Dict], fund_context: Optional[Dict] = None) -> Dict[str, Any]:
        """Generate scoring matrix heatmap for visual comparison (works for 1+ companies)"""
        if len(companies) == 0:
            return {}
        
        fund_size = fund_context.get('fund_size', 200_000_000) if fund_context else 200_000_000
        
        # Define scoring dimensions and weights
        dimensions = [
            "Portfolio Fit",
            "Carry Impact", 
            "Early/Late Positioning",
            "Risk-Adj Returns",
            "Growth Levers",
            "Execution Quality",
            "Market Fragmentation",
            "AI Story",
            "Cap Table Quality",
            "Reserve Strategy"
        ]
        
        weights = {
            "Portfolio Fit": 10,
            "Carry Impact": 20,
            "Early/Late Positioning": 25,
            "Risk-Adj Returns": 20,
            "Growth Levers": 10,
            "Execution Quality": 15,
            "Market Fragmentation": 5,
            "AI Story": 10,
            "Cap Table Quality": 10,
            "Reserve Strategy": 5
        }
        
        # For single company: calculate absolute scores
        if len(companies) == 1:
            company = companies[0]
            company_name = company.get('company', 'Company')
            
            # Calculate absolute scores for each dimension (0-10 scale)
            def calculate_absolute_score(company: Dict, dimension: str) -> float:
                """Calculate absolute score for a single company on a dimension"""
                # Base scores from company data
                stage = company.get('stage', '').lower()
                revenue = self._get_field_safe(company, 'revenue', 0)
                growth_rate = self._get_field_safe(company, 'growth_rate', 0)
                valuation = self._get_field_safe(company, 'valuation', 0)
                fund_fit_score = self._get_field_safe(company, 'fund_fit_score', 0)
                
                # Portfolio fit: based on fund_fit_score if available
                if dimension == "Portfolio Fit":
                    return min(10, max(0, fund_fit_score * 10)) if fund_fit_score else 7.0
                
                # Early/Late positioning: earlier stage = higher score
                elif dimension == "Early/Late Positioning":
                    stage_scores = {'pre-seed': 9, 'seed': 8, 'series a': 7, 'series b': 6, 'series c': 5, 'growth': 4}
                    return stage_scores.get(stage, 6.5)
                
                # Risk-adjusted returns: based on growth and stage
                elif dimension == "Risk-Adj Returns":
                    growth_score = min(10, growth_rate * 2) if growth_rate > 0 else 5.0
                    stage_multiplier = {'pre-seed': 1.2, 'seed': 1.1, 'series a': 1.0, 'series b': 0.9, 'series c': 0.8}.get(stage, 1.0)
                    return min(10, growth_score * stage_multiplier)
                
                # Growth levers: based on growth rate
                elif dimension == "Growth Levers":
                    return min(10, max(5, growth_rate * 2)) if growth_rate > 0 else 6.0
                
                # Execution quality: based on revenue and team
                elif dimension == "Execution Quality":
                    revenue_score = min(10, revenue / 10_000_000) if revenue > 0 else 5.0
                    team_size = self._get_field_safe(company, 'team_size', 0)
                    team_score = min(10, team_size / 20) if team_size > 0 else 5.0
                    return (revenue_score + team_score) / 2
                
                # Default score
                else:
                    return 7.0
            
            scores = {dim: calculate_absolute_score(company, dim) for dim in dimensions}
            
            # Calculate weighted overall score
            total_weight = sum(weights.values())
            weighted_score = sum(scores[dim] * weights[dim] for dim in dimensions) / total_weight if total_weight > 0 else 0
            
            # Create heatmap chart data for single company using helper
            heatmap_chart_data = format_heatmap_chart(
                dimensions=dimensions,
                companies=[company_name],
                scores=[[scores[dim] for dim in dimensions]],
                weights=weights,
                title="Investment Scoring Matrix"
            )
            
            # Validate heatmap chart data before prerendering
            if not heatmap_chart_data.get('data') or not heatmap_chart_data['data'].get('dimensions') or not heatmap_chart_data['data'].get('scores'):
                logger.error(f"[DECK_GEN] Invalid heatmap chart data structure: missing dimensions or scores")
                chart_data_for_slide = heatmap_chart_data
            else:
                # Pre-render heatmap chart with error handling
                prerendered_chart = await self._prerender_complex_chart(heatmap_chart_data)
                
                # If prerender failed (returned original chart_data), use that
                # If prerender succeeded (returned image object), check if it has original_data
                if prerendered_chart.get('type') == 'image' and prerendered_chart.get('original_data'):
                    # Prerender succeeded - use the image object
                    chart_data_for_slide = prerendered_chart
                elif prerendered_chart.get('type') == 'heatmap':
                    # Prerender failed but returned original - use it
                    chart_data_for_slide = prerendered_chart
                else:
                    # Fallback to original chart_data
                    logger.warning(f"[DECK_GEN] Prerender returned unexpected format, using original chart_data")
                    chart_data_for_slide = heatmap_chart_data
            
            # Create explanation text
            explanation = (
                f"Scoring Methodology: Each dimension is scored 0-10 based on company data. "
                f"Weighted overall score: {weighted_score:.1f}/10. "
                f"Dimensions: Portfolio Fit (10%), Carry Impact (20%), Early/Late Positioning (25%), "
                f"Risk-Adj Returns (20%), Growth Levers (10%), Execution Quality (15%), "
                f"Market Fragmentation (5%), AI Story (10%), Cap Table Quality (10%), Reserve Strategy (5%)."
            )
            
            return {
                "title": "Investment Scoring Matrix",
                "subtitle": f"Comprehensive scoring for {company_name}",
                "body": explanation,
                "companies": {
                    "company": company_name
                },
                "chart_data": chart_data_for_slide,
                "weighted_score": weighted_score,
                "dimension_scores": scores
            }
        
        # For multiple companies: comparison mode
        else:
            company_a, company_b = companies[0], companies[1]
            
            # Calculate all dimensions and extract scores
            portfolio_comparison = self._compare_portfolio_fit(company_a, company_b, fund_size)
            carry_comparison = self._compare_carry_impact(company_a, company_b, fund_size)
            early_late_comparison = self._compare_early_late_positioning(company_a, company_b)
            risk_adj_comparison = self._compare_risk_adjusted_returns(company_a, company_b)
            growth_comparison = self._compare_growth_levers(company_a, company_b)
            execution_comparison = self._compare_execution_quality(company_a, company_b)
            market_comparison = self._compare_market_fragmentation(company_a, company_b)
            ai_comparison = self._compare_ai_story(company_a, company_b)
            cap_table_comparison = self._compare_cap_table_quality(company_a, company_b)
            reserve_comparison = self._compare_reserve_strategy(company_a, company_b, fund_size)
            
            # Convert comparisons to scores (0-10 scale)
            def score_from_comparison(comparison):
                winner = comparison.get("winner", "company_a")
                return {"company_a": 8 if winner == "company_a" else 6, "company_b": 8 if winner == "company_b" else 6}
            
            scores = {
                "Portfolio Fit": score_from_comparison(portfolio_comparison),
                "Carry Impact": score_from_comparison(carry_comparison),
                "Early/Late Positioning": score_from_comparison(early_late_comparison),
                "Risk-Adj Returns": score_from_comparison(risk_adj_comparison),
                "Growth Levers": score_from_comparison(growth_comparison),
                "Execution Quality": score_from_comparison(execution_comparison),
                "Market Fragmentation": score_from_comparison(market_comparison),
                "AI Story": score_from_comparison(ai_comparison),
                "Cap Table Quality": score_from_comparison(cap_table_comparison),
                "Reserve Strategy": score_from_comparison(reserve_comparison)
            }
            
            weighted_scores = {"company_a": 0, "company_b": 0}
            total_weight = sum(weights.values())
            
            for dimension, dimension_scores in scores.items():
                weight = weights[dimension]
                weighted_scores["company_a"] += dimension_scores["company_a"] * weight
                weighted_scores["company_b"] += dimension_scores["company_b"] * weight
            
            # Normalize to 0-10 scale
            weighted_scores["company_a"] = weighted_scores["company_a"] / total_weight if total_weight > 0 else 0
            weighted_scores["company_b"] = weighted_scores["company_b"] / total_weight if total_weight > 0 else 0
            
            # Create heatmap chart data using helper
            heatmap_chart_data = format_heatmap_chart(
                dimensions=list(scores.keys()),
                companies=[company_a.get('company', 'Company A'), company_b.get('company', 'Company B')],
                scores=[[scores[dim]["company_a"] for dim in scores.keys()], 
                        [scores[dim]["company_b"] for dim in scores.keys()]],
                weights=weights,
                title="Investment Scoring Matrix"
            )
            
            # Validate heatmap chart data before prerendering
            dimensions = heatmap_chart_data.get('data', {}).get('dimensions', [])
            companies = heatmap_chart_data.get('data', {}).get('companies', [])
            scores = heatmap_chart_data.get('data', {}).get('scores', [])
            
            if not dimensions or not companies or not scores or len(dimensions) == 0 or len(companies) == 0 or len(scores) == 0:
                logger.error(f"[DECK_GEN] Invalid heatmap chart data (multi-company): dimensions={len(dimensions) if dimensions else 0}, companies={len(companies) if companies else 0}, scores={len(scores) if scores else 0}")
                logger.error(f"[DECK_GEN] Heatmap chart_data structure: {json.dumps(heatmap_chart_data, indent=2, default=str)}")
                chart_data_for_slide = heatmap_chart_data
            else:
                logger.info(f"[DECK_GEN] Heatmap chart data validated (multi-company): {len(dimensions)} dimensions, {len(companies)} companies, {len(scores)} score arrays")
                # Pre-render heatmap chart with error handling
                prerendered_chart = await self._prerender_complex_chart(heatmap_chart_data)
                
                # If prerender failed (returned original chart_data), use that
                # If prerender succeeded (returned image object), check if it has original_data
                if prerendered_chart.get('type') == 'image' and prerendered_chart.get('original_data'):
                    # Prerender succeeded - use the image object
                    logger.info(f"[DECK_GEN] Heatmap chart prerendered successfully (multi-company)")
                    chart_data_for_slide = prerendered_chart
                elif prerendered_chart.get('type') == 'heatmap':
                    # Prerender failed but returned original - use it
                    logger.info(f"[DECK_GEN] Heatmap chart prerender failed (multi-company), using raw chart_data")
                    chart_data_for_slide = prerendered_chart
                else:
                    # Fallback to original chart_data
                    logger.warning(f"[DECK_GEN] Prerender returned unexpected format (multi-company): {prerendered_chart.get('type')}, using original chart_data")
                    chart_data_for_slide = heatmap_chart_data
            
            # Create explanation text
            winner_name = company_a.get('company', 'Company A') if weighted_scores["company_a"] > weighted_scores["company_b"] else company_b.get('company', 'Company B')
            explanation = (
                f"Scoring Methodology: Each dimension scored 0-10 based on company data comparison. "
                f"Winner: {winner_name} ({max(weighted_scores.values()):.1f}/10 vs {min(weighted_scores.values()):.1f}/10). "
                f"Dimensions weighted: Portfolio Fit (10%), Carry Impact (20%), Early/Late Positioning (25%), "
                f"Risk-Adj Returns (20%), Growth Levers (10%), Execution Quality (15%), "
                f"Market Fragmentation (5%), AI Story (10%), Cap Table Quality (10%), Reserve Strategy (5%)."
            )
            
            return {
                "title": "Investment Scoring Matrix",
                "subtitle": "Head-to-head comparison across all dimensions",
                "body": explanation,
                "companies": {
                    "company_a": company_a.get('company', 'Company A'),
                    "company_b": company_b.get('company', 'Company B')
                },
                "chart_data": chart_data_for_slide,
                "weighted_scores": weighted_scores,
                "summary": {
                    "winner": "company_a" if weighted_scores["company_a"] > weighted_scores["company_b"] else "company_b",
                    "score_difference": abs(weighted_scores["company_a"] - weighted_scores["company_b"])
                }
            }
    
    def _format_money(self, value: float) -> str:
        """Format money values consistently using centralized formatter"""
        return DeckFormatter.format_currency(value)

    def _parse_portfolio_composition(self, fund_context: Dict[str, Any], companies: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Parse fund portfolio composition from context
        Examples: "1 seed to Series D, 2 Series A to C"
        """
        composition = {
            "seed_to_exit": 0,
            "a_to_exit": 0,
            "b_to_exit": 0,
            "c_to_exit": 0,
            "growth_to_exit": 0,
            "total_companies": 0,
            "stage_distribution": {},
            "expected_returns_by_stage": {}
        }
        
        # If explicit portfolio composition is provided
        portfolio_desc = fund_context.get('portfolio_description', '')
        if portfolio_desc:
            # Parse descriptions like "1 seed to Series D, 2 Series A to C"
            import re
            patterns = [
                (r'(\d+)\s+seed\s+to\s+(?:series\s+)?([A-E]|exit|ipo)', 'seed_to_{}'),
                (r'(\d+)\s+(?:series\s+)?a\s+to\s+(?:series\s+)?([B-E]|exit|ipo)', 'a_to_{}'),
                (r'(\d+)\s+(?:series\s+)?b\s+to\s+(?:series\s+)?([C-E]|exit|ipo)', 'b_to_{}'),
            ]
            
            for pattern, key_template in patterns:
                matches = re.findall(pattern, portfolio_desc.lower())
                for count, exit_stage in matches:
                    stage_key = key_template.format(exit_stage.lower() if exit_stage != 'ipo' else 'exit')
                    composition[stage_key] = int(count)
                    composition['total_companies'] += int(count)
        
        # Otherwise infer from existing portfolio
        if composition['total_companies'] == 0:
            portfolio_count = fund_context.get('portfolio_size', fund_context.get('portfolio_count', 10))
            # Typical venture portfolio distribution
            composition['seed_to_exit'] = int(portfolio_count * 0.2)  # 20% seed
            composition['a_to_exit'] = int(portfolio_count * 0.4)     # 40% Series A
            composition['b_to_exit'] = int(portfolio_count * 0.3)     # 30% Series B
            composition['c_to_exit'] = int(portfolio_count * 0.1)     # 10% Series C+
            composition['total_companies'] = portfolio_count
        
        # Calculate expected returns by entry stage
        composition['expected_returns_by_stage'] = {
            'seed': {'min': 0, 'expected': 10, 'max': 50},     # Seed: 0-50x, expected 10x
            'series_a': {'min': 0, 'expected': 5, 'max': 20},  # Series A: 0-20x, expected 5x
            'series_b': {'min': 0.5, 'expected': 3, 'max': 10}, # Series B: 0.5-10x, expected 3x
            'series_c': {'min': 0.8, 'expected': 2, 'max': 5},  # Series C: 0.8-5x, expected 2x
            'growth': {'min': 1, 'expected': 1.5, 'max': 3}     # Growth: 1-3x, expected 1.5x
        }
        
        return composition
    
    def _calculate_forward_cap_table(self, company: Dict[str, Any], our_investment: float, 
                                    fund_context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Calculate forward-looking cap table with our investment through exit
        NOW WITH YC SAFE MODELING
        """
        current_valuation = company.get('valuation', 100_000_000)
        stage = company.get('stage', 'Series A').lower()
        
        # NEW: Detect YC company
        investors = company.get('investors', [])
        is_yc = company.get('is_yc', False) or any('Y Combinator' in str(inv) for inv in investors)
        
        # Our entry
        post_money = current_valuation + our_investment
        our_entry_ownership = our_investment / post_money
        
        # NEW: If YC company at seed stage, model SAFE conversion at Series A
        yc_safe_amount = 0
        yc_safe_dilution_at_a = 0
        if is_yc and 'seed' in stage:
            # YC SAFE inference (from intelligent_gap_filler.py lines 392-400)
            year = company.get('founded_year', '2024')
            if year >= '2021':
                yc_safe_amount = 500_000  # $500k post-2021
            elif year >= '2017':
                yc_safe_amount = 150_000
            else:
                yc_safe_amount = 125_000
            
            # YC SAFE terms: typically 20% discount, uncapped (pre_post_cap_table.py lines 260-266)
            # Will convert at Series A
            series_a_valuation = current_valuation * 3  # Assume 3x step-up to Series A
            conversion_price = series_a_valuation * 0.80  # 20% discount
            yc_safe_dilution_at_a = yc_safe_amount / (conversion_price + yc_safe_amount)
        
        # Determine rounds to exit based on stage
        rounds_to_exit = {
            'seed': 4,  # Seed → A → B → C → Exit
            'series a': 3,  # A → B → C → Exit
            'series b': 2,  # B → C → Exit
            'series c': 1,  # C → Exit
        }.get(stage.lower(), 2)
        
        # Model future rounds
        future_rounds = []
        cumulative_prorata = 0
        current_ownership = our_entry_ownership
        
        for i in range(rounds_to_exit):
            round_name = ['Series A', 'Series B', 'Series C', 'Series D'][i]
            
            # Base dilution from your existing benchmarks
            base_dilution = {'Series A': 0.20, 'Series B': 0.15, 'Series C': 0.12, 'Series D': 0.10}[round_name]
            
            # Apply YOUR adjusted benchmarks (from _calculate_exit_ownership lines 5393-5410)
            # Better investors = less dilution
            has_tier1 = any('Sequoia' in str(inv) or 'a16z' in str(inv) or 'Benchmark' in str(inv) 
                           or 'Accel' in str(inv) or 'Founders Fund' in str(inv) for inv in investors)
            quality_mult = 0.85 if has_tier1 else 1.0
            
            # Geography adjustment (SF/NYC = less dilution)
            geo = company.get('geography', '')
            geo_mult = 0.95 if geo in ['SF', 'San Francisco', 'NYC', 'New York'] else 1.0
            
            # Calculate adjusted dilution
            dilution = base_dilution * quality_mult * geo_mult
            
            # NEW: Add YC SAFE dilution at Series A
            if i == 0 and is_yc and yc_safe_dilution_at_a > 0:
                total_dilution = dilution + yc_safe_dilution_at_a
                yc_converted = True
            else:
                total_dilution = dilution
                yc_converted = False
            
            # Calculate ownership after this round
            ownership_after_round = current_ownership * (1 - total_dilution)
            
            # Pro-rata investment to maintain ownership
            round_size = current_valuation * total_dilution
            pro_rata_investment = round_size * current_ownership
            
            future_rounds.append({
                'round': round_name,
                'dilution': total_dilution,
                'ownership_without_prorata': ownership_after_round,
                'pro_rata_cost': pro_rata_investment,
                'yc_safe_converted': yc_converted,  # NEW FLAG
                'yc_safe_dilution': yc_safe_dilution_at_a if yc_converted else 0  # NEW
            })
            
            current_ownership = ownership_after_round
            cumulative_prorata += pro_rata_investment
        
        # Calculate exit valuation for 3x return
        exit_for_3x = our_investment * 3 / current_ownership if current_ownership > 0 else 0
        
        # Calculate liquidation preference (assume 1x non-participating)
        our_liquidation_pref = our_investment * 1.0
        
        return {
            # Match expected key names for deck generation
            'our_investment': our_investment,
            'our_entry_ownership': our_entry_ownership,  # was 'entry_ownership'
            'our_exit_ownership': current_ownership,      # was 'exit_ownership_no_followon'
            'rounds_to_exit': rounds_to_exit,
            'future_rounds': future_rounds,
            'total_prorata_needed': cumulative_prorata,  # matches expected 'total_prorata_needed'
            'our_liquidation_pref': our_liquidation_pref,
            'exit_for_3x': exit_for_3x,
            'is_yc_company': is_yc,
            'yc_safe_amount': yc_safe_amount if is_yc else 0,
        }
    
    def _enhance_sankey_with_investment(self, existing_sankey: Dict[str, Any], 
                                       forward_data: Dict[str, Any],
                                       company_name: str) -> Dict[str, Any]:
        """
        Enhance existing Sankey diagram with our investment and future projections
        """
        if not existing_sankey or 'nodes' not in existing_sankey:
            return existing_sankey
        
        enhanced = {
            'nodes': list(existing_sankey.get('nodes', [])),
            'links': list(existing_sankey.get('links', []))
        }
        
        # Find the last node ID
        max_node_id = max([n.get('id', 0) for n in enhanced['nodes']] + [0])
        
        # Add our investment node
        our_node_id = max_node_id + 1
        enhanced['nodes'].append({
            'id': our_node_id,
            'name': f"Our Fund: ${forward_data['our_investment']/1e6:.1f}M ({forward_data['our_entry_ownership']*100:.1f}%)",
            'color': '#10b981'  # Green for our investment
        })
        
        # Add future round nodes
        future_node_ids = []
        for i, round_data in enumerate(forward_data['future_rounds']):
            node_id = our_node_id + i + 1
            future_node_ids.append(node_id)
            enhanced['nodes'].append({
                'id': node_id,
                'name': f"Future Round {i+1}",
                'color': '#6366f1'
            })
        
        # Add exit node
        exit_node_id = our_node_id + len(forward_data['future_rounds']) + 1
        enhanced['nodes'].append({
            'id': exit_node_id,
            'name': f"Exit: {forward_data['our_exit_ownership']*100:.1f}% ownership",
            'color': '#f59e0b'
        })
        
        # Add links for our investment flow
        # Link from current state to our investment
        if enhanced['nodes']:
            # Find the final ownership node from existing data
            final_nodes = [n for n in enhanced['nodes'] if 'Final' in n.get('name', '')]
            if final_nodes:
                last_node = final_nodes[-1]
                enhanced['links'].append({
                    'source': last_node['id'],
                    'target': our_node_id,
                    'value': forward_data['our_entry_ownership'] * 100
                })
        
        # Links through future rounds to exit
        for i, (round_data, node_id) in enumerate(zip(forward_data['future_rounds'], future_node_ids)):
            source = our_node_id if i == 0 else future_node_ids[i-1]
            enhanced['links'].append({
                'source': source,
                'target': node_id,
                'value': round_data['ownership_without_prorata'] * 100
            })
        
        # Link to exit
        if future_node_ids:
            enhanced['links'].append({
                'source': future_node_ids[-1],
                'target': exit_node_id,
                'value': forward_data['our_exit_ownership'] * 100
            })
        
        return enhanced
    
    def _calculate_dpi_impact_scenarios(self, companies: List[Dict[str, Any]], fund_size: float, 
                                       deployed_capital: float, remaining_capital: float,
                                       current_dpi: float, portfolio_composition: Dict[str, Any]) -> Dict[str, Any]:
        """
        Calculate how these investments impact fund DPI across different scenarios
        """
        scenarios = {
            'company1_contribution': 0,
            'company2_contribution': 0,
            'reserves_contribution': 0,
            'total_expected_contribution': 0,
            'total_followon_needed': 0,
            'avg_exit_ownership': 0,
            'company1_breakeven': 0,
            'company2_breakeven': 0
        }
        
        if not companies:
            return scenarios
        
        # Calculate for each company
        for i, company in enumerate(companies[:2]):
            # Get investment details
            check_size = self._get_optimal_check_size(company, {'fund_size': fund_size})
            valuation = company.get('valuation', 100_000_000)
            
            # Entry ownership
            entry_ownership = check_size / (valuation + check_size)
            
            # Get PWERM scenarios if available
            pwerm_scenarios = company.get('pwerm_scenarios', [])
            if pwerm_scenarios:
                # Use probability-weighted expected return
                expected_return = 0
                for scenario in pwerm_scenarios:
                    if hasattr(scenario, 'probability'):
                        expected_return += scenario.probability * scenario.moic
                    else:
                        expected_return += scenario.get('probability', 0) * scenario.get('moic', 1)
                
                expected_proceeds = check_size * expected_return
            else:
                # Fallback to stage-based expectations
                stage = company.get('stage', 'Series A').lower()
                if 'seed' in stage:
                    expected_multiple = 10
                elif 'a' in stage:
                    expected_multiple = 5
                elif 'b' in stage:
                    expected_multiple = 3
                else:
                    expected_multiple = 2
                
                expected_proceeds = check_size * expected_multiple
            
            # Calculate dilution to exit
            rounds_to_exit = 3 if 'seed' in company.get('stage', '').lower() else 2
            dilution_per_round = 0.20
            exit_ownership = entry_ownership * ((1 - dilution_per_round) ** rounds_to_exit)
            
            # Follow-on capital needed to maintain ownership
            followon_needed = 0
            remaining_ownership = entry_ownership
            for round_num in range(rounds_to_exit):
                round_dilution = dilution_per_round
                prorata_investment = (remaining_ownership / (1 - round_dilution)) * round_dilution * (valuation * (1.5 ** (round_num + 1)))
                followon_needed += prorata_investment
                remaining_ownership = remaining_ownership  # Maintained through pro-rata
            
            # Breakeven exit value (1x return)
            breakeven_exit = check_size / exit_ownership
            
            # Store results
            if i == 0:
                scenarios['company1_contribution'] = expected_proceeds
                scenarios['company1_breakeven'] = breakeven_exit
            else:
                scenarios['company2_contribution'] = expected_proceeds
                scenarios['company2_breakeven'] = breakeven_exit
            
            scenarios['total_expected_contribution'] += expected_proceeds
            scenarios['total_followon_needed'] += followon_needed
            scenarios['avg_exit_ownership'] += exit_ownership / min(2, len(companies))
        
        # Calculate reserves contribution (remaining portfolio)
        remaining_for_others = remaining_capital - sum([self._get_optimal_check_size(c, {'fund_size': fund_size}) for c in companies[:2]])
        # Assume 2.5x blended return on reserves
        scenarios['reserves_contribution'] = remaining_for_others * 2.5
        
        return scenarios

    def _get_optimal_check_size(self, company_data: Dict[str, Any], fund_context: Dict[str, Any] = None) -> float:
        """Get optimal check size using IntelligentGapFiller service with intelligent fallback
        
        This method uses the IntelligentGapFiller.score_fund_fit() service to calculate
        optimal check size based on fund context, company stage, and market conditions.
        Falls back to position sizing only if service calculation is unavailable.
        """
        # First try to get the calculated optimal check size from service
        optimal_check = company_data.get('optimal_check_size', 0)
        if optimal_check > 0:
            return optimal_check
        
        # Try to use IntelligentGapFiller service to calculate optimal check size
        if not fund_context:
            fund_context = self.shared_data.get('fund_context', {})
        
        try:
            # Use IntelligentGapFiller.score_fund_fit() to get optimal check size
            # This service considers fund size, deployment pace, portfolio construction, etc.
            fund_fit_result = self.gap_filler.score_fund_fit(
                company_data=company_data,
                inferred_data={},  # Will be calculated internally if needed
                context=fund_context
            )
            
            # Extract optimal check from service result
            service_check = fund_fit_result.get('selected_check', 0)
            if service_check > 0:
                logger.info(f"[CHECK_SIZE] Using IntelligentGapFiller service result: ${service_check/1e6:.1f}M")
                return service_check
                
        except Exception as e:
            logger.warning(f"[CHECK_SIZE] IntelligentGapFiller service failed: {e}, using fallback calculation")
        
        # Fallback: Use position sizing based on fund parameters (only if service unavailable)
        fund_size = fund_context.get('fund_size', DEFAULT_FUND_SIZE)
        is_lead = fund_context.get('is_lead', False) or fund_context.get('lead_investor', False)
        
        # Position sizing based on fund economics
        if is_lead:
            max_check_percentage = 0.05  # 5% max when leading
        else:
            max_check_percentage = 0.03  # 3% max when following
        
        # Get stage for more refined sizing
        stage = company_data.get('stage', 'Series A')
        
        # Stage-based adjustments within the max
        stage_multipliers = {
            'Seed': 0.3,       # 30% of max (smaller checks for seed)
            'Series A': 0.5,   # 50% of max  
            'Series B': 0.7,   # 70% of max
            'Series C': 0.9,   # 90% of max
            'Series C+': 1.0,  # 100% of max
            'Series D': 1.0,   # 100% of max
            'Late Stage': 1.0  # 100% of max
        }
        
        stage_multiplier = stage_multipliers.get(stage, 0.5)
        
        # Calculate the check size
        calculated_check = fund_size * max_check_percentage * stage_multiplier
        
        # Log the calculation for debugging
        logger.info(f"[CHECK_SIZE] Calculated fallback for {company_data.get('company', 'Unknown')}: "
                   f"${calculated_check/1e6:.1f}M (fund=${fund_size/1e6:.0f}M, "
                   f"stage={stage}, lead={is_lead})")
        
        return calculated_check

    def _calculate_exit_ownership(self, company_data: Dict[str, Any], entry_ownership: float, 
                                  with_followon: bool = False, fund_context: Dict[str, Any] = None) -> float:
        """Calculate exit ownership using sophisticated cap table evolution modeling
        
        Instead of simple multiplication (0.7x), this uses:
        - Actual round-by-round dilution based on stage
        - ESOP expansion per round
        - Geography and investor quality adjustments
        - Different scenarios (IPO vs M&A vs distressed)
        """
        # First check if we already have calculated exit ownership
        if with_followon and company_data.get('exit_ownership_with_followon'):
            return company_data['exit_ownership_with_followon']
        elif not with_followon and company_data.get('exit_ownership_no_followon'):
            return company_data['exit_ownership_no_followon']
        
        # Get current stage to determine remaining rounds
        current_stage = company_data.get('stage', 'Series A')
        
        # Define typical funding paths from each stage
        funding_paths = {
            'Seed': ['Series A', 'Series B', 'Series C'],
            'Series A': ['Series B', 'Series C'],
            'Series B': ['Series C', 'Series D'],
            'Series C': ['Series D', 'Series E'],
            'Series D': ['Series E'],
            'Series E': [],
            'Late Stage': []
        }
        
        remaining_rounds = funding_paths.get(current_stage, ['Series B', 'Series C'])
        
        # Dilution rates by round (from intelligent_gap_filler and valuation_engine)
        round_dilution = {
            'Series A': 0.20,
            'Series B': 0.15,
            'Series C': 0.12,
            'Series D': 0.10,
            'Series E': 0.08
        }
        
        # ESOP expansion per round
        esop_expansion = {
            'Series A': 0.05,
            'Series B': 0.03,
            'Series C': 0.02,
            'Series D': 0.02,
            'Series E': 0.01
        }
        
        # Start with entry ownership
        exit_ownership = entry_ownership
        
        # Apply quality adjustments
        # Better investors = less dilution
        investors = company_data.get('investors', [])
        has_tier1 = any('Sequoia' in inv or 'a16z' in inv or 'Benchmark' in inv 
                       or 'Accel' in inv or 'Founders Fund' in inv for inv in investors)
        quality_mult = 0.85 if has_tier1 else 1.0
        
        # Geography adjustment (SF/NYC = less dilution)
        geo = company_data.get('geography', '')
        geo_mult = 0.95 if geo in ['SF', 'San Francisco', 'NYC', 'New York'] else 1.0
        
        # Apply dilution for each remaining round
        for round_name in remaining_rounds:
            base_dilution = round_dilution.get(round_name, 0.15)
            esop = esop_expansion.get(round_name, 0.03)
            
            # Adjust dilution based on quality factors
            actual_dilution = base_dilution * quality_mult * geo_mult
            
            # If we're doing follow-on, we maintain ownership better
            if with_followon:
                # Assume we do 80% of our pro-rata in each round
                pro_rata_protection = 0.8
                effective_dilution = actual_dilution * (1 - pro_rata_protection)
            else:
                effective_dilution = actual_dilution
            
            # Apply dilution and ESOP expansion
            total_dilution = effective_dilution + esop
            exit_ownership *= (1 - total_dilution)
        
        # Log the calculation for transparency
        logger.info(f"[EXIT_OWNERSHIP] {company_data.get('company', 'Unknown')}: "
                   f"Entry={entry_ownership:.1%} → Exit={exit_ownership:.1%} "
                   f"(followon={with_followon}, rounds={len(remaining_rounds)}, quality={quality_mult})")
        
        return exit_ownership

    def _generate_gross_margin_reasoning(self, company: Dict[str, Any]) -> str:
        """Generate explanation for gross margin based on business model and GPU usage"""
        gross_margin = safe_get_value(company.get('gross_margin', company.get('inferred_gross_margin', 0.75)))
        business_model = str(company.get('business_model', '')).lower()
        
        # Check for GPU/AI costs
        gpu_metrics = company.get('gpu_metrics', {})
        gpu_cost_ratio = gpu_metrics.get('gpu_cost_ratio', 0)
        
        if gpu_cost_ratio > 0.3:
            return f"AI-heavy → {int(gpu_cost_ratio*100)}% GPU costs"
        elif gross_margin >= 0.85:
            return "pure software → minimal COGS"
        elif gross_margin >= 0.75:
            if 'marketplace' in business_model:
                return "marketplace → payment processing costs"
            else:
                return "SaaS → hosting & support costs"
        elif gross_margin >= 0.65:
            if 'services' in business_model or 'consulting' in business_model:
                return "services component → labor costs"
            else:
                return "mixed model → ops & delivery costs"
        elif gross_margin >= 0.50:
            return "hardware/physical → materials & logistics"
        else:
            return "low margin → high operational costs"
    
    def _calculate_acv(self, company: Dict[str, Any]) -> float:
        """Calculate ACV from revenue and customer count with fallbacks"""
        try:
            # Try to get revenue
            revenue = safe_get_value(company.get('revenue', company.get('arr', 0)))
            
            # Try to get customer count from various sources
            customer_count = None
            
            # First try from intelligent gap filler results
            if 'customer_count' in company:
                customer_count = safe_get_value(company['customer_count'])
            
            # Try from customers data structure
            customers_data = company.get('customers', {})
            if isinstance(customers_data, dict):
                customer_count = safe_get_value(customers_data.get('customer_count', 0))
                if customer_count == 0:
                    # Fallback to customer names length
                    customer_names = customers_data.get('customer_names', [])
                    if isinstance(customer_names, list) and len(customer_names) > 0:
                        customer_count = len(customer_names)
            
            # If we have both revenue and customer count, calculate ACV
            if revenue > 0 and customer_count and customer_count > 0:
                acv = revenue / customer_count
                return acv
            
            # Fallback: estimate ACV based on stage and business model
            stage = (company.get('stage') or 'seed').lower()
            business_model = str(company.get('business_model', '')).lower()
            
            # Stage-based ACV estimates
            stage_acvs = {
                'series-d': 200_000,
                'series-c': 150_000,
                'series-b': 100_000,
                'series-a': 50_000,
                'seed': 25_000,
                'pre-seed': 15_000
            }
            
            base_acv = stage_acvs.get(stage, 25_000)
            
            # Adjust based on business model
            if 'enterprise' in business_model or 'b2b' in business_model:
                base_acv *= 2
            elif 'saas' in business_model:
                base_acv *= 1.5
            elif 'marketplace' in business_model:
                base_acv *= 0.5
            
            return base_acv
            
        except Exception as e:
            logger.warning(f"Error calculating ACV: {e}")
            return 25_000  # Default fallback
    
    def _generate_acv_reasoning(self, company: Dict[str, Any]) -> str:
        """Generate explanation for ACV based on target market and business model"""
        acv = safe_get_value(company.get('acv', 0))
        customer_segment = company.get('customer_segment', '').lower()
        business_model = str(company.get('business_model', '')).lower()
        
        if acv > 0:
            if acv >= 500000:
                return "enterprise focus → high-touch sales"
            elif acv >= 100000:
                return "mid-market enterprise → field sales"
            elif acv >= 30000:
                return "SMB enterprise → inside sales"
            elif acv >= 5000:
                return "small business → self-serve + sales assist"
            else:
                return "consumer/prosumer → pure self-serve"
        else:
            # Infer from customer segment
            if 'enterprise' in customer_segment or 'enterprise' in business_model:
                return "enterprise model (inferred)"
            elif 'mid-market' in customer_segment:
                return "mid-market focus (inferred)"
            elif 'smb' in customer_segment or 'small' in customer_segment:
                return "SMB focus (inferred)"
            else:
                return "segment unclear"
    
    def _format_citation_entry(self, citation: Dict[str, Any], citation_number: int = 0) -> Dict[str, Any]:
        """Create a structured citation with clickable URL and number"""
        source = citation.get('source') or citation.get('metadata', {}).get('source') or 'Unknown source'
        metadata = citation.get('metadata') or {}
        title = metadata.get('title') or citation.get('content', '').strip()[:100]
        date = citation.get('date')
        url = citation.get('url') or citation.get('metadata', {}).get('url', '')
        snippet = citation.get('content', '')[:200] if citation.get('content') else ''
        citation_id = citation.get('id', citation_number)
        
        return {
            "number": f"[{citation_id + 1}]",
            "title": title or source,
            "source": source,
            "date": date,
            "url": url,
            "snippet": snippet,
            "clickable": bool(url)
        }

    def _parse_numeric(self, value: Any, default: float = 0.0) -> float:
        """Robust numeric parser for amounts/valuations"""
        if value is None:
            return default
        if isinstance(value, (int, float)):
            return float(value)
        if isinstance(value, Decimal):
            return float(value)
        if isinstance(value, str):
            cleaned = value.replace(',', '').replace('$', '').strip()
            if cleaned in ('', 'n/a', 'na', '-'):  # Common placeholders
                return default
            try:
                return float(cleaned)
            except ValueError:
                return default
        return default

    def _canonical_round_name(self, label: Optional[str]) -> Optional[str]:
        """Map arbitrary round labels to canonical stage names"""
        if not label:
            return None
        label_norm = str(label).lower()
        for canonical, keywords in self._ROUND_KEYWORDS:
            if any(keyword in label_norm for keyword in keywords):
                return canonical
        return None

    def _round_order_index(self, canonical_name: Optional[str]) -> int:
        if canonical_name and canonical_name in self._STAGE_ORDER:
            return self._STAGE_ORDER.index(canonical_name)
        # Unknown rounds go to the end but stay stable
        return len(self._STAGE_ORDER)

    def _infer_round_for_stage(self, company: Dict[str, Any], stage_name: str) -> Optional[Dict[str, Any]]:
        """Infer a funding round for a missing stage using benchmarks"""
        benchmark = self.gap_filler.STAGE_BENCHMARKS.get(stage_name)
        if not benchmark:
            return None

        burn = benchmark.get('burn_monthly', 100_000)
        runway = benchmark.get('runway_months', 18)
        amount = burn * runway

        valuation = benchmark.get('valuation_median')
        if not valuation:
            arr = benchmark.get('arr_median') or 1_000_000
            multiple = benchmark.get('valuation_multiple', 10)
            valuation = arr * multiple

        # Ensure valuation is above amount so founders retain ownership
        if valuation <= amount:
            valuation = amount * 1.5

        pre_money = max(valuation - amount, valuation * 0.7)

        geography = (company.get('geography') or "US").strip()
        geo_adjust = self.gap_filler.GEOGRAPHY_ADJUSTMENTS.get(geography, {}).get('valuation', 1.0)
        if geo_adjust and geo_adjust > 0:
            valuation *= geo_adjust
            pre_money *= geo_adjust

        # Generate realistic synthetic date - estimate based on stage
        from datetime import datetime, timedelta
        # Map stage to typical months ago
        stage_age_map = {
            'Seed': 24,  # 2 years ago
            'Series A': 18,  # 1.5 years ago
            'Series B': 12,  # 1 year ago
            'Series C': 6,   # 6 months ago
            'Series D+': 3   # 3 months ago
        }
        months_ago = stage_age_map.get(stage_name, 12)
        synthetic_date = (datetime.now() - timedelta(days=months_ago * 30)).strftime("%Y-%m-%d")

        return {
            'round': stage_name,
            'amount': float(amount),
            'pre_money_valuation': float(pre_money),
            'valuation': float(pre_money + amount),
            'investors': [],
            'date': synthetic_date,
            'inferred': True
        }

    def _build_funding_rounds_with_inference(self, company: Dict[str, Any]) -> Tuple[List[Dict[str, Any]], int]:
        """Return cleaned funding rounds augmented with inferred predecessors"""
        inferred_count = 0

        raw_rounds = company.get('funding_rounds')
        if isinstance(raw_rounds, dict):
            raw_rounds = [raw_rounds]
        elif not isinstance(raw_rounds, list):
            raw_rounds = []

        cleaned_rounds: List[Dict[str, Any]] = []
        for round_entry in raw_rounds:
            if not isinstance(round_entry, dict):
                continue

            label = round_entry.get('round') or round_entry.get('round_name') or ''
            label_str = str(label).strip()
            amount = 0.0
            for amount_candidate in (
                round_entry.get('amount'),
                round_entry.get('round_size'),
                round_entry.get('size'),
            ):
                parsed_amount = self._parse_numeric(amount_candidate)
                if parsed_amount:
                    amount = parsed_amount
                    break
            pre_money = self._parse_numeric(round_entry.get('pre_money_valuation'))
            post_money = self._parse_numeric(round_entry.get('valuation') or round_entry.get('post_money_valuation'))

            # ROOT CAUSE FIX: Don't filter out rounds that have valuation but no amount - calculate amount from valuation
            # This must happen BEFORE adding to cleaned_rounds so the round has amount > 0
            if post_money and not amount:
                # If we only have post-money, assume 20% dilution to estimate round size
                amount = post_money * 0.2
            elif pre_money and not amount:
                # If we only have pre-money, calculate amount (20% dilution)
                amount = pre_money * 0.25  # pre_money * 0.25 = amount (since pre_money = amount * 4)
            if amount and not pre_money and post_money:
                pre_money = post_money - amount
            if not pre_money and amount:
                # Fallback to typical 20% dilution
                pre_money = amount * 4

            post_money_value = post_money if post_money else (pre_money + amount if pre_money and amount else 0)

            # Only add round if it has financial data (amount or valuation)
            if amount > 0 or pre_money > 0 or post_money_value > 0:
                cleaned_rounds.append({
                    'round': label_str,
                    'amount': float(amount) if amount else 0.0,
                    'pre_money_valuation': float(pre_money) if pre_money else 0.0,
                    'valuation': float(post_money_value) if post_money_value else 0.0,
                    'investors': round_entry.get('investors') or [],
                    'date': round_entry.get('date'),
                    'inferred': bool(round_entry.get('inferred', False))
                })

        # Determine target stage index from company stage or existing rounds
        stage_name = self._canonical_round_name(company.get('stage'))
        stage_index = self._round_order_index(stage_name) if stage_name else None

        existing_indices = []
        canonical_labels = set()
        for item in cleaned_rounds:
            canonical = self._canonical_round_name(item.get('round'))
            if canonical:
                canonical_labels.add(canonical)
                existing_indices.append(self._round_order_index(canonical))

        if existing_indices:
            existing_max = max(existing_indices)
            stage_index = max(stage_index or existing_max, existing_max)

        if stage_index is None:
            stage_index = 1  # Default to Seed if nothing else is known

        # Infer missing predecessors up to target stage
        for idx in range(0, min(stage_index + 1, len(self._STAGE_ORDER))):
            canonical = self._STAGE_ORDER[idx]
            if canonical in canonical_labels:
                continue
            inferred_round = self._infer_round_for_stage(company, canonical)
            if inferred_round:
                cleaned_rounds.append(inferred_round)
                canonical_labels.add(canonical)
                inferred_count += 1

        # Sort rounds by canonical order, preserving originals for unknown types
        cleaned_rounds.sort(
            key=lambda r: (
                self._round_order_index(self._canonical_round_name(r.get('round'))),
                str(r.get('date') or '')
            )
        )

        # Filter out rounds with no financial data even after inference
        # Note: amount should already be calculated from valuation above, so this is just a safety check
        cleaned_rounds = [r for r in cleaned_rounds if r.get('amount', 0) > 0 or r.get('pre_money_valuation', 0) > 0 or r.get('valuation', 0) > 0]

        return cleaned_rounds, inferred_count

    def _build_cap_table_from_rounds(self, company: Dict[str, Any], rounds: List[Dict[str, Any]]) -> Dict[str, float]:
        """
        Build current_cap_table from rounds data when service fails.
        Extract founder names from company data and investor names from funding_rounds.
        Calculate ownership percentages from round amounts/valuations.
        """
        current_cap_table = {}
        
        # Extract founder names from company data
        founders = company.get('founders', [])
        if isinstance(founders, list):
            founder_names = [f if isinstance(f, str) else f.get('name', 'Founder') for f in founders if f]
        elif isinstance(founders, str):
            founder_names = [founders]
        else:
            founder_names = []
        
        # If no founders found, use default
        if not founder_names:
            founder_names = ['Founders']
        
        # Extract investor names from funding_rounds
        investor_names = set()
        total_invested = 0.0
        last_valuation = 0.0
        
        for round_data in rounds:
            if not isinstance(round_data, dict):
                continue
            
            # Get investors from this round
            investors = round_data.get('investors', [])
            if isinstance(investors, list):
                for inv in investors:
                    if isinstance(inv, str) and inv:
                        investor_names.add(inv)
                    elif isinstance(inv, dict) and inv.get('name'):
                        investor_names.add(inv['name'])
            
            # Track total invested and last valuation
            amount = self._parse_numeric(round_data.get('amount', 0))
            if amount and amount > 0:
                total_invested += amount
            
            valuation = self._parse_numeric(round_data.get('valuation') or round_data.get('post_money_valuation', 0))
            if valuation and valuation > 0:
                last_valuation = max(last_valuation, valuation)
        
        # Calculate ownership percentages
        # If we have valuation and total invested, calculate investor ownership
        if last_valuation > 0 and total_invested > 0:
            investor_pct = min((total_invested / last_valuation) * 100, 80.0)  # Cap at 80%
        elif total_invested > 0:
            # Estimate: assume 20% dilution per round
            num_rounds = len([r for r in rounds if self._parse_numeric(r.get('amount', 0)) > 0])
            investor_pct = min(num_rounds * 20.0, 80.0)
        else:
            investor_pct = 0.0
        
        # Distribute ownership
        remaining_pct = 100.0 - investor_pct
        
        # Founders get majority of remaining
        if len(founder_names) > 0:
            founder_pct = remaining_pct * 0.85  # 85% of remaining
            employee_pct = remaining_pct * 0.15  # 15% of remaining (ESOP)
            
            # Distribute founder ownership equally
            pct_per_founder = founder_pct / len(founder_names)
            for founder in founder_names:
                current_cap_table[founder] = round(pct_per_founder, 2)
        else:
            current_cap_table['Founders'] = round(remaining_pct * 0.85, 2)
            employee_pct = remaining_pct * 0.15
        
        # Add employee/ESOP
        if employee_pct > 0:
            current_cap_table['Employees'] = round(employee_pct, 2)
        
        # Add investors - distribute equally if multiple
        if investor_names and investor_pct > 0:
            if len(investor_names) == 1:
                current_cap_table[list(investor_names)[0]] = round(investor_pct, 2)
            else:
                # Group by round or distribute equally
                pct_per_investor = investor_pct / len(investor_names)
                for investor in investor_names:
                    current_cap_table[investor] = round(pct_per_investor, 2)
        elif investor_pct > 0:
            current_cap_table['Investors'] = round(investor_pct, 2)
        
        # Normalize to ensure total is 100%
        total = sum(current_cap_table.values())
        if total > 0 and abs(total - 100.0) > 0.01:
            factor = 100.0 / total
            current_cap_table = {k: round(v * factor, 2) for k, v in current_cap_table.items()}
        
        logger.info(f"[CAP_TABLE] Built cap table from rounds: {len(current_cap_table)} stakeholders, total={sum(current_cap_table.values()):.1f}%")
        return current_cap_table

    def _generate_breakpoint_insights(self, exit_scenarios_data: Dict, reality_check_table: List) -> List[str]:
        """Generate actual insights from calculated breakpoint data"""
        insights = []
        
        # Extract real breakpoint values from the data
        for company_name, scenarios in exit_scenarios_data.items():
            if scenarios.get('data_available'):
                breakpoints = scenarios.get('breakpoints', {})
                if breakpoints:
                    # Get actual calculated breakpoints
                    liq_pref = breakpoints.get('liquidation_preference', 0)
                    conversion = breakpoints.get('conversion_point', 0)
                    breakeven = breakpoints.get('breakeven_exit', 0)
                    
                    if liq_pref > 0:
                        insights.append(f"{company_name}: Liquidation preference of ${liq_pref/1e6:.1f}M must be cleared first")
                    if conversion > 0:
                        insights.append(f"{company_name}: Converts to common at ${conversion/1e6:.0f}M exit value")
                    if breakeven > 0:
                        insights.append(f"{company_name}: Need ${breakeven/1e6:.0f}M exit to break even on investment")
        
        # Add insights from reality check table
        if reality_check_table:
            for entry in reality_check_table[:2]:  # First 2 companies
                exit_val = entry.get('exit_value', '$0M')
                moic = entry.get('moic', '0x')
                if 'M' in str(exit_val) and 'x' in str(moic):
                    insights.append(f"{entry.get('company')}: {exit_val} exit returns {moic}")
        
        return insights[:4] if insights else ["Breakpoint analysis pending"]
    
    
    def _get_field_with_fallback(self, data: Dict, field: str, default: Any = None) -> Any:
        """
        Get field value with proper fallback hierarchy:
        1. Extracted value (field)
        2. Inferred value (inferred_field)
        3. Stage-based default value
        
        CRITICAL: This ensures we ALWAYS have a value, never None
        """
        # First check for extracted value
        extracted = data.get(field)
        if field == 'revenue':
            # Special handling for revenue - also check ARR
            extracted = extracted or data.get('arr')
        
        # If we have a valid extracted value, use it
        if extracted is not None and extracted != "" and extracted != 0:
            return safe_get_value(extracted, default)
        
        # Fall back to inferred value
        inferred = data.get(f'inferred_{field}')
        if inferred is not None and inferred != 0:
            return safe_get_value(inferred, default)
        
        # Last resort: use stage-based default
        stage = data.get('stage', 'Seed')
        stage_default = self._get_stage_default(field, stage)
        return stage_default if stage_default is not None else default
    
    async def _add_tam_pincer_slide(
        self,
        companies: List[Dict[str, Any]],
        add_slide
    ) -> None:
        """Former TAM slide generator, now disabled."""
        logger.info("[DECK_TAM] TAM slide generation disabled; skipping slide creation.")
        return

    def _build_growth_inference_payload(
        self, company: Dict[str, Any]
    ) -> Tuple[Dict[str, Any], Dict[str, Dict[str, Any]]]:
        """
        Normalize core fields required for growth inference and track their provenance.
        Returns:
            payload: dict ready for IntelligentGapFiller.calculate_required_growth_rates
            sources: map describing whether each field was provided, inferred, derived, or defaulted
        """
        payload = dict(company or {})
        sources: Dict[str, Dict[str, Any]] = {}
        company_name = company.get("company", "Unknown") if company else "Unknown"
        
        def _record(field: str, value: Any, source: str) -> Any:
            sources[field] = {"value": value, "source": source}
            return value
        
        # Stage
        stage = (company or {}).get("stage") or (company or {}).get("inferred_stage")
        stage_source = "provided" if stage else "derived"
        if not stage or stage in ("", "Unknown"):
            stage = self._determine_accurate_stage(company or {})
            stage_source = "derived" if stage and stage != "Unknown" else "default"
        if not stage or stage in ("", "Unknown"):
            stage = "Series A"
        _record("stage", stage, stage_source)
        payload["stage"] = stage
        
        # Category / sector
        if company:
            if company.get("category"):
                category = company["category"]
                category_source = "provided"
            elif company.get("vertical"):
                category = company["vertical"]
                category_source = "derived"
            elif company.get("business_model"):
                category = company["business_model"]
                category_source = "derived"
            else:
                category = "SaaS"
                category_source = "default"
        else:
            category = "SaaS"
            category_source = "default"
        _record("category", category, category_source)
        payload["category"] = category
        
        # Revenue / ARR
        revenue = (company or {}).get("revenue") or (company or {}).get("arr")
        revenue_source = "provided"
        if not revenue or revenue == 0:
            inferred_revenue = (company or {}).get("inferred_revenue")
            if inferred_revenue and inferred_revenue != 0:
                revenue = inferred_revenue
                revenue_source = "inferred"
            else:
                revenue = self._get_stage_default("revenue", stage) or 1_000_000
                revenue_source = "default"
        revenue = safe_get_value(revenue, 1_000_000) or 1_000_000
        _record("revenue", revenue, revenue_source)
        payload["revenue"] = revenue
        payload["arr"] = revenue
        payload["inferred_revenue"] = (company or {}).get("inferred_revenue") or revenue
        
        # Valuation
        valuation = (company or {}).get("valuation") or (company or {}).get("inferred_valuation")
        valuation_source = "provided" if (company or {}).get("valuation") else "inferred"
        if not valuation or valuation == 0:
            valuation = self._get_stage_default("valuation", stage) or 100_000_000
            valuation_source = "default"
        valuation = safe_get_value(valuation, 100_000_000) or 100_000_000
        _record("valuation", valuation, valuation_source)
        payload["valuation"] = valuation
        payload["inferred_valuation"] = valuation
        
        # Net retention
        nrr = (company or {}).get("nrr") or (company or {}).get("net_retention")
        nrr_source = "provided"
        if not nrr or nrr <= 0:
            nrr = 1.10
            nrr_source = "default"
        _record("net_retention", nrr, nrr_source)
        payload["nrr"] = nrr
        payload["net_retention"] = nrr
        
        # Profit margin
        profit_margin = (company or {}).get("profit_margin")
        pm_source = "provided"
        if profit_margin is None:
            profit_margin = -0.20
            pm_source = "default"
        _record("profit_margin", profit_margin, pm_source)
        payload["profit_margin"] = profit_margin
        
        # Last year revenue
        last_year_revenue = (company or {}).get("last_year_revenue")
        lyr_source = "provided"
        if not last_year_revenue or last_year_revenue <= 0:
            last_year_revenue = max(revenue / 1.5, 1)
            lyr_source = "derived"
        _record("last_year_revenue", last_year_revenue, lyr_source)
        payload["last_year_revenue"] = last_year_revenue
        
        # Total funding for context
        total_funding = (
            (company or {}).get("total_funding")
            or (company or {}).get("total_raised")
            or (company or {}).get("inferred_total_funding")
        )
        tf_source = "provided" if total_funding else "default"
        if not total_funding:
            total_funding = self._get_stage_default("total_funding", stage) or 10_000_000
        _record("total_funding", total_funding, tf_source)
        payload["total_funding"] = total_funding
        
        # Attach company name for telemetry
        payload["company"] = company_name
        
        return payload, sources
    
    def _ensure_growth_metrics(self, company: Dict[str, Any]) -> Dict[str, Any]:
        """
        Guarantee that a company dict carries growth metrics required by deck generation.
        Calculates using IntelligentGapFiller when missing and records telemetry.
        """
        if not company or not isinstance(company, dict):
            return {}
        
        payload, sources = self._build_growth_inference_payload(company)
        company_name = company.get("company", "Unknown")
        status = {
            "inputs": sources,
            "used_defaults": [field for field, meta in sources.items() if meta.get("source") == "default"],
            "success": False,
            "error": None
        }
        
        try:
            growth_metrics = self.gap_filler.calculate_required_growth_rates(payload)
        except Exception as exc:
            status["error"] = str(exc)
            company["growth_inference_status"] = status
            company["growth_inferred"] = False
            logger.warning(f"[GROWTH_INFERENCE] {company_name}: failed to calculate growth metrics - {exc}")
            return {}
        
        if not growth_metrics:
            status["error"] = "empty_growth_metrics"
            company["growth_inference_status"] = status
            company["growth_inferred"] = False
            logger.warning(f"[GROWTH_INFERENCE] {company_name}: gap filler returned empty growth metrics")
            return {}
        
        company["growth_metrics"] = growth_metrics
        projected = growth_metrics.get("projected_growth_rate")
        if projected:
            if not company.get("projected_growth_rate"):
                company["projected_growth_rate"] = projected
            if not company.get("growth_rate"):
                company["growth_rate"] = projected
            company.setdefault("inferred_growth_rate", projected)
        
        backward_actual = growth_metrics.get("backward_looking", {}).get("actual_growth_rate")
        if backward_actual is not None:
            company.setdefault("revenue_growth", backward_actual)
            normalized = backward_actual + 1 if -1 < backward_actual < 1 else backward_actual
            if not company.get("growth_rate"):
                company["growth_rate"] = normalized
        
        status["success"] = True
        status["projected_growth_rate"] = company.get("projected_growth_rate")
        company["growth_inference_status"] = status
        company["growth_inferred"] = True
        
        logger.info(
            f"[GROWTH_INFERENCE] {company_name}: success "
            f"(projected={status['projected_growth_rate']}, defaults={status['used_defaults']})"
        )
        
        return growth_metrics
    
    def _get_stage_default(self, field: str, stage: str) -> Any:
        """Get reasonable defaults based on stage to avoid None values"""
        # Normalize stage name
        stage = (stage or 'Seed').replace('Series ', '').strip()
        
        defaults = {
            'revenue': {
                'Pre-Seed': 100_000, 'Seed': 1_000_000, 'A': 5_000_000,
                'B': 20_000_000, 'C': 50_000_000, 'D': 100_000_000
            },
            'valuation': {
                'Pre-Seed': 5_000_000, 'Seed': 20_000_000, 'A': 80_000_000,
                'B': 250_000_000, 'C': 600_000_000, 'D': 1_500_000_000
            },
            'team_size': {
                'Pre-Seed': 5, 'Seed': 15, 'A': 40,
                'B': 120, 'C': 300, 'D': 600
            },
            'total_funding': {
                'Pre-Seed': 500_000, 'Seed': 3_000_000, 'A': 15_000_000,
                'B': 65_000_000, 'C': 165_000_000, 'D': 365_000_000
            },
            'gross_margin': 0.70,
            'growth_rate': 1.5,
            'burn_rate': 500_000,
            'runway_months': 18,
            'customer_count': 100,
            'ltv_cac_ratio': 3.0,
            'net_retention': 1.15
        }
        
        if field in defaults:
            if isinstance(defaults[field], dict):
                return defaults[field].get(stage, defaults[field].get('Seed', 1_000_000))
            return defaults[field]
        
        # Final fallback
        return 1_000_000 if field == 'revenue' else 0
    
    def _calculate_burn_multiple(self, company: Dict) -> float:
        """Calculate burn multiple (net burn / net new ARR)
        
        Lower is better - indicates efficient growth
        < 1.0 = Efficient (adding ARR faster than burning cash)
        1.0-2.0 = Normal 
        > 2.0 = Inefficient
        """
        try:
            # Get burn rate (monthly burn * 12 for annual)
            burn_rate = safe_get_value(company.get('burn_rate', 0), 0)
            if burn_rate == 0:
                # Try to estimate from funding and time
                funding = safe_get_value(company.get('total_funding', 0), 0)
                months_since = safe_get_value(company.get('months_since_funding', 12), 12)
                if funding > 0 and months_since > 0:
                    burn_rate = (funding / months_since) * 12
            
            # Get net new ARR (year over year growth in ARR)
            current_arr = self._get_revenue_safe(company)
            growth_rate = safe_get_value(company.get('revenue_growth', 0.5), 0.5)
            net_new_arr = current_arr * growth_rate
            
            if net_new_arr > 0 and burn_rate > 0:
                return round(burn_rate / net_new_arr, 2)
            return 0
        except:
            return 0
    
    def _calculate_rule_of_40(self, company: Dict) -> float:
        """Calculate Rule of 40 (growth rate % + profit margin %)
        
        > 40% = Excellent (balancing growth and profitability)
        30-40% = Good
        < 30% = Needs improvement
        """
        try:
            # Get growth rate as percentage
            growth_rate = safe_get_value(company.get('revenue_growth', 0.3), 0.3) * 100
            
            # Get profit margin (or use negative burn as proxy)
            profit_margin = safe_get_value(company.get('profit_margin', 0), 0)
            if profit_margin == 0:
                # Use gross margin - estimated burn as proxy
                gross_margin = safe_get_value(
                    company.get('gross_margin', company.get('inferred_gross_margin', 0.7)), 
                    0.7
                ) * 100
                # Estimate operating margin (gross margin - 50% for typical SaaS opex)
                profit_margin = gross_margin - 50
            else:
                profit_margin = profit_margin * 100
            
            return round(growth_rate + profit_margin, 1)
        except:
            return 0
    
    def _safe_divide(self, numerator: Any, denominator: Any, default: float = 0) -> float:
        """Safe division that never crashes"""
        try:
            if denominator is None or denominator == 0:
                return default
            if numerator is None:
                return default
            return float(numerator) / float(denominator)
        except (TypeError, ValueError, ZeroDivisionError):
            return default
    
    async def _generate_complete_metrics_analysis(self, company: Dict) -> Dict:
        """
        Single model call to analyze all unit economics with causal reasoning
        Explains WHY each metric is at its current level
        """
        try:
            # Prepare company data with safe defaults
            company_name = company.get('company', 'Unknown')
            stage = company.get('stage', 'Unknown')
            business_model = company.get('business_model', 'SaaS')
            
            # Get revenue with multiple fallbacks
            revenue = self._get_revenue_safe(company)
            
            # Calculate other metrics
            growth_rate = safe_get_value(company.get('revenue_growth', 0.5), 0.5) * 100
            burn_rate = safe_get_value(company.get('burn_rate', 0), 0)
            gross_margin = safe_get_value(
                company.get('gross_margin', company.get('inferred_gross_margin', 0.75)), 
                0.75
            ) * 100
            team_size = safe_get_value(company.get('team_size', 50), 50)
            total_funding = safe_get_value(company.get('total_funding', 0), 0)
            customer_count = safe_get_value(company.get('customer_count', 100), 100)
            target_market = company.get('target_market', 'B2B SaaS')
            product_description = company.get('product_description', 'Software platform')
            
            # Get dates
            founded_date = company.get('founded_date', 'Unknown')
            last_funding_date = company.get('last_funding_date', 'Unknown')
            
            prompt = f"""Analyze this company's unit economics and explain WHY each metric is at this level.
    
Company Data:
- Company: {company_name}
- Stage: {stage}
- Business Model: {business_model}
- Revenue: ${revenue:,.0f}
- Growth Rate: {growth_rate:.0f}%
- Burn Rate: ${burn_rate:,.0f}/month
- Gross Margin: {gross_margin:.0f}%
- Team Size: {team_size}
- Total Funding: ${total_funding:,.0f}
- Customer Count: {customer_count}
- Target Market: {target_market}
- Product: {product_description}
- Founded: {founded_date}
- Last Funding: {last_funding_date}

Calculate and explain these metrics:

1. BURN MULTIPLE = (Annual Burn / Net New ARR)
   - Calculate the value (use monthly burn * 12 / (revenue * growth rate))
   - Explain WHY it's at this level (hiring? R&D? Sales investment? Geographic expansion?)
   - What strategic choice does this reflect?
   - Is this appropriate for their stage and market?

2. RULE OF 40 = (YoY Growth % + Profit Margin %)
   - Calculate the value
   - Explain WHY they have this growth rate (market tailwind? Product-market fit? Competition?)
   - Explain WHY they have this margin (GPU costs? Services? Efficiency?)
   - What tradeoff are they making between growth and profitability?

3. LTV/CAC RATIO = (Customer Lifetime Value / Customer Acquisition Cost)
   - Estimate the value based on available data
   - Explain WHY CAC is high/low (sales model? Competition? Market maturity?)
   - Explain WHY LTV is high/low (retention? Expansion? Stickiness?)
   - What does this say about product-market fit?

4. ACV = (Annual Revenue / Customer Count)
   - Calculate the value
   - Explain WHY they can charge this much (value prop? Market position? Customer segment?)
   - How does this compare to alternatives?
   - What does this imply about go-to-market strategy?

5. GROSS MARGIN ANALYSIS
   - Explain WHY margins are at this level (cost structure? Delivery model? Pricing power?)
   - What are the main cost drivers?
   - How will this scale with growth?

6. OVERALL ASSESSMENT
   - What's the story these metrics tell together?
   - What's the primary constraint on growth?
   - What needs to change for them to reach $100M ARR efficiently?
   - Investment recommendation based on these unit economics

Return a JSON with this structure:
{{
  "burn_multiple": {{
    "value": <number>,
    "health": "good|warning|critical",
    "why": "Primary reason for this burn level",
    "evidence": "Specific data point supporting this",
    "implication": "What this means for the business"
  }},
  "rule_of_40": {{
    "value": <number>,
    "growth_component": <number>,
    "margin_component": <number>,
    "health": "excellent|good|poor",
    "why_growth": "Reason for growth rate",
    "why_margin": "Reason for margin level",
    "strategy": "What strategy this reflects"
  }},
  "ltv_cac": {{
    "value": <number>,
    "payback_months": <number>,
    "health": "strong|adequate|weak",
    "why_cac": "Main CAC driver",
    "why_ltv": "Main LTV driver",
    "pmf_signal": "What this says about product-market fit"
  }},
  "acv": {{
    "value": <number>,
    "segment": "enterprise|mid-market|smb|consumer",
    "why_pricing": "Reason they can charge this",
    "comparison": "vs market alternatives",
    "scalability": "Implications for growth"
  }},
  "gross_margin": {{
    "value": <number>,
    "health": "excellent|good|concerning",
    "why": "Primary margin driver",
    "cost_breakdown": "Main cost components",
    "trajectory": "How this changes with scale"
  }},
  "synthesis": {{
    "story": "2-3 sentence narrative connecting all metrics",
    "primary_issue": "Biggest challenge revealed by metrics",
    "key_strength": "Best metric and why it matters",
    "investment_view": "bullish|neutral|bearish",
    "required_fix": "What needs to improve for success"
  }}
}}"""

            # Call the model router for analysis
            response = await self.model_router.get_completion(
                prompt=prompt,
                json_mode=True,
                capability=ModelCapability.ANALYSIS
            )
            
            # Parse the response
            if isinstance(response, str):
                import json
                analysis = json.loads(response)
            else:
                analysis = response
            
            # Add calculated values as fallbacks if model didn't calculate
            if 'burn_multiple' in analysis and 'value' not in analysis['burn_multiple']:
                analysis['burn_multiple']['value'] = self._calculate_burn_multiple(company)
            
            if 'rule_of_40' in analysis and 'value' not in analysis['rule_of_40']:
                analysis['rule_of_40']['value'] = self._calculate_rule_of_40(company)
            
            if 'acv' in analysis and 'value' not in analysis['acv']:
                analysis['acv']['value'] = self._calculate_acv(company)
            
            return analysis
            
        except Exception as e:
            logger.error(f"Error generating complete metrics analysis: {e}")
            # Return fallback analysis
            return {
                "burn_multiple": {
                    "value": self._calculate_burn_multiple(company),
                    "health": "unknown",
                    "why": "Analysis unavailable",
                    "evidence": "Using calculated estimate",
                    "implication": "Further analysis needed"
                },
                "rule_of_40": {
                    "value": self._calculate_rule_of_40(company),
                    "growth_component": growth_rate,
                    "margin_component": -burn_rate * 12 / revenue * 100 if revenue > 0 else 0,
                    "health": "unknown",
                    "why_growth": "Analysis unavailable",
                    "why_margin": "Analysis unavailable",
                    "strategy": "Further analysis needed"
                },
                "ltv_cac": {
                    "value": safe_get_value(company.get('ltv_cac_ratio', 3.0), 3.0),
                    "payback_months": 18,
                    "health": "unknown",
                    "why_cac": "Analysis unavailable",
                    "why_ltv": "Analysis unavailable",
                    "pmf_signal": "Further analysis needed"
                },
                "acv": {
                    "value": self._calculate_acv(company),
                    "segment": "unknown",
                    "why_pricing": "Analysis unavailable",
                    "comparison": "Further analysis needed",
                    "scalability": "Further analysis needed"
                },
                "gross_margin": {
                    "value": gross_margin,
                    "health": "unknown",
                    "why": "Analysis unavailable",
                    "cost_breakdown": "Further analysis needed",
                    "trajectory": "Further analysis needed"
                },
                "synthesis": {
                    "story": "Metrics calculated but causal analysis unavailable",
                    "primary_issue": "Requires deeper analysis",
                    "key_strength": "Data available for calculation",
                    "investment_view": "neutral",
                    "required_fix": "Complete analysis needed"
                }
            }
    
    def _months_between_rounds(self, date1: str, date2: str) -> Optional[float]:
        """Calculate months between funding round dates"""
        from datetime import datetime
        try:
            if not date1 or not date2:
                logger.warning(f"Missing dates for comparison: date1={date1}, date2={date2}")
                return None
            
            # Parse ISO format dates and other common formats
            def parse_date(date_str):
                # Handle YYYY-MM format (e.g., "2024-04")
                if len(date_str) == 7 and '-' in date_str:
                    # Append first day of month for parsing
                    return datetime.strptime(date_str + '-01', '%Y-%m-%d')
                elif 'T' in date_str:
                    # ISO format with time
                    return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                elif len(date_str) >= 10 and '-' in date_str[:10]:
                    # Date only format YYYY-MM-DD
                    return datetime.strptime(date_str[:10], '%Y-%m-%d')
                else:
                    # Try other formats
                    for fmt in ['%Y-%m-%d', '%Y/%m/%d', '%d/%m/%Y', '%m/%d/%Y']:
                        try:
                            return datetime.strptime(date_str, fmt)
                        except:
                            continue
                    raise ValueError(f"Could not parse date: {date_str}")
            
            d1 = parse_date(date1)
            d2 = parse_date(date2)
            
            # Calculate months difference
            months = (d2.year - d1.year) * 12 + (d2.month - d1.month)
            # Add day difference as fraction
            days_diff = (d2.day - d1.day) / 30.0
            result = abs(months + days_diff)
            
            logger.info(f"Calculated {result:.1f} months between {date1[:10]} and {date2[:10]}")
            return result
            
        except Exception as e:
            logger.warning(f"Failed to parse dates {date1}, {date2}: {e}, using default 18 months")
            return 18.0  # Fallback to default assumption

    def _analyze_funding_pattern(self, company: Dict) -> Dict:
        """Analyze funding patterns to identify capital efficiency and growth trajectory"""
        try:
            funding_rounds = company.get('funding_rounds', [])
            total_funding = safe_get_value(company.get('total_funding', 0), 0)
            revenue = self._get_revenue_safe(company)
            
            # Calculate capital efficiency
            capital_efficiency = revenue / total_funding if total_funding > 0 else 0
            
            # Calculate time between rounds
            round_intervals = []
            if len(funding_rounds) >= 2:
                for i in range(1, len(funding_rounds)):
                    prev_date = funding_rounds[i-1].get('date')
                    curr_date = funding_rounds[i].get('date')
                    if prev_date and curr_date:
                        months = self._months_between_rounds(prev_date, curr_date)
                        if months is not None:
                            round_intervals.append(months)
            
            avg_interval = sum(round_intervals) / len(round_intervals) if round_intervals else 18
            
            # Calculate round size progression
            round_sizes = [r.get('amount', 0) for r in funding_rounds if r.get('amount')]
            size_multiples = []
            if len(round_sizes) >= 2:
                for i in range(1, len(round_sizes)):
                    if round_sizes[i-1] > 0:
                        size_multiples.append(round_sizes[i] / round_sizes[i-1])
            
            avg_size_multiple = sum(size_multiples) / len(size_multiples) if size_multiples else 2.5
            
            # Determine funding pattern and insight
            if capital_efficiency > 0.5 and avg_interval > 15:
                pattern = "Efficient Growth"
                health = "good"
                insight = f"Strong capital efficiency: ${capital_efficiency:.2f} revenue per $1 raised"
            elif capital_efficiency < 0.2 and avg_interval < 12:
                pattern = "Capital Hungry"
                health = "warning"
                insight = f"High burn rate: Only ${capital_efficiency:.2f} revenue per $1 raised"
            elif avg_size_multiple < 1.5 or avg_interval > 24:
                pattern = "Struggling to Scale"
                health = "critical"
                insight = "Slow funding progression indicates scaling challenges"
            else:
                pattern = "Standard Progression"
                health = "good"
                insight = f"Normal funding cadence with {avg_size_multiple:.1f}x round increases"
            
            return {
                "pattern": pattern,
                "health": health,
                "capital_efficiency": capital_efficiency,
                "avg_months_between": avg_interval,
                "avg_size_multiple": avg_size_multiple,
                "rounds_count": len(funding_rounds),
                "last_round": funding_rounds[-1].get('stage') if funding_rounds else 'Unknown',
                "months_since_last": safe_get_value(company.get('months_since_funding', 12), 12),
                "insight": insight
            }
        except Exception as e:
            logger.warning(f"Error analyzing funding pattern: {e}")
            return {
                "pattern": "Unknown",
                "health": "unknown",
                "capital_efficiency": 0,
                "avg_months_between": 18,
                "avg_size_multiple": 2.5,
                "rounds_count": 0,
                "last_round": "Unknown",
                "months_since_last": 12,
                "insight": "Funding analysis unavailable"
            }
    
    def _get_stage_enum(self, stage_str: str) -> Stage:
        """Safely convert stage string to Stage enum
        
        Handles all series including E, F, G+ by mapping to appropriate enums
        """
        if not stage_str:
            return Stage.SERIES_A
        
        # Use existing normalizer from gap_filler if available
        if hasattr(self, 'gap_filler') and hasattr(self.gap_filler, '_normalize_stage_key'):
            normalized = self.gap_filler._normalize_stage_key(stage_str)
        else:
            # Fallback normalization
            normalized = stage_str.strip()
        
        # Map normalized stage to valuation engine Stage enum
        stage_map = {
            "Pre-seed": Stage.PRE_SEED,
            "pre_seed": Stage.PRE_SEED,
            "Seed": Stage.SEED,
            "seed": Stage.SEED,
            "Series A": Stage.SERIES_A,
            "series_a": Stage.SERIES_A,
            "Series B": Stage.SERIES_B,
            "series_b": Stage.SERIES_B,
            "Series C": Stage.SERIES_C,
            "series_c": Stage.SERIES_C,
            "Series D": Stage.GROWTH,
            "Series D+": Stage.LATE,  # gap_filler maps E/F/G to D+
            "Growth": Stage.LATE
        }
        
        # Check direct mapping first
        if normalized in stage_map:
            return stage_map[normalized]
        
        # Handle late-stage series (D, E, F, G+)
        if "Series" in normalized or "series" in normalized.lower():
            import re
            match = re.search(r'[Ss]eries\s*([A-Z])', normalized, re.IGNORECASE)
            if match:
                letter = match.group(1).upper()
                if letter == 'D':
                    return Stage.GROWTH
                elif ord(letter) > ord('D'):  # E, F, G, H, etc.
                    return Stage.LATE
        
        # Handle other variations
        normalized_lower = normalized.lower()
        if "growth" in normalized_lower or "late" in normalized_lower:
            return Stage.LATE
        elif "pre" in normalized_lower and "seed" in normalized_lower:
            return Stage.PRE_SEED
        elif "seed" in normalized_lower:
            return Stage.SEED
        
        # Default fallback
        return Stage.SERIES_A
    
    def _serialize_memo_section(self, section: Dict[str, Any]) -> str:
        """Serialize a memo section (chart, table, list, heading, paragraph) into text.

        The frontend sends full memo sections with charts/tables/lists back as context,
        but previously only `content` was read — making charts and tables invisible to the agent.
        """
        section_type = section.get("type", "")
        content = section.get("content", "")

        if section_type == "chart":
            chart = section.get("chart", {})
            title = chart.get("title", "Chart")
            chart_type = chart.get("type", "unknown")
            data = chart.get("data", [])
            lines = [f"[Chart: {title} ({chart_type})]"]
            if isinstance(data, list):
                for item in data[:20]:
                    if isinstance(item, dict):
                        name = item.get("name") or item.get("label", "")
                        value = item.get("value", "")
                        lines.append(f"  {name}: {value}")
            elif isinstance(data, dict):
                labels = data.get("labels", [])
                datasets = data.get("datasets", [])
                if labels and datasets:
                    for ds in datasets[:5]:
                        ds_label = ds.get("label", "")
                        ds_data = ds.get("data", [])
                        pairs = [f"{l}={v}" for l, v in zip(labels, ds_data)]
                        lines.append(f"  {ds_label}: {', '.join(pairs[:10])}")
                # Sankey nodes/links
                nodes = data.get("nodes", [])
                links = data.get("links", [])
                if nodes:
                    lines.append(f"  Nodes: {', '.join(str(n.get('name', n)) for n in nodes[:15])}")
                if links:
                    for link in links[:10]:
                        lines.append(f"  {link.get('source','')} → {link.get('target','')}: {link.get('value','')}")
            return "\n".join(lines)

        if section_type == "table":
            table = section.get("table", {})
            headers = table.get("headers", [])
            rows = table.get("rows", [])
            caption = table.get("caption", "")
            lines = []
            if caption:
                lines.append(f"[Table: {caption}]")
            if headers:
                lines.append(" | ".join(str(h) for h in headers))
                lines.append("-" * (len(headers) * 12))
            for row in rows[:25]:
                lines.append(" | ".join(str(c) for c in row))
            return "\n".join(lines)

        if section_type == "list":
            items = section.get("items", [])
            return "\n".join(f"• {item}" for item in items[:20])

        # heading, paragraph, or anything with content
        return content

    def _serialize_memo_sections(self, sections: list, limit: int = 15) -> str:
        """Serialize a list of memo sections into a single text block."""
        parts = []
        for s in sections[:limit]:
            text = self._serialize_memo_section(s)
            if text:
                parts.append(text)
        return "\n".join(parts)

    def _get_field_safe(self, company: Dict[str, Any], field: str, default: Any = 0) -> Any:
        """Universal safe getter for company fields
        
        CORRECT Priority order:
        1. {field} (real extracted value - highest priority)
        2. inferred_{field} (gap filler inference - fallback)
        3. default (last resort)
        
        This ensures real data is never overwritten by inferred
        """
        # Check raw field FIRST (highest priority - real data)
        raw_value = company.get(field)
        if raw_value is not None:
            safe_value = safe_get_value(raw_value, default)
            if safe_value != default or raw_value == 0:  # 0 is a valid value
                return safe_value
        
        # Check inferred version SECOND (fallback if no real data)
        inferred_key = f"inferred_{field}"
        inferred_value = company.get(inferred_key)
        if inferred_value is not None:
            safe_value = safe_get_value(inferred_value, default)
            if safe_value != default or inferred_value == 0:  # 0 is a valid value
                return safe_value
        
        # Return default as last resort
        return default
    
    def _get_revenue_safe(self, company_data: Dict[str, Any]) -> float:
        """Get revenue - prefer inferred_revenue (has all adjustments) over raw revenue"""
        # Prefer inferred_revenue first (has time-based, geographic, and Tier 1 VC adjustments)
        inferred_revenue = self._get_field_safe(company_data, "inferred_revenue", default=0)
        if inferred_revenue and inferred_revenue > 0:
            return inferred_revenue
        # Fallback to raw revenue field
        return self._get_field_safe(company_data, "revenue", default=0)
    
    def _generate_date_labels(self, projection_years: int = 6, start_date=None) -> List[str]:
        """Generate real date labels for projections"""
        from datetime import datetime, timedelta
        from dateutil.relativedelta import relativedelta
        
        if start_date is None:
            start_date = datetime.now()
        elif isinstance(start_date, str):
            start_date = datetime.fromisoformat(start_date)
        
        labels = []
        for i in range(projection_years):
            future_date = start_date + relativedelta(years=i)
            # Format as "Oct 2025", "Oct 2026", etc.
            labels.append(future_date.strftime("%b %Y"))
        
        return labels
    
    def _safe_divide(self, numerator: Any, denominator: Any, default: float = 0) -> float:
        """Safe division that never crashes
        
        Handles None, 0, and converts to float safely
        """
        # Convert to safe values
        num = safe_get_value(numerator, 0)
        denom = safe_get_value(denominator, 0)
        
        # Protect against division by zero
        if denom == 0:
            return default
        
        try:
            return float(num) / float(denom)
        except (TypeError, ValueError, ZeroDivisionError):
            return default
    
    async def _execute_deck_generation(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Generate presentation deck with structured slides"""
        from datetime import datetime
        logger.critical(f"[DECK_GEN] 🔵🔵🔵 _execute_deck_generation CALLED with inputs: {inputs} 🔵🔵🔵")
        
        # CRITICAL FIX: Wrap entire function to ensure we ALWAYS return a result, never raise
        try:
            # CRITICAL FIX: Get companies from shared_data with lock protection
            # Also check results from company-data-fetcher skill directly
            async with self.shared_data_lock:
                companies = self.shared_data.get("companies", [])
                stored_fund_context = deepcopy(self.shared_data.get("fund_context", {}))
            
            # CRITICAL: Log fund_context availability
            logger.info(f"[DECK_GEN] Fund context from shared_data: {stored_fund_context}")
            logger.info(f"[DECK_GEN] Fund context keys: {list(stored_fund_context.keys()) if isinstance(stored_fund_context, dict) else 'not_dict'}")
            
            # Filter out None companies
            companies = [c for c in companies if c and isinstance(c, dict) and c.get('company')]
            
            # CRITICAL: Ensure all companies have inferred data (including team_size)
            companies = await self._ensure_companies_have_inferred_data(companies)
            
            # CRITICAL: Generate PWERM scenarios using valuation engine for deck slides
            logger.info(f"[DECK_GEN] Generating PWERM scenarios using valuation engine for {len(companies)} companies")
            for company in companies[:2]:  # Limit to 2 companies for deck
                try:
                    if not company.get('pwerm_scenarios'):
                        # Build valuation request
                        company_name = company.get('company', 'Unknown')
                        stage = self._determine_accurate_stage(company)
                        company_stage = self._get_stage_enum(stage)
                        revenue = self._get_field_safe(company, 'revenue') or self._get_field_safe(company, 'inferred_revenue') or 0
                        valuation = self._get_field_safe(company, 'valuation') or self._get_field_safe(company, 'inferred_valuation') or 0
                        growth_rate = self._get_field_safe(company, 'growth_rate') or 2.0
                        inferred_val = self._get_field_safe(company, 'inferred_valuation') or 0
                        
                        val_request = ValuationRequest(
                            company_name=company_name,
                            stage=company_stage,
                            revenue=revenue,
                            growth_rate=growth_rate,
                            last_round_valuation=valuation if valuation and valuation > 0 else None,
                            inferred_valuation=inferred_val,
                            total_raised=self._get_field_safe(company, "total_funding")
                        )
                        
                        # Use FULL PWERM calculation with stage-specific scenarios
                        pwerm_result = await self.valuation_engine._calculate_pwerm(val_request)
                        
                        # Get the full scenario distribution (10+ scenarios)
                        full_scenarios = pwerm_result.scenarios
                        
                        # Also get simplified bear/base/bull for easy display
                        simple_scenarios = self.valuation_engine.generate_simple_scenarios(val_request)
                        
                        # Add both full and simple scenarios to company data
                        company["exit_scenarios"] = simple_scenarios
                        company["full_exit_distribution"] = [
                            {
                                "scenario": s.scenario,
                                "probability": s.probability,
                                "exit_value": s.exit_value,
                                "time_to_exit": s.time_to_exit,
                                "moic": s.moic
                            } for s in full_scenarios
                        ]
                        company["pwerm_valuation"] = pwerm_result.fair_value
                        company["pwerm_scenarios"] = full_scenarios
                        
                        logger.info(f"[DECK_GEN] ✅ Generated PWERM scenarios for {company_name}: {len(full_scenarios)} scenarios, PWERM value: ${pwerm_result.fair_value:,.0f}")
                except Exception as e:
                    logger.error(f"[DECK_GEN] Failed to generate PWERM scenarios for {company.get('company', 'Unknown')}: {e}")
                    import traceback
                    logger.error(f"[DECK_GEN] Traceback: {traceback.format_exc()}")
            
            logger.info(f"[DECK_GEN] Starting deck generation with {len(companies)} companies from shared_data['companies']")
            logger.info(f"[DECK_GEN] Shared data keys: {list(self.shared_data.keys())}")
            
            # Normalize fund context immediately for any downstream math/logging
            incoming_fund_context = inputs.get("fund_context") if isinstance(inputs.get("fund_context"), dict) else {}
            fund_context: Dict[str, Any] = {}
            if isinstance(stored_fund_context, dict):
                fund_context.update(stored_fund_context)
            if incoming_fund_context:
                fund_context.update(incoming_fund_context)
            
            def _coerce_amount(value: Any, default: Optional[float] = None) -> Optional[float]:
                if value in (None, ""):
                    return default
                try:
                    if isinstance(value, (int, float, Decimal)):
                        return float(value)
                    if isinstance(value, str):
                        cleaned = value.replace(",", "").replace("$", "").strip().lower()
                        multiplier = 1.0
                        if cleaned.endswith("mm"):
                            cleaned = cleaned[:-2]
                            multiplier = 1_000_000
                        elif cleaned.endswith("m"):
                            cleaned = cleaned[:-1]
                            multiplier = 1_000_000
                        elif cleaned.endswith("b"):
                            cleaned = cleaned[:-1]
                            multiplier = 1_000_000_000
                        cleaned = cleaned.strip()
                        if not cleaned:
                            return default
                        return float(cleaned) * multiplier
                except (ValueError, TypeError):
                    return default
                return default
            
            baseline_fund_size = DEFAULT_FUND_SIZE
            baseline_deployed_ratio = 0.4
            baseline_remaining_ratio = 0.6
            
            fund_size = _coerce_amount(fund_context.get("fund_size")) or _coerce_amount(inputs.get("fund_size")) or baseline_fund_size
            deployed_capital = _coerce_amount(fund_context.get("deployed_capital"))
            remaining_capital = _coerce_amount(fund_context.get("remaining_capital")) or 0  # CRITICAL FIX: Default to 0 if None
            
            if deployed_capital is None and remaining_capital is not None:
                deployed_capital = max(fund_size - remaining_capital, 0)
            if remaining_capital is None and deployed_capital is not None:
                remaining_capital = max(fund_size - deployed_capital, 0)
            if deployed_capital is None and remaining_capital is None:
                deployed_capital = fund_size * baseline_deployed_ratio
                remaining_capital = fund_size - deployed_capital if fund_size else fund_size * baseline_remaining_ratio
            
            # CRITICAL FIX: Ensure remaining_capital is never None (default to 0)
            if remaining_capital is None:
                remaining_capital = 0
            if deployed_capital is None:
                deployed_capital = fund_size * baseline_deployed_ratio
                remaining_capital = fund_size - deployed_capital if fund_size else 0
            
            fund_context["fund_size"] = fund_size
            fund_context["deployed_capital"] = deployed_capital
            fund_context["remaining_capital"] = remaining_capital
            
            async with self.shared_data_lock:
                self.shared_data["fund_context"] = fund_context
            
            logger.info(
                "[DECK_GEN] Fund context normalized: fund=$%.0fM, deployed=$%.0fM, remaining=$%.0fM",
                fund_size / 1e6,
                deployed_capital / 1e6 if deployed_capital is not None else 0,
                remaining_capital / 1e6 if remaining_capital is not None else 0,
            )
            
            # CRITICAL: If no companies found, check company-data-fetcher result directly
            if not companies:
                logger.warning(f"[DECK_GEN] No companies in shared_data['companies']! Checking company-data-fetcher results...")
                async with self.shared_data_lock:
                    # Check if company-data-fetcher result is in shared_data
                    if "company-data-fetcher" in self.shared_data:
                        fetcher_result = self.shared_data["company-data-fetcher"]
                        if isinstance(fetcher_result, dict) and "companies" in fetcher_result:
                            companies = fetcher_result["companies"]
                            logger.info(f"[DECK_GEN] ✅ Found {len(companies)} companies in company-data-fetcher result")
                            # Store them in shared_data for future use
                            self.shared_data["companies"] = companies
                        elif isinstance(fetcher_result, list):
                            companies = fetcher_result
                            logger.info(f"[DECK_GEN] ✅ Found {len(companies)} companies as list in company-data-fetcher")
                            self.shared_data["companies"] = companies
            
            # DEBUG: Log detailed shared_data contents
            if companies:
                logger.info(f"[DECK_GEN] Companies found in shared_data['companies']:")
                for i, company in enumerate(companies):
                    logger.info(f"[DECK_GEN] Company {i}: {company.get('company', 'NO_COMPANY_FIELD')} (keys: {list(company.keys())})")
            else:
                logger.warning(f"[DECK_GEN] No companies in shared_data['companies']! Available keys: {list(self.shared_data.keys())}")
                
                # ENHANCED FALLBACK: Try multiple strategies to find companies
                logger.info(f"[DECK_GEN] Attempting enhanced company extraction...")
                
                # Strategy 1: Look for companies in any skill result
                async with self.shared_data_lock:
                    for key, value in self.shared_data.items():
                        if isinstance(value, dict) and "companies" in value:
                            companies_list = value["companies"]
                            if isinstance(companies_list, list) and companies_list:
                                logger.info(f"[DECK_GEN] Found companies in shared_data key '{key}'")
                                companies = companies_list
                                # Store in main companies key
                                self.shared_data["companies"] = companies
                                break
                        elif isinstance(value, list) and value and isinstance(value[0], dict) and "company" in value[0]:
                            logger.info(f"[DECK_GEN] Found companies list in shared_data key '{key}'")
                            companies = value
                            # Store in main companies key
                            self.shared_data["companies"] = companies
                            break
                
                # Strategy 2: Look for company data in any nested structure
                if not companies:
                    logger.info(f"[DECK_GEN] Strategy 1 failed, trying strategy 2...")
                    async with self.shared_data_lock:
                        for key, value in self.shared_data.items():
                            if isinstance(value, dict):
                                for subkey, subvalue in value.items():
                                    if isinstance(subvalue, list) and subvalue and isinstance(subvalue[0], dict) and "company" in subvalue[0]:
                                        logger.info(f"[DECK_GEN] Found companies in shared_data['{key}']['{subkey}']")
                                        companies = subvalue
                                        # Store in main companies key
                                        self.shared_data["companies"] = companies
                                        break
                                if companies:
                                    break
                
                # Strategy 3: Extract from any list that looks like company data
                if not companies:
                    logger.info(f"[DECK_GEN] Strategy 2 failed, trying strategy 3...")
                    async with self.shared_data_lock:
                        for key, value in self.shared_data.items():
                            if isinstance(value, list) and value:
                                logger.info(f"[DECK_GEN] Found list in key '{key}' with {len(value)} items")
                                if isinstance(value[0], dict) and ('company' in value[0] or 'name' in value[0]):
                                    logger.info(f"[DECK_GEN] Key '{key}' contains company-like data! Using it.")
                                    companies = value
                                    # Store in main companies key
                                    self.shared_data["companies"] = companies
                                    break
                
                if companies:
                    logger.info(f"[DECK_GEN] ✅ Successfully extracted {len(companies)} companies using fallback strategy")
                else:
                    logger.error(f"[DECK_GEN] ❌ All company extraction strategies failed")
            
            # ROOT CAUSE FIX: Ensure PWERM scenarios are generated for EACH company individually
            if companies:
                for company in companies:
                    company_name = company.get('company', 'Unknown')
                    if not company.get("pwerm_scenarios"):
                        logger.warning(f"[DECK_GEN] {company_name} missing PWERM scenarios - generating now")
                        try:
                            # Generate scenarios for this specific company
                            stage_map = {
                                "Pre-Seed": Stage.PRE_SEED if Stage else None,
                                "Pre Seed": Stage.PRE_SEED if Stage else None,
                                "Seed": Stage.SEED if Stage else None,
                                "Series A": Stage.SERIES_A if Stage else None,
                                "Series B": Stage.SERIES_B if Stage else None,
                                "Series C": Stage.SERIES_C if Stage else None,
                                "Growth": Stage.GROWTH if Stage else None,
                                "Late": Stage.LATE if Stage else None
                            }
                            
                            if Stage is None:
                                logger.error("[DECK_GEN] Stage enum not available - cannot generate PWERM scenarios")
                                continue
                            
                            company_stage = stage_map.get(company.get("stage", "Series A"), Stage.SERIES_A)
                            
                            # Get revenue with fallbacks
                            revenue = ensure_numeric(company.get("revenue"), 0)
                            if revenue == 0:
                                revenue = ensure_numeric(company.get("inferred_revenue"), 0)
                            if revenue == 0:
                                revenue = ensure_numeric(company.get("arr") or company.get("inferred_arr"), 1_000_000)
                            
                            # Get valuation with fallbacks
                            valuation = ensure_numeric(company.get("valuation"), 0)
                            if valuation == 0:
                                valuation = ensure_numeric(company.get("inferred_valuation"), 0)
                            if valuation == 0:
                                valuation = ensure_numeric(company.get("total_funding"), 0) * 3
                            
                            growth_rate = ensure_numeric(company.get("growth_rate"), 0)
                            if growth_rate == 0:
                                growth_rate = ensure_numeric(company.get("inferred_growth_rate"), 1.5)
                            
                            inferred_val = ensure_numeric(company.get("inferred_valuation"), None) if company.get("inferred_valuation") is not None else None
                            
                            val_request = ValuationRequest(
                                company_name=company_name,
                                stage=company_stage,
                                revenue=revenue,
                                growth_rate=growth_rate,
                                last_round_valuation=valuation if valuation > 0 else None,
                                inferred_valuation=inferred_val,
                                total_raised=self._get_field_safe(company, "total_funding")
                            )
                            
                            # Generate PWERM scenarios
                            pwerm_result = await self.valuation_engine._calculate_pwerm(val_request)
                            full_scenarios = pwerm_result.scenarios
                            
                            if full_scenarios and len(full_scenarios) > 0:
                                # Model cap table evolution for each scenario
                                check_size = self._get_optimal_check_size(company, fund_context or {})
                                post_money = valuation + check_size
                                our_investment = {
                                    'amount': check_size,
                                    'ownership': check_size / post_money if post_money > 0 else 0.08
                                }
                                
                                # Add cap table evolution to each scenario
                                for scenario in full_scenarios:
                                    self.valuation_engine.model_cap_table_evolution(
                                        scenario,
                                        company,
                                        our_investment
                                    )
                                
                                # Generate return curves for each scenario
                                self.valuation_engine.generate_return_curves(full_scenarios, our_investment)
                                
                                # Store scenarios in company
                                company["pwerm_scenarios"] = full_scenarios
                                company["pwerm_valuation"] = pwerm_result.fair_value
                                logger.info(f"[DECK_GEN] ✅ Generated {len(full_scenarios)} PWERM scenarios for {company_name}")
                            else:
                                logger.error(f"[DECK_GEN] ❌ Failed to generate PWERM scenarios for {company_name} - returned empty")
                        except Exception as e:
                            logger.error(f"[DECK_GEN] ❌ Error generating PWERM scenarios for {company_name}: {e}", exc_info=True)
                    else:
                        logger.info(f"[DECK_GEN] {company_name} already has {len(company.get('pwerm_scenarios', []))} PWERM scenarios")
            
            # Final validation - if still no companies, raise error to trigger retry or proper error handling
            if not companies:
                error_msg = "No companies found in shared_data after all extraction strategies. Cannot generate deck without company data."
                logger.error(f"[DECK_GEN] ❌ {error_msg}")
                logger.error(f"[DECK_GEN] ❌ Available shared_data keys: {list(self.shared_data.keys())}")
                # Return error result instead of fallback deck - this will be handled by the caller
                return {
                    "format": "deck",
                    "error": error_msg,
                    "error_type": "companies_not_found",
                    "slides": [],
                    "theme": "professional",
                    "metadata": {
                        "generated_at": datetime.now().isoformat(),
                        "status": "error",
                        "reason": "companies_not_found"
                    },
                    "companies": [],
                    "citations": [],
                    "charts": []
                }
            
            # Ensure growth metrics are available before slide generation
            for company in companies:
                needs_growth = (
                    not company.get("growth_metrics")
                    or not company.get("projected_growth_rate")
                    or not company.get("growth_rate")
                )
                if needs_growth:
                    self._ensure_growth_metrics(company)
            
            slides = []
            
            def _validate_chart_data(chart_data: Dict[str, Any], chart_type: str = None) -> Dict[str, Any]:
                """
                Validate chart_data structure and return validation result.
                
                Returns:
                    {
                        'valid': bool,
                        'chart_data': Dict (normalized/fixed chart_data),
                        'errors': List[str],
                        'warnings': List[str]
                    }
                """
                errors = []
                warnings = []
                validated_data = deepcopy(chart_data) if chart_data else {}
                
                # Check if chart_data is a dict
                if not isinstance(validated_data, dict):
                    errors.append("chart_data must be a dictionary")
                    return {
                        'valid': False,
                        'chart_data': validated_data,
                        'errors': errors,
                        'warnings': warnings
                    }
                
                # Validate type field
                chart_type_from_data = validated_data.get('type', chart_type)
                if not chart_type_from_data:
                    errors.append("chart_data must have a 'type' field")
                    chart_type_from_data = 'bar'  # Default fallback
                    validated_data['type'] = chart_type_from_data
                elif not isinstance(chart_type_from_data, str):
                    warnings.append(f"chart type should be string, got {type(chart_type_from_data)}")
                    chart_type_from_data = str(chart_type_from_data).lower()
                    validated_data['type'] = chart_type_from_data
                else:
                    chart_type_from_data = chart_type_from_data.lower()
                    validated_data['type'] = chart_type_from_data
                
                # Validate data field
                if 'data' not in validated_data:
                    errors.append("chart_data must have a 'data' field")
                    validated_data['data'] = {}
                elif validated_data['data'] is None:
                    warnings.append("chart_data 'data' field is None, replacing with empty structure")
                    validated_data['data'] = {}
                
                data = validated_data.get('data', {})
                
                # Type-specific validation
                if chart_type_from_data in ['bar', 'line', 'pie']:
                    # Chart.js format validation
                    if not isinstance(data, dict):
                        errors.append(f"Chart.js charts require 'data' to be a dict, got {type(data)}")
                        validated_data['data'] = {'labels': [], 'datasets': []}
                        data = validated_data['data']
                    else:
                        # Validate labels
                        if 'labels' not in data:
                            warnings.append("Chart.js chart missing 'labels', adding empty array")
                            data['labels'] = []
                        elif not isinstance(data['labels'], list):
                            warnings.append(f"Chart.js 'labels' should be array, got {type(data['labels'])}")
                            data['labels'] = list(data['labels']) if data['labels'] else []
                        
                        # Validate datasets
                        if 'datasets' not in data:
                            warnings.append("Chart.js chart missing 'datasets', adding empty array")
                            data['datasets'] = []
                        elif not isinstance(data['datasets'], list):
                            errors.append(f"Chart.js 'datasets' must be array, got {type(data['datasets'])}")
                            data['datasets'] = []
                        else:
                            # Validate each dataset
                            labels_len = len(data['labels'])
                            for i, dataset in enumerate(data['datasets']):
                                if not isinstance(dataset, dict):
                                    errors.append(f"Dataset {i} must be a dict")
                                    data['datasets'][i] = {'label': f'Dataset {i}', 'data': []}
                                    continue
                                
                                if 'data' not in dataset:
                                    warnings.append(f"Dataset {i} missing 'data' field")
                                    dataset['data'] = []
                                elif not isinstance(dataset['data'], list):
                                    warnings.append(f"Dataset {i} 'data' should be array, got {type(dataset['data'])}")
                                    dataset['data'] = list(dataset['data']) if dataset['data'] else []
                                
                                # Check length mismatch
                                data_len = len(dataset.get('data', []))
                                if labels_len > 0 and data_len != labels_len:
                                    warnings.append(f"Dataset {i} data length ({data_len}) doesn't match labels length ({labels_len})")
                
                elif chart_type_from_data == 'sankey':
                    # Sankey validation
                    if not isinstance(data, dict):
                        errors.append("Sankey chart requires 'data' to be a dict")
                        validated_data['data'] = {'nodes': [], 'links': []}
                        data = validated_data['data']
                    else:
                        if 'nodes' not in data or not isinstance(data['nodes'], list):
                            errors.append("Sankey chart requires 'nodes' array")
                            data['nodes'] = []
                        else:
                            for i, node in enumerate(data['nodes']):
                                if not isinstance(node, dict):
                                    errors.append(f"Sankey node {i} must be a dict")
                                    continue
                                if 'id' not in node:
                                    errors.append(f"Sankey node {i} missing 'id'")
                                if 'name' not in node:
                                    warnings.append(f"Sankey node {i} missing 'name'")
                        
                        if 'links' not in data or not isinstance(data['links'], list):
                            errors.append("Sankey chart requires 'links' array")
                            data['links'] = []
                        else:
                            for i, link in enumerate(data['links']):
                                if not isinstance(link, dict):
                                    errors.append(f"Sankey link {i} must be a dict")
                                    continue
                                if 'source' not in link:
                                    errors.append(f"Sankey link {i} missing 'source'")
                                if 'target' not in link:
                                    errors.append(f"Sankey link {i} missing 'target'")
                                if 'value' not in link:
                                    warnings.append(f"Sankey link {i} missing 'value'")
                
                elif chart_type_from_data == 'waterfall':
                    # Waterfall validation
                    if not isinstance(data, list):
                        if isinstance(data, dict) and 'items' in data:
                            data = data['items']
                            validated_data['data'] = data
                        else:
                            errors.append("Waterfall chart requires 'data' to be an array")
                            validated_data['data'] = []
                            data = validated_data['data']
                    
                    for i, item in enumerate(data):
                        if not isinstance(item, dict):
                            errors.append(f"Waterfall item {i} must be a dict")
                            continue
                        if 'name' not in item:
                            warnings.append(f"Waterfall item {i} missing 'name'")
                        if 'value' not in item:
                            errors.append(f"Waterfall item {i} missing 'value'")
                
                elif chart_type_from_data == 'heatmap':
                    # Heatmap validation
                    if not isinstance(data, dict):
                        errors.append("Heatmap chart requires 'data' to be a dict")
                        validated_data['data'] = {}
                        data = validated_data['data']
                    # Heatmap can have various structures, so we're lenient
                    if 'scores' not in data and 'data' not in data:
                        warnings.append("Heatmap chart missing 'scores' or 'data' field")
                
                # Check if we have any critical errors
                is_valid = len([e for e in errors if 'must' in e.lower() or 'requires' in e.lower()]) == 0
                
                return {
                    'valid': is_valid,
                    'chart_data': validated_data,
                    'errors': errors,
                    'warnings': warnings
                }
            
            def _normalize_chart_data(chart_data: Dict[str, Any]) -> Dict[str, Any]:
                """
                Normalize chart_data to consistent structure.
                Aggressively fixes issues to make charts work rather than replacing them.
                """
                if not chart_data or not isinstance(chart_data, dict):
                    # Only return empty structure if we have absolutely nothing
                    return {
                        'type': 'bar',
                        'title': 'Chart',
                        'data': {'labels': [], 'datasets': []},
                        'options': {}
                    }
                
                normalized = deepcopy(chart_data)
                
                # Normalize type
                if 'type' in normalized:
                    normalized['type'] = str(normalized['type']).lower()
                else:
                    normalized['type'] = 'bar'
                
                chart_type = normalized['type']
                
                # Normalize data field - try to extract from various locations
                if 'data' not in normalized or normalized['data'] is None:
                    # Try to find data in other fields
                    if 'values' in normalized:
                        # Legacy format - convert to Chart.js format
                        values = normalized.pop('values')
                        if isinstance(values, list) and len(values) > 0:
                            normalized['data'] = {
                                'labels': [f'Item {i+1}' for i in range(len(values))],
                                'datasets': [{'label': 'Value', 'data': values}]
                            }
                        else:
                            normalized['data'] = {}
                    elif 'datasets' in normalized:
                        # Data at top level - move it
                        datasets = normalized.pop('datasets', [])
                        labels = normalized.pop('labels', [])
                        normalized['data'] = {'labels': labels, 'datasets': datasets}
                    else:
                        normalized['data'] = {}
                
                data = normalized['data']
                
                # Type-specific normalization
                if chart_type in ['bar', 'line', 'pie']:
                    if not isinstance(data, dict):
                        # Try to convert if it's a list or other structure
                        if isinstance(data, list) and len(data) > 0:
                            # Assume it's an array of values
                            normalized['data'] = {
                                'labels': [f'Item {i+1}' for i in range(len(data))],
                                'datasets': [{'label': 'Data', 'data': data}]
                            }
                            data = normalized['data']
                        else:
                            data = {}
                            normalized['data'] = data
                    
                    # Ensure labels is an array - generate if missing
                    if 'labels' not in data or not data['labels']:
                        # Try to generate labels from datasets
                        if 'datasets' in data and isinstance(data['datasets'], list) and len(data['datasets']) > 0:
                            first_dataset = data['datasets'][0]
                            if isinstance(first_dataset, dict) and 'data' in first_dataset:
                                data_len = len(first_dataset['data']) if isinstance(first_dataset['data'], list) else 0
                                if data_len > 0:
                                    data['labels'] = [f'Item {i+1}' for i in range(data_len)]
                                else:
                                    data['labels'] = []
                        else:
                            data['labels'] = []
                    elif not isinstance(data['labels'], list):
                        data['labels'] = list(data['labels']) if data['labels'] else []
                    
                    # Ensure datasets is an array - create from available data if missing
                    if 'datasets' not in data or not data['datasets']:
                        # Try to find data elsewhere
                        if 'values' in normalized:
                            values = normalized.pop('values')
                            if isinstance(values, list):
                                data['datasets'] = [{'label': 'Data', 'data': values}]
                                if not data.get('labels'):
                                    data['labels'] = [f'Item {i+1}' for i in range(len(values))]
                        elif isinstance(data, dict) and 'data' in data:
                            # Data field contains the actual data array
                            values = data.pop('data')
                            if isinstance(values, list):
                                data['datasets'] = [{'label': 'Data', 'data': values}]
                                if not data.get('labels'):
                                    data['labels'] = [f'Item {i+1}' for i in range(len(values))]
                        else:
                            data['datasets'] = []
                    elif not isinstance(data['datasets'], list):
                        # Convert single dataset to array
                        if isinstance(data['datasets'], dict):
                            data['datasets'] = [data['datasets']]
                        else:
                            data['datasets'] = []
                    else:
                        # Normalize each dataset
                        labels_len = len(data['labels'])
                        for i, dataset in enumerate(data['datasets']):
                            if not isinstance(dataset, dict):
                                # Try to convert
                                if isinstance(dataset, (list, tuple)):
                                    data['datasets'][i] = {'label': f'Dataset {i+1}', 'data': list(dataset)}
                                    dataset = data['datasets'][i]
                                else:
                                    continue
                            
                            # Ensure data array exists - extract from various formats
                            if 'data' not in dataset or not dataset['data']:
                                # Try to find data in other fields
                                if 'values' in dataset:
                                    dataset['data'] = dataset.pop('values')
                                elif 'y' in dataset:
                                    dataset['data'] = dataset.pop('y')
                                else:
                                    dataset['data'] = []
                            elif not isinstance(dataset['data'], list):
                                # Convert to list
                                if isinstance(dataset['data'], (tuple, set)):
                                    dataset['data'] = list(dataset['data'])
                                elif isinstance(dataset['data'], (int, float)):
                                    dataset['data'] = [dataset['data']]
                                else:
                                    dataset['data'] = []
                            
                            # Ensure all values are numeric and clean NaN/None values
                            cleaned_data = []
                            for x in dataset['data']:
                                if x is None or x == '':
                                    cleaned_data.append(0.0)
                                elif isinstance(x, float) and math.isnan(x):
                                    cleaned_data.append(0.0)
                                else:
                                    try:
                                        cleaned_data.append(float(x))
                                    except (ValueError, TypeError):
                                        cleaned_data.append(0.0)
                            dataset['data'] = cleaned_data
                            
                            # Fix length mismatches intelligently
                            data_len = len(dataset['data'])
                            if labels_len > 0:
                                if data_len < labels_len:
                                    # Pad with last value or 0
                                    last_val = dataset['data'][-1] if dataset['data'] else 0
                                    dataset['data'].extend([last_val] * (labels_len - data_len))
                                elif data_len > labels_len:
                                    # Extend labels to match data
                                    for j in range(labels_len, data_len):
                                        data['labels'].append(f'Item {j+1}')
                                    labels_len = len(data['labels'])
                            
                            # Add default colors if missing
                            if chart_type == 'pie' and 'backgroundColor' not in dataset:
                                default_colors = [
                                    'rgba(99, 102, 241, 0.8)',
                                    'rgba(139, 92, 246, 0.8)',
                                    'rgba(236, 72, 153, 0.8)',
                                    'rgba(251, 146, 60, 0.8)',
                                    'rgba(34, 197, 94, 0.8)',
                                    'rgba(59, 130, 246, 0.8)'
                                ]
                                dataset['backgroundColor'] = default_colors[:len(dataset['data'])]
                            elif chart_type in ['bar', 'line'] and 'backgroundColor' not in dataset:
                                # Add default color for bar/line charts
                                colors = [
                                    'rgba(59, 130, 246, 0.8)',
                                    'rgba(16, 185, 129, 0.8)',
                                    'rgba(251, 146, 60, 0.8)',
                                    'rgba(139, 92, 246, 0.8)',
                                    'rgba(236, 72, 153, 0.8)'
                                ]
                                dataset['backgroundColor'] = colors[i % len(colors)]
                            
                            # Ensure label exists
                            if 'label' not in dataset or not dataset['label']:
                                dataset['label'] = f'Dataset {i+1}'
                        
                        # Remove empty datasets
                        data['datasets'] = [ds for ds in data['datasets'] if ds and isinstance(ds, dict) and ds.get('data')]
                        
                        # If we have no valid datasets but have labels, create a default dataset
                        if not data['datasets'] and data.get('labels'):
                            data['datasets'] = [{'label': 'Data', 'data': [0] * len(data['labels'])}]
                
                elif chart_type == 'sankey':
                    if not isinstance(data, dict):
                        data = {}
                        normalized['data'] = data
                    
                    if 'nodes' not in data or not isinstance(data['nodes'], list):
                        data['nodes'] = []
                    if 'links' not in data or not isinstance(data['links'], list):
                        data['links'] = []
                
                elif chart_type == 'waterfall':
                    if not isinstance(data, list):
                        if isinstance(data, dict) and 'items' in data:
                            data = data['items']
                            normalized['data'] = data
                        else:
                            normalized['data'] = []
                
                elif chart_type == 'heatmap':
                    if not isinstance(data, dict):
                        data = {}
                        normalized['data'] = data
                    
                    # Heatmap can have data at top level or nested
                    if 'dimensions' in normalized or 'scores' in normalized:
                        # Top-level structure - ensure it's complete
                        if 'data' not in normalized:
                            normalized['data'] = {}
                        # Copy top-level fields to data if needed
                        if 'dimensions' in normalized and 'dimensions' not in normalized['data']:
                            normalized['data']['dimensions'] = normalized.get('dimensions', [])
                        if 'companies' in normalized and 'companies' not in normalized['data']:
                            normalized['data']['companies'] = normalized.get('companies', [])
                        if 'scores' in normalized and 'scores' not in normalized['data']:
                            normalized['data']['scores'] = normalized.get('scores', [])
                        # Update data reference
                        data = normalized['data']
                
                # Ensure options exist with sensible defaults
                if 'options' not in normalized:
                    normalized['options'] = {}
                options = normalized['options']
                
                # Add type-specific defaults
                if chart_type in ['bar', 'line']:
                    if 'responsive' not in options:
                        options['responsive'] = True
                    if 'scales' not in options:
                        options['scales'] = {'y': {'beginAtZero': True}}
                elif chart_type == 'pie':
                    if 'responsive' not in options:
                        options['responsive'] = True
                    if 'plugins' not in options:
                        options['plugins'] = {}
                    if 'legend' not in options['plugins']:
                        options['plugins']['legend'] = {'position': 'right'}
                elif chart_type == 'sankey':
                    if 'responsive' not in options:
                        options['responsive'] = True
                elif chart_type == 'waterfall':
                    if 'responsive' not in options:
                        options['responsive'] = True
                elif chart_type == 'heatmap':
                    if 'responsive' not in options:
                        options['responsive'] = True
                
                # Ensure title exists
                if 'title' not in normalized:
                    normalized['title'] = 'Chart'
                
                return normalized
            
            def _create_placeholder_chart_data(chart_type: str, error_message: str = None) -> Dict[str, Any]:
                """
                Create placeholder chart_data when validation fails.
                Ensures slide still renders with error message.
                """
                chart_type = str(chart_type).lower() if chart_type else 'bar'
                
                # Default placeholder based on type
                if chart_type in ['bar', 'line']:
                    placeholder = {
                        'type': chart_type,
                        'title': f'Chart Data Unavailable' + (f' - {error_message[:50]}' if error_message else ''),
                        'data': {
                            'labels': ['No Data'],
                            'datasets': [{
                                'label': 'Data',
                                'data': [0],
                                'backgroundColor': 'rgba(156, 163, 175, 0.5)'
                            }]
                        },
                        'options': {
                            'plugins': {
                                'legend': {'display': False},
                                'tooltip': {'enabled': False}
                            }
                        }
                    }
                elif chart_type == 'pie':
                    placeholder = {
                        'type': 'pie',
                        'title': f'Chart Data Unavailable' + (f' - {error_message[:50]}' if error_message else ''),
                        'data': {
                            'labels': ['No Data'],
                            'datasets': [{
                                'data': [1],
                                'backgroundColor': 'rgba(156, 163, 175, 0.5)'
                            }]
                        },
                        'options': {
                            'plugins': {
                                'legend': {'display': False},
                                'tooltip': {'enabled': False}
                            }
                        }
                    }
                elif chart_type == 'sankey':
                    placeholder = {
                        'type': 'sankey',
                        'title': f'Chart Data Unavailable' + (f' - {error_message[:50]}' if error_message else ''),
                        'data': {
                            'nodes': [{'id': '0', 'name': 'No Data'}],
                            'links': []
                        }
                    }
                elif chart_type == 'waterfall':
                    placeholder = {
                        'type': 'waterfall',
                        'title': f'Chart Data Unavailable' + (f' - {error_message[:50]}' if error_message else ''),
                        'data': [{'name': 'No Data', 'value': 0}]
                    }
                elif chart_type == 'heatmap':
                    placeholder = {
                        'type': 'heatmap',
                        'title': f'Chart Data Unavailable' + (f' - {error_message[:50]}' if error_message else ''),
                        'data': {
                            'scores': [[]],
                            'dimensions': ['No Data'],
                            'companies': ['No Data']
                        }
                    }
                else:
                    # Generic fallback
                    placeholder = {
                        'type': 'bar',
                        'title': f'Chart Data Unavailable ({chart_type})' + (f' - {error_message[:50]}' if error_message else ''),
                        'data': {
                            'labels': ['No Data'],
                            'datasets': [{
                                'label': 'Data',
                                'data': [0],
                                'backgroundColor': 'rgba(156, 163, 175, 0.5)'
                            }]
                        },
                        'options': {}
                    }
                
                return placeholder
            
            def add_slide(template: str, content: Dict[str, Any]) -> None:
                """Helper to add properly formatted slides with error handling"""
                try:
                    if not template:
                        logger.warning("[DECK_GEN] ⚠️ Attempted to add slide with empty template, skipping")
                        return
                    if not content:
                        logger.warning("[DECK_GEN] ⚠️ Attempted to add slide with empty content, using placeholder")
                        content = {"title": "Slide", "body": "Content unavailable"}
                    
                    # Validate that slide has substantial content
                    def has_substantial_content(content_dict: Dict[str, Any]) -> bool:
                        """Check if slide has meaningful content"""
                        if not content_dict:
                            return False
                        
                        # Check for title (required for most slides)
                        title = content_dict.get('title', '')
                        if title and len(title.strip()) > 0:
                            # Title exists, now check for body content
                            body = content_dict.get('body', '')
                            bullets = content_dict.get('bullets', [])
                            companies = content_dict.get('companies', {})
                            chart_data = content_dict.get('chart_data')
                            
                            # Has body text
                            if body and len(body.strip()) > 10:
                                return True
                            
                            # Has bullets
                            if bullets and isinstance(bullets, list) and len(bullets) > 0:
                                # Check if bullets have actual content
                                non_empty_bullets = [b for b in bullets if b and isinstance(b, str) and len(b.strip()) > 5]
                                if len(non_empty_bullets) > 0:
                                    return True
                            
                            # Has company data
                            if companies and isinstance(companies, dict) and len(companies) > 0:
                                return True
                            
                            # Has chart data
                            if chart_data:
                                return True
                            
                            # Has other meaningful fields
                            meaningful_fields = ['subtitle', 'metrics', 'insights', 'data', 'analysis']
                            for field in meaningful_fields:
                                if content_dict.get(field):
                                    return True
                        
                        return False
                    
                    # Validate content before adding
                    if not has_substantial_content(content):
                        logger.warning(f"[DECK_GEN] ⚠️ Slide '{template}' has minimal content, may be skipped")
                        # Don't skip, but log warning - some slides might be intentionally minimal
                    
                    # Recursively find and validate all chart_data in nested structures
                    def find_and_validate_charts(obj: Any, path: str = '') -> None:
                        """Recursively find and validate all chart_data in nested structures"""
                        if isinstance(obj, dict):
                            for key, value in obj.items():
                                if key == 'chart_data' or key.endswith('_chart_data'):
                                    obj[key] = validate_and_normalize_chart(value, f'{path}.{key}' if path else key)
                                elif isinstance(value, (dict, list)):
                                    find_and_validate_charts(value, f'{path}.{key}' if path else key)
                        elif isinstance(obj, list):
                            for i, item in enumerate(obj):
                                if isinstance(item, (dict, list)):
                                    find_and_validate_charts(item, f'{path}[{i}]' if path else f'[{i}]')
                    
                    # Validate and normalize chart_data if present (including nested structures)
                    def validate_and_normalize_chart(chart_data: Any, path: str = 'chart_data') -> Any:
                        """Recursively validate and normalize chart_data - always tries to fix, never gives up"""
                        if not chart_data:
                            # Only use placeholder if we have absolutely nothing
                            logger.warning(f"[DECK_GEN] ⚠️ Empty chart_data at {path}, using placeholder")
                            return _create_placeholder_chart_data('bar', 'No data provided')
                        
                        if not isinstance(chart_data, dict):
                            # Try to convert to dict format
                            if isinstance(chart_data, list) and len(chart_data) > 0:
                                logger.info(f"[DECK_GEN] Converting list to chart format at {path}")
                                chart_data = format_bar_chart(
                                    labels=[f'Item {i+1}' for i in range(len(chart_data))],
                                    datasets=[{'label': 'Data', 'data': chart_data}]
                                )
                            else:
                                logger.warning(f"[DECK_GEN] ⚠️ Invalid chart_data type at {path}, using placeholder")
                                return _create_placeholder_chart_data('bar', 'Invalid data format')
                        
                        # Validate first to understand issues
                        validation_result = _validate_chart_data(chart_data)
                        
                        # Then normalize to fix them
                        normalized_chart = _normalize_chart_data(validation_result['chart_data'])
                        
                        # Re-validate after normalization to confirm fixes
                        final_validation = _validate_chart_data(normalized_chart)
                        
                        if validation_result['warnings']:
                            logger.debug(f"[DECK_GEN] Chart validation warnings at {path}: {validation_result['warnings']}")
                        if final_validation['warnings']:
                            logger.debug(f"[DECK_GEN] Chart normalization warnings at {path}: {final_validation['warnings']}")
                        
                        # Check if we have actual data to display
                        has_data = False
                        chart_type = normalized_chart.get('type', 'bar')
                        data = normalized_chart.get('data', {})
                        
                        if chart_type in ['bar', 'line', 'pie']:
                            datasets = data.get('datasets', [])
                            has_data = any(
                                isinstance(ds, dict) and 
                                isinstance(ds.get('data'), list) and 
                                len(ds.get('data', [])) > 0
                                for ds in datasets
                            )
                        elif chart_type == 'sankey':
                            has_data = (
                                len(data.get('nodes', [])) > 0 and
                                len(data.get('links', [])) > 0
                            )
                        elif chart_type == 'waterfall':
                            has_data = len(data) > 0 if isinstance(data, list) else len(data.get('items', [])) > 0
                        elif chart_type == 'heatmap':
                            has_data = bool(data.get('scores') or data.get('data'))
                        else:
                            # Unknown type, but try to render it anyway
                            has_data = bool(data)
                        
                        # Only use placeholder if we truly have no data after all fixes
                        if not has_data:
                            error_msg = 'No data available after normalization'
                            logger.warning(f"[DECK_GEN] ⚠️ No data in chart at {path} after normalization, using placeholder")
                            return _create_placeholder_chart_data(chart_type, error_msg)
                        
                        # Return the fixed chart
                        if validation_result['errors'] or final_validation['errors']:
                            error_count = len(validation_result['errors']) + len(final_validation['errors'])
                            logger.info(f"[DECK_GEN] Fixed chart at {path} (had {error_count} errors before/after normalization)")
                        
                        return normalized_chart
                    
                    if isinstance(content, dict):
                        # Use recursive function to find and validate all chart_data
                        find_and_validate_charts(content)
                    
                    slides.append({
                        "id": f"slide-{len(slides) + 1}",
                        "order": len(slides) + 1,
                        "template": template,
                        "content": content
                    })
                except Exception as e:
                    logger.error(f"[DECK_GEN] ❌ Failed to add slide (template={template}): {e}")
                    import traceback
                    logger.error(f"[DECK_GEN] ❌ Stack trace: {traceback.format_exc()}")
                    # Add error slide instead of failing completely
                    try:
                        slides.append({
                            "id": f"error-slide-{len(slides) + 1}",
                            "order": len(slides) + 1,
                            "template": "summary",
                            "content": {
                                "title": f"Slide Generation Error ({template})",
                                "subtitle": "This slide could not be generated",
                                "bullets": [f"Error: {str(e)[:200]}"]
                            }
                        })
                    except:
                        pass  # If even error slide fails, just skip it
            
            def convert_old_slide(old_slide: Dict[str, Any]) -> Dict[str, Any]:
                """Convert old slide format to new format"""
                return {
                    "id": f"slide-{len(slides) + 1}",
                    "order": len(slides) + 1,
                    "template": old_slide.get("type", "text"),
                    "content": old_slide.get("content", {})
                }
            
            # Title slide
            try:
                add_slide("title", {
                    "title": f"Investment Analysis Report",
                    "subtitle": f"Analysis of {len(companies)} companies",
                    "date": datetime.now().strftime("%B %Y")
                })
            except Exception as e:
                logger.error(f"[DECK_GEN] ❌ Title slide generation failed: {e}")
                # Title slide is critical, so we'll create a minimal one
                slides.append({
                    "id": "slide-1",
                    "order": 1,
                    "template": "title",
                    "content": {
                        "title": "Investment Analysis Report",
                        "subtitle": "Analysis Report",
                        "date": datetime.now().strftime("%B %Y")
                    }
                })
            
            # Executive summary - REMOVED due to LLM failures causing errors
            # Skipping executive summary generation to prevent deck generation failures
            
            # Portfolio Narrative Slide - REMOVED due to LLM failures causing errors
            # Skipping portfolio narrative generation to prevent deck generation failures
            
            # NEW: Scoring Matrix Slide (works for 1+ companies)
            if companies:
                try:
                    logger.info(f"[DECK_GEN] 🔍 CHECKPOINT 2C: Calling _generate_scoring_matrix with {len(companies)} companies")
                    logger.info(f"[DECK_GEN] 🔍 CHECKPOINT 2C: Company names: {[c.get('company', 'NO_COMPANY_FIELD') for c in companies[:2]]}")
                    logger.info(f"[DECK_GEN] 🔍 CHECKPOINT 2C: Fund context keys: {list(self.shared_data.get('fund_context', {}).keys())}")
                    scoring_matrix = await self._generate_scoring_matrix(companies[:2] if len(companies) >= 2 else companies, self.shared_data.get('fund_context', {}))
                    logger.info(f"[DECK_GEN] 🔍 CHECKPOINT 2C: _generate_scoring_matrix returned: {type(scoring_matrix)}")
                    logger.info(f"[DECK_GEN] 🔍 CHECKPOINT 2C: Scoring matrix content: {scoring_matrix}")
                    if scoring_matrix:
                        add_slide("scoring_matrix", scoring_matrix)
                        logger.info(f"[DECK_GEN] ✅ Scoring matrix generated")
                    else:
                        logger.warning(f"[DECK_GEN] ⚠️ Scoring matrix returned empty")
                except Exception as e:
                    logger.error(f"[DECK_GEN] ❌ CHECKPOINT 2C FAILED: Scoring matrix generation failed: {e}")
                    logger.error(f"[DECK_GEN] ❌ CHECKPOINT 2C FAILED: Exception type: {type(e)}")
                    logger.error(f"[DECK_GEN] ❌ CHECKPOINT 2C FAILED: Exception args: {e.args}")
                    import traceback
                    logger.error(f"[DECK_GEN] ❌ CHECKPOINT 2C FAILED: Traceback: {traceback.format_exc()}")
            
            # Company overview slide (comparison for multiple, detailed for single)
            if companies:
                try:
                    # Build company data without LLM-dependent narratives
                    companies_with_analysis = []
                    for company in companies[:2]:
                        # Extract valuation and funding data for pre/post money calculations
                        valuation = self._get_field_safe(company, 'valuation')
                        last_round_funding = self._get_field_safe(company, 'last_round_amount')
                        total_funding = self._get_field_safe(company, 'total_funding')
                        revenue = self._get_revenue_safe(company)
                        
                        # Calculate pre/post money valuations correctly
                        if last_round_funding > 0 and valuation > 0:
                            # Post-money is the valuation after investment
                            post_money = valuation
                            # Pre-money is post-money minus investment amount (standard formula)
                            pre_money = valuation - last_round_funding
                            
                            # Sanity check: pre-money should be positive and reasonable
                            if pre_money <= 0:
                                # If pre-money goes negative, likely the valuation is pre-money not post
                                pre_money = valuation
                                post_money = valuation + last_round_funding
                                logger.info(f"Adjusted to pre-money valuation for {company.get('company')}: pre=${pre_money/1e6:.1f}M, post=${post_money/1e6:.1f}M")
                        else:
                            # If no funding data, can't calculate accurately
                            # Try to infer from funding rounds if available
                            funding_rounds = company.get('funding_rounds', [])
                            if funding_rounds and isinstance(funding_rounds, list) and len(funding_rounds) > 0:
                                last_round = funding_rounds[-1] if isinstance(funding_rounds[-1], dict) else {}
                                round_amount = last_round.get('amount', 0)
                                if round_amount > 0:
                                    post_money = valuation
                                    pre_money = valuation - round_amount
                                else:
                                    # No funding amount available, can't calculate
                                    post_money = valuation
                                    pre_money = valuation  # Assume it's pre-money
                            else:
                                # No funding data at all
                                post_money = valuation
                                pre_money = valuation  # Assume it's pre-money
                        
                        # Calculate accurate revenue multiple - avoid showing same multiple for both companies
                        revenue_multiple = "N/A"
                        if revenue and revenue > 0 and valuation and valuation > 0:
                            multiple = self._safe_divide(valuation, revenue, default=0)
                            # Add some variance based on business model to avoid identical multiples
                            business_model = company.get('business_model', 'SaaS')
                            if 'AI' in business_model or 'ML' in business_model:
                                multiple *= 1.15  # AI companies get higher multiples
                            elif 'marketplace' in business_model.lower():
                                multiple *= 0.85  # Marketplaces typically lower
                            revenue_multiple = f"{multiple:.1f}x"
                        
                        # Calculate ownership from last round
                        ownership_pct = "N/A"
                        if last_round_funding > 0 and post_money > 0:
                            ownership_pct = f"{self._safe_divide(last_round_funding, post_money, 0) * 100:.1f}%"
                        
                        companies_with_analysis.append({
                            "name": company.get("company", "Unknown"),
                            "business_model": company.get("business_model", "N/A"),
                            "metrics": {
                                "Stage": self._determine_accurate_stage(company),
                                "Revenue": self._format_money(revenue),
                                "Pre-Money Val": self._format_money(pre_money),
                                "Post-Money Val": self._format_money(post_money),
                                "Revenue Multiple": revenue_multiple,
                                "Last Round": self._format_money(last_round_funding) if last_round_funding > 0 else "",
                                "Last Round Own%": ownership_pct,
                                "Total Funding": self._format_money(total_funding),
                                "Capital Efficiency": f"{self._safe_divide(revenue, max(total_funding, 1), 0):.2f}x" if total_funding > 0 else "",
                                "Team Size": f"{int(self._get_field_with_fallback(company, 'team_size', 0))} employees",
                                "Founded": company.get("founded_year", "Unknown")
                            },
                            "website": company.get("website_url", ""),
                            "recommendation": self._safe_generate_recommendation(company)  # ADD CLEAR RECOMMENDATION with error handling
                        })
                    
                    # Generate bullets for frontend rendering
                    bullets = []
                    for company_data in companies_with_analysis[:2]:
                        name = company_data.get("name", "Unknown")
                        metrics = company_data.get("metrics", {})
                        recommendation = company_data.get("recommendation", "")
                        bullets.append(
                            f"{name}: {metrics.get('Revenue', 'N/A')} revenue, "
                            f"{metrics.get('Post-Money Val', 'N/A')} valuation, "
                            f"{recommendation}"
                        )
                    
                    # Ensure recommendations are properly formatted
                    for company_data in companies_with_analysis:
                        rec = company_data.get("recommendation", {})
                        if isinstance(rec, dict):
                            # Ensure all recommendation fields are present
                            if "decision" not in rec:
                                rec["decision"] = "WATCH"
                            if "action" not in rec:
                                rec["action"] = "Monitor for updates"
                            if "reasoning" not in rec:
                                rec["reasoning"] = "Analysis in progress"
                            if "score" not in rec:
                                # Calculate score from metrics if available
                                metrics = company_data.get("metrics", {})
                                rec["score"] = "N/A"
                            if "color" not in rec:
                                decision = rec.get("decision", "WATCH")
                                rec["color"] = "green" if "BUY" in decision else "blue" if "WATCH" in decision else "yellow"
                    
                    add_slide("company_comparison", {
                        "title": "Company Overview & Financials",
                        "subtitle": "Pre/post money valuations and investment recommendations",
                        "companies": companies_with_analysis,
                        "bullets": bullets
                    })
                except Exception as e:
                    logger.error(f"[DECK_GEN] ❌ Company comparison slide generation failed: {e}")
                    import traceback
                    logger.error(f"[DECK_GEN] ❌ Stack trace: {traceback.format_exc()}")
            
            # Slide 4: Founder & Team Analysis
            if companies:
                try:
                    founders_team_data = {}
                    for company in companies[:2]:
                        company_name = company.get('company', 'Unknown')
                        
                        # Extract founder and team information
                        founders = company.get('founders', [])
                        team_size = self._get_field_safe(company, 'team_size', 10)
                        
                        # Analyze founder profiles
                        founder_profiles = []
                        founder_profile_analysis = company.get('founder_profile', {})
                        
                        for founder in founders if isinstance(founders, list) else []:
                            if isinstance(founder, dict):
                                # Get work history from founder_profile analysis if available
                                previous_companies = founder_profile_analysis.get('previous_companies', [])
                                background_text = founder.get('background', '')
                                
                                # Use formatted founder history helper
                                formatted_history = self._format_founder_history(founder_profile_analysis)
                                if formatted_history:
                                    background_text = formatted_history
                                elif previous_companies and not background_text:
                                    background_text = f"Previously at {', '.join(previous_companies[:3])}"
                                elif previous_companies and 'Previously at' not in background_text:
                                    background_text = f"{background_text}. Previously at {', '.join(previous_companies[:2])}"
                                
                                founder_profiles.append({
                                    'name': founder.get('name', 'Unknown'),
                                    'role': founder.get('role', 'Founder'),
                                    'background': background_text,
                                    'previous_exits': founder.get('previous_exits', False),
                                    'technical': founder.get('technical', False),
                                    'previous_companies': previous_companies[:3],  # Include previous companies
                                    'work_history': founder_profile_analysis.get('work_history', [])[:3]  # Include work history
                                })
                        
                        if not founder_profiles:
                            logger.warning(f"[FOUNDERS] No founder data available for {company_name}; skipping in founder slide")
                            continue
                        
                        # Team composition - ONLY use if we have real extracted data with citations
                        team_composition = company.get('team_composition', {})
                        # Don't make up team composition - it's misleading without real data
                        if not team_composition:
                            team_composition = {}  # Empty means we don't know
                        
                        # Calculate team quality score if not present (0-100 scale, convert to 0-10 for display)
                        team_quality = company.get('team_quality_score', 0)
                        if team_quality == 0 or team_quality is None:
                            # Calculate based on available data
                            team_quality = 50.0  # Base score
                            
                            # Add points for team size
                            if team_size and team_size > 0:
                                team_quality += min(30, team_size * 2)  # Up to 30 points for team size
                            
                            # Add points for founder quality
                            if any(f.get('previous_exits') for f in founder_profiles):
                                team_quality += 20  # Repeat entrepreneurs
                            
                            if any(f.get('technical') for f in founder_profiles):
                                team_quality += 15  # Technical founders
                            
                            # Add points for previous companies (experienced founders)
                            if any(f.get('previous_companies') for f in founder_profiles):
                                team_quality += 10
                            
                            # Cap at 100
                            team_quality = min(100.0, team_quality)
                            
                            logger.info(f"[TEAM_QUALITY] Calculated team quality score for {company_name}: {team_quality:.1f}/100")
                        
                        # Team quality indicators
                        quality_signals = []
                        
                        # Build quality signals from actual data
                        if any(f.get('previous_exits') for f in founder_profiles):
                            quality_signals.append("• Repeat entrepreneurs with exits")
                        
                        if any(f.get('technical') for f in founder_profiles):
                            quality_signals.append("• Technical founder(s)")
                        
                        stage = company.get('stage', 'Series A')
                        if team_size and team_size > 0:
                            quality_signals.append(f"• Team size: {team_size} employees")
                        
                        # Only include actual extracted background info, no fake top-tier nonsense
                        
                        # Risk factors based on actual data
                        risk_factors = []
                        if not any(f.get('technical') for f in founder_profiles):
                            risk_factors.append("• No technical co-founder")
                        
                        # No arbitrary team size judgments
                        
                        # Store ACTUAL founder data and analysis
                        founders_team_data[company_name] = {
                            'founders': founder_profiles,
                            'team_size': team_size,
                            'quality_signals': quality_signals[:4],  # Limit to 4 key signals
                            'risk_factors': risk_factors[:2],  # Limit to 2 key risks
                            'team_quality_score': team_quality,
                            'stage': stage,
                            'founder_analysis': company.get('founder_profile', {}),  # Real founder analysis
                            'confidence': company.get('confidence_score', 0)
                        }
                    
                    if founders_team_data:
                        # Generate bullets for frontend rendering with enhanced founder history
                        bullets = []
                        for company_name, data in founders_team_data.items():
                            founders = data.get('founders', [])
                            team_size = data.get('team_size', 0)
                            team_quality = data.get('team_quality_score', 0)
                            quality_signals = data.get('quality_signals', [])
                            
                            founder_names = [f.get('name', 'Unknown') for f in founders[:2] if f.get('name')]
                            founder_summary = ', '.join(founder_names) if founder_names else 'Unknown founders'
                            
                            bullets.append(
                                f"{company_name}: {founder_summary} leading {team_size} person team "
                                f"(Quality Score: {team_quality:.1f}/10)"
                            )
                            
                            # Add detailed founder backgrounds with work history
                            for founder in founders[:2]:
                                name = founder.get('name', '')
                                background = founder.get('background', '')
                                work_history = founder.get('work_history', [])
                                
                                if background:
                                    bullets.append(f"  • {name}: {background}")
                                elif work_history:
                                    work_parts = []
                                    for role in work_history[:2]:
                                        if isinstance(role, dict):
                                            company = role.get('company', '')
                                            title = role.get('title', '')
                                            if company and title:
                                                work_parts.append(f"{title} at {company}")
                                    if work_parts:
                                        bullets.append(f"  • {name}: {'; '.join(work_parts)}")
                            
                            if quality_signals:
                                for signal in quality_signals[:2]:
                                    bullets.append(f"  {signal}")
                        
                        add_slide("founder_team_analysis", {
                            "title": "Founder & Team Analysis",
                            "subtitle": "Leadership profiles and quality assessment",
                            "companies": founders_team_data,
                            "bullets": bullets
                        })
                    else:
                        logger.warning("[FOUNDERS] Skipping founder/team slide due to missing data for all companies")
                except Exception as e:
                    logger.error(f"[DECK_GEN] ❌ Founder/team analysis slide generation failed: {e}")
                    import traceback
                    logger.error(f"[DECK_GEN] ❌ Stack trace: {traceback.format_exc()}")
            
            # Slide 5: Growth to Exit (Merged Path to $100M + Exit Scenarios)
            if companies:
                import math
                
                # Calculate for BOTH companies
                companies_100m_data = {}
                
                for company in companies[:2]:
                    company_name = company.get('company', 'Unknown')
                    # ROOT CAUSE FIX: Use inferred_revenue if available - this is what's extracted from articles
                    # Start from inferred revenue, not from 1
                    current_revenue = self._get_field_with_fallback(company, 'revenue', 0)
                    if current_revenue == 0:
                        # Try inferred_revenue as fallback - this is what we want to use
                        current_revenue = safe_get_value(company.get('inferred_revenue', 0), 0)
                    if current_revenue == 0:
                        # Try inferred_arr
                        current_revenue = safe_get_value(company.get('inferred_arr', 0), 0)
                    if current_revenue == 0:
                        # Last resort: use stage-based default
                        stage = company.get('stage', 'Series A')
                        current_revenue = self._get_stage_default('revenue', stage)
                    
                    # Use the actual inferred revenue, don't force minimum
                    current_arr = max(current_revenue, 100_000)  # Minimum $100K to avoid divide by zero, but use actual if higher
                    stage = company.get('stage', 'Series A')
                    
                    # ALWAYS use service-calculated growth rates
                    growth_metrics = company.get('growth_metrics')
                    if not growth_metrics:
                        growth_metrics = self._ensure_growth_metrics(company)
                    
                    # Get the appropriate growth rate from service
                    yoy_growth = 0
                    if growth_metrics:
                        yoy_growth = safe_get_value(growth_metrics.get('projected_growth_rate'), 0)
                        if yoy_growth:
                            logger.info(f"Using service-calculated growth rate: {(yoy_growth-1)*100:.1f}% YoY for {company_name}")
                    
                    if not yoy_growth:
                        # Try to get from actual data
                        actual_growth = self._get_field_safe(company, 'growth_rate', 0)
                        if not actual_growth:
                            actual_growth = self._get_field_safe(company, 'revenue_growth', 0)
                        if actual_growth > 0:
                            # Fix: Handle ambiguous growth rate values
                            # If < 10, it's likely already a decimal percentage (1.5 = 150% growth)
                            # If >= 10, it's a percentage that needs division (50 = 50% growth)
                            if actual_growth < 10:
                                # Already a multiplier or very low percentage
                                # 1.5 means 50% growth, 3.0 means 200% growth
                                yoy_growth = actual_growth
                            else:
                                # Traditional percentage: 50 means 50% growth
                                yoy_growth = 1 + (actual_growth / 100)
                            
                            # Validate: yoy_growth must be > 1.0 for positive growth
                            if yoy_growth <= 1.0:
                                logger.warning(f"Growth rate {actual_growth} resulted in yoy_growth {yoy_growth}, using 1.2 (20% growth)")
                                yoy_growth = 1.2  # 20% minimum growth
                            
                            logger.info(f"Using actual growth rate: {(yoy_growth-1)*100:.1f}% YoY for {company_name}")
                        else:
                            # Last resort - request from service with minimal data
                            minimal_payload, _ = self._build_growth_inference_payload(company)
                            fallback_growth = self.gap_filler.calculate_required_growth_rates(minimal_payload)
                            yoy_growth = safe_get_value(fallback_growth.get('projected_growth_rate'), 1.5) if fallback_growth else 1.5  # 50% if service fails
                            logger.warning(f"Using fallback service growth rate: {(yoy_growth-1)*100:.1f}% YoY for {company_name}")
                    
                    # Dynamic target based on current ARR
                    if current_arr >= 1_000_000_000:  # Already at $1B+
                        target_arr = 10_000_000_000  # Target $10B
                        target_name = "$10B"
                        milestone = "decacorn"
                    elif current_arr >= 100_000_000:  # Already at $100M+
                        target_arr = 1_000_000_000  # Target $1B
                        target_name = "$1B"
                        milestone = "unicorn"
                    else:
                        target_arr = 100_000_000
                        target_name = "$100M"
                        milestone = "scale"
                    
                    # Add defensive validation before math operations
                    if current_arr > 0 and current_arr < target_arr:
                        try:
                            # Fix the critical bug: <= should be < for logarithm domain
                            # yoy_growth = 1.0 means 0% growth, log(1.0) = 0 causing division by zero
                            if yoy_growth < 1.0:
                                logger.warning(f"Growth rate {yoy_growth} is less than 1.0 for {company_name}, using fallback 1.2")
                                yoy_growth = 1.2
                            elif yoy_growth == 1.0:
                                # Exactly 1.0 means no growth - calculate linear time to target
                                # If no growth, we'll never reach the target, but we can show flat projection
                                logger.warning(f"Growth rate is exactly 1.0 (no growth) for {company_name}, showing flat projection")
                                # For flat growth, just show the current revenue extended over time
                                # Set years_to_target to a large value to indicate it won't be reached
                                years_to_target = float('inf')  # Will be capped later in projection generation
                            else:
                                # yoy_growth > 1.0, safe to use logarithm
                                # Validate current_arr is positive
                                if current_arr <= 0:
                                    logger.warning(f"Invalid current_arr {current_arr} for {company_name}, using fallback 1M")
                                    current_arr = 1_000_000
                                
                                # Safe math operations with validation
                                arr_ratio = target_arr / current_arr
                                if arr_ratio <= 0:
                                    logger.warning(f"Invalid ARR ratio {arr_ratio} for {company_name}")
                                    years_to_target = 5
                                else:
                                    years_to_target = math.log(arr_ratio) / math.log(yoy_growth)
                                    # Validate result is finite
                                    if not math.isfinite(years_to_target):
                                        logger.warning(f"Non-finite years_to_target for {company_name}, using fallback")
                                        years_to_target = 5
                                    else:
                                        years_to_target = max(0, years_to_target)
                                    
                        except (ValueError, ZeroDivisionError, OverflowError) as e:
                            logger.error(f"Math error calculating years_to_target for {company_name}: {e}")
                            years_to_target = 5  # Sensible fallback
                    elif current_arr >= target_arr:
                        years_to_target = 0  # Already achieved
                    else:
                        years_to_target = 5  # Default if no revenue
                    
                    # Generate J-curve projection - slow start, rapid middle, eventual plateau
                    projection_years = 6
                    projection_data = []
                    
                    # Determine company quality score for growth adjustment
                    quality_score = 1.0
                    investors_str = str(company.get('investors', '')).lower()
                    
                    # Top-tier investors boost growth
                    if any(vc in investors_str for vc in ['sequoia', 'andreessen', 'benchmark', 'accel', 'greylock']):
                        quality_score = 1.2
                    # Good investors moderate boost
                    elif any(vc in investors_str for vc in ['index', 'bessemer', 'redpoint', 'lightspeed']):
                        quality_score = 1.1
                    
                    # Team size adjustment
                    team_size = company.get('team_size') or company.get('inferred_team_size') or 50
                    if team_size > 100 and stage in ['Series A', 'Seed']:
                        quality_score *= 0.8  # Overstaffed for stage slows growth
                    elif team_size < 20 and stage == 'Series B':
                        quality_score *= 0.9  # Understaffed for stage
                    
                    # Market quality adjustment
                    if company.get('category') == 'ai_first':
                        quality_score *= 1.15  # AI companies grow faster currently
                    
                    # Generate three scenarios: bear, base, bull
                    # FIX: Use service-based exit scenarios instead of hardcoded multipliers
                    scenarios_data = {}
                    
                    # Start from inferred revenue (not from 1)
                    starting_revenue = current_arr / 1_000_000  # Convert to millions
                    current_revenue_millions = current_arr / 1_000_000
                    
                    # Step 1: Get exit scenarios from service (bear/base/bull with exit_value)
                    exit_scenarios = company.get('exit_scenarios')
                    if not exit_scenarios or not isinstance(exit_scenarios, dict):
                        # Generate scenarios if not available
                        try:
                            company_name = company.get('company', 'Unknown')
                            company_stage = self._get_stage_enum(stage)
                            revenue = self._get_field_safe(company, 'revenue') or self._get_field_safe(company, 'inferred_revenue') or current_arr
                            valuation = self._get_field_safe(company, 'valuation') or self._get_field_safe(company, 'inferred_valuation') or 0
                            growth_rate = self._get_field_safe(company, 'growth_rate') or yoy_growth
                            
                            val_request = ValuationRequest(
                                company_name=company_name,
                                stage=company_stage,
                                revenue=revenue,
                                growth_rate=growth_rate,
                                last_round_valuation=valuation if valuation and valuation > 0 else None,
                                total_raised=self._get_field_safe(company, "total_funding")
                            )
                            exit_scenarios = self.valuation_engine.generate_simple_scenarios(val_request)
                            # Store for future use
                            company["exit_scenarios"] = exit_scenarios
                        except Exception as e:
                            logger.warning(f"Failed to generate exit scenarios for {company_name}: {e}, using fallback")
                            exit_scenarios = None
                    
                    # Step 2: Get realistic_exit_multiple from gap_filler.score_fund_fit() service
                    realistic_exit_multiple = None
                    if exit_scenarios:
                        try:
                            # Build company_data for gap_filler
                            valuation = self._get_field_safe(company, 'valuation') or self._get_field_safe(company, 'inferred_valuation') or 0
                            revenue = self._get_field_safe(company, 'revenue') or self._get_field_safe(company, 'inferred_revenue') or current_arr
                            
                            company_data_for_gap_filler = {
                                'revenue': revenue,
                                'inferred_revenue': revenue,
                                'valuation': valuation,
                                'inferred_valuation': valuation,
                                'stage': stage,
                            }
                            
                            # Call gap_filler.score_fund_fit() to get realistic_exit_multiple
                            # Use empty context if no fund context available
                            fund_context = {}  # Can be enhanced with actual fund context if available
                            fit_result = self.gap_filler.score_fund_fit(
                                company_data_for_gap_filler,
                                {},  # Empty inferred_data dict
                                context=fund_context
                            )
                            
                            # Extract realistic_exit_multiple from result
                            if fit_result and isinstance(fit_result, dict):
                                realistic_exit_multiple = fit_result.get('realistic_exit_multiple')
                                if realistic_exit_multiple and realistic_exit_multiple > 0:
                                    logger.info(f"Got realistic_exit_multiple from gap_filler: {realistic_exit_multiple:.1f}x")
                        except Exception as e:
                            logger.warning(f"Failed to get realistic_exit_multiple from gap_filler: {e}, using fallback")
                    
                    # Fallback: calculate from current valuation/revenue ratio if gap_filler failed
                    if not realistic_exit_multiple or realistic_exit_multiple <= 0:
                        try:
                            valuation = self._get_field_safe(company, 'valuation') or self._get_field_safe(company, 'inferred_valuation') or 0
                            if valuation > 0 and current_arr > 0:
                                current_revenue_multiple = valuation / current_arr
                                # Validate: reasonable multiples are 0.5x to 100x
                                if 0.5 <= current_revenue_multiple <= 100:
                                    # Replicate gap_filler's logic for exit multiple
                                    if current_revenue_multiple <= 10:
                                        realistic_exit_multiple = 20
                                    elif current_revenue_multiple <= 20:
                                        realistic_exit_multiple = 10
                                    elif current_revenue_multiple <= 40:
                                        realistic_exit_multiple = 5
                                    else:
                                        realistic_exit_multiple = 3
                        except Exception:
                            pass
                    
                    # Final fallback: use default if still not available
                    if not realistic_exit_multiple or realistic_exit_multiple <= 0:
                        realistic_exit_multiple = 10.0  # Default exit multiple
                        logger.warning(f"Using default realistic_exit_multiple: {realistic_exit_multiple:.1f}x")
                    
                    # Default time_to_exit (used in generate_simple_scenarios)
                    default_time_to_exit = 5.0
                    
                    # Step 3: Calculate revenue projections for each scenario
                    for scenario_type in ['bear', 'base', 'bull']:
                        projection_data = []
                        
                        if exit_scenarios and scenario_type in exit_scenarios:
                            # Use service-provided exit scenario
                            scenario = exit_scenarios[scenario_type]
                            exit_value = scenario.get('exit_value', 0)
                            # Get time_to_exit from scenario if available, otherwise use default
                            time_to_exit = scenario.get('time_to_exit', default_time_to_exit)
                            
                            if exit_value > 0 and realistic_exit_multiple > 0:
                                # Calculate target revenue at exit: target_revenue = exit_value / realistic_exit_multiple
                                target_revenue = exit_value / realistic_exit_multiple
                                target_revenue_millions = target_revenue / 1_000_000
                                
                                # Calculate required CAGR to reach target revenue
                                # CAGR = (target_revenue / current_revenue)^(1/time_to_exit) - 1
                                if current_revenue_millions > 0 and target_revenue_millions > current_revenue_millions:
                                    try:
                                        revenue_ratio = target_revenue_millions / current_revenue_millions
                                        if revenue_ratio > 0:
                                            cagr = (revenue_ratio ** (1.0 / time_to_exit)) - 1.0
                                            # Convert CAGR to percentage for growth calculations
                                            base_growth_pct = cagr * 100
                                        else:
                                            # Fallback: use existing yoy_growth
                                            base_growth_pct = (yoy_growth - 1) * 100
                                    except (ValueError, ZeroDivisionError, OverflowError) as e:
                                        logger.warning(f"Error calculating CAGR for {scenario_type} scenario: {e}, using fallback")
                                        base_growth_pct = (yoy_growth - 1) * 100
                                else:
                                    # Target revenue not higher than current, use modest growth
                                    base_growth_pct = 20.0  # 20% growth
                            else:
                                # Invalid exit_value or realistic_exit_multiple, fallback to existing logic
                                base_growth_pct = (yoy_growth - 1) * 100
                        else:
                            # No exit scenarios available, fallback to existing logic
                            base_growth_pct = (yoy_growth - 1) * 100
                        
                        # Step 4: Project revenue over 6 years using CAGR
                        # Use CAGR directly - it already accounts for reaching target revenue over time_to_exit
                        cagr_decimal = base_growth_pct / 100.0 if base_growth_pct else 0
                        
                        for i in range(projection_years):
                            if i == 0:
                                # Start with current revenue (in millions)
                                projected_arr = round(starting_revenue, 2)
                            else:
                                # Compound revenue using CAGR: revenue = initial * (1 + CAGR)^years
                                projected_arr = starting_revenue * ((1 + cagr_decimal) ** i)
                                
                                # Round to 2 decimal places for clean display
                                projected_arr = round(projected_arr, 2)
                            
                            projection_data.append(projected_arr)
                        
                        scenarios_data[scenario_type] = projection_data
                    
                    # Add ownership evolution data if available
                    ownership_evolution = company.get('ownership_evolution', {})
                    ownership_data = {}
                    if ownership_evolution:
                        # Extract ownership progression through rounds
                        if 'our_entry_ownership' in ownership_evolution:
                            ownership_data['entry_ownership'] = round(ownership_evolution['our_entry_ownership'] * 100, 1)
                        if 'our_exit_ownership_no_followon' in ownership_evolution:
                            ownership_data['exit_no_followon'] = round(ownership_evolution['our_exit_ownership_no_followon'] * 100, 1)
                        if 'our_exit_ownership_with_followon' in ownership_evolution:
                            ownership_data['exit_with_followon'] = round(ownership_evolution['our_exit_ownership_with_followon'] * 100, 1)
                        
                        # Generate ownership milestones for each projection year
                        if 'entry_ownership' in ownership_data and 'exit_no_followon' in ownership_data:
                            entry_pct = ownership_data['entry_ownership']
                            exit_pct = ownership_data['exit_no_followon']
                            dilution_per_year = (entry_pct - exit_pct) / projection_years
                            
                            ownership_milestones = []
                            for i in range(projection_years):
                                # Assume dilution happens gradually with funding rounds
                                ownership_pct = entry_pct - (dilution_per_year * i)
                                ownership_milestones.append(round(ownership_pct, 1))
                            ownership_data['milestones'] = ownership_milestones
                    
                    companies_100m_data[company_name] = {
                        "current_arr": current_arr,
                        "current_arr_formatted": self._format_money(current_arr),
                        "years_to_target": round(years_to_target, 1),
                        "target": target_name,
                        "target_value": target_arr,
                        "milestone": milestone,
                        "growth_rate": yoy_growth,  # Store as multiplier (e.g., 2.0 for 100% growth)
                        "growth_rate_pct": int((yoy_growth - 1) * 100),  # Store percentage separately for display
                        "stage": stage,
                        "projection": scenarios_data['base'],  # Base scenario for backward compatibility
                        "scenarios": scenarios_data,  # All three scenarios
                        "already_achieved": current_arr >= target_arr,
                        "ownership_evolution": ownership_data  # Add ownership data
                    }
                
                # Create ONE slide with both companies with enhanced data
                # Determine appropriate title based on targets
                targets = [data['target'] for data in companies_100m_data.values()]
                if "$1B" in targets:
                    slide_title = "Path to Unicorn Status"
                    slide_subtitle = "Revenue growth to $1B valuation milestone"
                elif "$10B" in targets:
                    slide_title = "Path to Decacorn Status"
                    slide_subtitle = "Revenue growth to $10B valuation milestone"
                else:
                    slide_title = "Path to $100M ARR"
                    slide_subtitle = "Revenue growth trajectory and key milestones"
                
                # Build datasets for all scenarios
                datasets = []
                for idx, (company_name, data) in enumerate(companies_100m_data.items()):
                    # Neo-noir colors: neon green for first company, neon red for second
                    base_color = "rgba(0, 255, 159, 1)" if idx == 0 else "rgba(255, 71, 87, 1)"
                    light_color = "rgba(0, 255, 159, 0.5)" if idx == 0 else "rgba(255, 71, 87, 0.5)"
                    
                    # Base scenario - solid line with J-curve (higher tension for exponential growth curve)
                    datasets.append({
                        "label": f"{company_name} - Base ({data['growth_rate_pct']}% YoY)",
                        "data": data['scenarios']['base'],
                        "borderColor": base_color,
                        "backgroundColor": "transparent",
                        "fill": False,
                        "tension": 0.5,  # Increased for more pronounced J-curve
                        "pointRadius": 4,
                        "pointHoverRadius": 6,
                        "borderWidth": 2
                    })
                    
                    # Bull scenario - dashed line with lighter opacity (J-curve)
                    datasets.append({
                        "label": f"{company_name} - Bull",
                        "data": data['scenarios']['bull'],
                        "borderColor": light_color,
                        "backgroundColor": "transparent",
                        "fill": False,
                        "tension": 0.5,  # Increased for J-curve
                        "pointRadius": 2,
                        "pointHoverRadius": 4,
                        "borderDash": [5, 5],
                        "borderWidth": 1.5
                    })
                    
                    # Bear scenario - dashed line with lighter opacity (J-curve)
                    datasets.append({
                        "label": f"{company_name} - Bear",
                        "data": data['scenarios']['bear'],
                        "borderColor": light_color,
                        "backgroundColor": "transparent",
                        "fill": False,
                        "tension": 0.5,  # Increased for J-curve
                        "pointRadius": 2,
                        "pointHoverRadius": 4,
                        "borderDash": [2, 2],
                        "borderWidth": 1.5
                    })
                
                try:
                    # Format chart data properly for frontend
                    chart_data = format_line_chart(
                        labels=self._generate_date_labels(projection_years=6),
                        datasets=datasets,
                        title="ARR Growth Projection"
                    )
                    
                    add_slide("path_to_100m_comparison", {
                        "title": slide_title,
                        "subtitle": slide_subtitle,
                        "companies": companies_100m_data,
                        "chart_data": {
                            **chart_data,
                            "options": {
                            "responsive": True,
                            "maintainAspectRatio": False,
                            "scales": {
                                "y": {
                                    "type": "linear",
                                    # AUTO-SCALE Y-axis based on data
                                    "beginAtZero": False,  # Don't force zero, let it scale naturally
                                    "title": {
                                        "display": True,
                                        "text": "ARR ($M)",
                                        "font": {
                                            "size": 14,
                                            "weight": "bold"
                                        }
                                    },
                                    "ticks": {
                                        "font": {
                                            "size": 12
                                        },
                                        "stepSize": 25  # $25M steps (values already in millions)
                                    },
                                    "grid": {
                                        "display": True,
                                        "color": "rgba(0, 0, 0, 0.05)"
                                    }
                                },
                                "x": {
                                    "title": {
                                        "display": True,
                                        "text": "Date (Projected)",
                                        "font": {
                                            "size": 14,
                                            "weight": "bold"
                                        }
                                    },
                                    "ticks": {
                                        "font": {
                                            "size": 11
                                        }
                                    },
                                    "grid": {
                                        "display": False
                                    }
                                }
                            },
                            "plugins": {
                                "annotation": {
                                    "annotations": {
                                        "line100M": {
                                            "type": "line",
                                            "yMin": 100,
                                            "yMax": 100,
                                            "borderColor": "rgba(255, 99, 132, 0.5)",
                                            "borderWidth": 2,
                                            "borderDash": [5, 5],
                                            "label": {
                                                "content": "$100M ARR",
                                                "enabled": True,
                                                "position": "end"
                                            }
                                        },
                                        "line1B": {
                                            "type": "line",
                                            "yMin": 1000,
                                            "yMax": 1000,
                                            "borderColor": "rgba(255, 206, 86, 0.5)",
                                            "borderWidth": 2,
                                            "borderDash": [5, 5],
                                            "label": {
                                                "content": "$1B ARR",
                                                "enabled": True,
                                                "position": "end"
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    },
                        "insights": self._generate_path_to_100m_insights(companies_100m_data),
                        "metrics": {
                            company_name: {
                                "Current ARR": data['current_arr_formatted'],
                                "Target": data['target'],
                                "Time to Target": f"{data['years_to_target']} years" if not data['already_achieved'] else "Already achieved",
                                "Growth Rate": f"{data['growth_rate_pct']}% YoY",
                            "Stage": data['stage'],
                            # Add ownership evolution if available
                            **({
                                "Entry Ownership": f"{data['ownership_evolution']['entry_ownership']}%",
                                "Exit Ownership (No Follow-on)": f"{data['ownership_evolution']['exit_no_followon']}%"
                            } if data.get('ownership_evolution', {}).get('entry_ownership') else {})
                        }
                        for company_name, data in companies_100m_data.items()
                    }
                    })
                except Exception as e:
                    logger.error(f"[DECK_GEN] ❌ Path to 100M slide generation failed: {e}")
                    import traceback
                    logger.error(f"[DECK_GEN] ❌ Stack trace: {traceback.format_exc()}")
            
            # Slide 6: Business Analysis WITH TAM AND PRICING DATA
            if companies:
                companies_business_data = {}
                for company in companies[:2]:
                    # Use actual extracted data, not pattern matching
                    what_they_do = company.get('business_model', company.get('product_description', 'N/A'))
                    if what_they_do == 'N/A' or what_they_do in ['SaaS', 'Software', 'Technology', 'Tech']:
                        # Fallback to description if too generic
                        what_they_do = company.get('description', company.get('sector', 'Technology company'))
                    
                    # Use actual product description
                    what_they_sell = company.get('product_description', company.get('business_model', 'N/A'))
                    if what_they_sell == 'N/A' or what_they_sell in ['SaaS', 'Software']:
                        # If still generic, build from components
                        sector = company.get('sector', '').lower()
                        if 'ai' in sector or 'ml' in sector:
                            what_they_sell = f"AI/ML solutions for {company.get('target_market', 'businesses')}"
                        elif 'fintech' in sector:
                            what_they_sell = "Financial technology services"
                        else:
                            what_they_sell = company.get('product', 'Software solutions')
                    
                    # Use extracted customer segment data first
                    customer_segment = company.get('customer_segment', '')
                    geographies = company.get('geographies', [])
                    
                    # Build who_they_sell_to from extracted data
                    if customer_segment:
                        # Map segment to readable format
                        segment_map = {
                            'enterprise': 'Enterprise customers',
                            'mid-market': 'Mid-market companies',
                            'sme': 'SMBs and startups',
                            'mixed': 'Companies of all sizes'
                        }
                        who_they_sell_to = segment_map.get(customer_segment.lower(), customer_segment)
                        
                        # Add geography if available
                        if geographies:
                            geo_str = ', '.join(geographies[:2])  # First 2 regions
                            who_they_sell_to = f"{who_they_sell_to} in {geo_str}"
                    else:
                        # Fallback to target_market or customers
                        who_they_sell_to = company.get('target_market', company.get('target_customers', ''))
                        
                        if not who_they_sell_to:
                            # Use actual customer names if available
                            customers = company.get('customers', [])
                            if customers and isinstance(customers, list) and len(customers) > 0:
                                who_they_sell_to = ', '.join(customers[:3])  # Show first 3 customers
                            else:
                                # Only as last resort, use revenue-based guess
                                who_they_sell_to = "Various businesses"
                    
                    # Use actual pricing model data
                    pricing_model = company.get('pricing_model', 'N/A')
                    if pricing_model == 'N/A' or not pricing_model:
                        # Infer from business model with better specificity
                        if 'subscription' in what_they_do.lower() or 'saas' in what_they_do.lower():
                            pricing_model = 'Subscription (SaaS)'
                        elif 'transaction' in what_they_do.lower() or 'payment' in what_they_do.lower():
                            pricing_model = 'Transaction-based'
                        elif 'api' in what_they_do.lower() or 'usage' in what_they_do.lower():
                            pricing_model = 'Usage-based'
                        elif 'enterprise' in who_they_sell_to.lower():
                            pricing_model = 'Annual contracts'
                        else:
                            # Be specific about what we don't know
                            pricing_model = 'Pricing model unclear'
                    
                    # EXTRACT ACTUAL TAM DATA FROM MARKET_SIZE SERVICE (NOW DISABLED)
                    market_data = company.get('market_size', {}) or {}
                    tam_disabled = (
                        market_data.get('status') == 'tam_disabled'
                        or company.get('tam_processing_disabled')
                    )
                    
                    if tam_disabled:
                        tam = sam = som = 0
                        labor_tam = 0
                        labor_citation = ''
                        tam_reasoning = "TAM analysis disabled"
                    else:
                        tam = market_data.get('tam', company.get('tam', 0))
                        sam = market_data.get('sam', 0)
                        som = market_data.get('som', 0)
                        tam_source = market_data.get('source', '')
                        tam_citation = market_data.get('citation', '')
                        tam_methodology = market_data.get('methodology', '')
                        labor_tam = market_data.get('labor_tam', 0)
                        labor_citation = market_data.get('labor_citation', '')
                        
                        if tam_citation:
                            tam_reasoning = f"{tam_citation} ({tam_source})"
                        elif labor_citation and labor_tam > 0:
                            tam_reasoning = f"Labor TAM: {labor_citation}"
                        elif tam_methodology:
                            tam_reasoning = tam_methodology
                        else:
                            tam_reasoning = "Market size inferred from comparable companies"
                    
                    # Calculate ACV from actual data with proper type checking
                    revenue = self._get_field_with_fallback(company, 'revenue', 0)
                    
                    # Safely get customer count with multiple fallbacks
                    customers_data = company.get('customers')
                    if isinstance(customers_data, list):
                        customers_count = len(customers_data)
                    elif isinstance(customers_data, dict):
                        customers_count = safe_get_value(customers_data.get('count', 0), 0)
                    elif isinstance(customers_data, (int, float)):
                        customers_count = safe_get_value(customers_data, 0)
                    else:
                        customers_count = safe_get_value(company.get('customer_count', 100), 100)
                    
                    if customers_count == 0:
                        customers_count = 100  # Default assumption
                    
                    acv = revenue / customers_count if customers_count > 0 else 0
                    
                    # Generate ACV reasoning based on target market and business model
                    if acv > 0:
                        if acv >= 500000:
                            acv_source = "enterprise focus → high-touch sales"
                        elif acv >= 100000:
                            acv_source = "mid-market enterprise → field sales"
                        elif acv >= 30000:
                            acv_source = "SMB enterprise → inside sales"
                        elif acv >= 5000:
                            acv_source = "small business → self-serve + sales assist"
                        else:
                            acv_source = "consumer/prosumer → pure self-serve"
                    else:
                        # Infer from stage/business model
                        stage = company.get('stage', 'Series A')
                        if 'enterprise' in str(company.get('business_model', '')).lower():
                            acv_source = "enterprise model (inferred)"
                        elif stage in ['Series C', 'Series D']:
                            acv_source = "mature stage → likely enterprise"
                        else:
                            acv_source = "early stage → mixed model"
                    
                    # Get actual pricing tiers if available
                    pricing_tiers = company.get('pricing_tiers', [])
                    if not pricing_tiers and acv > 0:
                        # Estimate pricing tiers based on ACV
                        if acv > 100000:
                            pricing_tiers = ["Enterprise: $100K-500K/year", "Strategic: $500K+/year"]
                        elif acv > 20000:
                            pricing_tiers = ["Starter: $10K/year", "Growth: $25K/year", "Enterprise: $50K+/year"]
                        else:
                            pricing_tiers = ["Basic: $99/mo", "Pro: $299/mo", "Enterprise: Custom"]
                    
                    companies_business_data[company.get('company', 'Unknown')] = {
                        "what_they_do": what_they_do,
                        "what_they_sell": what_they_sell,
                        "who_they_sell_to": who_they_sell_to,
                        "sector": company.get('sector', 'Technology'),
                        "founded": company.get('founded_year', 'Unknown'),
                        "team_size": company.get('team_size', 'Unknown'),
                        "pricing_model": pricing_model,
                        "pricing_tiers": pricing_tiers,
                        "revenue": revenue,
                        "gross_margin": safe_get_value(company.get('gross_margin', company.get('inferred_gross_margin', 0.75))),
                        "gm_source": self._generate_gross_margin_reasoning(company),
                        "acv_source": acv_source,
                        "tam": tam,
                        "sam": sam,
                        "som": som,
                        "tam_reasoning": tam_reasoning,  # Add TAM reasoning
                        "labor_tam": labor_tam,  # Add labor TAM if applicable
                        "acv": acv,
                        "customer_count": customers_count,
                        "customer_type": company.get('customer_type', 'B2B'),
                        "geography": company.get('geography', 'North America'),
                        "market_position": company.get('market_position', 'Emerging player')
                    }
                
                # Create ONE slide with both companies side-by-side WITH ENHANCED DATA
                try:
                    # Generate bullets for frontend rendering with formatted business model
                    bullets = []
                    for company_name, data in companies_business_data.items():
                        # Find the corresponding company dict to get business model details
                        company = next((c for c in companies if c.get('company') == company_name), None)
                        
                        if company:
                            # Use formatted business model bullets
                            business_bullets = self._format_business_model_bullets(company)
                            bullets.extend([f"{company_name}: {b.replace('• ', '')}" for b in business_bullets])
                        
                        tam = data.get('tam', 0)
                        tam_reasoning = data.get('tam_reasoning', '')
                        pricing_model = data.get('pricing_model', '')
                        acv = data.get('acv', 0)
                        revenue = data.get('revenue', 0)
                        
                        if tam > 0 and tam_reasoning != "TAM analysis disabled":
                            bullets.append(f"{company_name}: TAM of {self._format_money(tam)}")
                        if pricing_model and not any('Pricing' in b for b in bullets):
                            bullets.append(f"{company_name}: {pricing_model} pricing model")
                        if acv > 0:
                            bullets.append(f"{company_name}: ACV of {self._format_money(acv)}")
                        if revenue > 0:
                            bullets.append(f"{company_name}: Current revenue of {self._format_money(revenue)}")
                    
                    add_slide("business_analysis_comparison", {
                        "title": "Business Analysis & Market Opportunity",
                        "subtitle": "Product, pricing, and TAM analysis",
                        "companies": companies_business_data,
                        "bullets": bullets,
                        "metrics_comparison": {
                            company_name: {
                                "TAM": (
                                    "TAM analysis disabled"
                                    if data.get('tam_reasoning') == "TAM analysis disabled"
                                    else (
                                        f"{self._format_money(data['tam'])} - {data.get('tam_reasoning', '')}"
                                        if data.get('tam') and data['tam'] > 0
                                        else ""
                                    )
                                ),
                                "Labor TAM": (
                                    ""
                                    if data.get('tam_reasoning') == "TAM analysis disabled"
                                    else (
                                        f"{self._format_money(data['labor_tam'])} - Labor replacement opportunity"
                                        if data.get('labor_tam') and data['labor_tam'] > 0
                                        else ""
                                    )
                                ),
                                "SAM": (
                                    ""
                                    if data.get('tam_reasoning') == "TAM analysis disabled"
                                    else (self._format_money(data['sam']) if data.get('sam') and data['sam'] > 0 else "")
                                ),
                                "SOM": (
                                    ""
                                    if data.get('tam_reasoning') == "TAM analysis disabled"
                                    else (self._format_money(data['som']) if data.get('som') and data['som'] > 0 else "")
                                ),
                                "Pricing": f"{data.get('pricing_model', '')} - {', '.join(data.get('pricing_tiers', [])[:2])}" if data.get('pricing_model') else "",
                                "ACV": f"{self._format_money(data['acv'])} ({data.get('acv_source', 'inferred from pricing')})" if data.get('acv') and data['acv'] > 0 else "",
                                "Customers": f"{data['customer_count']:,}" if data.get('customer_count') and data['customer_count'] > 0 else "",
                                "Gross Margin": f"{data['gross_margin']*100:.0f}% ({data.get('gm_source', 'category benchmark')})"
                            }
                            for company_name, data in companies_business_data.items()
                        }
                    })
                except Exception as e:
                    logger.error(f"[DECK_GEN] ❌ Error creating business analysis comparison slide: {e}")
                    import traceback
                    logger.error(f"[DECK_GEN] ❌ Stack trace: {traceback.format_exc()}")
                    # Create fallback slide with simplified data to maintain presentation integrity
                    try:
                        # Build simplified metrics comparison as fallback
                        simplified_metrics = {}
                        for company_name, data in companies_business_data.items():
                            simplified_metrics[company_name] = {
                                "TAM": self._format_money(data.get('tam', 0)) if data.get('tam', 0) > 0 else "N/A",
                                "Revenue": self._format_money(data.get('revenue', 0)) if data.get('revenue', 0) > 0 else "N/A",
                                "Pricing": data.get('pricing_model', 'N/A'),
                                "Gross Margin": f"{data.get('gross_margin', 0)*100:.0f}%" if data.get('gross_margin') else "N/A"
                            }
                        
                        add_slide("business_analysis_comparison", {
                            "title": "Business Analysis & Market Opportunity",
                            "subtitle": "Product, pricing, and TAM analysis (simplified due to data processing error)",
                            "companies": companies_business_data,
                            "metrics_comparison": simplified_metrics,
                            "error_note": f"Full metrics unavailable: {str(e)[:100]}"
                        })
                    except Exception as fallback_error:
                        # If even fallback fails, let add_slide's internal error handling create an error slide
                        logger.error(f"[DECK_GEN] ❌ Fallback slide creation also failed: {fallback_error}")
                        # add_slide will handle this and create an error slide
                        add_slide("summary", {
                            "title": "Business Analysis - Generation Error",
                            "subtitle": "This slide could not be fully generated",
                            "bullets": [
                                f"Error: {str(e)[:200]}",
                                "Simplified company data may be available in other slides"
                            ]
                        })
                
                # Add Competitive Landscape & Risk Analysis slide (NEW - CRITICAL)
                competitive_landscape_data = await self._generate_competitive_landscape_analysis(companies, companies_business_data)
                if competitive_landscape_data:
                    add_slide("competitive_landscape", competitive_landscape_data)
            
            # Comparison slide WITH CHART DATA using ComprehensiveDealAnalyzer
            if len(companies) > 1:
                # Use ComprehensiveDealAnalyzer for detailed company comparison
                deal_comparisons = []
                metrics_analyses = []  # Store comprehensive metrics analysis for each company
                
                for company in companies:
                    # Get fund context for analysis
                    fund_context = self.shared_data.get('fund_context', {})
                    investment_amount = fund_context.get('investment_amount', 5_000_000)
                    fund_size = fund_context.get('fund_size', 200_000_000)
                    logger.info(f"[DECK_GEN] Using fund_size: ${fund_size/1e6:.0f}M")
                    
                    # Analyze deal using ComprehensiveDealAnalyzer
                    deal_analysis = await self.comprehensive_deal_analyzer.analyze_deal(
                        company_data=company,
                        investment_amount=investment_amount,
                        fund_size=fund_size
                    )
                    deal_comparisons.append(deal_analysis)
                    
                    # Generate comprehensive metrics analysis with causal reasoning
                    try:
                        metrics_analysis = await self._generate_complete_metrics_analysis(company)
                        metrics_analyses.append(metrics_analysis)
                        logger.info(f"[DECK_GEN] Generated comprehensive metrics analysis for {company.get('company')}")
                    except Exception as e:
                        logger.error(f"[DECK_GEN] Failed to generate metrics analysis: {e}")
                        metrics_analyses.append(None)
                
                add_slide("comparison", {
                        "title": "Key Metrics & Business Health",
                        "companies": [{
                            "name": deal.company_name,
                            "valuation": companies[i].get("valuation", 0) if i < len(companies) else 0,
                            "revenue": self._get_revenue_safe(companies[i]) if i < len(companies) else 0,
                            "stage": companies[i].get("stage") if i < len(companies) else "Unknown",
                            "fund_fit_score": deal.fund_fit_score,
                            "fund_fit_bullets": self._format_fund_fit_bullets(deal, companies[i]) if i < len(companies) else [],
                            "ai_category": deal.ai_category,
                            "momentum_score": deal.momentum_score,
                            "venture_scale_potential": deal.venture_scale_potential,
                            "expected_return": deal.return_multiples.get('probability_weighted', 0),
                            "ownership_target": deal.ownership_target,
                            "down_round_risk": deal.down_round_risk,
                            "agent_washing_risk": deal.agent_washing_risk,
                            
                            # Business model data (only use fields that actually exist)
                            "business_model": {
                                "what_they_do": companies[i].get('business_model', '') if i < len(companies) else "",
                                "what_they_sell": companies[i].get('product_description', '') if i < len(companies) else "",
                                "who_they_sell_to": companies[i].get('target_market', '') if i < len(companies) else "",
                                "unit_of_work": companies[i].get('unit_economics', {}).get('unit_of_work', '') if i < len(companies) else "",
                                "pricing_model": companies[i].get('pricing_model', '') if i < len(companies) else ""
                            },
                            
                            # Enhanced metrics with comprehensive analysis
                            "metrics": (
                                # Use the comprehensive analysis if available
                                metrics_analyses[i] if i < len(metrics_analyses) and metrics_analyses[i] else {
                                    # Fallback to basic calculations if analysis failed
                                    "acv": {
                                        "value": self._calculate_acv(companies[i]) if i < len(companies) else 0,
                                        "rationale": self._generate_acv_reasoning(companies[i]) if i < len(companies) else "",
                                        "health": "unknown",
                                        "why_pricing": "Analysis unavailable"
                                    },
                                    "gross_margin": {
                                        "value": safe_get_value(companies[i].get("gross_margin", companies[i].get("inferred_gross_margin", 0.7))) if i < len(companies) else 0.7,
                                        "health": "unknown",
                                        "why": self._generate_gross_margin_reasoning(companies[i]) if i < len(companies) else "",
                                        "cost_breakdown": "GPU costs and infrastructure"
                                    },
                                    "ltv_cac": {
                                        "value": companies[i].get("ltv_cac_ratio", 3.0) if i < len(companies) else 3.0,
                                        "payback_months": companies[i].get('cac_payback_months', 18) if i < len(companies) else 18,
                                        "health": "unknown",
                                        "why_cac": "Sales model analysis needed",
                                        "why_ltv": "Retention analysis needed"
                                    },
                                    "burn_multiple": {
                                        "value": self._calculate_burn_multiple(companies[i]) if i < len(companies) else 0,
                                        "health": "unknown",
                                        "why": "Burn analysis needed"
                                    },
                                    "rule_of_40": {
                                        "value": self._calculate_rule_of_40(companies[i]) if i < len(companies) else 0,
                                        "health": "unknown",
                                        "why_growth": "Growth driver analysis needed",
                                        "why_margin": "Margin analysis needed"
                                    },
                                    "synthesis": {
                                        "story": "Metrics calculated but causal analysis unavailable",
                                        "primary_issue": "Requires deeper analysis",
                                        "key_strength": "Data available for calculation"
                                    }
                                }
                            ),
                            
                            # Add funding pattern analysis
                            "funding_pattern": self._analyze_funding_pattern(companies[i]) if i < len(companies) else {
                                "pattern": "Unknown",
                                "health": "unknown",
                                "capital_efficiency": 0,
                                "avg_months_between": 18,
                                "avg_size_multiple": 2.5,
                                "months_since_last": 12,
                                "insight": "Funding analysis unavailable"
                            }
                        } for i, deal in enumerate(deal_comparisons)],
                        
                        # Generate comprehensive analysis for all companies in one call
                        "comprehensive_analysis_raw": await self._generate_comprehensive_business_analysis(companies),
                        "chart_data": {
                            **format_bar_chart(
                                labels=["ACV ($K)", "LTV/CAC", "Gross Margin %", "YoY Growth %"],
                                datasets=[
                                    {
                                        "label": companies[0].get("company", "Company 1") if companies else "Company 1",
                                        "data": [
                                            # ACV (Average Contract Value in thousands)
                                            (companies[0].get("acv", self._get_revenue_safe(companies[0]) / max(len(companies[0].get("customers", [])) if isinstance(companies[0].get("customers"), list) else safe_get_value(companies[0].get("customers", 100)), 1)) / 1000) if companies else 50,
                                            # LTV/CAC ratio
                                            companies[0].get("ltv_cac_ratio", 3.0) if companies else 3.0,
                                            # Gross Margin % - use actual or inferred
                                            (safe_get_value(companies[0].get("gross_margin", companies[0].get("inferred_gross_margin", 0.7))) * 100) if companies else 70,
                                            # YoY Growth %
                                            (safe_get_value(companies[0].get("revenue_growth", 0.5)) * 100) if companies else 50
                                        ],
                                        "backgroundColor": "rgba(59, 130, 246, 0.9)"
                                    },
                                    {
                                        "label": companies[1].get("company", "Company 2") if len(companies) > 1 else "Company 2",
                                        "data": [
                                            # ACV (Average Contract Value in thousands)
                                            (companies[1].get("acv", self._get_revenue_safe(companies[1]) / max(len(companies[1].get("customers", [])) if isinstance(companies[1].get("customers"), list) else safe_get_value(companies[1].get("customers", 100)), 1)) / 1000) if len(companies) > 1 else 35,
                                            # LTV/CAC ratio
                                            companies[1].get("ltv_cac_ratio", 2.5) if len(companies) > 1 else 2.5,
                                            # Gross Margin % - use actual or inferred
                                            (safe_get_value(companies[1].get("gross_margin", companies[1].get("inferred_gross_margin", 0.65))) * 100) if len(companies) > 1 else 65,
                                            # YoY Growth %
                                            (safe_get_value(companies[1].get("revenue_growth", 0.5)) * 100) if len(companies) > 1 else 45
                                        ],
                                        "backgroundColor": "rgba(16, 185, 129, 0.9)"
                                    }
                                ],
                                title="Key Business Metrics Comparison"
                            ),
                            "options": {
                                "scales": {
                                    "y": {
                                        "beginAtZero": True,
                                        "title": {
                                            "display": True,
                                            "text": "Value"
                                        }
                                    }
                                },
                                "plugins": {
                                    "tooltip": {
                                        "enabled": True
                                    }
                                }
                            }
                        },
                        
                        # Comparative investment thesis
                        "comparative_analysis": await self._generate_investment_thesis_comparison(companies) if len(companies) >= 2 else {},
                        "comparative_analysis_bullets": self._format_comparative_analysis_to_bullets(
                            await self._generate_investment_thesis_comparison(companies) if len(companies) >= 2 else {}
                        ),
                        
                        "deal_analysis": deal_comparisons,
                        
                        # Extract charts from ComprehensiveDealAnalyzer
                        "deal_charts": self._extract_deal_charts(deal_comparisons)
                })
                
                # Add formatted analysis bullets as separate content
                comprehensive_analysis = await self._generate_comprehensive_business_analysis(companies)
                comparative_analysis = await self._generate_investment_thesis_comparison(companies) if len(companies) >= 2 else {}
                
                analysis_bullets = self._format_analysis_to_bullets(comprehensive_analysis)
                comparative_bullets = self._format_comparative_analysis_to_bullets(comparative_analysis)
                
                # Add fund fit slide if we have fund fit data
                fund_fit_bullets_all = []
                for i, deal in enumerate(deal_comparisons):
                    if i < len(companies):
                        fund_fit_bullets = self._format_fund_fit_bullets(deal, companies[i])
                        if fund_fit_bullets:
                            fund_fit_bullets_all.extend([
                                f"{deal.company_name if hasattr(deal, 'company_name') else companies[i].get('company', 'Unknown')}: {b.replace('• ', '')}"
                                for b in fund_fit_bullets
                            ])
                
                if fund_fit_bullets_all:
                    add_slide("fund_fit_analysis", {
                        "title": "Fund Fit Analysis",
                        "subtitle": "Investment alignment and recommendation",
                        "bullets": fund_fit_bullets_all
                    })
                
                # Add analysis summary slide with formatted bullets
                if analysis_bullets or comparative_bullets:
                    all_analysis_bullets = analysis_bullets + comparative_bullets
                    if all_analysis_bullets:
                        add_slide("analysis_summary", {
                            "title": "Investment Analysis Summary",
                            "subtitle": "Key insights and recommendations",
                            "bullets": all_analysis_bullets[:15]  # Limit to top 15 bullets
                        })
            
                # Add CAP TABLE WATERFALL CHART with REAL DATA
                # CRITICAL: Always generate cap table slides - ensure they appear before exit scenarios
                cap_tables_shared = self.shared_data.get("cap_tables", {})
                
                # Ensure we have companies to process
                if not companies or len(companies) == 0:
                    logger.warning("[CAP_TABLE] No companies available for cap table generation")
                else:
                    logger.info(f"[CAP_TABLE] Generating cap table slides for {len(companies[:2])} companies")
                
                for idx, company in enumerate(companies[:2]):  # Do for first 2 companies
                    company_name = company.get("company", "Example Company")
                    logger.info(f"[CAP_TABLE] Processing cap table for company {idx + 1}: {company_name}")
                    
                    # Generate comprehensive cap table data using PrePostCapTable service
                    cap_table_data = None
                    if cap_tables_shared:
                        cap_table_data = cap_tables_shared.get(company_name)
                        if not cap_table_data:
                            for shared_name, shared_data in cap_tables_shared.items():
                                if isinstance(shared_name, str) and shared_name.lower() == company_name.lower():
                                    cap_table_data = shared_data
                                    break
                    waterfall_labels = []
                    waterfall_values = []
                    sankey_data = None

                    if not cap_table_data and self.cap_table_service:
                        try:
                            # ROOT CAUSE FIX: Ensure funding rounds are detected/extracted before building
                            # If funding_rounds is empty, try to extract from other company data
                            if not company.get('funding_rounds') or (isinstance(company.get('funding_rounds'), list) and len(company.get('funding_rounds', [])) == 0):
                                logger.warning(f"[CAP_TABLE] No funding_rounds found for {company_name} - attempting to extract from company data")
                                # Try to extract from total_funding, valuation, stage
                                total_funding = ensure_numeric(company.get('total_funding'), 0)
                                valuation = ensure_numeric(company.get('valuation') or company.get('inferred_valuation'), 0)
                                stage = company.get('stage', 'Series A')
                                
                                if total_funding > 0 or valuation > 0:
                                    # Create a round from available data
                                    inferred_round = {
                                        'round': stage if stage else 'Series A',
                                        'amount': total_funding if total_funding > 0 else valuation * 0.2,  # Assume 20% dilution
                                        'valuation': valuation if valuation > 0 else total_funding * 3,
                                        'pre_money_valuation': valuation * 0.8 if valuation > 0 else total_funding * 2.4,
                                        'inferred': True
                                    }
                                    company['funding_rounds'] = [inferred_round]
                                    logger.info(f"[CAP_TABLE] Created inferred round for {company_name}: {stage}, ${inferred_round['amount']/1e6:.1f}M")
                            
                            # Build comprehensive funding rounds using _build_funding_rounds_with_inference
                            # This ensures inferred rounds are properly constructed before PrePostCapTable calculation
                            rounds, inferred_added = self._build_funding_rounds_with_inference(company)
                            
                            # ROOT CAUSE FIX: If still no rounds after inference, create basic rounds from stage/valuation
                            if not rounds or len(rounds) == 0:
                                logger.warning(f"[CAP_TABLE] No rounds after inference for {company_name} - creating from stage/valuation")
                                total_funding = ensure_numeric(company.get('total_funding'), 0)
                                valuation = ensure_numeric(company.get('valuation') or company.get('inferred_valuation'), 0)
                                stage = company.get('stage', 'Series A')
                                
                                if valuation > 0 or total_funding > 0:
                                    # Create a basic round structure
                                    basic_round = {
                                        'round': stage,
                                        'amount': total_funding if total_funding > 0 else max(valuation * 0.2, 5_000_000),
                                        'valuation': valuation if valuation > 0 else total_funding * 3 if total_funding > 0 else 50_000_000,
                                        'pre_money_valuation': valuation * 0.8 if valuation > 0 else total_funding * 2.4 if total_funding > 0 else 40_000_000,
                                        'date': None,
                                        'investors': [],
                                        'inferred': True
                                    }
                                    rounds = [basic_round]
                                    inferred_added += 1
                                    logger.info(f"[CAP_TABLE] Created basic round for {company_name}: {stage}, ${basic_round['amount']/1e6:.1f}M")
                            
                            if inferred_added:
                                logger.info(f"[CAP_TABLE] Added {inferred_added} inferred rounds for {company_name}")
                            
                            # Enhance rounds with liquidation preferences if we have search results
                            if rounds and company.get('search_results'):
                                search_content = company.get('search_results', '')
                                enhanced_rounds = self.gap_filler.extract_liquidation_preferences(
                                    rounds, search_content
                                )
                                # Store liquidation preferences for visualization
                                company['liquidation_preferences'] = enhanced_rounds
                                rounds = enhanced_rounds
                            
                            stage = company.get('stage', 'Series A')
                            
                            # ROOT CAUSE FIX: Validate rounds have amount > 0 before calling service
                            valid_rounds = [r for r in rounds if r.get('amount', 0) > 0]
                            if not valid_rounds and rounds:
                                logger.warning(f"[CAP_TABLE] All rounds filtered out for {company_name} - rounds had no amount")
                            
                            # Call the service with proper data structure
                            # ROOT CAUSE FIX: Always call cap table service if we have any rounds (even inferred)
                            if valid_rounds:
                                logger.info(f"[CAP_TABLE] Calling cap table service for {company_name} with {len(rounds)} rounds (inferred: {inferred_added})")
                                try:
                                    cap_table_data = self.cap_table_service.calculate_full_cap_table_history(
                                        company_data={'funding_rounds': rounds, 'company': company_name, 'stage': stage}
                                    )
                                    # ROOT CAUSE FIX: Ensure current_cap_table is always populated
                                    if cap_table_data:
                                        if 'current_cap_table' not in cap_table_data or not cap_table_data.get('current_cap_table'):
                                            logger.warning(f"[CAP_TABLE] Service returned data without current_cap_table for {company_name} - extracting from history")
                                            # Extract current cap table from last snapshot in history
                                            if cap_table_data.get('history') and len(cap_table_data['history']) > 0:
                                                last_snapshot = cap_table_data['history'][-1]
                                                cap_table_data['current_cap_table'] = last_snapshot.get('post_money_ownership', {})
                                                logger.info(f"[CAP_TABLE] Extracted current_cap_table from history: {len(cap_table_data['current_cap_table'])} stakeholders")
                                    else:
                                        logger.error(f"[CAP_TABLE] Service returned None for {company_name}")
                                        cap_table_data = {"history": [], "current_cap_table": {}, "ownership_evolution": {}}
                                except Exception as e:
                                    logger.error(f"[CAP_TABLE] Cap table calculation failed for {company_name}: {e}", exc_info=True)
                                    cap_table_data = {"history": [], "current_cap_table": {}, "ownership_evolution": {}}
                                
                                # ROOT CAUSE FIX: Validate service returns current_cap_table with data
                                if cap_table_data and cap_table_data.get('current_cap_table'):
                                    current_cap = cap_table_data.get('current_cap_table', {})
                                    if not current_cap or len(current_cap) == 0:
                                        logger.warning(f"[CAP_TABLE] Service returned empty current_cap_table for {company_name} - building from rounds")
                                        # Build from rounds: extract founders/investors from round data
                                        cap_table_data['current_cap_table'] = self._build_cap_table_from_rounds(company, rounds)
                                    else:
                                        logger.info(f"[CAP_TABLE] ✅ Service returned data with current_cap_table: {len(current_cap)} stakeholders")
                                elif cap_table_data:
                                    logger.warning(f"[CAP_TABLE] Service returned data but no current_cap_table: keys={list(cap_table_data.keys())}")
                                    # Build from rounds: extract founders/investors from round data
                                    cap_table_data['current_cap_table'] = self._build_cap_table_from_rounds(company, rounds)
                                else:
                                    logger.error(f"[CAP_TABLE] Service returned no data for {company_name}")
                                    # Build from rounds if we have rounds
                                    if rounds:
                                        cap_table_data = {"history": [], "current_cap_table": self._build_cap_table_from_rounds(company, rounds), "ownership_evolution": {}}
                            else:
                                logger.warning(f"[CAP_TABLE] No funding rounds available for {company_name}")
                                cap_table_data = {"history": [], "current_cap_table": {}, "ownership_evolution": {}}
                                    
                        except Exception as e:
                            logger.error(f"[CAP_TABLE] Could not generate cap table for {company_name}: {e}", exc_info=True)
                    
                    if cap_table_data and not waterfall_labels and 'waterfall_data' in cap_table_data:
                        try:
                            waterfall_raw = cap_table_data.get('waterfall_data', [])
                            for item in waterfall_raw:
                                if isinstance(item, dict):
                                    waterfall_labels.append(item.get('name', ''))
                                    waterfall_values.append(item.get('value', 0))
                        except Exception as err:
                            logger.warning(f"[CAP_TABLE] Failed to parse waterfall data for {company_name}: {err}")

                    if cap_table_data and not sankey_data and 'sankey_data' in cap_table_data:
                        sankey_data = cap_table_data.get('sankey_data')

                    # Use real waterfall data if available from service
                    if waterfall_labels and waterfall_values:
                        # We have proper waterfall data from the service
                        labels = waterfall_labels
                        waterfall_data = waterfall_values
                        
                        # Calculate final founder ownership from waterfall
                        founder_pct = 100
                        for val in waterfall_values:
                            if val < 0:  # Dilution (negative values)
                                founder_pct += val  # val is already negative, so this subtracts
                        
                        bullets = [
                            f"Initial founder ownership: 100%",
                            f"Current founder ownership: {founder_pct:.1f}%",
                            f"Total dilution: {100 - founder_pct:.1f}%",
                            f"Number of funding rounds: {len([l for l in labels if 'Dilution' in l])}"
                        ]
                        
                        # Add investor names to bullets if available
                        if investor_names_list:
                            bullets.append(f"Key investors: {', '.join(investor_names_list[:5])}")  # Show top 5 investors
                        
                        # Get ACTUAL investor and employee ownership from cap table
                        # Don't assume 100 - founders = investors (that ignores employees!)
                        investor_pct = 0
                        employee_pct = 0
                        investor_names_list = []  # Extract actual investor names
                        
                        if cap_table_data and 'current_cap_table' in cap_table_data:
                            current_cap = cap_table_data['current_cap_table']
                            # Extract investor names and ownership
                            for owner, pct in current_cap.items():
                                owner_str = str(owner)
                                pct_float = float(pct) if isinstance(pct, (int, float)) else 0
                                
                                # Check if this is an investor (not founder, not employee)
                                is_investor = (
                                    ('Investor' in owner_str or 'Series' in owner_str or 'Seed' in owner_str or 'Round' in owner_str) 
                                    and 'Founder' not in owner_str 
                                    and 'Employee' not in owner_str 
                                    and 'Option' not in owner_str 
                                    and 'ESOP' not in owner_str
                                    and pct_float > 0.1  # Only include significant ownership
                                )
                                
                                if is_investor:
                                    investor_pct += pct_float
                                    # Extract clean investor name (remove round labels)
                                    clean_name = owner_str.replace(' (Lead)', '').replace(' (SAFE)', '').strip()
                                    if clean_name not in investor_names_list:
                                        investor_names_list.append(f"{clean_name} ({pct_float:.1f}%)")
                                
                                # Sum employee ownership
                                if 'Employee' in owner_str or 'Option' in owner_str or 'ESOP' in owner_str:
                                    employee_pct += pct_float
                        
                        # Validate total doesn't exceed 100%
                        total_ownership = founder_pct + investor_pct + employee_pct
                        if total_ownership > 100:
                            logger.error(f"[OWNERSHIP_BUG] Total ownership is {total_ownership:.1f}% (impossible!) - normalizing")
                            # Normalize
                            scale_factor = 100 / total_ownership
                            founder_pct *= scale_factor
                            investor_pct *= scale_factor
                            employee_pct *= scale_factor
                        
                        metrics = {
                            "Founder Ownership": f"{founder_pct:.1f}%",
                            "Investor Ownership": f"{investor_pct:.1f}%",
                            "Employee Ownership": f"{employee_pct:.1f}%",
                            "Last Round Valuation": self._format_money(company.get('valuation', 0)),
                            "Total Raised": self._format_money(company.get('total_funding', 0))
                        }
                        
                    elif cap_table_data and 'history' in cap_table_data and len(cap_table_data['history']) > 0:
                        # Build waterfall from cap table history
                        labels = ["Initial (100%)"]
                        waterfall_data = []
                        
                        prev_founder_pct = 100
                        for snapshot in cap_table_data['history']:
                            round_name = snapshot['round_name']
                            # Sum up all founder/co-founder ownership
                            founder_pct = sum(float(pct) for owner, pct in snapshot['post_money_ownership'].items() 
                                            if 'Founder' in str(owner) or 'founder' in str(owner).lower())
                            
                            dilution = founder_pct - prev_founder_pct
                            labels.append(f"{round_name} ({founder_pct:.0f}%)")
                            waterfall_data.append(dilution)
                            prev_founder_pct = founder_pct
                        
                        # Add final state
                        labels.append(f"Current ({prev_founder_pct:.0f}%)")
                        waterfall_data.append(0)  # No change for final
                        
                        bullets = [
                            f"Initial founder ownership: 100%",
                            f"Current founder ownership: {prev_founder_pct:.1f}%",
                            f"Total dilution: {100 - prev_founder_pct:.1f}%",
                            f"Number of funding rounds: {len(cap_table_data['history'])}"
                        ]
                        
                        metrics = {
                            "Founder Ownership": f"{prev_founder_pct:.1f}%",
                            "Investor Ownership": f"{100 - prev_founder_pct:.1f}%",
                            "Last Round Valuation": self._format_money(company.get('valuation', 0)),
                            "Total Raised": self._format_money(company.get('total_funding', 0))
                        }
                        founder_pct = prev_founder_pct
                    else:
                        # Enhanced intelligent defaults with realistic dilution progression
                        stage = company.get('stage', 'Series A')
                        stage_dilution = {
                            'Seed': {
                                'founders': 85, 
                                'labels': ["Initial (100%)", "Seed (85%)", "Current (85%)"], 
                                'data': [100, -15, 0]
                            },
                            'Series A': {
                                'founders': 68, 
                                'labels': ["Initial (100%)", "Seed (85%)", "Series A (68%)", "Current (68%)"], 
                                'data': [100, -15, -17, 0]
                            },
                            'Series B': {
                                'founders': 51, 
                                'labels': ["Initial (100%)", "Seed (85%)", "Series A (68%)", "Series B (51%)", "Current (51%)"], 
                                'data': [100, -15, -17, -17, 0]
                            },
                            'Series C': {
                                'founders': 38, 
                                'labels': ["Initial (100%)", "Seed (90%)", "Series A (72%)", "Series B (54%)", "Series C (38%)", "Current (38%)"], 
                                'data': [100, -10, -18, -18, -16, 0]
                            },
                            'Series D': {
                                'founders': 28,
                                'labels': ["Initial (100%)", "Seed (90%)", "A (72%)", "B (54%)", "C (38%)", "D (28%)", "Current (28%)"],
                                'data': [100, -10, -18, -18, -16, -10, 0]
                            },
                            'Series E': {
                                'founders': 22,
                                'labels': ["Initial (100%)", "Seed (90%)", "A (72%)", "B (54%)", "C (38%)", "D (28%)", "E (22%)", "Current (22%)"],
                                'data': [100, -10, -18, -18, -16, -10, -6, 0]
                            }
                        }
                        
                        # Handle variations in stage naming
                        stage_key = stage
                        for key in stage_dilution.keys():
                            if key in stage:
                                stage_key = key
                                break
                        
                        dilution_info = stage_dilution.get(stage_key, stage_dilution['Series A'])
                        labels = dilution_info['labels']
                        waterfall_data = dilution_info['data']
                        founder_pct = dilution_info['founders']
                        
                        bullets = [
                            f"Initial founder ownership: 100%",
                            f"Estimated current founder ownership: {founder_pct}%",
                            f"Total estimated dilution: {100 - founder_pct}%",
                            f"Current stage: {stage}"
                        ]
                        
                        metrics = {
                            "Founder Ownership": f"~{founder_pct}%",
                            "Investor Ownership": f"~{100 - founder_pct}%",
                            "Valuation": self._format_money(company.get('valuation', 0)),
                            "Total Funding": self._format_money(company.get('total_funding', 0))
                        }
                    
                    chart_payload = None
                    if cap_table_data:
                        try:
                            chart_payload = self._build_cap_table_chart_from_history(cap_table_data)
                        except Exception as err:
                            logger.warning(f"[CAP_TABLE] Failed to build chart from history for {company_name}: {err}")

                    if not chart_payload:
                        chart_payload = {
                            "labels": self._get_cap_table_labels(company),
                            "datasets": self._create_proper_cap_table_datasets(company)
                        }

                    # Create the slide with forward-looking cap table evolution
                    # Extract CURRENT ownership snapshot for pie chart from actual cap table data
                    current_ownership_labels = []
                    current_ownership_values = []
                    current_ownership_colors = []
                    current_cap_table = {}  # Initialize to empty dict
                    
                    # Priority 1: Use actual current_cap_table from cap_table_data (includes inferred rounds from PrePostCapTable)
                    if cap_table_data and cap_table_data.get("current_cap_table"):
                        current_cap_table = cap_table_data.get("current_cap_table", {})
                        logger.info(f"[CAP_TABLE] Found current_cap_table from PrePostCapTable for {company_name} with {len(current_cap_table)} stakeholders")
                        # Color mapping for stakeholders
                        stakeholder_colors = {
                            'Founders': 'rgba(59, 130, 246, 0.9)',
                            'Employees': 'rgba(251, 146, 60, 0.9)',
                            'Employee Pool': 'rgba(251, 146, 60, 0.9)',
                            'ESOP': 'rgba(251, 146, 60, 0.9)',
                            'Our Fund': 'rgba(16, 185, 129, 0.9)',
                            'Our Investment': 'rgba(16, 185, 129, 0.9)',
                        }
                        default_colors = [
                            'rgba(59, 130, 246, 0.9)',   # Blue
                            'rgba(251, 146, 60, 0.9)',   # Orange
                            'rgba(16, 185, 129, 0.9)',   # Green
                            'rgba(239, 68, 68, 0.9)',    # Red
                            'rgba(139, 92, 246, 0.9)',   # Purple
                            'rgba(236, 72, 153, 0.9)',   # Pink
                            'rgba(156, 163, 175, 0.9)',  # Gray
                        ]
                        
                        # Sort stakeholders by ownership percentage (descending)
                        sorted_stakeholders = sorted(
                            current_cap_table.items(),
                            key=lambda x: x[1] if isinstance(x[1], (int, float)) else 0,
                            reverse=True
                        )
                        
                        for idx, (stakeholder, pct) in enumerate(sorted_stakeholders):
                            if isinstance(pct, (int, float)) and pct > 0.1:  # Only show >0.1%
                                current_ownership_labels.append(stakeholder)
                                # Round to 1 decimal place for percentage formatting
                                current_ownership_values.append(round(float(pct), 1))
                                # Use stakeholder-specific color or default
                                color = stakeholder_colors.get(stakeholder) or default_colors[idx % len(default_colors)]
                                current_ownership_colors.append(color)
                        
                        logger.info(f"[CAP_TABLE] Extracted {len(current_ownership_labels)} stakeholders for pie chart from current_cap_table: {', '.join(current_ownership_labels[:5])}")
                    
                    # Priority 2: Extract from chart_payload (line chart data) if no direct cap table
                    elif chart_payload and chart_payload.get("labels") and chart_payload.get("datasets"):
                        # Get the last datapoint from each dataset (current/exit ownership)
                        last_index = len(chart_payload["labels"]) - 1
                        for dataset in chart_payload["datasets"]:
                            stakeholder = dataset.get("label", "")
                            value = dataset.get("data", [])[last_index] if last_index < len(dataset.get("data", [])) else 0
                            if value > 0.1:  # Only show stakeholders with >0.1% ownership
                                current_ownership_labels.append(stakeholder)
                                # Round to 1 decimal place for percentage formatting
                                current_ownership_values.append(round(value, 1))
                                current_ownership_colors.append(dataset.get("backgroundColor", "rgba(200, 200, 200, 0.9)"))
                    
                    # Priority 3: Try to get ownership from advanced_cap_table if available
                    if not current_ownership_labels and self.advanced_cap_table and self.advanced_cap_table.share_entries:
                        try:
                            ownership_df = self.advanced_cap_table.calculate_ownership(fully_diluted=True)
                            if ownership_df is not None and not ownership_df.empty:
                                logger.info(f"[CAP_TABLE] Found ownership data from advanced_cap_table for {company_name}")
                                for shareholder, row in ownership_df.iterrows():
                                    ownership_pct = row.get('ownership_pct', 0)
                                    if ownership_pct and ownership_pct > 0.1:
                                        current_ownership_labels.append(str(shareholder))
                                        # Round to 1 decimal place for percentage formatting
                                        current_ownership_values.append(round(float(ownership_pct), 1))
                                        # Determine color based on shareholder type
                                        shareholder_lower = str(shareholder).lower()
                                        if 'founder' in shareholder_lower:
                                            current_ownership_colors.append('rgba(59, 130, 246, 0.9)')
                                        elif 'employee' in shareholder_lower or 'option' in shareholder_lower:
                                            current_ownership_colors.append('rgba(251, 146, 60, 0.9)')
                                        else:
                                            current_ownership_colors.append('rgba(16, 185, 129, 0.9)')
                                logger.info(f"[CAP_TABLE] Extracted {len(current_ownership_labels)} stakeholders from advanced_cap_table")
                        except Exception as e:
                            logger.warning(f"[CAP_TABLE] Failed to get ownership from advanced_cap_table: {e}")
                    
                    # Priority 4: Use final_cap_table_at_exit if available (future scenario)
                    if not current_ownership_labels and cap_table_data and cap_table_data.get("final_cap_table_at_exit"):
                        final_cap_table = cap_table_data.get("final_cap_table_at_exit", {})
                        logger.info(f"[CAP_TABLE] Using final_cap_table_at_exit as fallback for {company_name}")
                        for stakeholder, pct in final_cap_table.items():
                            if isinstance(pct, (int, float)) and pct > 0.1:
                                current_ownership_labels.append(stakeholder)
                                # Round to 1 decimal place for percentage formatting
                                current_ownership_values.append(round(float(pct), 1))
                                current_ownership_colors.append("rgba(200, 200, 200, 0.9)")
                    
                    # Create pie chart with proper structure from cap table construction data
                    pie_chart_data = None
                    if current_ownership_labels and current_ownership_values:
                        logger.info(f"[CAP_TABLE] Creating pie chart for {company_name} with {len(current_ownership_labels)} segments")
                        pie_chart_data = format_pie_chart(
                            labels=current_ownership_labels,
                            data=current_ownership_values,
                            title=f"Current Ownership - {company_name}"
                        )
                        # Add colors to the dataset
                        if pie_chart_data.get("data", {}).get("datasets"):
                            pie_chart_data["data"]["datasets"][0]["backgroundColor"] = current_ownership_colors
                        # Add options with labels showing investor names and ownership percentages
                        # Format labels to show investor name and percentage clearly
                        pie_chart_data["options"] = {
                            "responsive": True,
                            "plugins": {
                                "legend": {
                                    "position": "right",
                                    "labels": {
                                        "usePointStyle": True,
                                        "padding": 15,
                                        "font": {
                                            "size": 11,
                                            "weight": "500"
                                        },
                                        "generateLabels": "function(chart) { const data = chart.data; if (data.labels.length && data.datasets.length) { const dataset = data.datasets[0]; const total = dataset.data.reduce((a, b) => a + b, 0); return data.labels.map((label, i) => { const value = dataset.data[i]; const percentage = ((value / total) * 100).toFixed(1); const displayText = label.length > 30 ? label.substring(0, 27) + '...' : label; return { text: displayText + ' (' + percentage + '%)', fillStyle: dataset.backgroundColor[i], hidden: false, index: i }; }); } return []; }"
                                    },
                                    "maxWidth": 300,
                                    "maxHeight": 400
                                },
                                "tooltip": {
                                    "enabled": True,
                                    "backgroundColor": "rgba(0, 0, 0, 0.8)",
                                    "titleFont": {
                                        "size": 13,
                                        "weight": "bold"
                                    },
                                    "bodyFont": {
                                        "size": 12
                                    },
                                    "padding": 10,
                                    "callbacks": {
                                        "title": "function(context) { return context[0].label || ''; }",
                                        "label": "function(context) { const label = context.label || ''; const value = context.parsed || 0; const total = context.dataset.data.reduce((a, b) => a + b, 0); const percentage = ((value / total) * 100).toFixed(1); return 'Ownership: ' + percentage + '%'; }"
                                    }
                                },
                                "datalabels": {
                                    "enabled": True,
                                    "color": "#fff",
                                    "font": {
                                        "weight": "bold",
                                        "size": 10
                                    },
                                    "formatter": "function(value, context) { const total = context.dataset.data.reduce((a, b) => a + b, 0); const percentage = ((value / total) * 100).toFixed(1); return percentage >= 3 ? percentage + '%' : ''; }",
                                    "anchor": "center",
                                    "align": "center"
                                }
                            }
                        }
                        logger.info(f"[CAP_TABLE] Pie chart created successfully for {company_name}")
                    else:
                        logger.warning(f"[CAP_TABLE] No ownership data available for pie chart: labels={len(current_ownership_labels)}, values={len(current_ownership_values)}")
                        # ROOT CAUSE FIX: Create fallback pie chart using metrics data to ensure chart is always generated
                        fallback_labels = []
                        fallback_values = []
                        fallback_colors = []
                        
                        # Extract ownership from metrics if available
                        if metrics:
                            founder_pct = metrics.get("Founder Ownership", "0%").replace("%", "").replace("~", "")
                            investor_pct = metrics.get("Investor Ownership", "0%").replace("%", "").replace("~", "")
                            employee_pct = metrics.get("Employee Ownership", "0%").replace("%", "").replace("~", "")
                            
                            try:
                                founder_val = float(founder_pct) if founder_pct else 0
                                investor_val = float(investor_pct) if investor_pct else 0
                                employee_val = float(employee_pct) if employee_pct else 0
                                
                                # Only add if value > 0
                                if founder_val > 0:
                                    fallback_labels.append("Founders")
                                    fallback_values.append(founder_val)
                                    fallback_colors.append("rgba(59, 130, 246, 0.9)")
                                if investor_val > 0:
                                    fallback_labels.append("Investors")
                                    fallback_values.append(investor_val)
                                    fallback_colors.append("rgba(16, 185, 129, 0.9)")
                                if employee_val > 0:
                                    fallback_labels.append("Employees")
                                    fallback_values.append(employee_val)
                                    fallback_colors.append("rgba(251, 146, 60, 0.9)")
                                
                                # If we have data, create the pie chart
                                if fallback_labels and fallback_values:
                                    pie_chart_data = format_pie_chart(
                                        labels=fallback_labels,
                                        data=fallback_values,
                                        title=f"Current Ownership - {company_name}"
                                    )
                                    if pie_chart_data.get("data", {}).get("datasets"):
                                        pie_chart_data["data"]["datasets"][0]["backgroundColor"] = fallback_colors
                                    logger.info(f"[CAP_TABLE] Created fallback pie chart for {company_name} using metrics data")
                            except (ValueError, TypeError) as e:
                                logger.warning(f"[CAP_TABLE] Failed to parse metrics for fallback pie chart: {e}")
                        
                        # If still no pie chart, create a basic one with estimated values based on stage
                        if not pie_chart_data:
                            stage_lower = company.get('stage', 'Series A').lower()
                            if 'series d' in stage_lower or 'series e' in stage_lower:
                                est_founders, est_investors, est_employees = 28, 60, 12
                            elif 'series c' in stage_lower:
                                est_founders, est_investors, est_employees = 38, 50, 12
                            elif 'series b' in stage_lower:
                                est_founders, est_investors, est_employees = 51, 37, 12
                            elif 'series a' in stage_lower:
                                est_founders, est_investors, est_employees = 68, 20, 12
                            else:  # Seed
                                est_founders, est_investors, est_employees = 85, 5, 10
                            
                            pie_chart_data = format_pie_chart(
                                labels=["Founders", "Investors", "Employees"],
                                data=[est_founders, est_investors, est_employees],
                                title=f"Estimated Ownership - {company_name}"
                            )
                            if pie_chart_data.get("data", {}).get("datasets"):
                                pie_chart_data["data"]["datasets"][0]["backgroundColor"] = [
                                    "rgba(59, 130, 246, 0.9)",   # Founders - blue
                                    "rgba(16, 185, 129, 0.9)",   # Investors - green
                                    "rgba(251, 146, 60, 0.9)"    # Employees - orange
                                ]
                            logger.info(f"[CAP_TABLE] Created estimated pie chart for {company_name} based on stage {company.get('stage', 'Unknown')}")
                    
                    # Extract investor details from cap table history for detailed display
                    investor_details = []
                    if cap_table_data and 'history' in cap_table_data:
                        for snapshot in cap_table_data.get('history', []):
                            post_ownership = snapshot.get('post_money_ownership', {})
                            round_name = snapshot.get('round_name', '')
                            for owner, pct in post_ownership.items():
                                owner_str = str(owner)
                                pct_float = float(pct) if isinstance(pct, (int, float)) else 0
                                # Check if this is an investor (not founder, not employee)
                                is_investor = (
                                    ('Investor' in owner_str or 'Series' in owner_str or 'Seed' in owner_str or 'Round' in owner_str) 
                                    and 'Founder' not in owner_str 
                                    and 'Employee' not in owner_str 
                                    and 'Option' not in owner_str 
                                    and 'ESOP' not in owner_str
                                    and pct_float > 0.1
                                )
                                if is_investor:
                                    clean_name = owner_str.replace(' (Lead)', '').replace(' (SAFE)', '').strip()
                                    investor_details.append({
                                        "name": clean_name,
                                        "round": round_name,
                                        "ownership": pct_float
                                    })
                    
                    cap_table_content = {
                        "title": f"Cap Table - {company_name}",
                        "subtitle": "Current ownership and future dilution scenarios",
                        "chart_data": pie_chart_data,
                        "bullets": bullets,
                        "metrics": metrics,
                        "current_cap_table": current_cap_table if cap_table_data and cap_table_data.get("current_cap_table") else {},
                        "investor_details": investor_details[:10]  # Include top 10 investors with details
                    }

                    # Always use pie chart for cap table slides (preferred over Sankey)
                    # Sankey is used for DPI slides, not cap table slides
                    if pie_chart_data:
                        # Ensure pie chart is set (it's already set above, but make it explicit)
                        cap_table_content["chart_data"] = pie_chart_data
                        logger.info(f"[CAP_TABLE] ✅ Using pie chart for {company_name} cap table slide with {len(current_ownership_labels)} stakeholders: {current_ownership_labels[:5]}")
                    else:
                        logger.warning(f"[CAP_TABLE] ⚠️ No pie chart data generated for {company_name}")
                        logger.warning(f"[CAP_TABLE] ⚠️ cap_table_data keys: {list(cap_table_data.keys()) if cap_table_data else 'None'}")
                        logger.warning(f"[CAP_TABLE] ⚠️ current_ownership_labels: {len(current_ownership_labels)}, current_ownership_values: {len(current_ownership_values)}")
                        # Still add the slide even without pie chart - it will show other data
                        logger.info(f"[CAP_TABLE] Adding cap table slide for {company_name} without pie chart (will show table/metrics)")
                    if cap_table_data:
                        cap_table_content["cap_table_history"] = cap_table_data.get("history", [])
                    
                    # Add FUTURE cap table showing our entry and dilution through future rounds
                    try:
                        from app.services.pre_post_cap_table import PrePostCapTable
                        pre_post_calc = PrePostCapTable()
                        
                        # Get fund context and next round predictions
                        fund_context = self.shared_data.get('fund_context', {})
                        our_investment = fund_context.get('investment_amount', 5_000_000)
                        next_round_data = company.get('next_round', {})
                        
                        # Calculate our entry impact
                        our_entry = pre_post_calc.calculate_our_entry_impact(
                            company_data=company,
                            our_investment=our_investment,
                            round_name=f"Our Entry ({company.get('stage', 'Series B')})"
                        )
                        
                        # Get actual dilution scenarios from the service
                        dilution_calc = company.get('dilution_scenarios', {})
                        
                        # Calculate future rounds with REAL data
                        current_stage = company.get('stage', 'Series B')
                        current_valuation = safe_get_value(company.get('valuation'), 100_000_000)
                        
                        # Use next round predictions for accurate modeling
                        next_round_size = next_round_data.get('next_round_size', 50_000_000)
                        next_round_valuation = next_round_data.get('next_round_valuation_post', current_valuation * 2)
                        next_round_timing = next_round_data.get('next_round_timing', 18)
                        next_round_stage = next_round_data.get('next_round_stage', 'Series C')
                        
                        # Stage-based dilution rates (more accurate than flat 20%)
                        dilution_by_stage = {
                            'Series A': 0.25,  # 25% dilution
                            'Series B': 0.20,  # 20% dilution
                            'Series C': 0.15,  # 15% dilution
                            'Series D': 0.12,  # 12% dilution
                            'Growth': 0.10,   # 10% dilution
                            'Exit': 0.0        # No more dilution
                        }
                        
                        # Model cap table evolution through future rounds
                        # Start with actual current ownership from cap_table_data if available (includes inferred rounds)
                        current_cap = cap_table_data.get("current_cap_table", {}) if cap_table_data else {}
                        current_founders = sum(v for k, v in current_cap.items() if 'founder' in k.lower()) if current_cap else company.get('founder_ownership', 30)
                        current_employees = sum(v for k, v in current_cap.items() if any(term in k.lower() for term in ['employee', 'esop', 'option'])) if current_cap else company.get('employee_ownership', 15)
                        current_investors = sum(v for k, v in current_cap.items() if 'investor' in k.lower() or 'series' in k.lower() or 'seed' in k.lower()) if current_cap else company.get('investor_ownership', 55)
                        
                        ownership_scenarios = {
                            "Current": {
                                "our_ownership": 0,
                                "founder_ownership": current_founders,
                                "employee_ownership": current_employees,
                                "investor_ownership": current_investors
                            },
                            "Post Our Investment": {
                                "our_ownership": our_entry.get('our_ownership', our_investment / (current_valuation + our_investment) * 100),
                                "founder_ownership": our_entry.get('founder_ownership_after', 25),
                                "employee_ownership": our_entry.get('employee_ownership_after', 15),
                                "investor_ownership": our_entry.get('existing_investor_ownership_after', 60 - our_entry.get('our_ownership', 10))
                            }
                        }
                        
                        # Track ownership through future rounds
                        our_ownership_with_reserves = float(ownership_scenarios["Post Our Investment"]["our_ownership"])
                        our_ownership_no_reserves = float(ownership_scenarios["Post Our Investment"]["our_ownership"])
                        founder_ownership = float(ownership_scenarios["Post Our Investment"]["founder_ownership"])
                        employee_ownership = float(ownership_scenarios["Post Our Investment"]["employee_ownership"])
                        
                        # Next round (e.g., Series C)
                        next_dilution = dilution_by_stage.get(next_round_stage, 0.15)
                        
                        # Calculate pro-rata investment needed
                        our_prorata = next_round_size * (our_ownership_with_reserves / 100)
                        
                        # With reserves scenario
                        ownership_scenarios[f"{next_round_stage} (w/ ${our_prorata/1e6:.1f}M reserves)"] = {
                            "our_ownership": our_ownership_with_reserves,  # Maintain ownership by investing pro-rata
                            "founder_ownership": founder_ownership * (1 - next_dilution),
                            "employee_ownership": employee_ownership + 2,  # ESOP refresh
                            "investor_ownership": 100 - our_ownership_with_reserves - (founder_ownership * (1 - next_dilution)) - (employee_ownership + 2)
                        }
                        
                        # Without reserves scenario
                        our_ownership_no_reserves *= (1 - next_dilution)
                        ownership_scenarios[f"{next_round_stage} (no reserves)"] = {
                            "our_ownership": our_ownership_no_reserves,
                            "founder_ownership": founder_ownership * (1 - next_dilution),
                            "employee_ownership": employee_ownership + 2,
                            "investor_ownership": 100 - our_ownership_no_reserves - (founder_ownership * (1 - next_dilution)) - (employee_ownership + 2)
                        }
                        
                        # Following round (e.g., Series D or Exit)
                        following_stage = "Series D" if next_round_stage != "Series D" else "Exit"
                        following_dilution = dilution_by_stage.get(following_stage, 0.12)
                        
                        if following_stage != "Exit":
                            # Another funding round
                            following_round_size = next_round_size * 1.5
                            our_prorata_2 = following_round_size * (our_ownership_with_reserves / 100)
                            total_reserves = our_prorata + our_prorata_2
                            
                            ownership_scenarios[f"{following_stage} (w/ ${total_reserves/1e6:.1f}M total reserves)"] = {
                                "our_ownership": our_ownership_with_reserves,
                                "founder_ownership": founder_ownership * (1 - next_dilution) * (1 - following_dilution),
                                "employee_ownership": employee_ownership + 4,  # Two ESOP refreshes
                                "investor_ownership": 100 - our_ownership_with_reserves - (founder_ownership * (1 - next_dilution) * (1 - following_dilution)) - (employee_ownership + 4)
                            }
                            
                            our_ownership_no_reserves *= (1 - following_dilution)
                            ownership_scenarios[f"{following_stage} (no reserves)"] = {
                                "our_ownership": our_ownership_no_reserves,
                                "founder_ownership": founder_ownership * (1 - next_dilution) * (1 - following_dilution),
                                "employee_ownership": employee_ownership + 4,
                                "investor_ownership": 100 - our_ownership_no_reserves - (founder_ownership * (1 - next_dilution) * (1 - following_dilution)) - (employee_ownership + 4)
                            }
                        
                        # Create waterfall chart data showing ownership evolution
                        waterfall_data = format_bar_chart(
                            labels=list(ownership_scenarios.keys()),
                            datasets=[
                                {
                                    "label": "Our Fund",
                                    "data": [round(s["our_ownership"], 1) for s in ownership_scenarios.values()],
                                    "backgroundColor": "rgba(16, 185, 129, 0.9)",
                                    "borderColor": "rgba(16, 185, 129, 1)",
                                    "borderWidth": 1
                                },
                                {
                                    "label": "Founders",
                                    "data": [round(s["founder_ownership"], 1) for s in ownership_scenarios.values()],
                                    "backgroundColor": "rgba(59, 130, 246, 0.9)",
                                    "borderColor": "rgba(59, 130, 246, 1)",
                                    "borderWidth": 1
                                },
                                {
                                    "label": "Employees",
                                    "data": [round(s["employee_ownership"], 1) for s in ownership_scenarios.values()],
                                    "backgroundColor": "rgba(251, 146, 60, 0.9)",
                                    "borderColor": "rgba(251, 146, 60, 1)",
                                    "borderWidth": 1
                                },
                                {
                                    "label": "Other Investors",
                                    "data": [round(s["investor_ownership"], 1) for s in ownership_scenarios.values()],
                                    "backgroundColor": "rgba(139, 92, 246, 0.9)",
                                    "borderColor": "rgba(139, 92, 246, 1)",
                                    "borderWidth": 1
                                }
                            ],
                            title="Ownership Evolution Through Rounds"
                        )
                        waterfall_data["options"] = {
                                "responsive": True,
                                "scales": {
                                    "x": {
                                        "stacked": True,
                                        "ticks": {
                                            "maxRotation": 45,
                                            "minRotation": 45
                                        }
                                    },
                                    "y": {
                                        "stacked": True,
                                        "max": 100,
                                        "title": {
                                            "display": True,
                                            "text": "Ownership %"
                                        },
                                        "ticks": {
                                            "format": "{value}%"
                                        }
                                    }
                                },
                                "plugins": {
                                    "tooltip": {
                                        "callbacks": {
                                            "format": "{label}: {value}%"
                                        }
                                    }
                                }
                            }
                        
                        cap_table_content["future_chart_data"] = waterfall_data
                        
                        # Create future_pie_charts array from ownership scenarios
                        future_pie_charts = []
                        for scenario_name, scenario_data in ownership_scenarios.items():
                            pie_chart = format_pie_chart(
                                labels=["Our Fund", "Founders", "Employees", "Other Investors"],
                                data=[
                                    scenario_data["our_ownership"],
                                    scenario_data["founder_ownership"],
                                    scenario_data["employee_ownership"],
                                    scenario_data["investor_ownership"]
                                ],
                                title=scenario_name
                            )
                            # Add colors to match the bar chart
                            pie_chart["data"]["datasets"][0]["backgroundColor"] = [
                                "rgba(16, 185, 129, 0.9)",   # Our Fund - green
                                "rgba(59, 130, 246, 0.9)",   # Founders - blue
                                "rgba(251, 146, 60, 0.9)",   # Employees - orange
                                "rgba(139, 92, 246, 0.9)"    # Other Investors - purple
                            ]
                            future_pie_charts.append({
                                "data": pie_chart["data"],
                                "title": pie_chart["title"]
                            })
                        
                        cap_table_content["future_pie_charts"] = future_pie_charts
                        
                        # Add metrics about our entry and future scenarios
                        # Add next round intelligence to the metrics
                        cap_table_content["our_entry_metrics"] = {
                            "Our Entry Ownership": f"{ownership_scenarios['Post Our Investment']['our_ownership']:.1f}%",
                            "Investment Amount": f"${our_investment/1e6:.1f}M",
                            "Pre-Money Valuation": f"${current_valuation/1e6:.0f}M",
                            "Post-Money Valuation": f"${(current_valuation + our_investment)/1e6:.0f}M",
                            "Next Round Timing": f"{next_round_timing:.0f} months ({next_round_data.get('next_round_timing_label', 'Normal')})",
                            "Next Round Size": f"${next_round_size/1e6:.0f}M {next_round_stage}",
                            "Pro-rata Needed": f"${our_prorata/1e6:.1f}M",
                            "Total Reserves": f"${total_reserves/1e6:.1f}M" if 'total_reserves' in locals() else f"${our_prorata/1e6:.1f}M",
                            "Exit w/ Reserves": f"{our_ownership_with_reserves:.1f}%",
                            "Exit w/o Reserves": f"{our_ownership_no_reserves:.1f}%",
                            "Dilution Risk": next_round_data.get('down_round_risk', 'MEDIUM')
                        }
                        
                        logger.info(f"[CAP_TABLE] Added future cap table for {company_name} with our ${our_investment/1e6:.1f}M entry")
                    except Exception as e:
                        logger.warning(f"[CAP_TABLE] Failed to calculate future cap table for {company_name}: {e}")

                    # ROOT CAUSE FIX: Ensure we always have minimum required content before adding slide
                    if not cap_table_content.get("bullets"):
                        cap_table_content["bullets"] = [
                            f"Cap table analysis for {company_name}",
                            f"Stage: {company.get('stage', 'Unknown')}",
                            f"Valuation: {self._format_money(company.get('valuation', 0))}"
                        ]
                    if not cap_table_content.get("metrics"):
                        cap_table_content["metrics"] = {
                            "Valuation": self._format_money(company.get('valuation', 0)),
                            "Total Funding": self._format_money(company.get('total_funding', 0)),
                            "Stage": company.get('stage', 'Unknown')
                        }
                    
                    # Validate chart_data structure before adding slide
                    chart_data = cap_table_content.get('chart_data')
                    if chart_data:
                        # Validate pie chart structure
                        if isinstance(chart_data, dict):
                            data = chart_data.get('data', {})
                            if isinstance(data, dict):
                                labels = data.get('labels', [])
                                datasets = data.get('datasets', [])
                                if not labels or not datasets or len(labels) == 0 or len(datasets) == 0:
                                    logger.warning(f"[CAP_TABLE] Invalid pie chart data structure for {company_name}: missing labels or datasets")
                                    # Remove invalid chart_data to prevent rendering errors
                                    cap_table_content.pop('chart_data', None)
                                else:
                                    logger.info(f"[CAP_TABLE] ✅ Valid pie chart data: {len(labels)} labels, {len(datasets)} datasets")
                            else:
                                logger.warning(f"[CAP_TABLE] Invalid chart_data.data structure for {company_name}: expected dict, got {type(data)}")
                                cap_table_content.pop('chart_data', None)
                        else:
                            logger.warning(f"[CAP_TABLE] Invalid chart_data type for {company_name}: expected dict, got {type(chart_data)}")
                            cap_table_content.pop('chart_data', None)
                    
                    # Validate future_chart_data if present
                    future_chart_data = cap_table_content.get('future_chart_data')
                    if future_chart_data:
                        # Future chart data should be waterfall data (line/bar chart format)
                        if isinstance(future_chart_data, dict):
                            if not future_chart_data.get('labels') or not future_chart_data.get('datasets'):
                                logger.warning(f"[CAP_TABLE] Invalid future_chart_data structure for {company_name}")
                                cap_table_content.pop('future_chart_data', None)
                    
                    # Log chart_data structure for debugging
                    chart_data_final = cap_table_content.get('chart_data')
                    if chart_data_final:
                        logger.info(f"[CAP_TABLE] 📊 Adding cap table slide for {company_name} with chart_data: type={chart_data_final.get('type')}, has_data={bool(chart_data_final.get('data'))}, labels_count={len(chart_data_final.get('data', {}).get('labels', []))}, datasets_count={len(chart_data_final.get('data', {}).get('datasets', []))}")
                    else:
                        logger.warning(f"[CAP_TABLE] ⚠️ Adding cap table slide for {company_name} WITHOUT chart_data (will show table/metrics only)")
                    
                    add_slide("cap_table", cap_table_content)
                    logger.info(f"[CAP_TABLE] ✅ Successfully added cap table slide for {company_name}")
                
                # Add a more detailed cap table breakdown with COMPARISON using REAL DATA
                if len(companies) > 1:
                    # Create side-by-side cap table comparison
                    company1 = companies[0]
                    company2 = companies[1]
                    
                    # Get real cap table data from PrePostCapTable service
                    def get_company_cap_table_data(company):
                        """Get cap table data using PrePostCapTable service"""
                        rounds, inferred_added = self._build_funding_rounds_with_inference(company)

                        if not rounds:
                            logger.warning(f"[CAP_TABLE] No funding history for {company.get('company', 'Unknown')}; skipping cap table comparison")
                            return None
                        if inferred_added:
                            logger.info(f"[CAP_TABLE] Added {inferred_added} inferred rounds for comparison of {company.get('company', 'Unknown')}")

                        # CRITICAL: Store cleaned rounds back to company data
                        company['funding_rounds'] = rounds
                        
                        # Call the PrePostCapTable service with full company data (not just rounds)
                        try:
                            cap_result = self.cap_table_service.calculate_full_cap_table_history(
                                company_data=company  # Pass full company dict with funding_rounds, geography, is_yc, etc
                            )
                        except Exception as e:
                            logger.warning(f"PrePostCapTable service error: {e}")
                            cap_result = None
                        
                        # Extract ownership from the current cap table
                        if cap_result and 'current_cap_table' in cap_result:
                            current_cap_table = cap_result['current_cap_table']
                            
                            # Aggregate ownership by type
                            founders_pct = 0
                            investors_pct = 0
                            employees_pct = 0
                            
                            for owner, pct in current_cap_table.items():
                                if 'Founder' in owner:
                                    founders_pct += pct
                                elif 'Employee' in owner or 'Option' in owner:
                                    employees_pct += pct
                                else:
                                    investors_pct += pct
                            
                            return {
                                'founders': round(founders_pct),
                                'investors': round(investors_pct),
                                'employees': round(employees_pct),
                                'rounds': rounds,
                                'raw_data': cap_result
                            }
                        
                        # Service should always return data
                        raise Exception("PrePostCapTable service did not return cap table data")
                    
                    # Get ownership data for both companies
                    company1_ownership = get_company_cap_table_data(company1)
                    company2_ownership = get_company_cap_table_data(company2)
                    if not company1_ownership or not company2_ownership:
                        logger.warning("[CAP_TABLE] Unable to build comparison due to missing cap table data")
                    else:
                        company1_stage = company1.get('stage', 'Series A')
                        company2_stage = company2.get('stage', 'Series A')
                        
                        # Create Sankey data for cap table visualization
                        def create_sankey_data(company_ownership, company_name):
                            """Convert ownership data to Sankey format with proper flow"""
                            try:
                                # VALIDATE input data exists
                                if not company_ownership or not isinstance(company_ownership, dict):
                                    logger.warning(f"[SANKEY] No ownership data for {company_name}")
                                    return None
                                
                                # If we have raw_data from PrePostCapTable, use its sankey_data
                                if company_ownership.get('raw_data') and company_ownership['raw_data'].get('sankey_data'):
                                    return company_ownership['raw_data']['sankey_data']
                                
                                # Otherwise, create a proper Sankey visualization
                                rounds = company_ownership.get('rounds', [])
                                if not rounds or not isinstance(rounds, list):
                                    logger.warning(f"[SANKEY] No funding rounds for {company_name}")
                                    return None
                                
                                nodes = []
                                links = []
                                
                                # Create unique nodes
                                node_map = {}
                                node_idx = 0
                                
                                # Initial founder node
                                nodes.append({"id": node_idx, "name": "Founders (100%)"})
                                node_map['founders_start'] = node_idx
                                node_idx += 1
                                
                                # Process each round to show dilution flow
                                current_founder_ownership = 100
                                for i, round_data in enumerate(rounds):
                                    if not isinstance(round_data, dict):
                                        continue
                                        
                                    round_name = round_data.get('round_name', f'Round {i+1}')
                                    amount = safe_get_value(round_data.get('amount', 0), 0)
                                    valuation = safe_get_value(round_data.get('valuation', 1), 1)
                                    
                                    # Ensure positive values
                                    amount = max(amount, 0)
                                    valuation = max(valuation, 1)  # Avoid divide by zero
                                    
                                    # Calculate dilution with safe division
                                    dilution_pct = min(25, self._safe_divide(amount * 100, valuation, 20))
                                    new_founder_ownership = current_founder_ownership * (1 - dilution_pct / 100)
                                    
                                    # Investor node for this round
                                    investor_name = f"{round_name} Investors"
                                    nodes.append({"id": node_idx, "name": investor_name})
                                    node_map[investor_name] = node_idx
                                    investor_node = node_idx
                                    node_idx += 1
                                    
                                    # Intermediate founder node after this round
                                    founder_after = f"Founders after {round_name}"
                                    nodes.append({"id": node_idx, "name": f"Founders ({new_founder_ownership:.0f}%)"})
                                    node_map[founder_after] = node_idx
                                    founder_node = node_idx
                                    node_idx += 1
                                    
                                    # Links showing the flow
                                    if i == 0:
                                        # From initial founders to first round
                                        links.append({
                                            "source": node_map['founders_start'],
                                            "target": investor_node,
                                            "value": dilution_pct
                                        })
                                        links.append({
                                            "source": node_map['founders_start'],
                                            "target": founder_node,
                                            "value": new_founder_ownership
                                        })
                                    else:
                                        # From previous founder node to this round
                                        prev_founder = f"Founders after {rounds[i-1].get('round_name', f'Round {i}')}"
                                        links.append({
                                            "source": node_map[prev_founder],
                                            "target": investor_node,
                                            "value": dilution_pct
                                        })
                                        links.append({
                                            "source": node_map[prev_founder],
                                            "target": founder_node,
                                            "value": new_founder_ownership
                                        })
                                    
                                    current_founder_ownership = new_founder_ownership
                                    
                                # Final ownership distribution
                                final_founders = company_ownership.get('founders', current_founder_ownership)
                                final_investors = company_ownership.get('investors', 100 - final_founders - 10)
                                final_employees = company_ownership.get('employees', 10)
                                
                                # Final nodes
                                nodes.append({"id": node_idx, "name": f"Final: Founders ({final_founders}%)"})
                                final_founder_node = node_idx
                                node_idx += 1
                                
                                nodes.append({"id": node_idx, "name": f"Final: Investors ({final_investors}%)"})
                                final_investor_node = node_idx
                                node_idx += 1
                                
                                nodes.append({"id": node_idx, "name": f"Final: Employees ({final_employees}%)"})
                                final_employee_node = node_idx
                                
                                # Links to final state
                                # Safe array access with explicit length check
                                last_round_name = (rounds[-1].get('round_name', f'Round {len(rounds)}') 
                                                 if rounds and len(rounds) > 0 else None)
                                last_founder = f"Founders after {last_round_name}" if last_round_name else 'founders_start'
                                if last_founder in node_map:
                                    links.append({
                                        "source": node_map[last_founder],
                                        "target": final_founder_node,
                                        "value": final_founders
                                    })
                                
                                # Aggregate investor flows - track actual ownership per round
                                investor_nodes = {}
                                for investor_name in node_map:
                                    if 'Investors' in investor_name and 'Final' not in investor_name:
                                        # Extract ownership percentage from node name if available
                                        import re
                                        match = re.search(r'\((\d+(?:\.\d+)?)\%\)', investor_name)
                                        if match:
                                            investor_ownership = float(match.group(1))
                                        else:
                                            # Fallback to equal split if no percentage available
                                            num_investor_rounds = len([k for k in node_map if 'Investors' in k and 'Final' not in k])
                                            investor_ownership = final_investors / max(1, num_investor_rounds)
                                        
                                        # Track unique investors to avoid duplication
                                        investor_round = investor_name.split(' - ')[0] if ' - ' in investor_name else investor_name
                                        if investor_round not in investor_nodes:
                                            investor_nodes[investor_round] = {
                                                "node": node_map[investor_name],
                                                "ownership": investor_ownership
                                            }
                                        else:
                                            # Accumulate ownership for same investor across rounds
                                            investor_nodes[investor_round]["ownership"] += investor_ownership
                                
                                # Create single link per unique investor
                                for investor_data in investor_nodes.values():
                                    links.append({
                                        "source": investor_data["node"],
                                        "target": final_investor_node,
                                        "value": min(investor_data["ownership"], final_investors)
                                    })
                                
                                return {"nodes": nodes, "links": links}
                            except Exception as e:
                                logger.warning(f"Failed to create Sankey data: {e}")
                                return None
                        
                        # Create cap table dilution Sankey
                        company1_sankey = create_sankey_data(company1_ownership, company1.get("company"))
                        company2_sankey = create_sankey_data(company2_ownership, company2.get("company"))
                        
                        if company1_sankey and company2_sankey:
                            # Add liquidation preference data to Sankey visualization
                            company1_prefs = company1.get('liquidation_preferences', [])
                            company2_prefs = company2.get('liquidation_preferences', [])
                            
                            # Calculate total liquidation preference stack
                            company1_total_prefs = sum(r.get('liquidation_preference', 1) * r.get('amount', 0) for r in company1_prefs if r)
                            company2_total_prefs = sum(r.get('liquidation_preference', 1) * r.get('amount', 0) for r in company2_prefs if r)
                            
                            # Calculate forward-looking cap table with our investment
                            company1_forward = self._calculate_forward_cap_table(
                                company1, 
                                self._get_optimal_check_size(company1, fund_context),
                                fund_context
                            )
                            company2_forward = self._calculate_forward_cap_table(
                                company2,
                                self._get_optimal_check_size(company2, fund_context),
                                fund_context
                            )
                            
                            # Build enhanced Sankey data including our investment
                            company1_enhanced_sankey = self._enhance_sankey_with_investment(
                                company1_sankey, 
                                company1_forward,
                                company1.get("company", "Company 1")
                            )
                            company2_enhanced_sankey = self._enhance_sankey_with_investment(
                                company2_sankey,
                                company2_forward,
                                company2.get("company", "Company 2")
                            )
                            
                            # Format side-by-side Sankey chart data properly for frontend using helper
                            side_by_side_sankey_chart = format_side_by_side_sankey_chart(
                                company1_data=company1_enhanced_sankey,
                                company2_data=company2_enhanced_sankey,
                                company1_name=company1.get("company", "Company 1"),
                                company2_name=company2.get("company", "Company 2"),
                                title="Ownership Flow: Current → Our Entry → Exit",
                                company1_liquidation_prefs=company1_total_prefs + company1_forward['our_liquidation_pref'],
                                company2_liquidation_prefs=company2_total_prefs + company2_forward['our_liquidation_pref'],
                                company1_forward=company1_forward,
                                company2_forward=company2_forward
                            )
                            
                            # Pre-render side-by-side Sankey chart
                            prerendered_side_sankey = await self._prerender_complex_chart(side_by_side_sankey_chart)
                            
                            # Use Sankey visualization with chart_data for frontend
                            add_slide("cap_table_forward_looking", {
                                    "title": "Cap Table Evolution with Our Investment",
                                    "subtitle": "Forward-looking ownership projection through exit",
                                "chart_data": prerendered_side_sankey,
                                "insights": [
                                    f"{company1.get('company')} - Our entry: ${company1_forward['our_investment']/1e6:.1f}M → {company1_forward['our_entry_ownership']*100:.1f}% ownership",
                                    f"{company1.get('company')} - At exit: {company1_forward['our_exit_ownership']*100:.1f}% after {company1_forward['rounds_to_exit']} more rounds",
                                    f"{company2.get('company')} - Our entry: ${company2_forward['our_investment']/1e6:.1f}M → {company2_forward['our_entry_ownership']*100:.1f}% ownership",
                                    f"{company2.get('company')} - At exit: {company2_forward['our_exit_ownership']*100:.1f}% after {company2_forward['rounds_to_exit']} more rounds",
                                    f"Pro-rata to maintain: {company1.get('company')}: ${company1_forward['total_prorata_needed']/1e6:.1f}M | {company2.get('company')}: ${company2_forward['total_prorata_needed']/1e6:.1f}M",
                                    f"Exit value for 3x: {company1.get('company')}: ${company1_forward['exit_for_3x']/1e6:.0f}M | {company2.get('company')}: ${company2_forward['exit_for_3x']/1e6:.0f}M"
                                ]
                            })
                        else:
                            # Fall back to pie charts if we can't create Sankey
                            add_slide("cap_table_comparison", {
                                    "title": "Cap Table Comparison",
                                    "subtitle": "Current ownership distribution",
                                "company1": {
                                    "name": company1.get("company", "Company 1"),
                                    "stage": company1_stage,
                                    "chart_data": {
                                        "type": "pie",
                                        "title": f"{company1.get('company', 'Company 1')} Ownership",
                                        "data": {
                                            "labels": ["Founders", "Investors", "Employees"],
                                            "datasets": [{
                                                "data": [
                                                    company1_ownership['founders'],
                                                    company1_ownership['investors'],
                                                    company1_ownership['employees']
                                                ],
                                                "backgroundColor": ["rgba(59, 130, 246, 0.3)", "rgba(59, 130, 246, 0.6)", "rgba(59, 130, 246, 0.9)"]
                                            }]
                                        }
                                    },
                                    "metrics": {
                                        "Founder Ownership": f"{company1_ownership['founders']}%",
                                        "Investor Ownership": f"{company1_ownership['investors']}%",
                                        "Employee Pool": f"{company1_ownership['employees']}%"
                                    }
                                },
                                "company2": {
                                    "name": company2.get("company", "Company 2"),
                                    "stage": company2_stage,
                                    "chart_data": {
                                        "type": "pie",
                                        "title": f"{company2.get('company', 'Company 2')} Ownership",
                                        "data": {
                                            "labels": ["Founders", "Investors", "Employees"],
                                            "datasets": [{
                                                "data": [
                                                    company2_ownership['founders'],
                                                    company2_ownership['investors'],
                                                    company2_ownership['employees']
                                                ],
                                                "backgroundColor": ["rgba(59, 130, 246, 0.3)", "rgba(59, 130, 246, 0.6)", "rgba(59, 130, 246, 0.9)"]
                                            }]
                                        }
                                    },
                                    "metrics": {
                                        "Founder Ownership": f"{company2_ownership['founders']}%",
                                        "Investor Ownership": f"{company2_ownership['investors']}%",
                                        "Employee Pool": f"{company2_ownership['employees']}%"
                                    }
                                },
                                "insights": [
                                    f"{company1.get('company')} founders retain {company1_ownership['founders']}% after {company1_stage}",
                                    f"{company2.get('company')} founders retain {company2_ownership['founders']}% after {company2_stage}",
                                    f"Difference in dilution: {abs(company1_ownership['founders'] - company2_ownership['founders'])}%"
                                ]
                            })
                
            # Prepare charts array with actual data - formatted in millions
            charts = []
            if len(companies) > 0:
                # Combined valuation and revenue comparison chart with gradient colors
                charts.append({
                    "type": "bar",
                    "title": "Valuation & Revenue Comparison",
                    "data": {
                        "labels": [c.get("company", "Unknown") for c in companies],
                        "datasets": [
                            {
                                "label": "Valuation",
                                "data": [c.get("valuation", 0) / 1_000_000 for c in companies],
                                "backgroundColor": "rgba(99, 102, 241, 0.8)",  # Modern indigo
                                "borderColor": "rgba(99, 102, 241, 1)",
                                "borderWidth": 2,
                                "borderRadius": 8,
                                "yAxisID": "y1"
                            },
                            {
                                "label": "Revenue", 
                                "data": [(c.get("revenue", 0) or c.get("inferred_revenue", 0)) / 1_000_000 for c in companies],
                                "backgroundColor": "rgba(34, 197, 94, 0.8)",  # Modern green
                                "borderColor": "rgba(34, 197, 94, 1)",
                                "borderWidth": 2,
                                "borderRadius": 8,
                                "yAxisID": "y1"
                            }
                        ]
                    },
                    "options": {
                        "scales": {
                            "y1": {
                                "type": "linear",
                                "position": "left",
                                "title": {
                                    "display": True,
                                    "text": "USD (Millions)"
                                }
                            }
                        }
                    }
                })
                
                # Add Path to $100M ARR chart to charts array
                if companies:
                    path_data = []
                    for company in companies[:2]:
                        company_name = company.get('company', 'Unknown')
                        current_revenue = self._get_field_with_fallback(company, 'revenue', 0)
                        stage = company.get('stage', 'Series A')
                        
                        # Calculate growth trajectory
                        growth_rates = {
                            'Seed': 3.0,  # 200% YoY
                            'Series A': 2.5,  # 150% YoY
                            'Series B': 2.0,  # 100% YoY
                            'Series C': 1.5,  # 50% YoY
                            'Growth': 1.3   # 30% YoY
                        }
                        yoy_growth = growth_rates.get(stage, 2.0)
                        
                        # Generate 7-year projection
                        years = list(range(8))
                        revenues = [current_revenue * (yoy_growth ** i) / 1_000_000 for i in years]
                        
                        path_data.append({
                            "label": company_name,
                            "data": revenues,
                            "borderColor": "rgba(59, 130, 246, 1)" if not path_data else "rgba(156, 163, 175, 1)",
                            "fill": False
                        })
                    
                    charts.append({
                        "type": "line",
                        "title": "Path to $100M ARR",
                        "data": {
                            "labels": ["Year 0", "Year 1", "Year 2", "Year 3", "Year 4", "Year 5", "Year 6", "Year 7"],
                            "datasets": path_data
                        },
                        "options": {
                            "scales": {
                                "y": {
                                    "type": "linear",
                                    "title": {
                                        "display": True,
                                        "text": "ARR ($M)"
                                    },
                                    "ticks": {
                                        "callback": "function(value) { return '$' + value.toFixed(0) + 'M'; }"
                                    }
                                }
                            },
                            "plugins": {
                                "annotation": {
                                    "annotations": {
                                        "line1": {
                                            "type": "line",
                                            "yMin": 100,
                                            "yMax": 100,
                                            "borderColor": "rgb(255, 99, 132)",
                                            "borderWidth": 2,
                                            "borderDash": [5, 5],
                                            "label": {
                                                "content": "$100M Target",
                                                "enabled": True,
                                                "position": "end"
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    })
                
                # Add TAM Pincer Chart (disabled when TAM processing is off)
                tam_chart_data = []
                for company in companies[:2]:
                    market_data = company.get('market_size', {}) or {}
                    if market_data.get('status') == 'tam_disabled' or company.get('tam_processing_disabled'):
                        continue
                    
                    company_name = company.get('company', 'Unknown')
                    traditional_tam = market_data.get('tam', company.get('tam', 0)) / 1_000_000
                    labor_tam = market_data.get('labor_tam', company.get('labor_tam', 0)) / 1_000_000
                    
                    # If no labor TAM, estimate based on business model
                    if not labor_tam or labor_tam == 0:
                        business_model = company.get('business_model', 'SaaS')
                        if traditional_tam and traditional_tam > 0:
                            if 'AI' in business_model or 'automation' in business_model.lower():
                                labor_tam = traditional_tam * 2.5  # AI companies can address larger labor market
                            else:
                                labor_tam = traditional_tam * 0.3  # Traditional SaaS addresses smaller labor portion
                        else:
                            labor_tam = 0  # No TAM data available
                    
                    tam_chart_data.append({
                        "company": company_name,
                        "traditional": traditional_tam,
                        "labor": labor_tam
                    })
                
                if tam_chart_data:
                    charts.append({
                        "type": "bar",
                        "title": "TAM Analysis: Traditional vs Labor Markets",
                        "data": {
                            "labels": [d["company"] for d in tam_chart_data],
                            "datasets": [
                                {
                                    "label": "Traditional TAM ($M)",
                                    "data": [d["traditional"] for d in tam_chart_data],
                                    "backgroundColor": "rgba(59, 130, 246, 0.9)"
                                },
                                {
                                    "label": "Labor TAM ($M)",
                                    "data": [d["labor"] for d in tam_chart_data],
                                    "backgroundColor": "rgba(156, 163, 175, 0.9)"
                                }
                            ]
                        },
                        "options": {
                            "scales": {
                                "y": {
                                    "title": {
                                        "display": True,
                                        "text": "Market Size ($M)"
                                    }
                                }
                            }
                        }
                    })
                
                # Add Capital Efficiency Chart
                efficiency_data = []
                for company in companies[:2]:
                    company_name = company.get('company', 'Unknown')
                    # Use safe getters - prioritize inferred values
                    revenue = self._get_field_safe(company, 'revenue')
                    total_funding = self._get_field_safe(company, 'total_funding', default=1)  # Default 1 to avoid div/0
                    team_size = self._get_field_safe(company, 'team_size', default=50)  # Default 50 if missing
                    
                    # Calculate metrics with safe division
                    revenue_per_dollar = self._safe_divide(revenue, total_funding, default=0)
                    revenue_per_employee = self._safe_divide(revenue, team_size, default=0)
                    burn_multiple = self._safe_divide(total_funding, revenue, default=0)
                    
                    efficiency_data.append({
                        "company": company_name,
                        "revenue_per_dollar": revenue_per_dollar,
                        "revenue_per_employee": (revenue_per_employee or 0) / 1000,  # In thousands
                        "burn_multiple": min(burn_multiple, 10) if burn_multiple > 0 else 0  # Cap at 10 for visualization
                    })
                
                if efficiency_data:
                    charts.append({
                        "type": "radar",
                        "title": "Capital Efficiency Metrics",
                        "data": {
                            "labels": ["Revenue/$1 Raised", "Revenue/Employee ($K)", "Efficiency Score"],
                            "datasets": [
                                {
                                    "label": efficiency_data[0]["company"],
                                    "data": [
                                        efficiency_data[0]["revenue_per_dollar"],
                                        efficiency_data[0]["revenue_per_employee"],
                                        10 - efficiency_data[0]["burn_multiple"]  # Inverse for better visualization
                                    ],
                                    "borderColor": "rgba(59, 130, 246, 1)",
                                    "backgroundColor": "rgba(66, 133, 244, 0.2)"
                                },
                                {
                                    "label": efficiency_data[1]["company"] if len(efficiency_data) > 1 else "Company 2",
                                    "data": [
                                        efficiency_data[1]["revenue_per_dollar"] if len(efficiency_data) > 1 else 0,
                                        efficiency_data[1]["revenue_per_employee"] if len(efficiency_data) > 1 else 0,
                                        10 - efficiency_data[1]["burn_multiple"] if len(efficiency_data) > 1 else 0
                                    ],
                                    "borderColor": "#0F9D58",
                                    "backgroundColor": "rgba(15, 157, 88, 0.2)"
                                }
                            ]
                        }
                    })
                
                # Add Fund Fit Scoring Chart
                fund_scores = []
                for company in companies[:2]:
                    company_name = company.get('company', 'Unknown')
                    stage = company.get('stage', 'Series A')
                    valuation = self._get_field_safe(company, 'valuation')
                    revenue = self._get_field_safe(company, 'revenue')
                    
                    # Calculate fund fit scores (0-10 scale)
                    stage_fit = 8 if stage in ['Series A', 'Series B'] else 5
                    valuation_fit = 9 if valuation < 200_000_000 else (6 if valuation < 500_000_000 else 3)
                    market_fit = company.get('market_score', 7)  # Use market score if available
                    team_fit = company.get('team_score', 7)  # Use team score if available
                    
                    fund_scores.append({
                        "company": company_name,
                        "stage_fit": stage_fit,
                        "valuation_fit": valuation_fit,
                        "market_fit": market_fit,
                        "team_fit": team_fit,
                        "overall": (stage_fit + valuation_fit + market_fit + team_fit) / 4
                    })
                
                if fund_scores:
                    charts.append({
                        "type": "bar",
                        "title": "Fund Fit Analysis",
                        "data": {
                            "labels": ["Stage Fit", "Valuation Fit", "Market Fit", "Team Fit", "Overall"],
                            "datasets": [
                                {
                                    "label": fund_scores[0]["company"],
                                    "data": [
                                        fund_scores[0]["stage_fit"],
                                        fund_scores[0]["valuation_fit"],
                                        fund_scores[0]["market_fit"],
                                        fund_scores[0]["team_fit"],
                                        fund_scores[0]["overall"]
                                    ],
                                    "backgroundColor": "rgba(59, 130, 246, 0.9)"
                                },
                                {
                                    "label": fund_scores[1]["company"] if len(fund_scores) > 1 else "Company 2",
                                    "data": [
                                        fund_scores[1]["stage_fit"] if len(fund_scores) > 1 else 0,
                                        fund_scores[1]["valuation_fit"] if len(fund_scores) > 1 else 0,
                                        fund_scores[1]["market_fit"] if len(fund_scores) > 1 else 0,
                                        fund_scores[1]["team_fit"] if len(fund_scores) > 1 else 0,
                                        fund_scores[1]["overall"] if len(fund_scores) > 1 else 0
                                    ],
                                    "backgroundColor": "rgba(156, 163, 175, 0.9)"
                                }
                            ]
                        },
                        "options": {
                            "scales": {
                                "y": {
                                    "min": 0,
                                    "max": 10,
                                    "title": {
                                        "display": True,
                                        "text": "Score (0-10)"
                                    }
                                }
                            }
                        }
                    })
                
                # Add Exit Scenarios Chart with proper liquidation waterfalls
                if companies:
                    exit_chart_data = []
                    for company in companies[:2]:
                        company_name = company.get('company', 'Unknown')
                        scenarios = company.get('pwerm_scenarios', [])
                        
                        if scenarios and len(scenarios) > 0:
                            # Get top 5 scenarios for visualization
                            top_scenarios = scenarios[:5] if len(scenarios) >= 5 else scenarios
                            
                            # Extract scenario names and values
                            scenario_names = []
                            exit_values = []
                            probabilities = []
                            moics = []
                            
                            for s in top_scenarios:
                                scenario_name = s.scenario if hasattr(s, 'scenario') else s.get('scenario', 'Unknown')
                                # Shorten scenario names for chart
                                if 'Blockbuster' in scenario_name:
                                    short_name = 'Blockbuster IPO'
                                elif 'Strong IPO' in scenario_name:
                                    short_name = 'IPO >$1B'
                                elif 'Strategic Premium' in scenario_name:
                                    short_name = 'Strategic (Premium)'
                                elif 'Strategic Acquisition' in scenario_name and 'strong fit' in scenario_name:
                                    short_name = 'Strategic (Good)'
                                elif 'Quick Strategic' in scenario_name:
                                    short_name = 'Quick Exit'
                                elif 'Modest' in scenario_name:
                                    short_name = 'Modest M&A'
                                elif 'Acquihire' in scenario_name:
                                    short_name = 'Acquihire'
                                else:
                                    short_name = scenario_name[:20]  # Truncate long names
                                
                                scenario_names.append(short_name)
                                exit_val = s.exit_value if hasattr(s, 'exit_value') else s.get('exit_value', 0)
                                # Ensure exit_val is numeric before division
                                exit_val = exit_val if exit_val is not None else 0
                                exit_values.append(exit_val / 1_000_000)  # Convert to millions
                                prob = s.probability if hasattr(s, 'probability') else s.get('probability', 0)
                                probabilities.append(prob * 100)  # Convert to percentage
                                moic = s.moic if hasattr(s, 'moic') else s.get('moic', 1.0)
                                moics.append(moic)
                            
                            exit_chart_data.append({
                                "company": company_name,
                                "scenarios": scenario_names,
                                "exit_values": exit_values,
                                "probabilities": probabilities,
                                "moics": moics
                            })
                    
                    if exit_chart_data and len(exit_chart_data) > 0:
                        # Create a grouped bar chart for exit scenarios
                        charts.append({
                            "type": "bar",
                            "title": "Exit Scenario Analysis (PWERM)",
                            "data": {
                                "labels": exit_chart_data[0]["scenarios"] if exit_chart_data else [],
                                "datasets": [
                                    {
                                        "label": f"{exit_chart_data[0]['company']} Exit Value ($M)",
                                        "data": exit_chart_data[0]["exit_values"] if exit_chart_data else [],
                                        "backgroundColor": "rgba(59, 130, 246, 0.9)",
                                        "yAxisID": "y1"
                                    },
                                    {
                                        "label": f"{exit_chart_data[0]['company']} Probability (%)",
                                        "data": exit_chart_data[0]["probabilities"] if exit_chart_data else [],
                                        "backgroundColor": "rgba(66, 133, 244, 0.3)",
                                        "yAxisID": "y2"
                                    }
                                ]
                            },
                            "options": {
                                "scales": {
                                    "y1": {
                                        "type": "linear",
                                        "position": "left",
                                        "title": {
                                            "display": True,
                                            "text": "Exit Value ($M)"
                                        }
                                    },
                                    "y2": {
                                        "type": "linear",
                                        "position": "right",
                                        "title": {
                                            "display": True,
                                            "text": "Probability (%)"
                                        },
                                        "max": 30  # Cap at 30% for better visualization
                                    }
                                }
                            }
                        })
                        
                        # Add Sankey chart for M&A liquidation waterfall
                        for idx, company in enumerate(companies[:2]):
                            if idx >= len(exit_chart_data):
                                break
                                
                            company_name = company.get('company', 'Unknown')
                            total_funding = company.get('total_funding', 0)
                            
                            # Create Sankey data for M&A exit waterfall
                            # Filter out None values before checking
                            valid_exit_values = [v for v in exit_chart_data[idx]["exit_values"] if v is not None]
                            if valid_exit_values:
                                best_exit = max(valid_exit_values) * 1_000_000  # Convert back from millions
                            else:
                                # Use a reasonable default exit value based on valuation or $100M
                                best_exit = max(company.get('valuation', 100_000_000), 100_000_000)
                            
                            # Create nodes for Sankey
                            nodes = []
                            links = []
                            node_idx = 0
                            
                            # Exit value node
                            nodes.append({"id": node_idx, "name": f"Exit: ${best_exit/1e6:.0f}M"})
                            exit_node = node_idx
                            node_idx += 1
                            
                            # Calculate liquidation preferences using AdvancedCapTable service
                            funding_rounds = company.get('funding_rounds', [])
                            liquidation_preferences = company.get('liquidation_preferences', {})
                            cap_table = company.get('cap_table', {})
                            
                            # Use AdvancedCapTable service to calculate proper liquidation waterfall
                            try:
                                waterfall_result = self.advanced_cap_table.calculate_liquidation_waterfall(
                                    exit_value=best_exit,
                                    cap_table=cap_table,
                                    liquidation_preferences=liquidation_preferences,
                                    funding_rounds=funding_rounds
                                )
                                
                                # Extract distributions from service result
                                distributions = waterfall_result.get('distributions', [])
                                total_prefs = waterfall_result.get('total_distributed', 0)
                                
                                # Create nodes from service-calculated distributions
                                pref_nodes = []
                                for dist in distributions:
                                    if isinstance(dist, dict):
                                        shareholder = dist.get('shareholder', 'Unknown')
                                        amount = dist.get('total', 0)
                                        if amount > 0 and 'pref' in shareholder.lower() or 'preferred' in shareholder.lower():
                                            nodes.append({"id": node_idx, "name": f"{shareholder} Pref"})
                                            pref_nodes.append((node_idx, amount))
                                            
                                            # Link from exit to preference
                                            links.append({
                                                "source": exit_node,
                                                "target": node_idx,
                                                "value": amount / 1_000_000  # Convert to millions
                                            })
                                            node_idx += 1
                                
                                # If service didn't return detailed breakdown, fall back to calculated total
                                if not distributions:
                                    total_prefs = waterfall_result.get('summary', {}).get('investors', 0)
                                    
                            except Exception as e:
                                logger.warning(f"Failed to use AdvancedCapTable for liquidation waterfall: {e}, using fallback")
                                # Fallback: Use service-calculated preferences if available, otherwise simple calculation
                                total_prefs = 0
                                pref_nodes = []
                                
                                # Fallback to simple calculation if service fails
                                for round_data in reversed(funding_rounds):
                                    round_name = round_data.get('round', 'Unknown')
                                    amount = round_data.get('amount', 0)
                                    investors = round_data.get('investors', ['Investors'])
                                    liq_multiple = round_data.get('liquidation_preference', 1.0)
                                    
                                    pref_amount = min(amount * liq_multiple, best_exit - total_prefs)
                                    if pref_amount > 0:
                                        investor_name = investors[0] if investors else f"{round_name} Lead"
                                        nodes.append({"id": node_idx, "name": f"{investor_name} Pref"})
                                        pref_nodes.append((node_idx, pref_amount))
                                        
                                        links.append({
                                            "source": exit_node,
                                            "target": node_idx,
                                            "value": pref_amount / 1_000_000
                                        })
                                        
                                        total_prefs += pref_amount
                                        node_idx += 1
                            
                            # Remaining after preferences
                            remaining = max(0, best_exit - total_prefs)
                            
                            if remaining > 0:
                                # Common stock distribution node
                                nodes.append({"id": node_idx, "name": "Common Stock Pool"})
                                common_node = node_idx
                                node_idx += 1
                                
                                # Link from exit to common pool
                                links.append({
                                    "source": exit_node,
                                    "target": common_node,
                                    "value": remaining / 1_000_000
                                })
                                
                                # Distribute common based on ownership
                                # Get actual ownership percentages
                                cap_table = company.get('cap_table', {})
                                # Get actual ownership from cap table data
                                founder_ownership = cap_table.get('founders', cap_table.get('Founders', 0))
                                investor_ownership = cap_table.get('investors', cap_table.get('Investors', 0))
                                employee_ownership = cap_table.get('employees', cap_table.get('Employees', 0))
                                
                                # Founders node
                                nodes.append({"id": node_idx, "name": f"Founders (${remaining * founder_ownership / 1e6:.1f}M)"})
                                links.append({
                                    "source": common_node,
                                    "target": node_idx,
                                    "value": remaining * founder_ownership / 1_000_000
                                })
                                node_idx += 1
                                
                                # Investors common stock
                                nodes.append({"id": node_idx, "name": f"Investors Common (${remaining * investor_ownership / 1e6:.1f}M)"})
                                links.append({
                                    "source": common_node,
                                    "target": node_idx,
                                    "value": remaining * investor_ownership / 1_000_000
                                })
                                node_idx += 1
                                
                                # Employee pool
                                nodes.append({"id": node_idx, "name": f"Employees (${remaining * employee_ownership / 1e6:.1f}M)"})
                                links.append({
                                    "source": common_node,
                                    "target": node_idx,
                                    "value": remaining * employee_ownership / 1_000_000
                                })
                            
                            # Add as Sankey chart
                            sankey_chart = {
                                "type": "sankey",
                                "title": f"{company_name} - M&A Exit Waterfall (${best_exit/1e6:.0f}M Exit)",
                                "data": {
                                    "nodes": nodes,
                                    "links": links
                                },
                                "options": {
                                    "nodeAlign": "left",
                                    "nodeWidth": 30,
                                    "nodePadding": 10,
                                    "tooltip": {
                                        "callbacks": {
                                            "label": "function(context) { return context.raw.target + ': $' + context.raw.value.toFixed(1) + 'M'; }"
                                        }
                                    }
                                }
                            }
                            
                            # Pre-render complex chart
                            prerendered_chart = await self._prerender_complex_chart(sankey_chart)
                            charts.append(prerendered_chart)
            
            # PWERM Exit Scenarios with Comprehensive Ownership & Breakpoints
            if companies:
                exit_scenarios_data = {}
                for company in companies[:2]:
                    company_name = company.get('company', 'Unknown')
                    
                    # Get all the data we need for comprehensive analysis
                    valuation = self._get_field_with_fallback(company, 'valuation', 0)
                    total_funding = company.get('total_funding')
                    stage = company.get('stage', 'Series A')
                    
                    # Get ownership evolution data
                    ownership_data = company.get('ownership_evolution', {})
                    entry_ownership = ownership_data.get('entry_ownership')
                    exit_no_followon = ownership_data.get('exit_ownership_no_followon')
                    exit_with_followon = ownership_data.get('exit_ownership_with_followon')
                    
                    # Calculate if missing (DON'T SKIP - per NOPLSFIXROOTCAUSE.MD)
                    if not entry_ownership or not exit_no_followon or not exit_with_followon:
                        logger.warning(f"Missing ownership data for {company_name}, calculating fallback...")
                        
                        # Calculate from available data
                        check_size = self._get_optimal_check_size(company, fund_context or {})
                        valuation = self._get_field_with_fallback(company, 'valuation', 0)
                        entry_ownership = check_size / (valuation + check_size) if (valuation + check_size) > 0 else 0.10
                        
                        # Use gap filler for exit ownership
                        stage = company.get('stage', 'Series A')
                        rounds_to_exit = {"Seed": 4, "Series A": 3, "Series B": 2, "Series C": 1}.get(stage, 2)
                        
                        dilution_calc = self.gap_filler.calculate_exit_dilution_scenarios(
                            initial_ownership=entry_ownership,
                            rounds_to_exit=rounds_to_exit,
                            company_data=company  # Pass full company data for context
                        )
                        
                        exit_no_followon = dilution_calc.get("without_pro_rata", entry_ownership * 0.5)
                        exit_with_followon = dilution_calc.get("with_pro_rata", entry_ownership * 0.8)
                        
                        # Update the company data so it's available for other charts
                        ownership_data = {
                            "entry_ownership": entry_ownership,
                            "exit_ownership_no_followon": exit_no_followon,
                            "exit_ownership_with_followon": exit_with_followon,
                            "followon_capital_required": check_size * 2
                        }
                        company["ownership_evolution"] = ownership_data
                        
                        logger.info(f"Calculated fallback ownership for {company_name}: Entry={entry_ownership:.1%}, Exit={exit_no_followon:.1%}")
                    followon_required = ownership_data.get('followon_capital_required', 15_000_000)
                    
                    # Get optimal check size from fund fit (ensure positive fallback)
                    raw_check_size = company.get('optimal_check_size')
                    if not raw_check_size or raw_check_size <= 0:
                        inferred_default = max(company.get('valuation', 100_000_000) * 0.08, 5_000_000)
                        check_size = inferred_default
                        company['optimal_check_size'] = check_size
                    else:
                        check_size = raw_check_size
                    
                    # ROOT CAUSE FIX: Ensure PWERM scenarios exist - they should have been generated earlier
                    scenarios = company.get('pwerm_scenarios', [])
                    pwerm_valuation = company.get('pwerm_valuation', valuation)
                    
                    # If no PWERM scenarios, this is a ROOT CAUSE issue - scenarios should exist by now
                    if not scenarios:
                        logger.error(f"[EXIT_SCENARIOS] ROOT CAUSE: No PWERM scenarios found for {company_name} - should have been generated in deck generation start")
                        # Generate scenarios NOW using valuation engine (root cause fix)
                        try:
                            stage_map = {
                                "Pre-Seed": Stage.PRE_SEED if Stage else None,
                                "Pre Seed": Stage.PRE_SEED if Stage else None,
                                "Seed": Stage.SEED if Stage else None,
                                "Series A": Stage.SERIES_A if Stage else None,
                                "Series B": Stage.SERIES_B if Stage else None,
                                "Series C": Stage.SERIES_C if Stage else None,
                                "Growth": Stage.GROWTH if Stage else None,
                                "Late": Stage.LATE if Stage else None
                            }
                            
                            if Stage is None:
                                logger.error(f"[EXIT_SCENARIOS] Stage enum not available - cannot generate scenarios for {company_name}")
                                continue
                            
                            company_stage = stage_map.get(company.get("stage", "Series A"), Stage.SERIES_A)
                            
                            revenue = ensure_numeric(company.get("revenue"), 0)
                            if revenue == 0:
                                revenue = ensure_numeric(company.get("inferred_revenue"), 0)
                            if revenue == 0:
                                revenue = ensure_numeric(company.get("arr") or company.get("inferred_arr"), 1_000_000)
                            
                            valuation_for_pwerm = ensure_numeric(company.get("valuation"), 0)
                            if valuation_for_pwerm == 0:
                                valuation_for_pwerm = ensure_numeric(company.get("inferred_valuation"), 0)
                            if valuation_for_pwerm == 0:
                                valuation_for_pwerm = ensure_numeric(company.get("total_funding"), 0) * 3
                            
                            growth_rate = ensure_numeric(company.get("growth_rate"), 0)
                            if growth_rate == 0:
                                growth_rate = ensure_numeric(company.get("inferred_growth_rate"), 1.5)
                            
                            inferred_val = ensure_numeric(company.get("inferred_valuation"), None) if company.get("inferred_valuation") is not None else None
                            
                            val_request = ValuationRequest(
                                company_name=company_name,
                                stage=company_stage,
                                revenue=revenue,
                                growth_rate=growth_rate,
                                last_round_valuation=valuation_for_pwerm if valuation_for_pwerm > 0 else None,
                                inferred_valuation=inferred_val,
                                total_raised=self._get_field_safe(company, "total_funding")
                            )
                            
                            # Generate PWERM scenarios
                            pwerm_result = await self.valuation_engine._calculate_pwerm(val_request)
                            scenarios = pwerm_result.scenarios
                            
                            if scenarios and len(scenarios) > 0:
                                # Model cap table evolution for each scenario
                                check_size = self._get_optimal_check_size(company, fund_context or {})
                                post_money = valuation_for_pwerm + check_size
                                our_investment = {
                                    'amount': check_size,
                                    'ownership': check_size / post_money if post_money > 0 else 0.08
                                }
                                
                                for scenario in scenarios:
                                    self.valuation_engine.model_cap_table_evolution(
                                        scenario,
                                        company,
                                        our_investment
                                    )
                                
                                self.valuation_engine.generate_return_curves(scenarios, our_investment)
                                
                                company['pwerm_scenarios'] = scenarios
                                company['pwerm_valuation'] = pwerm_result.fair_value
                                logger.info(f"[EXIT_SCENARIOS] ✅ Generated {len(scenarios)} PWERM scenarios for {company_name} (root cause fix)")
                            else:
                                logger.error(f"[EXIT_SCENARIOS] ❌ Failed to generate PWERM scenarios for {company_name}")
                                continue
                        except Exception as e:
                            logger.error(f"[EXIT_SCENARIOS] ❌ Error generating PWERM scenarios for {company_name}: {e}", exc_info=True)
                            continue
                    
                    if scenarios and len(scenarios) > 0:
                        # Use real PWERM scenarios with comprehensive data
                        scenario_list = []
                        
                        # Calculate weighted exit multiple for follow-on analysis
                        weighted_exit_multiple = 0
                        for s in scenarios[:6]:  # Top 6 scenarios for display
                            prob = s.probability if hasattr(s, 'probability') else s.get('probability', 0)
                            exit_val = s.exit_value if hasattr(s, 'exit_value') else s.get('exit_value', 0)
                            if valuation > 0:
                                weighted_exit_multiple += prob * (exit_val / valuation)
                        
                        # Calculate liquidation preference breakpoint using AdvancedCapTable service
                        funding_rounds = company.get('funding_rounds', [])
                        liquidation_preferences = company.get('liquidation_preferences', {})
                        cap_table = company.get('cap_table', {})
                        
                        # Use service to calculate actual liquidation preference breakpoint
                        try:
                            # Calculate breakpoints for base case exit
                            base_exit = valuation if valuation > 0 else total_funding * 2
                            waterfall_breakpoints = self.advanced_cap_table.calculate_waterfall_breakpoints(
                                base_case_exit=Decimal(str(base_exit)),
                                bull_multiplier=2.0,
                                bear_multiplier=0.5
                            )
                            
                            # Extract liquidation preference breakpoint from service
                            # The breakpoint is where preferred investors get their preference paid
                            liq_pref_breakpoint = float(waterfall_breakpoints.get('liquidation_preference_total', total_funding))
                            
                        except Exception as e:
                            logger.warning(f"Failed to calculate liquidation breakpoint using AdvancedCapTable: {e}, using fallback")
                            # Fallback: Calculate from funding rounds
                            liq_pref_breakpoint = 0
                            for round_data in funding_rounds:
                                amount = round_data.get('amount', 0)
                                liq_multiple = round_data.get('liquidation_preference', 1.0)
                                liq_pref_breakpoint += amount * liq_multiple
                            
                            if liq_pref_breakpoint == 0:
                                liq_pref_breakpoint = total_funding * 1.0  # Final fallback
                        
                        for s in scenarios[:6]:  # Show top 6 scenarios
                            # Extract scenario data - PWERMScenario objects have direct attributes
                            scenario_name = s.scenario if hasattr(s, 'scenario') else s.get('scenario', 'Unknown')
                            probability = s.probability if hasattr(s, 'probability') else s.get('probability', 0)
                            exit_value = s.exit_value if hasattr(s, 'exit_value') else s.get('exit_value', 0)
                            time_to_exit = s.time_to_exit if hasattr(s, 'time_to_exit') else s.get('time_to_exit', 3)
                            
                            # Calculate our proceeds based on exit type
                            # Check if this is an IPO scenario (converts to common)
                            is_ipo = 'IPO' in scenario_name or 'NYSE' in scenario_name or 'NASDAQ' in scenario_name or 'public' in scenario_name.lower()
                            
                            if is_ipo:
                                # IPO: All preferred converts to common, simple pro-rata
                                proceeds_no_followon = exit_value * exit_no_followon
                                proceeds_with_followon = exit_value * exit_with_followon
                                ownership_at_exit_no_followon = exit_no_followon
                                ownership_at_exit_with_followon = exit_with_followon
                            else:
                                # M&A: Use AdvancedCapTable service to calculate liquidation waterfall
                                try:
                                    # Calculate actual liquidation waterfall for this exit value
                                    waterfall_result = self.advanced_cap_table.calculate_liquidation_waterfall(
                                        exit_value=exit_value,
                                        cap_table=cap_table,
                                        liquidation_preferences=liquidation_preferences,
                                        funding_rounds=funding_rounds
                                    )
                                    
                                    # Extract our proceeds from the waterfall distribution
                                    distributions = waterfall_result.get('distributions', [])
                                    total_distributed = waterfall_result.get('total_distributed', 0)
                                    
                                    # Find our investment in the distributions
                                    # This is a simplified approach - in reality we'd need to match by investor name
                                    # For now, calculate based on ownership percentage
                                    remaining_after_prefs = max(0, exit_value - total_distributed)
                                    
                                    # Our proceeds are pro-rata of remaining after preferences
                                    proceeds_no_followon = remaining_after_prefs * exit_no_followon
                                    proceeds_with_followon = remaining_after_prefs * exit_with_followon
                                    ownership_at_exit_no_followon = exit_no_followon
                                    ownership_at_exit_with_followon = exit_with_followon
                                    
                                except Exception as e:
                                    logger.warning(f"Failed to use AdvancedCapTable for M&A liquidation: {e}, using fallback")
                                    # Fallback: Simple calculation
                                    if exit_value < liq_pref_breakpoint:
                                        # We're junior to existing investors, likely get 0
                                        proceeds_no_followon = 0
                                        proceeds_with_followon = 0
                                        ownership_at_exit_no_followon = 0
                                        ownership_at_exit_with_followon = 0
                                    else:
                                        # Above liquidation preference, we participate pro-rata
                                        remaining_after_prefs = exit_value - liq_pref_breakpoint
                                        proceeds_no_followon = remaining_after_prefs * exit_no_followon
                                        proceeds_with_followon = remaining_after_prefs * exit_with_followon
                                        ownership_at_exit_no_followon = exit_no_followon
                                        ownership_at_exit_with_followon = exit_with_followon
                            
                            # Calculate MOICs
                            moic_no_followon = proceeds_no_followon / check_size if check_size > 0 else 0
                            moic_with_followon = proceeds_with_followon / (check_size + followon_required) if (check_size + followon_required) > 0 else 0
                            
                            scenario_data = {
                                "scenario": scenario_name,
                                "probability": probability,
                                "exit_value": exit_value,
                                "time_to_exit": time_to_exit,
                                "exit_type": "IPO" if is_ipo else "M&A",  # NEW: Track exit type
                                # Ownership data
                                "entry_ownership": entry_ownership,
                                "exit_ownership_no_followon": ownership_at_exit_no_followon,
                                "exit_ownership_with_followon": ownership_at_exit_with_followon,
                                # Proceeds data
                                "proceeds_no_followon": proceeds_no_followon,
                                "proceeds_with_followon": proceeds_with_followon,
                                # Returns data
                                "moic_no_followon": moic_no_followon,
                                "moic_with_followon": moic_with_followon,
                                # Breakpoint data
                                "below_liquidation": exit_value < liq_pref_breakpoint and not is_ipo,  # IPOs don't have liquidation preferences
                                "liquidation_breakpoint": liq_pref_breakpoint if not is_ipo else None,  # Not applicable for IPOs
                                "conversion_to_common": is_ipo  # NEW: Flag for preferred conversion
                            }
                            
                            scenario_list.append(scenario_data)
                        
                        # Get detailed follow-on scenarios with round-by-round data
                        followon_detail = ownership_data.get('followon_scenarios', {})
                        round_details = followon_detail.get('with_followon', {}).get('round_details', [])
                        
                        # Use our new comprehensive method for investor-specific scenarios
                        investor_scenarios = {}
                        try:
                            investor_scenarios = self._calculate_investor_specific_exit_scenarios(
                                company_data=company,
                                our_investment=check_size,
                                check_size=check_size,
                                stage=stage
                            )
                        except Exception as e:
                            logger.warning(f"Investor-specific scenario calculation failed: {e}")
                            # Fallback to basic calculations if new method fails
                            investor_scenarios = {}
                        
                        # Extract key data from investor scenarios
                        cap_table_history = investor_scenarios.get('cap_table_data', {})
                        investor_stack = investor_scenarios.get('investor_stack', [])
                        detailed_exit_scenarios = investor_scenarios.get('exit_scenarios', [])
                        investor_breakpoints = investor_scenarios.get('breakpoints', {})
                        ownership_analysis = investor_scenarios.get('ownership_analysis', {})
                        preference_analysis = investor_scenarios.get('preference_analysis', {})
                        
                        # Override ownership data with investor-specific calculations
                        if ownership_analysis:
                            entry_ownership = ownership_analysis.get('our_entry_ownership', entry_ownership * 100) / 100
                            exit_no_followon = ownership_analysis.get('our_exit_ownership_no_followon', exit_no_followon * 100) / 100
                            exit_with_followon = ownership_analysis.get('our_exit_ownership_with_followon', exit_with_followon * 100) / 100
                        
                        # Use investor-specific breakpoints
                        real_liq_pref = investor_breakpoints.get('liquidation_preference_satisfied', liq_pref_breakpoint)
                        real_common_threshold = investor_breakpoints.get('common_meaningful_proceeds', liq_pref_breakpoint + 10_000_000)
                        real_conversion_point = investor_breakpoints.get('our_conversion_point', real_liq_pref * 2)
                        
                        # Keep existing PWERM and advanced cap table calculations as fallback
                        pwerm_data = self.shared_data.get("pwerm", {})
                        company_pwerm = pwerm_data.get(company_name, {})
                        waterfall_data = company_pwerm.get("waterfall", {})
                        waterfall_breakpoints = waterfall_data.get("breakpoints", [])
                        
                        # If investor scenarios failed, try fallback methods
                        if not investor_scenarios:
                            # 2. Use PrePostCapTable for complete cap table history
                            ownership_evolution = {}
                            if company.get("funding_rounds"):
                                try:
                                    cap_table_history = self.cap_table_service.calculate_full_cap_table_history(company)
                                    # Extract ownership evolution from cap table history
                                    if cap_table_history and 'sankey_data' in cap_table_history:
                                        ownership_evolution = cap_table_history.get('ownership_evolution', {})
                                except Exception as e:
                                    logger.warning(f"Cap table history calculation failed: {e}")
                            
                            # 3. Use AdvancedCapTable for waterfall breakpoints
                            advanced_breakpoints = {}
                            if valuation > 0:
                                try:
                                    advanced_breakpoints = self.advanced_cap_table.calculate_waterfall_breakpoints(
                                        base_case_exit=Decimal(str(valuation)),
                                        bull_multiplier=2.0,
                                        bear_multiplier=0.5
                                    )
                                except Exception as e:
                                    logger.warning(f"Advanced cap table breakpoints failed: {e}")
                        
                        # 3. Calculate future round impact
                        next_round_timing = company.get('next_round_timing', 12)  # months
                        next_round_size = company.get('next_round_size', total_funding * 0.5)
                        next_round_valuation = valuation * 2  # Typical 2x step-up
                        
                        # Project ownership through next round
                        pro_rata_calc = {}
                        if entry_ownership > 0:
                            try:
                                pro_rata_calc = self.cap_table_service.calculate_pro_rata_investment(
                                    current_ownership=Decimal(str(entry_ownership)),
                                    new_money_raised=Decimal(str(next_round_size)),
                                    pre_money_valuation=Decimal(str(next_round_valuation))
                                )
                            except Exception as e:
                                logger.warning(f"Pro-rata calculation failed: {e}")
                        
                        # If we don't have investor scenarios data, extract from other sources
                        if not investor_scenarios:
                            # Extract real breakpoints from multiple sources  
                            real_liq_pref = liq_pref_breakpoint  # Default to simple calculation
                            real_conversion_point = None
                            real_common_threshold = None
                            investor_breakevens = {}
                            ownership_evolution = {}
                            
                            # First try waterfall breakpoints from PWERM
                            if waterfall_breakpoints:
                                for bp in waterfall_breakpoints:
                                    if 'liquidation preferences satisfied' in bp.get('description', ''):
                                        real_liq_pref = bp['value']
                                    elif 'Common shareholders receive' in bp.get('description', ''):
                                        real_common_threshold = bp['value']
                            
                            # Then overlay with AdvancedCapTable breakpoints for more detail
                            advanced_breakpoints = {}
                            if valuation > 0:
                                try:
                                    advanced_breakpoints = self.advanced_cap_table.calculate_waterfall_breakpoints(
                                        base_case_exit=Decimal(str(valuation)),
                                        bull_multiplier=2.0,
                                        bear_multiplier=0.5
                                    )
                                except Exception as e:
                                    logger.warning(f"Advanced cap table breakpoints failed: {e}")

                            if advanced_breakpoints:
                                base_scenario = advanced_breakpoints.get('base', {}) or {}
                                if base_scenario:
                                    total_liq_prefs = base_scenario.get('total_liquidation_preferences')
                                    if total_liq_prefs is not None:
                                        real_liq_pref = float(total_liq_prefs)

                                    breakevens = base_scenario.get('investor_breakeven_points')
                                    if isinstance(breakevens, dict):
                                        investor_breakevens = breakevens

                                    common_thresh = base_scenario.get('common_participation_threshold')
                                    if common_thresh is not None:
                                        real_common_threshold = float(common_thresh)
                            
                            # Use ownership evolution data if available
                            if ownership_evolution:
                                # Override with more accurate ownership data from cap table reconstruction
                                if 'our_entry_ownership' in ownership_evolution:
                                    entry_ownership = ownership_evolution['our_entry_ownership']
                                if 'our_exit_ownership_no_followon' in ownership_evolution:
                                    exit_no_followon = ownership_evolution['our_exit_ownership_no_followon']
                                if 'our_exit_ownership_with_followon' in ownership_evolution:
                                    exit_with_followon = ownership_evolution['our_exit_ownership_with_followon']
                            
                            # Calculate actual conversion point based on ownership
                            if entry_ownership > 0:
                                # Conversion point = where converting to common beats staying as preferred
                                # This is typically where: exit_value * ownership% > liquidation_preference
                                real_conversion_point = real_liq_pref / entry_ownership
                            else:
                                real_conversion_point = real_liq_pref * 2  # Default fallback
                        
                        # CRITICAL: Calculate the $150M problem for 0 DPI funds
                        # What happens at realistic exit values?
                        realistic_exits = [50_000_000, 86_000_000, 150_000_000, 250_000_000, 500_000_000]
                        exit_waterfall_results = []
                        
                        # Get ACTUAL fund context from API request, not hardcoded
                        fund_context = self.shared_data.get('fund_context', {})
                        actual_fund_size = fund_context.get('fund_size', DEFAULT_FUND_SIZE)
                        actual_current_dpi = fund_context.get('current_dpi', fund_context.get('dpi', 0.0))
                        
                        for exit_val in realistic_exits:
                            # Calculate DPI impact using enhanced valuation engine
                            dpi_impact = self.valuation_engine.calculate_fund_dpi_impact(
                                investment_amount=check_size,
                                entry_stage=company.get('stage', 'Series B'),
                                exit_value=exit_val,
                                total_preferences_ahead=total_funding,  # All preferences ahead of us
                                fund_size=actual_fund_size,  # Use ACTUAL fund size from context
                                fund_dpi=actual_current_dpi  # Use ACTUAL current DPI
                            )
                            exit_waterfall_results.append({
                                'exit_value': exit_val,
                                'our_proceeds': dpi_impact['our_return'],
                                'moic': dpi_impact['moic'],
                                'dpi_contribution': dpi_impact['dpi_contribution'],
                                'reality_check': dpi_impact.get('reality_check', '')
                            })
                        
                        # Calculate future breakpoints after next round
                        future_liq_pref = total_funding + next_round_size
                        future_conversion_point = future_liq_pref
                        if pro_rata_calc and 'new_ownership_with_pro_rata' in pro_rata_calc:
                            future_ownership = float(pro_rata_calc['new_ownership_with_pro_rata'])
                            if future_ownership > 0:
                                future_conversion_point = future_liq_pref / future_ownership
                        
                        # Add comprehensive summary data
                        company_scenarios = {
                            "scenarios": scenario_list,
                            "entry_economics": {
                                "investment": check_size,
                                "post_money_valuation": valuation,
                                "entry_ownership": entry_ownership * 100,  # As percentage
                                "price_per_share": valuation / 100_000_000  # Approximate
                            },
                            "exit_economics": {
                                "ownership_no_followon": exit_no_followon * 100,
                                "ownership_with_followon": exit_with_followon * 100,
                                "followon_capital": followon_required,
                                "total_capital_with_followon": check_size + followon_required,
                                "reserve_ratio": (check_size + followon_required) / check_size if check_size > 0 else 2.0
                            },
                            "realistic_exits": exit_waterfall_results,  # THE $150M PROBLEM DATA
                            "breakpoints": {
                                "liquidation_preference": real_liq_pref,
                                "conversion_point": real_conversion_point,
                                "common_threshold": real_common_threshold,  # Now dynamically calculated
                                "our_breakeven": investor_breakpoints.get('our_breakeven', check_size / entry_ownership if entry_ownership > 0 else valuation),
                                "our_2x": investor_breakpoints.get('our_2x', (check_size * 2) / entry_ownership if entry_ownership > 0 else valuation * 2),
                                "our_3x": investor_breakpoints.get('our_3x', (check_size * 3) / entry_ownership if entry_ownership > 0 else valuation * 3),
                                # Future round impacts
                                "next_round_dilution": future_liq_pref,
                                "next_round_conversion": future_conversion_point,
                                "next_round_timing": next_round_timing
                            },
                            "weighted_outcomes": {
                                "expected_exit_value": pwerm_valuation,
                                "expected_multiple": weighted_exit_multiple,
                                "probability_of_loss": sum(s.get("probability", 0) for s in scenario_list if s.get("moic_no_followon", 0) < 1.0),
                                "probability_of_3x": sum(s.get("probability", 0) for s in scenario_list if s.get("moic_with_followon", 0) >= 3.0)
                            },
                            "followon_requirements": {
                                "round_details": round_details,  # Per-round investment requirements
                                "summary": followon_detail.get('followon_summary', {}),
                                "total_reserves_needed": followon_detail.get('followon_summary', {}).get('total_reserves_needed', 0),
                                "can_maintain_ownership": followon_detail.get('followon_summary', {}).get('can_maintain_ownership', False)
                            },
                            # Add investor stack and detailed exit scenarios from our new method
                            "investor_stack": investor_stack if investor_scenarios else [],
                            "detailed_exit_scenarios": detailed_exit_scenarios if investor_scenarios else [],
                            "preference_analysis": preference_analysis if investor_scenarios else {},
                            "ownership_breakdown": {
                                "common": ownership_analysis.get('common_ownership_pct', 0) if ownership_analysis else 0,
                                "preferred": 100 - ownership_analysis.get('common_ownership_pct', 0) if ownership_analysis else 100,
                                "our_position": preference_analysis.get('our_position_in_stack', 0) if preference_analysis else 0
                            }
                        }
                        
                        # ROOT CAUSE FIX: Ensure company_scenarios includes 'scenarios' key for probability cloud
                        company_scenarios['scenarios'] = scenario_list  # Ensure scenarios list is included
                        company_scenarios['pwerm_scenarios'] = scenarios  # Include raw PWERM scenarios for probability cloud
                        exit_scenarios_data[company_name] = company_scenarios
                        logger.info(f"[EXIT_SCENARIOS] ✅ Populated exit_scenarios_data for {company_name} with {len(scenario_list)} scenarios")
                        
                    else:
                        logger.error(f"[EXIT_SCENARIOS] ROOT CAUSE: No PWERM scenarios found for {company_name} after generation attempts - this should not happen")
                        # Don't mark as unavailable - scenarios should have been generated above
                        # If we reach here, something went wrong with scenario generation
                        continue
                
                # Create charts for exit scenarios
                exit_charts = []
                
                # Generate probability cloud visualization with breakpoints
                for company_name, scenarios_data in exit_scenarios_data.items():
                    # Use actual scenarios if pwerm_scenarios not available
                    pwerm_scenarios = None
                    if scenarios_data:
                        # Try to get scenarios from multiple sources
                        if 'pwerm_scenarios' in scenarios_data:
                            pwerm_scenarios = scenarios_data['pwerm_scenarios']
                        elif 'scenarios' in scenarios_data:
                            # Use scenarios list and convert to PWERM format if needed
                            scenarios = scenarios_data['scenarios']
                            if scenarios and len(scenarios) > 0:
                                pwerm_scenarios = scenarios
                        
                    if pwerm_scenarios:
                        
                        # Calculate breakpoint distributions
                        breakpoint_data = self.valuation_engine.calculate_breakpoint_distributions(pwerm_scenarios)
                        
                        # Generate scenario curves for probability cloud
                        scenario_curves = []
                        # Get check size for this company
                        company_obj = next((c for c in companies[:2] if c.get('company') == company_name), None)
                        if not company_obj:
                            continue
                        check_size = self._get_optimal_check_size(company_obj, fund_context or {})
                        
                        for scenario in pwerm_scenarios[:10]:  # Top 10 most probable
                            # Handle both dict and object scenarios
                            if isinstance(scenario, dict):
                                scenario_name = scenario.get('scenario', scenario.get('name', 'Unknown'))
                                probability = scenario.get('probability', 0)
                                exit_value = scenario.get('exit_value', 0)
                                moic = scenario.get('moic_no_followon', scenario.get('moic', 0))
                            else:
                                scenario_name = getattr(scenario, 'scenario', getattr(scenario, 'name', 'Unknown'))
                                probability = getattr(scenario, 'probability', 0)
                                exit_value = getattr(scenario, 'exit_value', 0)
                                moic = getattr(scenario, 'moic_no_followon', getattr(scenario, 'moic', 0))
                            
                            exit_values = [10e6, 25e6, 50e6, 75e6, 100e6, 150e6, 250e6, 500e6, 750e6, 1e9, 2e9, 5e9]
                            return_multiples = []
                            
                            # Get ownership from scenarios_data if available
                            entry_economics = scenarios_data.get('entry_economics', {})
                            entry_ownership = entry_economics.get('entry_ownership', 10) / 100 if entry_economics else 0.10
                            
                            for exit_val in exit_values:
                                # Calculate our return at this exit value
                                # Simple calculation: ownership * exit value, accounting for liquidation preference
                                liq_pref = 0
                                if isinstance(scenario, dict):
                                    liq_pref = scenario.get('liquidation_preference', 0)
                                    ownership_at_exit = scenario.get('exit_ownership_no_followon', entry_ownership)
                                else:
                                    liq_pref = getattr(scenario, 'liquidation_preference', 0)
                                    ownership_at_exit = getattr(scenario, 'exit_ownership_no_followon', entry_ownership)
                                
                                if exit_val > liq_pref and ownership_at_exit:
                                    # Above liquidation preference, we get our ownership share
                                    our_proceeds = ownership_at_exit * (exit_val - liq_pref) 
                                else:
                                    # Below liquidation preference, we might get nothing or partial
                                    our_proceeds = min((ownership_at_exit or 0.1) * exit_val, exit_val * 0.1)  # Rough estimate
                                
                                moic_calc = our_proceeds / check_size if check_size > 0 else 0
                                return_multiples.append(moic_calc)
                            
                            scenario_curves.append({
                                "name": scenario_name,
                                "probability": probability,
                                "return_curve": {
                                    "exit_values": exit_values,
                                    "return_multiples": return_multiples
                                }
                            })
                        
                        # Create probability cloud chart
                        probability_chart = {
                            "type": "probability_cloud",
                            "title": f"{company_name} - Return Scenarios with Breakpoints",
                            "data": {
                                "scenario_curves": scenario_curves,
                                "breakpoints": breakpoint_data,
                                "probability_bands": {
                                    "p10": breakpoint_data.get("our_breakeven", {}).get("p10", 0),
                                    "p25": breakpoint_data.get("our_breakeven", {}).get("p25", 0),
                                    "p50": breakpoint_data.get("our_breakeven", {}).get("p50", 0),
                                    "p75": breakpoint_data.get("our_3x", {}).get("p75", 0),
                                    "p90": breakpoint_data.get("our_3x", {}).get("p90", 0)
                                }
                            },
                            "options": {
                                "scales": {
                                    "y1": {
                                        "type": "linear",
                                        "position": "left",
                                        "title": {
                                            "display": True,
                                            "text": "Our Proceeds ($M)"
                                        }
                                    },
                                    "y2": {
                                        "type": "linear",
                                        "position": "right",
                                        "title": {
                                            "display": True,
                                            "text": "DPI Contribution (%)"
                                        },
                                        "grid": {
                                            "drawOnChartArea": False
                                        }
                                    }
                                },
                                "plugins": {
                                    "annotation": {
                                        "annotations": {
                                            "breakeven": {
                                                "type": "line",
                                                "yMin": scenarios_data['entry_economics']['investment']/1e6,
                                                "yMax": scenarios_data['entry_economics']['investment']/1e6,
                                                "borderColor": "rgba(0, 0, 0, 0.5)",
                                                "borderWidth": 2,
                                                "borderDash": [5, 5],
                                                "label": {
                                                    "content": "Breakeven",
                                                    "enabled": True,
                                                    "position": "end"
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                        exit_charts.append(probability_chart)
                    
                    # Then add the existing PWERM scenarios
                    if scenarios_data and 'scenarios' in scenarios_data:
                        scenarios = scenarios_data['scenarios'][:6]  # Top 6 scenarios
                        
                        # Create exit waterfall Sankey
                        nodes = []
                        links = []
                        node_idx = 0
                        
                        # Source node for exit value
                        for scenario in scenarios:
                            exit_value = scenario.get('exit_value', 0)
                            scenario_name = scenario.get('scenario', 'Unknown')
                            probability = scenario.get('probability', 0)
                            
                            if exit_value > 0 and probability > 0.05:  # Only show scenarios > 5% probability
                                # Create nodes for this scenario
                                scenario_node_name = f"{scenario_name} (${exit_value/1e6:.0f}M, {probability*100:.0f}%)"
                                nodes.append({"id": node_idx, "name": scenario_node_name})
                                scenario_node = node_idx
                                node_idx += 1
                                
                                # Calculate distributions
                                liq_pref = scenario.get('liquidation_preference', 0)
                                proceeds_no_followon = scenario.get('proceeds_no_followon', 0)
                                proceeds_with_followon = scenario.get('proceeds_with_followon', 0)
                                
                                # Add distribution nodes
                                if liq_pref > 0:
                                    nodes.append({"id": node_idx, "name": f"Liquidation Pref"})
                                    links.append({
                                        "source": scenario_node,
                                        "target": node_idx,
                                        "value": liq_pref / 1_000_000
                                    })
                                    node_idx += 1
                                
                                # Our proceeds
                                nodes.append({"id": node_idx, "name": f"Our Return"})
                                links.append({
                                    "source": scenario_node,
                                    "target": node_idx,
                                    "value": max(proceeds_no_followon, proceeds_with_followon) / 1_000_000
                                })
                                node_idx += 1
                        
                        if nodes and links:
                            exit_sankey_chart = {
                                "type": "sankey",
                                "title": f"{company_name} - Exit Waterfall by Scenario",
                                "data": {
                                    "nodes": nodes,
                                    "links": links
                                }
                            }
                            
                            # Pre-render complex chart
                            prerendered_exit_chart = await self._prerender_complex_chart(exit_sankey_chart)
                            exit_charts.append(prerendered_exit_chart)
                        
                        # Add breakpoint visualization chart with real data from services
                        breakpoints = scenarios_data.get('breakpoints', {})
                        if breakpoints:
                            liq_pref_point = breakpoints.get('liquidation_preference', 0)
                            conversion_point = breakpoints.get('conversion_point', 0)
                            common_threshold = breakpoints.get('common_threshold', 0)
                            
                            # Get our specific breakeven points
                            our_breakeven = breakpoints.get('our_breakeven', 0)
                            our_2x = breakpoints.get('our_2x', 0)
                            our_3x = breakpoints.get('our_3x', 0)
                            
                            # Get future round impacts
                            next_round_dilution = breakpoints.get('next_round_dilution', 0)
                            next_round_conversion = breakpoints.get('next_round_conversion', 0)
                            next_round_timing = breakpoints.get('next_round_timing', 12)
                            
                            # Get our ownership and returns at different points
                            entry_ownership = scenarios_data.get('entry_economics', {}).get('entry_ownership', 10) / 100
                            
                            # Create shorter labels to avoid overlapping
                            breakpoint_chart = {
                                "type": "line",
                                "title": f"{company_name} - Cap Table Based Exit Analysis",
                                "data": {
                                    "labels": ["Start", 
                                             f"Liq ${liq_pref_point/1e6:.0f}M", 
                                             f"Conv ${conversion_point/1e6:.0f}M",
                                             f"1x ${our_breakeven/1e6:.0f}M",
                                             f"2x ${our_2x/1e6:.0f}M",
                                             f"3x ${our_3x/1e6:.0f}M",
                                             f"Next ${next_round_dilution/1e6:.0f}M"],
                                    "datasets": [
                                        {
                                            "label": "Exit Value (Current)",
                                            "data": [0, liq_pref_point/1e6, conversion_point/1e6, 
                                                   our_breakeven/1e6, our_2x/1e6, our_3x/1e6, next_round_dilution/1e6],
                                            "borderColor": "rgb(75, 192, 192)",
                                            "backgroundColor": "rgba(75, 192, 192, 0.2)",
                                            "tension": 0.1
                                        },
                                        {
                                            "label": "Our Returns (No Follow-on)",
                                            "data": [0, 0, conversion_point * entry_ownership / 1e6, 
                                                   our_breakeven * entry_ownership / 1e6, 
                                                   our_2x * entry_ownership / 1e6, 
                                                   our_3x * entry_ownership / 1e6,
                                                   next_round_dilution * entry_ownership * 0.6 / 1e6],  # Diluted
                                            "borderColor": "rgb(255, 99, 132)",
                                            "backgroundColor": "rgba(255, 99, 132, 0.2)",
                                            "borderWidth": 2,
                                            "borderDash": [5, 5],
                                            "tension": 0.1
                                        },
                                        {
                                            "label": "Our Returns (With Pro-Rata)",
                                            "data": [0, 0, conversion_point * entry_ownership / 1e6, 
                                                   our_breakeven * entry_ownership / 1e6, 
                                                   our_2x * entry_ownership / 1e6, 
                                                   our_3x * entry_ownership / 1e6,
                                                   next_round_dilution * entry_ownership / 1e6],  # Maintained
                                            "borderColor": "rgb(54, 162, 235)",
                                            "backgroundColor": "rgba(54, 162, 235, 0.2)",
                                            "borderWidth": 3,
                                            "tension": 0.1
                                        }
                                    ]
                                },
                                "options": {
                                    "responsive": True,
                                    "plugins": {
                                        "legend": {
                                            "position": "top"
                                        },
                                        "title": {
                                            "display": True,
                                            "text": f"{company_name} - Real Breakpoints from Cap Table Analysis"
                                        },
                                    },
                                    "scales": {
                                        "x": {
                                            "ticks": {
                                                "maxRotation": 45,
                                                "minRotation": 45,
                                                "fontSize": 10
                                            }
                                        },
                                        "y": {
                                            "title": {
                                                "display": True,
                                                "text": "Exit Value / Returns ($M)"
                                            }
                                        }
                                    },
                                        "annotation": {
                                            "annotations": {
                                                "liqPref": {
                                                    "type": "line",
                                                    "yMin": liq_pref_point/1e6,
                                                    "yMax": liq_pref_point/1e6,
                                                    "borderColor": "rgba(255, 99, 132, 0.5)",
                                                    "borderWidth": 2,
                                                    "borderDash": [5, 5],
                                                    "label": {
                                                        "content": f"Liquidation Preference: ${liq_pref_point/1e6:.1f}M",
                                                        "enabled": True,
                                                        "position": "start"
                                                    }
                                                },
                                                "conversion": {
                                                    "type": "line",
                                                    "yMin": conversion_point/1e6,
                                                    "yMax": conversion_point/1e6,
                                                    "borderColor": "rgba(54, 162, 235, 0.5)",
                                                    "borderWidth": 2,
                                                    "borderDash": [10, 5],
                                                    "label": {
                                                        "content": f"Conversion Point: ${conversion_point/1e6:.1f}M",
                                                        "enabled": True,
                                                        "position": "center"
                                                    }
                                                },
                                                "nextRound": {
                                                    "type": "line",
                                                    "yMin": next_round_conversion/1e6,
                                                    "yMax": next_round_conversion/1e6,
                                                    "borderColor": "rgba(255, 206, 86, 0.5)",
                                                    "borderWidth": 2,
                                                    "borderDash": [2, 2],
                                                    "label": {
                                                        "content": f"Post-Next Round ({next_round_timing:.0f}mo): ${next_round_conversion/1e6:.1f}M",
                                                        "enabled": True,
                                                        "position": "end"
                                                    }
                                                },
                                                "returnZone": {
                                                    "type": "box",
                                                    "xMin": 0,
                                                    "xMax": 6,
                                                    "yMin": our_breakeven/1e6,
                                                    "yMax": our_2x/1e6,
                                                    "backgroundColor": "rgba(0, 255, 0, 0.05)",
                                                    "borderColor": "rgba(0, 255, 0, 0.3)",
                                                    "label": {
                                                        "content": "Target Return Zone (1x-2x)",
                                                        "enabled": True
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            exit_charts.append(breakpoint_chart)
                            
                            # Add ownership evolution chart showing dilution path
                            if cap_table_history and 'history' in cap_table_history:
                                rounds = []
                                our_ownership_path = []
                                
                                for round_data in cap_table_history['history']:
                                    round_name = round_data.get('round', 'Unknown')
                                    rounds.append(round_name)
                                    
                                    # Find our ownership in this round
                                    cap_table = round_data.get('cap_table', {})
                                    our_stake = 0
                                    for investor, ownership in cap_table.items():
                                        if 'Our' in investor or 'Fund' in investor:
                                            our_stake = ownership
                                            break
                                    our_ownership_path.append(our_stake)
                                
                                # Add projected future rounds
                                if rounds:
                                    rounds.extend(['Next Round', 'Round After', 'Exit'])
                                    # Project dilution assuming no follow-on
                                    last_ownership = our_ownership_path[-1] if our_ownership_path else entry_ownership * 100
                                    our_ownership_path.append(last_ownership * 0.75)  # 25% dilution
                                    our_ownership_path.append(last_ownership * 0.60)  # 40% total dilution
                                    our_ownership_path.append(last_ownership * 0.50)  # 50% at exit
                                
                                ownership_evolution_chart = {
                                    "type": "line",
                                    "title": f"{company_name} - Ownership Evolution Through Rounds",
                                    "data": {
                                        "labels": rounds,
                                        "datasets": [
                                            {
                                                "label": "Our Ownership % (No Follow-on)",
                                                "data": our_ownership_path,
                                                "borderColor": "rgb(255, 99, 132)",
                                                "backgroundColor": "rgba(255, 99, 132, 0.2)",
                                                "borderWidth": 2,
                                                "borderDash": [5, 5],
                                                "tension": 0.2
                                            },
                                            {
                                                "label": "Our Ownership % (With Pro-Rata)",
                                                "data": [o if i < len(rounds)-3 else our_ownership_path[len(our_ownership_path)-4] 
                                                        for i, o in enumerate(our_ownership_path)],
                                                "borderColor": "rgb(54, 162, 235)",
                                                "backgroundColor": "rgba(54, 162, 235, 0.2)",
                                                "borderWidth": 3,
                                                "tension": 0.2
                                            }
                                        ]
                                    },
                                    "options": {
                                        "responsive": True,
                                        "plugins": {
                                            "title": {
                                                "display": True,
                                                "text": "Ownership % Evolution Based on Cap Table Reconstruction"
                                            }
                                        },
                                        "scales": {
                                            "y": {
                                                "beginAtZero": True,
                                                "max": 100,
                                                "title": {
                                                    "display": True,
                                                    "text": "Ownership %"
                                                }
                                            }
                                        }
                                    }
                                }
                                exit_charts.append(ownership_evolution_chart)
                
                # Add probability cloud visualization for future breakpoints
                # ROOT CAUSE FIX: Always generate probability cloud, ensure it has data
                if companies and len(companies) > 0:
                    # Get PWERM scenarios with cap table evolution for first company
                    first_company = companies[0]
                    check_size = self._get_optimal_check_size(first_company, fund_context)
                    
                    # Generate comprehensive probability cloud data using our new method
                    probability_cloud_data = self._generate_probability_cloud_data(first_company, check_size)
                    
                    # ROOT CAUSE FIX: Ensure scenario_curves always exists, create default if missing
                    if not probability_cloud_data.get('scenario_curves'):
                        logger.warning("[PROB_CLOUD] No scenario_curves generated, creating default")
                        probability_cloud_data['scenario_curves'] = []
                        probability_cloud_data['breakpoint_clouds'] = probability_cloud_data.get('breakpoint_clouds', [])
                        probability_cloud_data['decision_zones'] = probability_cloud_data.get('decision_zones', [])
                        probability_cloud_data['config'] = probability_cloud_data.get('config', {
                            "x_axis": "Exit Value ($M)",
                            "y_axis": "Probability (%)"
                        })
                    
                    # Always add the probability cloud chart
                    probability_cloud_chart = format_probability_cloud_chart(
                        scenario_curves=probability_cloud_data['scenario_curves'],
                        breakpoint_clouds=probability_cloud_data.get('breakpoint_clouds', []),
                        decision_zones=probability_cloud_data.get('decision_zones', []),
                        config=probability_cloud_data.get('config', {
                            "x_axis": "Exit Value ($M)",
                            "y_axis": "Probability (%)"
                        }),
                        insights=probability_cloud_data.get('insights'),
                        title="Investment Return Scenarios with Defensive Breakpoints"
                    )
                    
                    # Validate probability cloud chart structure
                    chart_data = probability_cloud_chart.get('data', {})
                    scenario_curves = chart_data.get('scenario_curves', [])
                    if not scenario_curves or len(scenario_curves) == 0:
                        logger.warning("[PROB_CLOUD] No scenario curves in probability cloud chart, skipping")
                    else:
                        # Validate that scenario curves have the required structure
                        valid_curves = []
                        for curve in scenario_curves:
                            # Frontend expects return_curve with exit_values and return_multiples
                            if curve.get('return_curve') or curve.get('curve'):
                                valid_curves.append(curve)
                            else:
                                logger.warning(f"[PROB_CLOUD] Scenario curve missing return_curve: {curve.get('name', 'Unknown')}")
                        
                        if len(valid_curves) == 0:
                            logger.warning("[PROB_CLOUD] No valid scenario curves after validation, skipping chart")
                        else:
                            # Update chart data with validated curves
                            probability_cloud_chart['data']['scenario_curves'] = valid_curves
                            # Don't pre-render - let frontend render with D3.js for interactivity
                            exit_charts.append(probability_cloud_chart)
                
                # Add reality check table for the $150M problem
                reality_check_table = []
                for company_name, scenarios_data in exit_scenarios_data.items():
                    if scenarios_data and 'realistic_exits' in scenarios_data:
                        for exit_scenario in scenarios_data['realistic_exits']:
                            if exit_scenario['exit_value'] in [86_000_000, 150_000_000]:  # Key scenarios
                                reality_check_table.append({
                                    'company': company_name,
                                    'exit_value': f"${exit_scenario['exit_value']/1e6:.0f}M",
                                    'our_proceeds': f"${exit_scenario['our_proceeds']/1e6:.1f}M",
                                    'moic': f"{exit_scenario['moic']:.2f}x",
                                    'dpi_contribution': f"{exit_scenario['dpi_contribution']:.1f}%",
                                    'reality': exit_scenario.get('reality_check', '')
                                })
                
                # Split into two slides to avoid "Invalid chart structure"
                # Slide 1: Probability Cloud - ROOT CAUSE FIX: Always add if exit_charts has data
                if exit_charts and len(exit_charts) > 0:
                    # Extract the first chart (probability cloud) and add as chart_data
                    probability_chart = exit_charts[0]
                    add_slide("probability_cloud", {
                        "title": "Exit Probability Cloud",
                        "subtitle": "Return distribution across exit scenarios",
                        "chart_data": probability_chart,  # Add chart_data directly for frontend rendering
                        "chart_config": {
                            "show_probability_clouds": True,
                            "show_realistic_exits": True
                        }
                    })
                    logger.info("[DECK_GEN] ✅ Probability cloud slide added")
                else:
                    # ROOT CAUSE FIX: If no exit_charts, create one from probability_cloud_data
                    logger.warning("[DECK_GEN] No exit_charts in array, creating probability cloud directly")
                    if companies and len(companies) > 0:
                        first_company = companies[0]
                        check_size = self._get_optimal_check_size(first_company, fund_context)
                        probability_cloud_data = self._generate_probability_cloud_data(first_company, check_size)
                        probability_chart = format_probability_cloud_chart(
                            scenario_curves=probability_cloud_data.get('scenario_curves', []),
                            breakpoint_clouds=probability_cloud_data.get('breakpoint_clouds', []),
                            decision_zones=probability_cloud_data.get('decision_zones', []),
                            config=probability_cloud_data.get('config', {
                                "x_axis": "Exit Value ($M)",
                                "y_axis": "Probability (%)"
                            }),
                            insights=probability_cloud_data.get('insights'),
                            title="Investment Return Scenarios with Defensive Breakpoints"
                        )
                        
                        # Validate probability cloud chart structure
                        chart_data = probability_chart.get('data', {})
                        scenario_curves = chart_data.get('scenario_curves', [])
                        if scenario_curves and len(scenario_curves) > 0:
                            # Validate that scenario curves have the required structure
                            valid_curves = [curve for curve in scenario_curves if curve.get('return_curve') or curve.get('curve')]
                            if len(valid_curves) > 0:
                                probability_chart['data']['scenario_curves'] = valid_curves
                                add_slide("probability_cloud", {
                                    "title": "Exit Probability Cloud",
                                    "subtitle": "Return distribution across exit scenarios",
                                    "chart_data": probability_chart,
                                    "chart_config": {
                                        "show_probability_clouds": True,
                                        "show_realistic_exits": True
                                    }
                                })
                                logger.info("[DECK_GEN] ✅ Probability cloud slide created directly")
                            else:
                                logger.warning("[DECK_GEN] No valid scenario curves in probability cloud chart, skipping slide")
                        else:
                            logger.warning("[DECK_GEN] No scenario curves in probability cloud chart, skipping slide")
                
                # Slide 2: Breakpoint Analysis
                try:
                    # Find breakpoint chart from exit_charts (the line chart showing breakpoints)
                    breakpoint_chart_data = None
                    for chart in exit_charts:
                        if isinstance(chart, dict) and chart.get("type") == "line" and "Breakpoint" in chart.get("title", ""):
                            breakpoint_chart_data = chart
                            break
                    
                    # If not found, try to find any line chart with breakpoint-related title
                    if not breakpoint_chart_data:
                        for chart in exit_charts:
                            if isinstance(chart, dict) and chart.get("type") == "line" and "Cap Table" in chart.get("title", ""):
                                breakpoint_chart_data = chart
                                break
                    
                    # Keep breakpoint chart as line chart (don't convert to bar)
                    # The chart already includes the pro-rata blue line
                    
                    add_slide("breakpoint_analysis", {
                        "title": "Breakpoint Analysis",
                        "subtitle": "Key inflection points for returns",
                        "companies": exit_scenarios_data,
                        "reality_check": reality_check_table,
                        "insights": self._generate_breakpoint_insights(exit_scenarios_data, reality_check_table),
                        "chart_data": breakpoint_chart_data,  # Add chart_data to slide
                        "chart_config": {
                            "show_breakpoints": True,
                            "show_ownership_evolution": True,
                            "highlight_liquidation_preference": True
                        }
                    })
                except Exception as e:
                    logger.error(f"[DECK_GEN] ❌ Breakpoint analysis slide generation failed: {e}")
                    import traceback
                    logger.error(f"[DECK_GEN] ❌ Stack trace: {traceback.format_exc()}")
                
                # Add PWERM Exit Scenarios Slide (Slide 12)
                try:
                    if exit_scenarios_data:
                        # Format PWERM scenarios for display
                        pwerm_display_data = []
                        all_scenarios = []  # Flat list for frontend table rendering
                        
                        for company_name, scenarios_data in exit_scenarios_data.items():
                            if 'scenarios' in scenarios_data and scenarios_data['scenarios']:
                                company_scenarios = []
                                for s in scenarios_data['scenarios'][:6]:  # Top 6 scenarios
                                    scenario_obj = {
                                        "scenario": s.get('scenario', 'Unknown'),
                                        "probability": f"{s.get('probability', 0) * 100:.0f}%",
                                        "exit_value": f"${s.get('exit_value', 0)/1e6:.0f}M",
                                        "time": f"{s.get('time_to_exit', 0):.1f} years",
                                        "moic_no_followon": f"{s.get('moic_no_followon', 0):.1f}x",
                                        "moic_with_followon": f"{s.get('moic_with_followon', 0):.1f}x",
                                        "exit_type": s.get('exit_type', 'M&A')
                                    }
                                    company_scenarios.append(scenario_obj)
                                    
                                    # Also add to flat scenarios array for frontend
                                    all_scenarios.append({
                                        "name": s.get('scenario', 'Unknown'),
                                        "scenario": s.get('scenario', 'Unknown'),
                                        "probability": s.get('probability', 0),
                                        "exit_value": s.get('exit_value', 0),
                                        "multiple": s.get('moic_no_followon', 0),
                                        "company": company_name
                                    })
                                
                                pwerm_display_data.append({
                                    "company": company_name,
                                    "scenarios": company_scenarios,
                                    "expected_return": scenarios_data.get('expected_return', 0),
                                    "probability_weighted_moic": scenarios_data.get('probability_weighted_moic', 0)
                                })
                        
                        if pwerm_display_data:
                            # Create a chart for exit scenarios - grouped bar chart showing exit values and probabilities
                            exit_scenarios_chart_data = {
                                "type": "bar",
                                "title": "Exit Scenarios - Value and Probability",
                                "data": {
                                    "labels": [s.get('scenario', 'Unknown') for s in all_scenarios[:12]],  # Top 12 scenarios
                                    "datasets": [
                                        {
                                            "label": "Exit Value ($M)",
                                            "data": [s.get('exit_value', 0) / 1e6 for s in all_scenarios[:12]],
                                            "backgroundColor": "rgba(59, 130, 246, 0.8)",
                                            "yAxisID": "y"
                                        },
                                        {
                                            "label": "Probability (%)",
                                            "data": [s.get('probability', 0) * 100 for s in all_scenarios[:12]],
                                            "backgroundColor": "rgba(16, 185, 129, 0.8)",
                                            "yAxisID": "y1",
                                            "type": "line"
                                        }
                                    ]
                                },
                                "options": {
                                    "responsive": True,
                                    "scales": {
                                        "y": {
                                            "type": "linear",
                                            "position": "left",
                                            "title": {
                                                "display": True,
                                                "text": "Exit Value ($M)"
                                            }
                                        },
                                        "y1": {
                                            "type": "linear",
                                            "position": "right",
                                            "title": {
                                                "display": True,
                                                "text": "Probability (%)"
                                            },
                                            "grid": {
                                                "drawOnChartArea": False
                                            }
                                        }
                                    }
                                }
                            }
                            
                            add_slide("exit_scenarios_pwerm", {
                                "title": "Exit Scenarios (PWERM)",
                                "subtitle": "Probability-weighted expected return analysis",
                                "scenarios": all_scenarios,  # Add flat scenarios array for frontend table rendering
                                "companies": pwerm_display_data,
                                "chart_data": exit_scenarios_chart_data,  # Add chart data for visualization
                                "methodology": "Based on industry exit data and company stage",
                                "chart_type": "bar",  # Display as bar chart with table
                                "highlights": [
                                    "IPO scenarios convert to common stock",
                                    "M&A scenarios subject to liquidation preferences",
                                    "Follow-on investment preserves ownership through dilution"
                                ]
                            })
                except Exception as e:
                    logger.error(f"[DECK_GEN] ❌ Exit scenarios PWERM slide generation failed: {e}")
                    import traceback
                    logger.error(f"[DECK_GEN] ❌ Stack trace: {traceback.format_exc()}")
                
                # Add Fund Return Impact Analysis with Follow-on Strategy
                # Get fund context from shared data - USE ACTUAL FUND CONTEXT
                fund_context = self.shared_data.get('fund_context', {}) or {}
                
                # Extract fund parameters - default to CLAUDE.md reference fund if absent
                fund_size = fund_context.get('fund_size')
                deployed_capital = fund_context.get('deployed_capital')
                remaining_capital = fund_context.get('remaining_capital')
                
                if not fund_size:
                    # Fall back to default $260M fund assumptions to keep deck generation resilient
                    fund_size = DEFAULT_FUND_SIZE
                    logger.info(f"[FUND_IMPACT] No fund context provided - falling back to default ${DEFAULT_FUND_SIZE/1e6:.0f}M fund")
                
                if deployed_capital is None and remaining_capital is None:
                    deployed_capital = fund_size * 0.4
                    remaining_capital = fund_size * 0.6
                elif deployed_capital is None:
                    deployed_capital = fund_size - remaining_capital
                elif remaining_capital is None:
                    remaining_capital = fund_size - deployed_capital
                
                # CRITICAL FIX: Ensure fund_size and remaining_capital are valid before division in logger
                safe_fund_size_log = fund_size if fund_size and fund_size > 0 else DEFAULT_FUND_SIZE
                safe_remaining_capital_log = remaining_capital if remaining_capital and remaining_capital > 0 else 0
                logger.info(f"[FUND_IMPACT] Using fund context: ${safe_fund_size_log/1e6:.0f}M fund, ${safe_remaining_capital_log/1e6:.0f}M remaining")
                portfolio_size = fund_context.get('portfolio_size', fund_context.get('portfolio_count', 18))  # Real portfolio size
                current_dpi = fund_context.get('current_dpi', fund_context.get('dpi', 0.5))  # Actual 0.5x DPI
                target_dpi = fund_context.get('target_dpi', fund_context.get('target_tvpi', 3.0))
                fund_year = fund_context.get('fund_year', 4)  # Year 4 of fund
                
                # CRITICAL FIX: Ensure fund_size and remaining_capital are valid before division in logger
                safe_fund_size = fund_size if fund_size and fund_size > 0 else DEFAULT_FUND_SIZE
                safe_remaining_capital = remaining_capital if remaining_capital and remaining_capital > 0 else 0
                logger.info(f"[FUND_IMPACT] Using fund context: ${safe_fund_size/1e6:.0f}M fund, ${safe_remaining_capital/1e6:.0f}M remaining, {portfolio_size} investments, {current_dpi:.1f}x current DPI")
                
                # Calculate remaining deployment
                remaining_to_deploy = remaining_capital
                
                # Use optimal check sizes from fund fit analysis
                company1_check = self._get_optimal_check_size(companies[0], fund_context) if len(companies) > 0 else fund_context.get('typical_check_size', 5_000_000)
                company2_check = self._get_optimal_check_size(companies[1], fund_context) if len(companies) > 1 else fund_context.get('typical_check_size', 5_000_000)
                # CRITICAL FIX: Ensure check sizes are valid numbers
                if company1_check is None or company1_check <= 0:
                    company1_check = fund_context.get('typical_check_size', 5_000_000)
                if company2_check is None or company2_check <= 0:
                    company2_check = fund_context.get('typical_check_size', 5_000_000)
                avg_check_size = (company1_check + company2_check) / 2
                # Defensive check: ensure avg_check_size is valid
                if avg_check_size is None or avg_check_size <= 0:
                    avg_check_size = 5_000_000  # Default fallback
                    logger.warning(f"[DECK_GEN] ⚠️ avg_check_size was invalid, using fallback: ${avg_check_size/1e6:.1f}M")
                portfolio_size = int(remaining_to_deploy / avg_check_size) if avg_check_size > 0 else 0  # Calculate based on remaining capital
                reserve_ratio = 2.0  # 2x reserves for follow-on
                
                # Calculate blended portfolio returns with and without follow-on
                blended_scenarios = {
                    "no_followon": [],
                    "with_followon": []
                }
                
                # Portfolio composition assumptions
                portfolio_distribution = {
                    "home_runs": 2,  # 20x+ returns
                    "winners": 3,    # 5-10x returns
                    "modest": 5,     # 2-3x returns
                    "return_capital": 8,  # 1x returns
                    "partial_loss": 4,    # 0.5x returns
                    "total_loss": 3       # 0x returns
                }
                
                # Calculate for each company using actual PWERM scenarios
                company_scenarios = []
                if companies:
                    for i, company in enumerate(companies[:2]):
                        # Get company specifics including actual funding dates
                        company_name = company.get('company', f'Company {i+1}')
                        valuation = self._get_field_with_fallback(company, 'valuation', 0)
                        # CRITICAL FIX: Calculate check_size for this company (was missing, causing NoneType division error)
                        check_size = self._get_optimal_check_size(company, fund_context)
                        # Defensive check: ensure check_size and valuation are valid numbers
                        # Fix circular dependency: ensure check_size is valid BEFORE using it to calculate valuation
                        if check_size is None or check_size <= 0:
                            # Use valuation if available, otherwise use default
                            if valuation and valuation > 0:
                                check_size = max(valuation * 0.05, 5_000_000)
                            else:
                                check_size = 5_000_000  # Default fallback
                            logger.warning(f"[DECK_GEN] ⚠️ check_size was None/zero for {company_name}, using fallback: ${check_size/1e6:.1f}M")
                        # Now that check_size is guaranteed to be valid, we can use it to calculate valuation if needed
                        if valuation is None or valuation <= 0:
                            valuation = check_size / 0.1  # Assume 10% ownership if no valuation
                            logger.warning(f"[DECK_GEN] ⚠️ valuation was None/zero for {company_name}, using fallback: ${valuation/1e6:.1f}M")
                        # Final safety check: ensure both are still valid numbers (not None)
                        if check_size is None:
                            check_size = 5_000_000
                        if valuation is None:
                            valuation = 50_000_000
                        # CRITICAL: Keep original stage string for display!
                        # Don't collapse Pre-Seed into Seed - they're different
                        stage_str = company.get('stage', 'Series A')
                        
                        # Convert stage string to Stage enum ONLY for calculations
                        # But preserve the original stage_str for display purposes
                        stage_map = {
                            'pre-seed': Stage.PRE_SEED,     # NOW WE HAVE PRE_SEED!
                            'pre seed': Stage.PRE_SEED,     
                            'preseed': Stage.PRE_SEED,      
                            'seed': Stage.SEED,
                            'seed extension': Stage.SEED,
                            'series a': Stage.SERIES_A,
                            'series b': Stage.SERIES_B,
                            'series c': Stage.SERIES_C,
                            'series d': Stage.GROWTH,
                            'series e': Stage.LATE,
                            'growth': Stage.GROWTH,
                            'late stage': Stage.LATE,
                            'late': Stage.LATE,
                            'ipo': Stage.PUBLIC,
                            'public': Stage.PUBLIC
                        }
                        stage = stage_map.get(stage_str.lower(), Stage.SERIES_A)
                        
                        # Ensure original stage string is preserved in company data
                        if 'stage' not in company or company['stage'] != stage_str:
                            logger.info(f"[STAGE_PRESERVATION] Keeping original stage '{stage_str}' for {company_name}")
                        
                        # Extract last funding date from funding rounds
                        funding_rounds = company.get('funding_rounds', [])
                        last_funding_date = None
                        if funding_rounds:
                            # Get the most recent round with a date
                            for round_data in reversed(funding_rounds):
                                if round_data.get('date'):
                                    company['last_funding_date'] = round_data['date']
                                    break
                        
                        # Get PWERM scenarios if available
                        pwerm_scenarios = company.get('pwerm_scenarios', [])
                        
                        # If we have actual PWERM scenarios, use their funding paths
                        if pwerm_scenarios:
                            # Group scenarios by funding path type
                            funding_paths = {}
                            for scenario in pwerm_scenarios:
                                # Extract funding path from scenario (IPO implies Series B,C,D; M&A implies B,maybe C, etc.)
                                # Handle both PWERMScenario objects and dicts
                                if hasattr(scenario, 'scenario_type'):
                                    scenario_type = scenario.scenario_type
                                else:
                                    scenario_type = scenario.get('scenario_type', 'BASE') if isinstance(scenario, dict) else 'BASE'
                                
                                if scenario_type not in funding_paths:
                                    funding_paths[scenario_type] = []
                                funding_paths[scenario_type].append(scenario)
                            
                            # Calculate follow-on needs for each path type
                            path_based_followon = []
                            
                            # Get next round predictions for accurate follow-on modeling
                            next_round_data = company.get('next_round', {})
                            next_round_size = next_round_data.get('next_round_size', 50_000_000)
                            next_round_timing = next_round_data.get('next_round_timing', 18)
                            next_round_stage = next_round_data.get('next_round_stage', 'Series C')
                            # CRITICAL FIX: Ensure check_size is valid before using it
                            safe_check_for_prorata = check_size if check_size and check_size > 0 else 5_000_000
                            our_prorata_needed = next_round_data.get('our_prorata_amount', safe_check_for_prorata * 0.2)
                            down_round_risk = next_round_data.get('down_round_risk', 'MEDIUM')
                            
                            # Use the actual funding path scenarios from ValuationEngineService
                            # Use inferred_revenue if revenue is None - CRITICAL FIX
                            revenue = ensure_numeric(company.get('revenue'), 0)
                            if revenue == 0:
                                revenue = ensure_numeric(company.get('inferred_revenue'), 0)
                                if revenue == 0:
                                    revenue = ensure_numeric(company.get('arr'), 0)
                                    if revenue == 0:
                                        revenue = ensure_numeric(company.get('inferred_arr'), 1_000_000)
                            
                            # Use inferred_growth_rate if growth_rate is None
                            growth_rate = ensure_numeric(company.get('growth_rate'), 0)
                            if growth_rate == 0:
                                growth_rate = ensure_numeric(company.get('inferred_growth_rate'), 1.0)
                            
                            # Use inferred_valuation if valuation is None - CRITICAL FIX
                            valuation_val = ensure_numeric(company.get('valuation'), 0)
                            if valuation_val == 0:
                                valuation_val = ensure_numeric(company.get('inferred_valuation'), 0)
                                if valuation_val == 0:
                                    # Calculate from total_funding as fallback
                                    valuation_val = ensure_numeric(company.get('total_funding'), 0) * 3
                            
                            # Extract inferred_valuation if available
                            inferred_val = ensure_numeric(company.get("inferred_valuation"), None) if company.get("inferred_valuation") is not None else None
                            val_request = ValuationRequest(
                                company_name=company.get('name', 'Unknown'),
                                revenue=revenue,
                                growth_rate=growth_rate,
                                stage=stage,
                                last_round_valuation=valuation_val if valuation_val and valuation_val > 0 else None,
                                inferred_valuation=inferred_val,
                                total_raised=self._get_field_safe(company, 'total_funding')
                            )
                            
                            # Get funding path scenarios from ValuationEngineService
                            # CRITICAL FIX: Ensure both check_size and valuation are valid before division
                            safe_check = check_size if check_size and check_size > 0 else 5_000_000
                            safe_valuation = valuation if valuation and valuation > 0 else safe_check / 0.1  # Assume 10% ownership if no valuation
                            initial_ownership_calc = safe_check / (safe_valuation * 1.2) if safe_valuation > 0 else 0.1
                            funding_path_data = self.valuation_engine._calculate_funding_path_scenarios(
                                company_data=company,
                                initial_investment=safe_check,
                                initial_ownership=initial_ownership_calc,
                                time_to_exit=5.0 if 'Seed' in stage else 3.0
                            )
                            
                            # For each funding path (conservative, aggressive, bootstrapped)
                            for path_name, path_data in funding_path_data.items():
                                if 'dilution_events' in path_data:
                                    total_prorata_needed = sum(event['pro_rata_needed'] for event in path_data['dilution_events'])
                                    # CRITICAL FIX: Use safe values for fallback ownership calculations
                                    fallback_ownership = safe_check / (safe_valuation * 1.2) if safe_valuation > 0 else 0.1
                                    final_ownership_no_followon = path_data['dilution_events'][-1]['ownership_after'] if path_data['dilution_events'] else fallback_ownership
                                    final_ownership_with_followon = path_data['dilution_events'][-1]['ownership_if_pro_rata'] if path_data['dilution_events'] else fallback_ownership
                                    
                                    # Build qualitative narrative for this path
                                    narrative = f"{path_data.get('description', '')}. "
                                    if path_data['dilution_events']:
                                        narrative += "Funding timeline from today:\n"
                                        from datetime import datetime
                                        current_date = datetime.now()
                                        
                                        for i, event in enumerate(path_data['dilution_events']):
                                            # Use actual date if available, otherwise calculate from current date
                                            if 'date' in event:
                                                event_date = event['date']
                                                if isinstance(event_date, datetime):
                                                    date_str = event_date.strftime("%b %Y")
                                                else:
                                                    date_str = event.get('date_str', f"+{event['year']:.1f} years")
                                            else:
                                                # Calculate date from now
                                                from datetime import timedelta
                                                future_date = current_date + timedelta(days=int(event['year'] * 365))
                                                date_str = future_date.strftime("%b %Y")
                                            
                                            narrative += f"• {date_str}: {event['round']} (${event['pre_money']/1_000_000:.0f}M pre-money)\n"
                                    
                                    # Map to bear/base/bull scenarios with actual funding paths
                                    if path_name == "conservative":
                                        # BEAR CASE
                                        funding_path = "Pre-seed→seed→A→B"
                                        exit_narrative = "Strategic Acquisition ($400-600M)"
                                        scenario_type = "BEAR"
                                        exit_value_range = "$400-600M"
                                        liquidation_stack = "Debt (if any) → Series B → Series A → Seed → Common"
                                    elif path_name == "aggressive":
                                        # BULL CASE
                                        funding_path = "Pre-seed→seed→A→B→C→D→E"
                                        exit_narrative = "NYSE IPO or Megacap Acquisition ($2B+)"
                                        scenario_type = "BULL"
                                        exit_value_range = "$2B-5B+"
                                        liquidation_stack = "Debt → Series E → D → C → B → A → Seed → Common"
                                    else:  # bootstrapped
                                        # BASE CASE
                                        funding_path = "Pre-seed→seed→A→B→Debt"
                                        exit_narrative = "PE Buyout or Growth Recap ($800M-1.2B)"
                                        scenario_type = "BASE"
                                        exit_value_range = "$800M-1.2B"
                                        liquidation_stack = "Debt (senior) → Series B → Series A → Seed → Common"
                                    
                                    # Add debt impact analysis
                                    debt_narrative = ""
                                    if "Debt" in funding_path or "debt" in path_name.lower():
                                        debt_narrative = "\n💰 Debt Impact:\n"
                                        debt_narrative += "• Senior position in liquidation preference\n"
                                        debt_narrative += "• No dilution but adds to preference stack\n"
                                        debt_narrative += "• Good for: SaaS with predictable revenue (low default risk)\n"
                                        debt_narrative += "• Bad for: Pre-revenue or high-burn AI companies\n"
                                    
                                    path_based_followon.append({
                                        "scenario_type": scenario_type,  # BEAR/BASE/BULL
                                        "path": path_name,
                                        "funding_path": funding_path,  # Full sequence
                                        "exit_value_range": exit_value_range,
                                        "liquidation_stack": liquidation_stack,
                                        "description": path_data.get('description', ''),
                                        "narrative": narrative + debt_narrative,
                                        "exit_narrative": exit_narrative,
                                        "rounds": [event['round'] for event in path_data['dilution_events']],
                                        "round_details": [{
                                            "name": event['round'],
                                            "year": event['year'],
                                            "date": event.get('date_str', f"Year {event['year']}"),
                                            "pre_money": event['pre_money'],
                                            "post_money": event['post_money'],
                                            "our_dilution": f"{event['dilution']*100:.1f}%",
                                            "pro_rata_cost": f"${event['pro_rata_needed']/1_000_000:.1f}M",
                                            "ownership_before": f"{event['ownership_before']*100:.1f}%",
                                            "ownership_after": f"{event['ownership_after']*100:.1f}%",
                                            "ownership_if_pro_rata": f"{event['ownership_if_pro_rata']*100:.1f}%"
                                        } for event in path_data['dilution_events']],
                                        "total_follow_on_needed": total_prorata_needed,
                                        "ownership_without_followon": final_ownership_no_followon,
                                        "ownership_with_followon": final_ownership_with_followon,
                                        "cap_table_impact": {
                                            "founders": f"{(1 - sum(e['dilution'] for e in path_data['dilution_events']))*40:.1f}%",  # Assume 40% founder start
                                            "employees": "10-15%",  # Option pool
                                            "investors": f"{sum(e['dilution'] for e in path_data['dilution_events'])*100:.1f}%",
                                            "our_stake": f"{final_ownership_with_followon*100:.1f}% (with follow-on)"
                                        },
                                        "decision_criteria": f"Follow-on makes sense if company maintains {'>50%' if path_name == 'aggressive' else '>30%'} YoY growth"
                                    })
                            
                            # Use actual PWERM-weighted exit multiples
                            # Handle both dict and PWERMScenario objects - use MOIC not exit_multiple
                            weighted_exit_multiple = 0
                            for s in pwerm_scenarios:
                                if hasattr(s, 'moic') and hasattr(s, 'probability'):
                                    # PWERMScenario uses 'moic' not 'exit_multiple'
                                    weighted_exit_multiple += s.moic * s.probability
                                elif isinstance(s, dict):
                                    # Dict might have either moic or exit_multiple
                                    moic = s.get('moic', s.get('exit_multiple', 5.0))
                                    weighted_exit_multiple += moic * s.get('probability', 0.1)
                                else:
                                    weighted_exit_multiple += 5.0 * 0.1  # Default fallback
                            if weighted_exit_multiple == 0:
                                weighted_exit_multiple = 5.0  # Default if calculation fails
                            
                            # Calculate follow-on scenarios with PWERM-based parameters
                            # CRITICAL FIX: Ensure valuation is valid before division
                            safe_valuation = valuation if valuation and valuation > 0 else check_size / 0.1  # Assume 10% ownership if no valuation
                            safe_avg_check = avg_check_size if avg_check_size and avg_check_size > 0 else check_size
                            followon_analysis = self._calculate_followon_scenarios(
                                initial_investment=safe_avg_check,
                                initial_ownership=safe_avg_check / (safe_valuation * 1.2) if safe_valuation > 0 else 0.1,
                                exit_multiple=weighted_exit_multiple,
                                rounds_to_exit=len(path_data['dilution_events']) if path_data.get('dilution_events') else 3,
                                dilution_per_round=0.20,
                                reserve_ratio=reserve_ratio
                            )
                            
                            # Add funding path details to analysis
                            followon_analysis["funding_paths"] = path_based_followon
                            followon_analysis["pwerm_weighted_exit"] = weighted_exit_multiple
                            
                        else:
                            # Fallback to stage-based estimates if no PWERM scenarios
                            if 'Seed' in stage:
                                expected_exit_multiple = 15.0
                                rounds_to_exit = 4
                            elif 'Series A' in stage:
                                expected_exit_multiple = 10.0
                                rounds_to_exit = 3
                            elif 'Series B' in stage:
                                expected_exit_multiple = 5.0
                                rounds_to_exit = 2
                            else:
                                expected_exit_multiple = 3.0
                                rounds_to_exit = 2
                            
                            # CRITICAL FIX: Ensure valuation is valid before division
                            safe_valuation = valuation if valuation and valuation > 0 else check_size / 0.1  # Assume 10% ownership if no valuation
                            safe_avg_check = avg_check_size if avg_check_size and avg_check_size > 0 else check_size
                            followon_analysis = self._calculate_followon_scenarios(
                                initial_investment=safe_avg_check,
                                initial_ownership=safe_avg_check / (safe_valuation * 1.2) if safe_valuation > 0 else 0.1,
                                exit_multiple=expected_exit_multiple,
                                rounds_to_exit=rounds_to_exit,
                                dilution_per_round=0.20,
                                reserve_ratio=reserve_ratio
                            )
                        
                        company_scenarios.append({
                            "company": company_name,
                            "stage": stage,
                            "scenarios": followon_analysis
                        })
                
                # Calculate fund-level blended returns using PWERM probabilities
                # Aggregate PWERM scenarios across analyzed companies
                aggregated_scenarios = []
                for company in companies[:2]:
                    if company.get('pwerm_scenarios'):
                        for scenario in company['pwerm_scenarios']:
                            # Handle both PWERMScenario objects and dicts
                            if hasattr(scenario, 'scenario'):
                                # It's a PWERMScenario object
                                aggregated_scenarios.append({
                                    'company': company.get('company', 'Unknown'),
                                    'scenario': scenario.scenario,
                                    'probability': scenario.probability,
                                    'exit_value': scenario.exit_value,
                                    'moic': scenario.moic
                                })
                            else:
                                # It's a dict
                                aggregated_scenarios.append({
                                    'company': company.get('company', 'Unknown'),
                                    'scenario': scenario.get('scenario', ''),
                                    'probability': scenario.get('probability', 0),
                                    'exit_value': scenario.get('exit_value', 0),
                                    'moic': scenario.get('moic', 1.0)
                                })
                
                # If we have PWERM scenarios, use them for portfolio distribution
                if aggregated_scenarios:
                    # Group scenarios by outcome quality matching ValuationEngineService naming
                    ipo_scenarios = [s for s in aggregated_scenarios if 'IPO' in s['scenario'] or 'SPAC' in s['scenario']]
                    strategic_scenarios = [s for s in aggregated_scenarios if 'Strategic' in s['scenario'] or 'Acquisition' in s['scenario']]
                    financial_scenarios = [s for s in aggregated_scenarios if 'Financial' in s['scenario'] or 'Secondary' in s['scenario'] or 'PE' in s['scenario']]
                    recap_scenarios = [s for s in aggregated_scenarios if 'Recap' in s['scenario'] or 'Modest' in s['scenario']]
                    liquidation_scenarios = [s for s in aggregated_scenarios if 'Liquidation' in s['scenario'] or 'Write' in s['scenario'] or 'Shut' in s['scenario']]
                    
                    # Calculate probability-weighted portfolio distribution
                    ipo_prob = sum(s['probability'] for s in ipo_scenarios) / len(companies) if companies else 0
                    strategic_prob = sum(s['probability'] for s in strategic_scenarios) / len(companies) if companies else 0
                    financial_prob = sum(s['probability'] for s in financial_scenarios) / len(companies) if companies else 0
                    recap_prob = sum(s['probability'] for s in recap_scenarios) / len(companies) if companies else 0
                    liquidation_prob = sum(s['probability'] for s in liquidation_scenarios) / len(companies) if companies else 0
                    
                    # Adjust portfolio distribution based on PWERM probabilities
                    portfolio_distribution["home_runs"] = int(portfolio_size * ipo_prob * 0.5)  # Half of IPOs become home runs
                    portfolio_distribution["winners"] = int(portfolio_size * (ipo_prob * 0.5 + strategic_prob * 0.7))
                    portfolio_distribution["modest"] = int(portfolio_size * (strategic_prob * 0.3 + financial_prob))
                    portfolio_distribution["return_capital"] = int(portfolio_size * recap_prob)
                    portfolio_distribution["partial_loss"] = int(portfolio_size * liquidation_prob * 0.5)
                    portfolio_distribution["total_loss"] = int(portfolio_size * liquidation_prob * 0.5)
                    
                    # Ensure total equals portfolio size
                    total_allocated = sum(portfolio_distribution.values())
                    if total_allocated < portfolio_size:
                        portfolio_distribution["return_capital"] += portfolio_size - total_allocated
                
                # Calculate returns with PWERM-based or default distribution
                total_deployed_no_followon = check_size * portfolio_size
                total_proceeds_no_followon = 0
                
                # Apply portfolio distribution with scenario-based multiples
                if aggregated_scenarios:
                    # Use actual PWERM exit multiples
                    avg_ipo_multiple = sum(s['moic'] for s in ipo_scenarios) / len(ipo_scenarios) if ipo_scenarios else 20
                    avg_strategic_multiple = sum(s['moic'] for s in strategic_scenarios) / len(strategic_scenarios) if strategic_scenarios else 7
                    avg_financial_multiple = sum(s['moic'] for s in financial_scenarios) / len(financial_scenarios) if financial_scenarios else 2.5
                    
                    total_proceeds_no_followon += portfolio_distribution["home_runs"] * check_size * avg_ipo_multiple
                    total_proceeds_no_followon += portfolio_distribution["winners"] * check_size * avg_strategic_multiple
                    total_proceeds_no_followon += portfolio_distribution["modest"] * check_size * avg_financial_multiple
                else:
                    # Fallback to default multiples
                    total_proceeds_no_followon += portfolio_distribution["home_runs"] * check_size * 20
                    total_proceeds_no_followon += portfolio_distribution["winners"] * check_size * 7
                    total_proceeds_no_followon += portfolio_distribution["modest"] * check_size * 2.5
                
                total_proceeds_no_followon += portfolio_distribution["return_capital"] * check_size * 1
                total_proceeds_no_followon += portfolio_distribution["partial_loss"] * check_size * 0.5
                # Total loss contributes 0
                
                blended_multiple_no_followon = total_proceeds_no_followon / total_deployed_no_followon if total_deployed_no_followon > 0 else 0
                
                # Scenario 2: With follow-on strategy using funding paths
                winners_count = portfolio_distribution["home_runs"] + portfolio_distribution["winners"]
                
                # Use actual funding path data to calculate follow-on needs
                if company_scenarios and company_scenarios[0]['scenarios'].get('funding_paths'):
                    # Average follow-on needed across funding paths
                    avg_followon_per_winner = 0
                    for company_data in company_scenarios:
                        if company_data['scenarios'].get('funding_paths'):
                            for path in company_data['scenarios']['funding_paths']:
                                avg_followon_per_winner += path.get('total_follow_on_needed', check_size)
                    avg_followon_per_winner = avg_followon_per_winner / (len(company_scenarios) * 3) if company_scenarios else check_size
                    
                    follow_on_deployed = winners_count * avg_followon_per_winner
                else:
                    follow_on_deployed = winners_count * check_size * (reserve_ratio - 1)
                
                total_deployed_with_followon = total_deployed_no_followon + follow_on_deployed
                
                # Calculate proceeds with maintained ownership through follow-on
                total_proceeds_with_followon = 0
                
                if aggregated_scenarios and company_scenarios:
                    # Use ownership preservation from funding paths
                    ownership_multiplier = 1.0
                    for company_data in company_scenarios:
                        if company_data['scenarios'].get('with_followon'):
                            no_followon_ownership = company_data['scenarios']['no_followon']['final_ownership']
                            with_followon_ownership = company_data['scenarios']['with_followon']['final_ownership']
                            ownership_multiplier = with_followon_ownership / no_followon_ownership if no_followon_ownership > 0 else 1.5
                            break
                    
                    total_proceeds_with_followon += portfolio_distribution["home_runs"] * check_size * avg_ipo_multiple * ownership_multiplier
                    total_proceeds_with_followon += portfolio_distribution["winners"] * check_size * avg_strategic_multiple * ownership_multiplier
                    total_proceeds_with_followon += portfolio_distribution["modest"] * check_size * avg_financial_multiple  # No follow-on for modest
                else:
                    # Fallback to estimated improvement
                    total_proceeds_with_followon += portfolio_distribution["home_runs"] * check_size * 35  # Better with follow-on
                    total_proceeds_with_followon += portfolio_distribution["winners"] * check_size * 12   # Better with follow-on
                    total_proceeds_with_followon += portfolio_distribution["modest"] * check_size * 2.5   # Same (no follow-on)
                
                total_proceeds_with_followon += portfolio_distribution["return_capital"] * check_size * 1
                total_proceeds_with_followon += portfolio_distribution["partial_loss"] * check_size * 0.5
                
                blended_multiple_with_followon = total_proceeds_with_followon / total_deployed_with_followon if total_deployed_with_followon > 0 else 0
                
                # Calculate fund return scenarios with both strategies
                fund_return_scenarios = []
                return_multiples = [0.0, 0.3, 0.5, 0.7, 1.0, 2.0, 3.0, 5.0, 10.0, 20.0]
                
                for multiple in return_multiples:
                    proceeds = check_size * multiple
                    
                    # Calculate impact on fund DPI (Distributed to Paid-in)
                    # Assuming this is one of 25 investments
                    contribution_to_fund = proceeds / fund_size
                    
                    # Calculate what DPI this would create if all other investments return 1x
                    other_investments_return = (portfolio_size - 1) * check_size * 1.0  # Others return capital
                    total_distributions = proceeds + other_investments_return
                    total_dpi = total_distributions / fund_size
                    
                    # Calculate required performance from rest of portfolio to hit 3x
                    target_fund_return = fund_size * 3.0  # 3x target
                    required_from_others = target_fund_return - proceeds
                    required_multiple_others = required_from_others / ((portfolio_size - 1) * check_size) if portfolio_size > 1 else 0
                    
                    fund_return_scenarios.append({
                        "multiple": multiple,
                        "proceeds": proceeds,
                        "fund_contribution": contribution_to_fund,
                        "dpi_if_others_1x": total_dpi,
                        "required_from_others": required_multiple_others
                    })
                
                # Calculate DPI Contribution Ladder - what returns needed for specific DPI targets
                dpi_contribution_targets = [0.05, 0.1, 0.25, 0.5, 1.0]  # DPI contribution levels
                dpi_ladder = []
                
                # Average ownership and exit ownership from companies
                avg_initial_ownership = 0.08  # Default 8%
                avg_exit_ownership = 0.056  # After 30% dilution (more realistic)
                
                if companies:
                    # Use actual ownership data from fund fit
                    if len(companies) >= 2:
                        company1_ownership = companies[0].get('actual_ownership_pct', 0.08)
                        company2_ownership = companies[1].get('actual_ownership_pct', 0.08)
                        avg_initial_ownership = (company1_ownership + company2_ownership) / 2
                        
                        company1_exit = companies[0].get('exit_ownership_pct') or self._calculate_exit_ownership(companies[0], company1_ownership, with_followon=True, fund_context=fund_context)
                        company2_exit = companies[1].get('exit_ownership_pct') or self._calculate_exit_ownership(companies[1], company2_ownership, with_followon=True, fund_context=fund_context)
                        avg_exit_ownership = (company1_exit + company2_exit) / 2
                    else:
                        # Single company case
                        company1_ownership = companies[0].get('actual_ownership_pct', 0.08)
                        avg_initial_ownership = company1_ownership
                        
                        company1_exit = companies[0].get('exit_ownership_pct') or self._calculate_exit_ownership(companies[0], company1_ownership, with_followon=True, fund_context=fund_context)
                        avg_exit_ownership = company1_exit
                
                for dpi_target in dpi_contribution_targets:
                    # How much proceeds needed for this DPI contribution?
                    required_proceeds = fund_size * dpi_target
                    
                    # What multiple on our investment?
                    required_multiple = required_proceeds / avg_check_size if avg_check_size > 0 else 0
                    
                    # What exit valuation needed given our ownership?
                    required_exit_value = required_proceeds / avg_exit_ownership if avg_exit_ownership > 0 else 999999999
                    
                    # Is this achievable based on current valuations?
                    avg_current_valuation = 100_000_000
                    if companies:
                        if len(companies) >= 2:
                            val1 = companies[0].get('valuation', 100_000_000)
                            val2 = companies[1].get('valuation', 100_000_000)
                            avg_current_valuation = (val1 + val2) / 2
                        else:
                            avg_current_valuation = companies[0].get('valuation', 100_000_000)
                    
                    exit_multiple_needed = required_exit_value / avg_current_valuation if avg_current_valuation > 0 else 0
                    
                    # Determine achievability
                    is_achievable = "✅ Likely" if required_multiple <= 10 else "⚠️ Challenging" if required_multiple <= 25 else "❌ Unlikely"
                    
                    dpi_ladder.append({
                        "dpi_contribution": dpi_target,
                        "dpi_contribution_pct": f"{dpi_target*100:.0f}%",
                        "required_proceeds": required_proceeds,
                        "required_multiple": required_multiple,
                        "required_exit_value": required_exit_value,
                        "exit_multiple_on_company": exit_multiple_needed,
                        "achievability": is_achievable,
                        "ownership_assumption": f"{avg_exit_ownership*100:.1f}%"
                    })
                
                # Create enhanced DPI slide with fund-level Sankey diagram
                # Get company names for clarity
                company_names_str = " & ".join([c.get('company', 'Unknown') for c in companies[:2]]) if companies else "Portfolio Companies"
                
                # Parse fund portfolio composition from context if available
                portfolio_composition = self._parse_portfolio_composition(fund_context, companies)
                
                # Calculate actual DPI impact based on exit scenarios
                dpi_impact_scenarios = self._calculate_dpi_impact_scenarios(
                    companies=companies,
                    fund_size=fund_size,
                    deployed_capital=deployed_capital,
                    remaining_capital=remaining_capital,
                    current_dpi=current_dpi,
                    portfolio_composition=portfolio_composition
                )
                
                # Build fund-level Sankey diagram data
                fund_sankey_data = {
                    "nodes": [
                        {"id": 0, "name": f"${fund_size/1e6:.0f}M Fund", "color": "#1e293b"},
                        {"id": 1, "name": f"Deployed: ${deployed_capital/1e6:.0f}M", "color": "#64748b"},
                        {"id": 2, "name": f"Remaining: ${remaining_capital/1e6:.0f}M", "color": "#3b82f6"},
                        {"id": 3, "name": f"Current Portfolio ({portfolio_size} cos)", "color": "#94a3b8"},
                        {"id": 4, "name": f"Realized: {current_dpi:.1f}x DPI", "color": "#10b981"},
                        {"id": 5, "name": f"Unrealized: ${(deployed_capital * 2.5 - deployed_capital * current_dpi)/1e6:.0f}M", "color": "#f59e0b"},
                        {"id": 6, "name": f"{companies[0].get('company', 'Company 1')}: ${company1_check/1e6:.1f}M", "color": "#8b5cf6"},
                        {"id": 7, "name": f"{companies[1].get('company', 'Company 2') if len(companies) > 1 else 'Company 2'}: ${company2_check/1e6:.1f}M", "color": "#ec4899"},
                        {"id": 8, "name": f"Reserves: ${(remaining_capital - company1_check - company2_check)/1e6:.0f}M", "color": "#6366f1"},
                        {"id": 9, "name": f"Target: {target_dpi:.1f}x DPI", "color": "#22c55e"}
                    ],
                    "links": [
                        {"source": 0, "target": 1, "value": deployed_capital/1e6, "color": "#64748b40"},
                        {"source": 0, "target": 2, "value": remaining_capital/1e6, "color": "#3b82f640"},
                        {"source": 1, "target": 3, "value": deployed_capital/1e6, "color": "#94a3b840"},
                        {"source": 3, "target": 4, "value": deployed_capital * current_dpi/1e6, "color": "#10b98140"},
                        {"source": 3, "target": 5, "value": (deployed_capital * 2.5 - deployed_capital * current_dpi)/1e6, "color": "#f59e0b40"},
                        {"source": 2, "target": 6, "value": company1_check/1e6, "color": "#8b5cf640"},
                        {"source": 2, "target": 7, "value": company2_check/1e6, "color": "#ec489940"},
                        {"source": 2, "target": 8, "value": (remaining_capital - company1_check - company2_check)/1e6, "color": "#6366f140"},
                        {"source": 4, "target": 9, "value": deployed_capital * current_dpi/1e6, "color": "#22c55e40"},
                        {"source": 5, "target": 9, "value": (deployed_capital * 2.5 - deployed_capital * current_dpi)/1e6, "color": "#22c55e40"},
                        {"source": 6, "target": 9, "value": dpi_impact_scenarios['company1_contribution']/1e6, "color": "#8b5cf640"},
                        {"source": 7, "target": 9, "value": dpi_impact_scenarios['company2_contribution']/1e6, "color": "#ec489940"},
                        {"source": 8, "target": 9, "value": dpi_impact_scenarios['reserves_contribution']/1e6, "color": "#6366f140"}
                    ]
                }
                
                # Calculate ownership sensitivity analysis
                ownership_scenarios = []
                for valuation_multiple in [0.8, 1.0, 1.2]:  # -20%, current, +20% valuation
                    for company in companies[:2]:
                        company_name = company.get('company', 'Unknown')
                        current_val = company.get('valuation', 100_000_000)
                        check = self._get_optimal_check_size(company, fund_context)
                        
                        scenario_val = current_val * valuation_multiple
                        ownership_pct = (check / (scenario_val + check)) * 100
                        
                        # Calculate exit value needed for different returns
                        exit_for_1x = check / (ownership_pct / 100)
                        exit_for_3x = (check * 3) / (ownership_pct / 100)
                        exit_for_10x = (check * 10) / (ownership_pct / 100)
                        
                        ownership_scenarios.append({
                            "company": company_name,
                            "valuation_scenario": f"{int((valuation_multiple - 1) * 100):+d}%",
                            "entry_valuation": scenario_val,
                            "check_size": check,
                            "ownership": ownership_pct,
                            "exit_1x": exit_for_1x,
                            "exit_3x": exit_for_3x,
                            "exit_10x": exit_for_10x,
                            "dpi_at_3x": (check * 3) / fund_size
                        })
                
                # ROOT CAUSE FIX: Ensure Sankey always has valid nodes and links before formatting
                # Validate fund_sankey_data structure first
                nodes = fund_sankey_data.get("nodes", [])
                links = fund_sankey_data.get("links", [])
                
                if not nodes or len(nodes) == 0:
                    logger.error("[DECK_GEN] fund_sankey_data has no nodes, cannot create Sankey")
                    # Don't add slide if no nodes
                elif not links or len(links) == 0:
                    logger.error("[DECK_GEN] fund_sankey_data has no links, cannot create Sankey")
                    # Don't add slide if no links
                else:
                    # Format Sankey chart data properly for frontend using helper
                    sankey_chart_data = format_sankey_chart(
                        nodes=nodes,
                        links=links,
                        title="Fund DPI Impact Flow"
                    )
                    
                    # Validate chart data structure after formatting
                    formatted_nodes = sankey_chart_data.get('data', {}).get('nodes', [])
                    formatted_links = sankey_chart_data.get('data', {}).get('links', [])
                    
                    if not formatted_nodes or not formatted_links or len(formatted_nodes) == 0 or len(formatted_links) == 0:
                        logger.warning(f"[DECK_GEN] Invalid Sankey chart data after formatting: nodes={len(formatted_nodes) if formatted_nodes else 0}, links={len(formatted_links) if formatted_links else 0}, rebuilding...")
                        # ROOT CAUSE FIX: Rebuild with correct structure
                        sankey_chart_data = {
                            "type": "sankey",
                            "data": {
                                "nodes": nodes,
                                "links": links
                            },
                            "title": "Fund DPI Impact Flow"
                        }
                        formatted_nodes = nodes
                        formatted_links = links
                    
                    # Final validation - ensure we have valid data
                    if not formatted_nodes or not formatted_links or len(formatted_nodes) == 0 or len(formatted_links) == 0:
                        logger.error(f"[DECK_GEN] Sankey chart data still invalid after rebuild, skipping slide: nodes={len(formatted_nodes) if formatted_nodes else 0}, links={len(formatted_links) if formatted_links else 0}")
                    else:
                        logger.info(f"[DECK_GEN] Sankey chart data validated: {len(formatted_nodes)} nodes, {len(formatted_links)} links")
                        
                        # CRITICAL: Ensure chart_data structure is exactly what frontend expects
                        # Frontend expects: {type: "sankey", data: {nodes: [], links: []}}
                        # Don't pre-render - let frontend render with D3.js for better interactivity
                        chart_data_for_slide = {
                            "type": "sankey",
                            "data": {
                                "nodes": formatted_nodes,
                                "links": formatted_links
                            },
                            "title": "Fund DPI Impact Flow"
                        }
                        
                        # Log the structure for debugging
                        logger.info(f"[DECK_GEN] DPI Sankey chart_data structure: type={chart_data_for_slide.get('type')}, has_data={bool(chart_data_for_slide.get('data'))}, nodes_count={len(chart_data_for_slide.get('data', {}).get('nodes', []))}, links_count={len(chart_data_for_slide.get('data', {}).get('links', []))}")
                        
                        add_slide("fund_dpi_impact_sankey", {
                            "title": f"Fund DPI Impact Analysis",
                            "subtitle": f"${fund_size/1e6:.0f}M fund | {current_dpi:.1f}x current → {target_dpi:.1f}x target | Analyzing: {company_names_str}",
                            "chart_data": chart_data_for_slide,
                            "portfolio_composition": portfolio_composition,
                            "ownership_sensitivity": ownership_scenarios,
                            "dpi_scenarios": dpi_impact_scenarios,
                            "key_metrics": {
                                "gap_to_target": (target_dpi - current_dpi) * fund_size,
                                "these_deals_contribution": dpi_impact_scenarios['total_expected_contribution'],
                                "percent_of_gap": (dpi_impact_scenarios['total_expected_contribution'] / ((target_dpi - current_dpi) * fund_size)) * 100 if (target_dpi - current_dpi) * fund_size > 0 else 0,
                                "capital_required": company1_check + company2_check,
                                "follow_on_reserves": dpi_impact_scenarios['total_followon_needed'],
                                "ownership_at_exit": dpi_impact_scenarios['avg_exit_ownership']
                            },
                            "insights": [
                                f"Need ${((target_dpi - current_dpi) * fund_size)/1e6:.0f}M in distributions to reach {target_dpi}x DPI",
                                f"These 2 deals: ${(company1_check + company2_check)/1e6:.1f}M deployed → ${dpi_impact_scenarios['total_expected_contribution']/1e6:.0f}M expected returns",
                                f"Contribution to DPI: {dpi_impact_scenarios['total_expected_contribution']/fund_size:.2f}x ({(dpi_impact_scenarios['total_expected_contribution'] / ((target_dpi - current_dpi) * fund_size)) * 100:.0f}% of gap)" if (target_dpi - current_dpi) * fund_size > 0 else "Contribution to DPI: Calculating...",
                                f"Follow-on reserves needed: ${dpi_impact_scenarios['total_followon_needed']/1e6:.1f}M to maintain {dpi_impact_scenarios['avg_exit_ownership']*100:.1f}% ownership at exit",
                                f"Breakeven exits: {companies[0].get('company')}: ${dpi_impact_scenarios['company1_breakeven']/1e6:.0f}M | {companies[1].get('company') if len(companies) > 1 else 'Company 2'}: ${dpi_impact_scenarios['company2_breakeven']/1e6:.0f}M"
                            ]
                        })
                        logger.info("[DECK_GEN] ✅ Fund DPI impact Sankey slide added")
                    
                    # Add dedicated Fund Metrics slide with DPI, TVPI, IRR
                    fund_metrics_data = self.shared_data.get('fund-metrics-calculator', {})
                    if not fund_metrics_data or 'fund_metrics' not in fund_metrics_data:
                        # Calculate fund metrics if not already calculated
                        try:
                            fund_metrics_result = await self._execute_fund_metrics({
                                "context": fund_context
                            })
                            fund_metrics_data = fund_metrics_result.get('fund_metrics', {})
                        except Exception as e:
                            logger.warning(f"[FUND_METRICS] Failed to calculate fund metrics: {e}")
                            fund_metrics_data = {}
                    else:
                        fund_metrics_data = fund_metrics_data.get('fund_metrics', {})
                    
                    # Create fund metrics slide with DPI, TVPI, IRR
                    if fund_metrics_data:
                        performance_metrics = fund_metrics_data.get('performance_metrics', {})
                        portfolio_metrics = fund_metrics_data.get('portfolio_metrics', {})
                        deployment_metrics = fund_metrics_data.get('deployment_metrics', {})
                        
                        fund_metrics_bullets = [
                            f"Current DPI: {performance_metrics.get('dpi', current_dpi):.2f}x",
                            f"TVPI: {performance_metrics.get('tvpi', current_dpi + (safe_remaining_capital / safe_fund_size if safe_fund_size > 0 else 0)):.2f}x",
                            f"RVPI: {performance_metrics.get('rvpi', safe_remaining_capital / safe_fund_size if safe_fund_size > 0 else 0):.2f}x",
                            f"Deployed: {performance_metrics.get('deployed_percentage', (safe_fund_size - safe_remaining_capital) / safe_fund_size if safe_fund_size > 0 else 0) * 100:.1f}%",
                            f"Portfolio: {portfolio_metrics.get('total_companies', portfolio_size)} companies",
                            f"Exits: {portfolio_metrics.get('exited_companies', 0)}"
                        ]
                        
                        add_slide("fund_metrics", {
                            "title": "Fund Performance Metrics",
                            "subtitle": f"${safe_fund_size/1e6:.0f}M Fund | Year {fund_year}",
                            "bullets": fund_metrics_bullets,
                            "metrics": {
                                "DPI": f"{performance_metrics.get('dpi', current_dpi):.2f}x",
                                "TVPI": f"{performance_metrics.get('tvpi', current_dpi + (safe_remaining_capital / safe_fund_size if safe_fund_size > 0 else 0)):.2f}x",
                                "RVPI": f"{performance_metrics.get('rvpi', safe_remaining_capital / safe_fund_size if safe_fund_size > 0 else 0):.2f}x",
                                "Deployed": f"${(safe_fund_size - safe_remaining_capital)/1e6:.0f}M",
                                "Remaining": f"${safe_remaining_capital/1e6:.0f}M"
                            },
                            "fund_metrics": fund_metrics_data
                        })
                        logger.info("[DECK_GEN] ✅ Fund metrics slide added with DPI, TVPI, RVPI")
                
                # Add comparison chart for blended portfolio returns
                charts.append({
                    "type": "bar",
                    "title": "Portfolio Returns: With vs Without Follow-on",
                    "data": {
                        "labels": ["No Follow-on", "With Follow-on (2x Reserves)"],
                        "datasets": [
                            {
                                "label": "Total Capital Deployed ($M)",
                                "data": [
                                    total_deployed_no_followon / 1_000_000,
                                    total_deployed_with_followon / 1_000_000
                                ],
                                "backgroundColor": "rgba(156, 163, 175, 0.9)"
                            },
                            {
                                "label": "Total Proceeds ($M)",
                                "data": [
                                    total_proceeds_no_followon / 1_000_000,
                                    total_proceeds_with_followon / 1_000_000
                                ],
                                "backgroundColor": "rgba(59, 130, 246, 0.9)"
                            },
                            {
                                "label": "Blended Multiple",
                                "data": [
                                    blended_multiple_no_followon,
                                    blended_multiple_with_followon
                                ],
                                "backgroundColor": "rgba(251, 191, 36, 0.9)"
                            }
                        ]
                    },
                    "options": {
                        "scales": {
                            "y": {
                                "title": {
                                    "display": True,
                                    "text": "Value"
                                }
                            }
                        }
                    }
                })
                
                # Add chart showing fund return scenarios
                charts.append({
                    "type": "line",
                    "title": "Fund Return Sensitivity Analysis",
                    "data": {
                        "labels": [f"{m}x" for m in return_multiples],
                        "datasets": [
                            {
                                "label": "This Investment's Contribution to Fund (%)",
                                "data": [s["fund_contribution"] * 100 for s in fund_return_scenarios],
                                "borderColor": "rgba(59, 130, 246, 1)",
                                "backgroundColor": "rgba(66, 133, 244, 0.1)",
                                "fill": True
                            },
                            {
                                "label": "Cumulative Fund DPI (if others return 1x)",
                                "data": [s["dpi_if_others_1x"] for s in fund_return_scenarios],
                                "borderColor": "#0F9D58",
                                "backgroundColor": "rgba(15, 157, 88, 0.1)",
                                "fill": True
                            }
                        ]
                    },
                    "options": {
                        "scales": {
                            "y": {
                                "title": {
                                    "display": True,
                                    "text": "Impact on Fund Returns"
                                }
                            }
                        },
                        "plugins": {
                            "annotation": {
                                "annotations": {
                                    "target": {
                                        "type": "line",
                                        "yMin": 3,
                                        "yMax": 3,
                                        "borderColor": "rgb(255, 99, 132)",
                                        "borderWidth": 2,
                                        "borderDash": [5, 5],
                                        "label": {
                                            "content": "3x Fund Target",
                                            "enabled": True,
                                            "position": "end"
                                        }
                                    }
                                }
                            }
                        }
                    }
                })
                
                # Add company-specific follow-on analysis chart if we have company scenarios
                if company_scenarios:
                    followon_chart_data = {
                        "labels": [],
                        "no_followon_multiples": [],
                        "with_followon_multiples": [],
                        "no_followon_ownership": [],
                        "with_followon_ownership": [],
                        "capital_deployed_no": [],
                        "capital_deployed_with": []
                    }
                    
                    for company_data in company_scenarios:
                        company_name = company_data["company"]
                        scenarios = company_data["scenarios"]
                        
                        followon_chart_data["labels"].append(company_name)
                        followon_chart_data["no_followon_multiples"].append(scenarios["no_followon"]["multiple"])
                        followon_chart_data["with_followon_multiples"].append(scenarios["with_followon"]["multiple"])
                        followon_chart_data["no_followon_ownership"].append(scenarios["no_followon"]["final_ownership"] * 100)
                        followon_chart_data["with_followon_ownership"].append(scenarios["with_followon"]["final_ownership"] * 100)
                        followon_chart_data["capital_deployed_no"].append(scenarios["no_followon"]["capital_deployed"] / 1_000_000)
                        followon_chart_data["capital_deployed_with"].append(scenarios["with_followon"]["capital_deployed"] / 1_000_000)
                    
                    charts.append({
                        "type": "radar",
                        "title": "Company-Specific Follow-on Impact",
                        "data": {
                            "labels": ["Exit Multiple", "Final Ownership %", "Capital Deployed $M", "IRR %"],
                            "datasets": []
                        }
                    })
                    
                    for i, company_data in enumerate(company_scenarios):
                        company_name = company_data["company"]
                        scenarios = company_data["scenarios"]
                        
                        # Add dataset for no follow-on
                        charts[-1]["data"]["datasets"].append({
                            "label": f"{company_name} - No Follow-on",
                            "data": [
                                scenarios["no_followon"]["multiple"],
                                scenarios["no_followon"]["final_ownership"] * 100,
                                scenarios["no_followon"]["capital_deployed"] / 1_000_000,
                                scenarios["no_followon"]["irr"]
                            ],
                            "borderColor": f"rgba(156, 163, 175, {0.9 - i*0.3})",
                            "backgroundColor": f"rgba(156, 163, 175, {0.2 - i*0.1})",
                            "borderDash": [5, 5]
                        })
                        
                        # Add dataset for with follow-on
                        charts[-1]["data"]["datasets"].append({
                            "label": f"{company_name} - With Follow-on",
                            "data": [
                                scenarios["with_followon"]["multiple"],
                                scenarios["with_followon"]["final_ownership"] * 100,
                                scenarios["with_followon"]["capital_deployed"] / 1_000_000,
                                scenarios["with_followon"]["irr"]
                            ],
                            "borderColor": f"rgba(59, 130, 246, {0.9 - i*0.3})",
                            "backgroundColor": f"rgba(59, 130, 246, {0.2 - i*0.1})"
                        })
                    
                    # Add timeline chart for funding scenarios
                    from datetime import datetime
                    current_date = datetime.now()
                    
                    # Enhanced timeline with real dates and historical data
                    colors = ["#4e79a7", "#f28e2c", "#e15759", "#76b7b2", "#59a14f", "#edc949", "#af7aa1", "#ff9da7"]
                    timeline_data = {
                        "type": "timeline_valuation",  # Special type for timeline charts
                        "title": "Valuation Evolution Timeline",
                        "subtitle": "Historical funding and projected future rounds with ownership %",
                        "x_axis_type": "time",  # This tells frontend to use date scale
                        "y_axis_label": "Post-Money Valuation ($M)",
                        "datasets": [],
                        "annotations": []  # For marking key events
                    }
                    
                    for idx, scenario in enumerate(company_scenarios[:2]):  # Focus on top 2 companies
                        company_name = scenario.get('company', 'Unknown')
                        company_data = scenario.get('company_data', {})
                        
                        # Extract last funding date from company's actual funding history
                        last_funding_date = current_date
                        funding_rounds = company_data.get('funding_rounds', [])
                        
                        if funding_rounds:
                            for round_info in reversed(funding_rounds):
                                if round_info.get('date'):
                                    try:
                                        date_str = round_info['date']
                                        # Try multiple date formats
                                        for fmt in ['%Y-%m-%d', '%Y-%m', '%B %Y', '%b %Y', '%Y']:
                                            try:
                                                last_funding_date = datetime.strptime(date_str, fmt)
                                                break
                                            except:
                                                continue
                                        break
                                    except:
                                        pass
                        
                        # Create dataset for this company
                        dataset = {
                            "label": company_name,
                            "data": [],
                            "borderColor": colors[idx] if idx < len(colors) else "#4e79a7",
                            "backgroundColor": f"{colors[idx] if idx < len(colors) else '#4e79a7'}20",
                            "pointRadius": 6,
                            "pointHoverRadius": 9,
                            "borderWidth": 2,
                            "tension": 0.3,  # Smooth curve
                            "fill": False
                        }
                        
                        # Add historical funding rounds
                        for round_info in funding_rounds:
                            if round_info.get('amount'):
                                # Parse date or use estimate
                                round_date = last_funding_date
                                if round_info.get('date'):
                                    try:
                                        date_str = round_info['date']
                                        for fmt in ['%Y-%m-%d', '%Y-%m', '%B %Y', '%b %Y', '%Y']:
                                            try:
                                                round_date = datetime.strptime(date_str, fmt)
                                                break
                                            except:
                                                continue
                                    except:
                                        pass
                                
                                # Calculate post-money valuation
                                pre_money = round_info.get('pre_money_valuation', 0)
                                amount = round_info.get('amount', 0)
                                post_money = pre_money + amount if pre_money > 0 else amount * 5
                                
                                dataset['data'].append({
                                    "x": round_date.strftime('%Y-%m-%d'),
                                    "y": post_money / 1_000_000,
                                    "round": round_info.get('round_name', round_info.get('round', 'Unknown')),
                                    "amount": amount,
                                    "historical": True,
                                    "ownership": None,
                                    "pro_rata": 0,
                                    "tooltip": f"{round_info.get('round_name', 'Round')}: ${amount/1e6:.1f}M at ${post_money/1e6:.0f}M post"
                                })
                        
                        # Add projected future rounds from scenarios
                        if scenario['scenarios'].get('funding_paths'):
                            for path in scenario['scenarios']['funding_paths']:
                                if path.get('scenario_type') == 'with_followon':
                                    # Calculate future dates based on typical cadence
                                    for j, round_detail in enumerate(path.get('round_details', [])):
                                        # 18-24 months between rounds is typical
                                        months_ahead = 18 * (j + 1) if j < 2 else 24 * (j - 1)
                                        future_date = last_funding_date + timedelta(days=months_ahead * 30)
                                        
                                        dataset['data'].append({
                                            "x": future_date.strftime('%Y-%m-%d'),
                                            "y": round_detail['post_money'] / 1_000_000,
                                            "round": round_detail['name'],
                                            "amount": round_detail.get('amount', 0),
                                            "historical": False,
                                            "projected": True,
                                            "ownership": round_detail.get('ownership_after', 0),
                                            "pro_rata": round_detail.get('pro_rata_cost', 0),
                                            "tooltip": f"{round_detail['name']} (Projected): ${round_detail.get('amount', 0)/1e6:.1f}M, {round_detail.get('ownership_after', 0):.1f}% ownership, ${round_detail.get('pro_rata_cost', 0)/1e6:.1f}M pro-rata"
                                        })
                        
                        # Sort data by date
                        dataset['data'].sort(key=lambda x: x['x'])
                        timeline_data['datasets'].append(dataset)
                    
                    # Add annotations for key events
                    timeline_data['annotations'].append({
                        "type": "line",
                        "mode": "vertical",
                        "scaleID": "x",
                        "value": current_date.strftime('%Y-%m-%d'),
                        "borderColor": "rgba(255, 99, 132, 0.5)",
                        "borderWidth": 2,
                        "borderDash": [5, 5],
                        "label": {
                            "content": "Today",
                            "enabled": True,
                            "position": "top"
                        }
                    })
                    
                    charts.append(timeline_data)
                    
                    # Add summary table data
                    add_slide("followon_strategy_table", {
                        "title": "Follow-on Strategy Decision Framework",
                        "subtitle": "Company-specific analysis of reserve deployment with timeline",
                        "companies": company_scenarios,
                        "timeline_chart": timeline_data,
                        "summary": {
                            "total_initial": sum(cs["scenarios"]["no_followon"]["capital_deployed"] for cs in company_scenarios),
                            "total_with_reserves": sum(cs["scenarios"]["with_followon"]["capital_deployed"] for cs in company_scenarios),
                            "avg_multiple_improvement": sum(cs["scenarios"]["delta"]["multiple_delta"] for cs in company_scenarios) / len(company_scenarios) if company_scenarios else 0,
                            "recommended_strategy": "SELECTIVE FOLLOW-ON" if blended_multiple_with_followon > blended_multiple_no_followon * 1.2 else "MINIMAL FOLLOW-ON"
                        }
                    })
            
            # Investment Recommendations with TRANSPARENT SCORING
            if companies:
                recommendations_with_scoring = []
                
                for company in companies[:2]:
                    company_name = company.get('company', 'Unknown')
                    
                    # TRANSPARENT SCORING - Show the math
                    scoring = self._generate_transparent_scoring(company)
                    
                    recommendations_with_scoring.append({
                        'company': company_name,
                        'recommendation': scoring['recommendation'],
                        'action': scoring['action'],
                        'total_score': scoring['total_score'],
                        'scores': scoring['component_scores'],
                        'methodology': scoring['methodology'],
                        'reasoning': scoring['reasoning']
                    })
                
                if recommendations_with_scoring:
                    add_slide("investment_recommendations", {
                        "title": "Investment Recommendations",
                        "subtitle": "Transparent scoring methodology with quantified risk/return",
                        "recommendations": recommendations_with_scoring
                    })
            
            # Add Next Round Intelligence slide (NEW)
            if companies and len(companies) > 0:
                next_round_intelligence = []
                for company in companies[:2]:
                    company_name = company.get('company', 'Unknown')
                    next_round = company.get('next_round', {})
                    
                    if next_round:
                        intelligence = {
                            "company": company_name,
                            "timing": f"{next_round.get('next_round_timing', 12):.0f} months",
                            "urgency": next_round.get('next_round_timing_label', 'Normal timing'),
                            "stage": next_round.get('next_round_stage', 'Series C'),
                            "size": f"${next_round.get('next_round_size', 50_000_000)/1e6:.0f}M",
                            "valuation_pre": f"${next_round.get('next_round_valuation_pre', 200_000_000)/1e6:.0f}M",
                            "valuation_step_up": f"{next_round.get('valuation_step_up', 2.0):.1f}x",
                            "down_round_risk": next_round.get('down_round_risk', 'MEDIUM'),
                            "down_round_probability": f"{next_round.get('down_round_probability', 0.25)*100:.0f}%",
                            "revenue_milestone": f"${next_round.get('revenue_milestone', 10_000_000)/1e6:.1f}M to hit",
                            "milestone_confidence": next_round.get('milestone_confidence', 'On track'),
                            "our_prorata": f"${next_round.get('our_prorata_amount', 5_000_000)/1e6:.1f}M",
                            "dilution_expected": f"{next_round.get('dilution_expected', 0.15)*100:.0f}%",
                            "market_sentiment": next_round.get('market_sentiment', 'Neutral')
                        }
                        next_round_intelligence.append(intelligence)
                
                if next_round_intelligence:
                    add_slide("next_round_intelligence", {
                        "title": "Next Round Intelligence & Timing",
                        "subtitle": "Predictive analysis of upcoming funding rounds",
                        "companies": next_round_intelligence,
                        "insights": [
                            f"Window to invest: Next {max(c.get('next_round', {}).get('next_round_timing', 18) for c in companies[:2]):.0f} months",
                            f"Total pro-rata needed: ${sum(c.get('next_round', {}).get('our_prorata_amount', 0) for c in companies[:2])/1e6:.1f}M",
                            f"Down round risks: {', '.join([c.get('company', 'Unknown') + ': ' + c.get('next_round', {}).get('down_round_risk', 'MEDIUM') for c in companies[:2]])}",
                            f"Market sentiment: {companies[0].get('next_round', {}).get('market_sentiment', 'Neutral')}"
                        ]
                    })
            
            # ORIGINAL Investment Recommendations & Fund Fit Analysis using REAL fund fit data
            if companies:  # Re-enabled to use actual fund fit calculations
                # Use actual fund fit scores and ownership data
                recommendations = []
                fund_fit_data = {}
                
                # Get fund context BEFORE the loop - fix variable scope issue
                fund_context = self.shared_data.get('fund_context', {})
                fund_size = fund_context.get('fund_size', DEFAULT_FUND_SIZE)
                remaining_to_deploy = fund_context.get('remaining_capital', fund_size * 0.6)  # Default to 60% of fund if not specified
                
                for company in companies[:2]:
                    company_name = company.get('company', 'Unknown')
                    stage = company.get('stage', 'Unknown')
                    valuation = safe_get_value(company.get('valuation', 0))
                    revenue = safe_get_value(company.get('revenue', company.get('inferred_revenue', 0)))
                    
                    # Use ACTUAL fund fit data calculated by IntelligentGapFiller
                    fund_fit_score = company.get('fund_fit_score', 50)
                    optimal_check = self._get_optimal_check_size(company, fund_context)
                    actual_ownership = company.get('actual_ownership_pct', 0.05)
                    exit_ownership = company.get('exit_ownership_pct') or self._calculate_exit_ownership(company, actual_ownership, with_followon=False, fund_context=fund_context)
                    exit_proceeds = company.get('exit_proceeds', 0)
                    expected_irr = company.get('expected_irr', 0)
                    fund_fit_reasons = company.get('fund_fit_reasons', [])
                    fund_fit_action = company.get('fund_fit_action', 'Review')
                    
                    # Calculate ownership target based on fund parameters
                    # Target 8% but adjust based on stage and fund constraints
                    ownership_target = fund_context.get('target_ownership', 0.08) * 100  # Convert to percentage
                    concentration_limit_pct = (fund_context.get('concentration_limit', fund_size * 0.10) / valuation) * 100 if valuation > 0 else 10.0
                    
                    fund_fit_data[company_name] = {
                        "required_check": optimal_check,
                        "ownership_target": min(ownership_target, concentration_limit_pct),  # Use fund's actual target
                        "actual_ownership": actual_ownership * 100,
                        "exit_ownership": exit_ownership * 100,
                        "fit_score": fund_fit_score / 100,  # Convert to 0-1 scale
                        "stage": stage,
                        "valuation": valuation
                    }
                    
                    # Generate recommendation using REAL fund fit data - CONSISTENT LOGIC
                    if fund_fit_score > 70:
                        decision = "STRONG BUY"
                        action = "Schedule diligence meeting"
                        rec = f"{decision}: {self._format_money(optimal_check)} for {actual_ownership*100:.1f}% ownership"
                        color = "green"
                    elif fund_fit_score > 50:
                        decision = "CONSIDER"
                        action = "Request more information"
                        rec = f"{decision}: {action} before proceeding"
                        color = "yellow"
                    else:
                        decision = "PASS"
                        action = "Does not meet fund thesis"
                        rec = f"{decision}: {action}"
                        color = "red"
                    
                    # Build reasoning from actual fund fit reasons
                    reasoning_points = fund_fit_reasons[:3] if fund_fit_reasons else [f"Stage: {stage}, Valuation: {self._format_money(valuation)}"]
                    
                    recommendations.append({
                        "company": company_name,
                        "decision": decision,
                        "action": action,
                        "recommendation": rec,
                        "color": color,
                        "reasoning": " | ".join(reasoning_points),
                        "ownership_details": f"Entry: {actual_ownership*100:.1f}% → Exit: {exit_ownership*100:.1f}% (after dilution)",
                        "expected_proceeds": self._format_money(exit_proceeds) if exit_proceeds > 0 else "TBD",
                        "expected_irr": f"{expected_irr:.0f}%" if expected_irr > 0 else "TBD",
                        "fund_fit_score": fund_fit_score
                    })
                
                add_slide("investment_recommendations", {
                    "title": "Investment Recommendations",
                    "subtitle": f"Fund Fit Analysis for ${fund_size/1e6:.0f}M Fund (${remaining_to_deploy/1e6:.0f}M remaining to deploy)",
                    "recommendations": recommendations,
                    "fund_fit": fund_fit_data
                })
            
            # Get citations from citation manager and add as final slide
            citations = self.citation_manager.get_all_citations() if hasattr(self, 'citation_manager') else []
            
            # Filter out irrelevant/generic citations
            def is_relevant_citation(cite):
                """Filter out generic or irrelevant citations"""
                if not cite:
                    return False
                cite_text = str(cite).lower()
                # Remove generic placeholders
                irrelevant_phrases = [
                    'market analysis',
                    'internal calculation',
                    'benchmark data',
                    'industry standard',
                    'n/a',
                    'unknown source',
                    'calculated',
                    'inferred',
                    'estimated'
                ]
                # Keep if it has a URL, company name, or specific source
                has_url = 'http' in cite_text or 'www.' in cite_text
                has_company = any(c.get('company', '').lower() in cite_text for c in companies if c.get('company'))
                is_specific = len(cite_text) > 20 and not any(phrase in cite_text for phrase in irrelevant_phrases)
                
                return has_url or has_company or is_specific
            
            filtered_citations = [cite for cite in citations if is_relevant_citation(cite)]
            
            formatted_citations = [
                self._format_citation_entry(cite, idx)
                for idx, cite in enumerate(filtered_citations)
            ] if filtered_citations else []
            
            # Build methodology list with company-specific sources
            methodology = [
                "Data extracted from company websites and public sources",
                "Financial metrics calculated using industry benchmarks",
                "Valuation models based on PWERM methodology",
                "Cap table analysis using standard dilution assumptions"
            ]
            
            # Add company-specific sources if available
            for company in companies[:2]:
                website = company.get('website_url')
                if website:
                    methodology.append(f"{company.get('company')} website: {website}")

            # Always add citations slide if we have any citations
            if filtered_citations or methodology:  # Add slide if we have citations or methodology
                add_slide("citations", {
                    "title": "Sources & References",
                    "subtitle": f"{len(formatted_citations)} sources cited" if formatted_citations else "Methodology & Data Sources",
                    "citations": formatted_citations if formatted_citations else [],
                    "methodology": methodology,
                    "citation_count": len(formatted_citations),
                    "show_fallback": len(formatted_citations) == 0
                })
            
            # CRITICAL VALIDATION: Ensure we have slides before returning
            if not slides or len(slides) == 0:
                logger.error(f"[DECK_GEN] ❌ CRITICAL ERROR: No slides generated despite having {len(companies)} companies!")
                logger.error(f"[DECK_GEN] ❌ This indicates a bug in slide generation logic")
                logger.error(f"[DECK_GEN] ❌ Companies available: {[c.get('company', 'NO_COMPANY_FIELD') for c in companies]}")
                
                # Generate minimal error slides to prevent empty deck
                slides = [
                    {
                        "id": "error-slide-1",
                        "order": 1,
                        "template": "title",
                        "content": {
                            "title": "Deck Generation Error",
                            "subtitle": "Unable to generate slides",
                            "body": f"An error occurred during deck generation for {len(companies)} companies. Please try again."
                        }
                    },
                    {
                        "id": "error-slide-2",
                        "order": 2,
                        "template": "summary",
                        "content": {
                            "title": "Error Details",
                            "bullets": [
                                f"Companies processed: {len(companies)}",
                                "Slide generation failed",
                                "Please contact support if this persists"
                            ]
                        }
                    }
                ]
                logger.warning(f"[DECK_GEN] ⚠️ Generated {len(slides)} error slides as fallback")
            
            # Log what we're returning
            logger.info(f"[DECK_GEN] Generated {len(slides)} slides successfully")
            logger.info(f"[DECK_GEN] Slide types: {[s.get('template') for s in slides]}")
            logger.info(f"[DECK_GEN] Citations count: {len(citations)}")
            
            # Return standardized format for frontend consumption
            result = {
                "format": "deck",
                "slides": slides,
                "slide_count": len(slides),
                "theme": "professional",
                "metadata": {
                    "generated_at": datetime.now().isoformat(),
                    "company_count": len(companies),
                    "total_slides": len(slides)
                },
                "citations": citations,
                "formatted_citations": formatted_citations,
                "charts": charts  # Now populated with actual chart data!
            }
            
            logger.info(f"[DECK_GEN] ✅ Returning deck with {len(result['slides'])} slides")
            logger.info(f"[DECK_GEN] ✅ Result keys: {list(result.keys())}")
            logger.info(f"[DECK_GEN] ✅ Result format: {result.get('format')}")
            logger.info(f"[DECK_GEN] ✅ Result slides is list: {isinstance(result.get('slides'), list)}")
            return result
            
        except Exception as e:
            logger.error(f"[DECK_GEN] ❌ CRITICAL ERROR: Deck generation failed: {e}")
            logger.error(f"[DECK_GEN] ❌ Exception type: {type(e).__name__}")
            import traceback
            logger.error(f"[DECK_GEN] ❌ Traceback: {''.join(traceback.format_exc())}")
            
            # CRITICAL FIX: Return error result instead of raising, so format_deck can see it
            # This ensures deck-storytelling is in results with error info
            return {
                "format": "deck",
                "slides": [],
                "error": str(e),
                "error_type": type(e).__name__,
                "theme": "professional",
                "metadata": {
                    "generated_at": datetime.now().isoformat(),
                    "status": "error",
                    "reason": str(e)
                },
                "citations": [],
                "charts": []
            }
    
    def _generate_path_to_100m_insights(self, companies_100m_data: Dict[str, Any]) -> List[str]:
        """Generate insights for Path to $100M slide, handling None values properly"""
        insights = []
        
        try:
            # Convert dict keys and values to lists for safe access
            company_names = list(companies_100m_data.keys())
            company_values = list(companies_100m_data.values())
            
            # First company insight
            if len(company_names) > 0 and len(company_values) > 0:
                first_name = company_names[0]
                first_data = company_values[0]
                years_to_100m = first_data.get('years_to_100m', 0)
                insights.append(f"{first_name} reaches $100M in {years_to_100m:.1f} years")
            
            # Second company insight (if exists)
            if len(company_names) > 1 and len(company_values) > 1:
                second_name = company_names[1]
                second_data = company_values[1]
                years_to_100m = second_data.get('years_to_100m', 0)
                insights.append(f"{second_name} reaches $100M in {years_to_100m:.1f} years")
            
            # Growth rates
            growth_rates = []
            for name, data in companies_100m_data.items():
                growth_rate_pct = data.get('growth_rate_pct', 100)  # Use percentage field for display
                growth_rates.append(f"{name}: {growth_rate_pct}%")
            if growth_rates:
                insights.append(f"Required growth rates: {', '.join(growth_rates)}")
            
            # Current ARR
            current_arrs = []
            for name, data in companies_100m_data.items():
                current_arr = data.get('current_arr', 0)
                arr_in_millions = current_arr / 1_000_000 if current_arr else 0
                current_arrs.append(f"{name}: ${arr_in_millions:.1f}M")
            if current_arrs:
                insights.append(f"Current ARR: {', '.join(current_arrs)}")
            
        except Exception as e:
            logger.warning(f"Error generating Path to $100M insights: {e}")
            insights = ["Analysis in progress"]
        
        return insights
    
    async def _execute_excel_generation(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Generate Excel spreadsheet with formulas and formatting"""
        try:
            companies = self.shared_data.get("companies", [])
            commands = []
            
            # Headers with formatting
            headers = ["Company", "Stage", "Revenue", "Valuation", "Total Funding", "Team Size", "Founded", "Business Model", "Gross Margin"]
            for i, header in enumerate(headers):
                cell = f"{chr(65 + i)}1"
                commands.append(f'sheet.write("{cell}", "{header}").style("bold", true).style("backgroundColor", "#4285F4").style("color", "white")')
            
            # Data rows
            for row_idx, company in enumerate(companies, start=2):
                commands.append(f'sheet.write("A{row_idx}", "{company.get("company", "")}")')
                commands.append(f'sheet.write("B{row_idx}", "{company.get("stage", "")}")')
                
                revenue = self._get_field_safe(company, "revenue")
                if revenue:
                    commands.append(f'sheet.write("C{row_idx}", {revenue}).format("currency")')
                
                valuation = self._get_field_safe(company, "valuation")
                if valuation:
                    commands.append(f'sheet.write("D{row_idx}", {valuation}).format("currency")')
                
                funding = self._get_field_safe(company, "total_funding")
                if funding:
                    commands.append(f'sheet.write("E{row_idx}", {funding}).format("currency")')
                
                commands.append(f'sheet.write("F{row_idx}", {company.get("team_size", 0)})')
                commands.append(f'sheet.write("G{row_idx}", "{company.get("founded_year", "")}")')
                commands.append(f'sheet.write("H{row_idx}", "{company.get("business_model", "")}")')
                
                margin = company.get("key_metrics", {}).get("gross_margin", 0)
                if margin:
                    commands.append(f'sheet.write("I{row_idx}", {margin}).format("percentage")')
            
            # Add summary formulas
            if len(companies) > 0:
                last_row = len(companies) + 1
                summary_row = last_row + 2
                
                commands.append(f'sheet.write("A{summary_row}", "TOTALS").style("bold", true)')
                commands.append(f'sheet.formula("C{summary_row}", "=SUM(C2:C{last_row})").format("currency")')
                commands.append(f'sheet.formula("D{summary_row}", "=AVERAGE(D2:D{last_row})").format("currency")')
                commands.append(f'sheet.formula("E{summary_row}", "=SUM(E2:E{last_row})").format("currency")')
                commands.append(f'sheet.formula("F{summary_row}", "=SUM(F2:F{last_row})")')
                
                # Add chart
                commands.append(f'sheet.createChart("column", "K2", {{"data": "A1:E{last_row}", "title": "Company Metrics Comparison"}})')
            
            # Return standardized format for frontend consumption
            return {
                "format": "spreadsheet",
                "commands": commands,
                "metadata": {
                    "rows": len(companies) + 3,
                    "columns": 9,
                    "generated_at": datetime.now().isoformat(),
                    "company_count": len(companies)
                }
            }
            
        except Exception as e:
            logger.error(f"Excel generation error: {e}")
            return {"error": str(e), "format": "spreadsheet"}
    
    async def _populate_memo_service_data(self) -> None:
        """Populate shared_data with the service outputs that LightweightMemoService expects.

        Mirrors what _execute_deck_generation does per-company (PWERM, cap table
        evolution, return curves) and stores results under the keys that memo
        templates declare as optional_data:
          - shared_data["scenario_analysis"]
          - shared_data["cap_table_history"]
          - shared_data["revenue_projections"]

        Skips any sub-step that already has data so this is idempotent when the
        skill chain ran upstream services first.
        """
        try:
            companies = self.shared_data.get("companies", [])
            companies = [c for c in companies if c and isinstance(c, dict) and c.get("company")]
            if not companies:
                logger.debug("[MEMO] _populate_memo_service_data: no companies, skipping")
                return

            # ── Selective Tavily enrichment for companies missing critical data ──
            if self.tavily_api_key:
                gaps = [c for c in companies
                        if not (c.get("revenue") or c.get("inferred_revenue"))
                        or not c.get("valuation")]
                if gaps:
                    logger.info(f"[MEMO] Enriching {len(gaps)} companies with gaps via Tavily")
                    try:
                        from app.services.micro_skills.gap_resolver import resolve_gaps

                        async def _tavily_fn(query: str) -> dict:
                            return await self._tavily_search(query)

                        async def _llm_fn(prompt: str, system: str = "") -> str:
                            from app.services.model_router import ModelCapability
                            resp = await self.model_router.get_completion(
                                prompt=prompt, system_prompt=system,
                                capability=ModelCapability.EXTRACTION,
                                max_tokens=2000, caller_context="memo_gap_fill",
                            )
                            return resp.get("response", "") if isinstance(resp, dict) else str(resp)

                        enriched = await resolve_gaps(
                            companies=gaps,
                            fund_id=self.shared_data.get("fund_context", {}).get("fund_id"),
                            tavily_search_fn=_tavily_fn,
                            llm_extract_fn=_llm_fn,
                        )
                        # Merge enriched data back
                        enriched_map = {(e.get("company") or "").lower(): e
                                        for e in (enriched.get("companies") or enriched if isinstance(enriched, list) else [])}
                        for c in companies:
                            key = (c.get("company") or "").lower()
                            if key in enriched_map:
                                for field, val in enriched_map[key].items():
                                    if val and not c.get(field):
                                        c[field] = val
                    except Exception as enrich_err:
                        logger.warning(f"[MEMO] Tavily enrichment failed (non-fatal): {enrich_err}")

            # ── Sourcing context: comparable companies + scoring ──────────
            if not self.shared_data.get("sourcing_context"):
                try:
                    from app.services.sourcing_service import query_companies as sourcing_query, score_companies as sourcing_score

                    fund_id = (self.shared_data.get("fund_context") or {}).get("fund_id")
                    # Gather sectors from portfolio for comparable lookup
                    sectors = {c.get("sector") or c.get("industry", "") for c in companies}
                    sectors.discard("")
                    comparables: List[Dict[str, Any]] = []
                    for sector in list(sectors)[:5]:  # cap to avoid excessive queries
                        sr = await sourcing_query(
                            filters={"sector": sector},
                            sort_by="name", sort_desc=False,
                            limit=100, fund_id=fund_id,
                        )
                        comparables.extend(sr.get("companies", []))

                    if comparables:
                        # De-dup by name
                        seen = set()
                        unique = []
                        for comp in comparables:
                            key = (comp.get("name") or "").lower()
                            if key and key not in seen:
                                seen.add(key)
                                unique.append(comp)

                        scored_comps = sourcing_score(unique)
                        self.shared_data["sourcing_context"] = {
                            "comparables": scored_comps[:50],  # top 50 by score
                            "total_in_db": len(unique),
                            "sectors_queried": list(sectors),
                        }

                        # Enrich each portfolio company with its sector benchmarks
                        sector_benchmarks: Dict[str, Dict[str, Any]] = {}
                        for comp in scored_comps:
                            s = (comp.get("sector") or "").lower()
                            if s and s not in sector_benchmarks:
                                sector_comps = [x for x in scored_comps if (x.get("sector") or "").lower() == s]
                                arrs = [x.get("arr") for x in sector_comps if (x.get("arr") or 0) > 0]
                                vals = [x.get("valuation") for x in sector_comps if (x.get("valuation") or 0) > 0]
                                sector_benchmarks[s] = {
                                    "median_arr": sorted(arrs)[len(arrs) // 2] if arrs else 0,
                                    "median_valuation": sorted(vals)[len(vals) // 2] if vals else 0,
                                    "count": len(sector_comps),
                                }

                        for c in companies:
                            s = (c.get("sector") or c.get("industry") or "").lower()
                            if s in sector_benchmarks:
                                c["_sector_benchmark"] = sector_benchmarks[s]

                        logger.info(f"[MEMO] Sourcing context: {len(unique)} comparables across {len(sectors)} sectors")
                except Exception as src_err:
                    logger.warning(f"[MEMO] Sourcing enrichment failed (non-fatal): {src_err}")

            fund_context = self.shared_data.get("fund_context") or {}
            cap_table_history: Dict[str, Any] = dict(self.shared_data.get("cap_table_history") or {})
            scenario_analysis: Dict[str, Any] = dict(self.shared_data.get("scenario_analysis") or {})
            revenue_projections: Dict[str, Any] = dict(self.shared_data.get("revenue_projections") or {})

            for company in companies:
                company_name = company.get("company", "Unknown")
                try:
                    # ── PWERM + cap-table evolution ──────────────────────────────
                    if not company.get("pwerm_scenarios"):
                        stage = self._determine_accurate_stage(company)
                        company_stage = self._get_stage_enum(stage)
                        revenue = (
                            self._get_field_safe(company, "revenue")
                            or self._get_field_safe(company, "inferred_revenue")
                            or 0
                        )
                        valuation = (
                            self._get_field_safe(company, "valuation")
                            or self._get_field_safe(company, "inferred_valuation")
                            or 0
                        )
                        growth_rate = self._get_field_safe(company, "growth_rate") or 2.0
                        inferred_val = self._get_field_safe(company, "inferred_valuation") or 0

                        val_request = ValuationRequest(
                            company_name=company_name,
                            stage=company_stage,
                            revenue=revenue,
                            growth_rate=growth_rate,
                            last_round_valuation=valuation if valuation > 0 else None,
                            inferred_valuation=inferred_val,
                            total_raised=self._get_field_safe(company, "total_funding"),
                        )

                        pwerm_result = await self.valuation_engine._calculate_pwerm(val_request)
                        full_scenarios = pwerm_result.scenarios or []

                        if full_scenarios:
                            check_size = self._get_optimal_check_size(company, fund_context)
                            post_money = valuation + check_size
                            our_investment = {
                                "amount": check_size,
                                "ownership": check_size / post_money if post_money > 0 else 0.08,
                            }
                            for scenario in full_scenarios:
                                self.valuation_engine.model_cap_table_evolution(
                                    scenario, company, our_investment
                                )
                            self.valuation_engine.generate_return_curves(full_scenarios, our_investment)

                            company["pwerm_scenarios"] = full_scenarios
                            company["pwerm_valuation"] = pwerm_result.fair_value
                            company["exit_scenarios"] = self.valuation_engine.generate_simple_scenarios(val_request)
                            company["full_exit_distribution"] = [
                                {
                                    "scenario": s.scenario,
                                    "probability": s.probability,
                                    "exit_value": s.exit_value,
                                    "time_to_exit": s.time_to_exit,
                                    "moic": s.moic,
                                }
                                for s in full_scenarios
                            ]

                            scenario_analysis[company_name] = {
                                "scenarios": company["full_exit_distribution"],
                                "pwerm_valuation": pwerm_result.fair_value,
                            }
                            logger.info(
                                "[MEMO] Populated PWERM for %s: %d scenarios, value $%s",
                                company_name, len(full_scenarios), f"{pwerm_result.fair_value:,.0f}",
                            )

                    # ── Cap table history ─────────────────────────────────────────
                    if company_name not in cap_table_history:
                        try:
                            cap_hist = self.cap_table_service.calculate_full_cap_table_history(company)
                            if cap_hist:
                                cap_table_history[company_name] = cap_hist
                                logger.info("[MEMO] Populated cap_table_history for %s", company_name)
                        except Exception as ct_err:
                            logger.warning("[MEMO] cap_table_history failed for %s: %s", company_name, ct_err)

                    # ── Revenue projections ───────────────────────────────────────
                    if company_name not in revenue_projections:
                        try:
                            self._ensure_growth_metrics(company)
                            revenue_projections[company_name] = {
                                "current_revenue": (
                                    self._get_field_safe(company, "revenue")
                                    or self._get_field_safe(company, "inferred_revenue")
                                    or 0
                                ),
                                "growth_rate": self._get_field_safe(company, "growth_rate") or 2.0,
                                "projected_growth_rate": company.get("projected_growth_rate"),
                                "growth_metrics": company.get("growth_metrics"),
                            }
                        except Exception as rp_err:
                            logger.warning("[MEMO] revenue_projections failed for %s: %s", company_name, rp_err)

                except Exception as company_err:
                    logger.error("[MEMO] _populate_memo_service_data failed for %s: %s", company_name, company_err, exc_info=True)

            # ── Revenue decay projections via RevenueProjectionService ───
            # Upgrade raw revenue_projections from basic {current, growth_rate}
            # to full year-by-year decay curves that ChartDataService can plot.
            try:
                from app.services.revenue_projection_service import RevenueProjectionService
                for company_name, rp in list(revenue_projections.items()):
                    if isinstance(rp, dict) and not rp.get("yearly"):
                        base_rev = ensure_numeric(rp.get("current_revenue"), 0)
                        growth = ensure_numeric(rp.get("growth_rate"), 0.5)
                        if base_rev > 0:
                            company = next((c for c in companies if c.get("company") == company_name), {})
                            yearly = RevenueProjectionService.project_revenue_with_decay(
                                base_revenue=base_rev,
                                initial_growth=growth,
                                years=6,
                                quality_score=1.0,
                                stage=company.get("stage"),
                                sector=company.get("sector") or company.get("business_model"),
                                investor_quality=company.get("investor_quality"),
                                geography=company.get("geography"),
                                return_projections=True,
                            )
                            if isinstance(yearly, list):
                                rp["yearly"] = yearly
                                logger.debug("[MEMO] Decay projections for %s: %d years", company_name, len(yearly))
            except Exception as rps_err:
                logger.warning("[MEMO] RevenueProjectionService enrichment failed: %s", rps_err)

            # ── ScenarioTreeService: portfolio-level bull/base/bear ─────
            if not self.shared_data.get("fund_scenarios"):
                try:
                    from app.services.scenario_tree_service import ScenarioTreeService
                    sts = ScenarioTreeService()
                    companies_for_scenarios = {
                        c.get("company", "Unknown"): c for c in companies
                        if c.get("company")
                    }
                    if companies_for_scenarios:
                        tree = sts.build_portfolio_scenarios(companies_for_scenarios, fund_context, years=5)
                        scenario_all_charts = sts.to_all_charts(tree)
                        scenario_analysis["_portfolio_tree"] = {
                            "path_count": len(tree.paths),
                            "companies": tree.companies,
                            "expected_value": {
                                "nav": tree.expected_value.nav if tree.expected_value else 0,
                                "tvpi": tree.expected_value.tvpi if tree.expected_value else 0,
                                "dpi": tree.expected_value.dpi if tree.expected_value else 0,
                                "irr": tree.expected_value.irr if tree.expected_value else 0,
                            } if tree.expected_value else {},
                        }
                        logger.info("[MEMO] ScenarioTree: %d paths for %d companies",
                                    len(tree.paths), len(companies_for_scenarios))
                except Exception as sts_err:
                    logger.warning("[MEMO] ScenarioTreeService failed: %s", sts_err)
                    scenario_all_charts = {}
            else:
                scenario_all_charts = {}

            # ── FPA: Monte Carlo + Sensitivity (if available) ──────────
            fpa_result = {}
            monte_carlo_result = {}
            if not self.shared_data.get("fpa_result"):
                try:
                    fpa_svc = self._get_fpa_regression_service() if hasattr(self, "_get_fpa_regression_service") else None
                    if fpa_svc and companies:
                        # Sensitivity: how does each company's growth affect portfolio NAV
                        revenues = [ensure_numeric(c.get("revenue") or c.get("inferred_revenue"), 0) for c in companies]
                        growth_rates = [ensure_numeric(c.get("growth_rate"), 0.5) for c in companies]
                        if any(r > 0 for r in revenues):
                            fpa_result = fpa_svc.sensitivity_analysis(
                                base_inputs={
                                    "revenues": revenues,
                                    "growth_rates": growth_rates,
                                },
                                variable_ranges={f"growth_{c.get('company', i)}": (0.1, 3.0)
                                                 for i, c in enumerate(companies)},
                                model_fn=lambda inputs: sum(
                                    r * g for r, g in zip(inputs.get("revenues", revenues),
                                                          inputs.get("growth_rates", growth_rates))
                                ),
                            )
                            logger.debug("[MEMO] FPA sensitivity computed")

                        # Monte Carlo on portfolio revenue
                        total_rev = sum(r for r in revenues if r > 0)
                        if total_rev > 0:
                            monte_carlo_result = fpa_svc.monte_carlo_simulation(
                                base_scenario={"revenue": total_rev, "growth": 0.5},
                                distributions={"revenue": ("normal", total_rev, total_rev * 0.3),
                                               "growth": ("normal", 0.5, 0.2)},
                                iterations=1000,
                            )
                            logger.debug("[MEMO] FPA Monte Carlo computed")
                except Exception as fpa_err:
                    logger.warning("[MEMO] FPA analysis failed: %s", fpa_err)

            # ── Fund-level metrics (TVPI, DPI, IRR) ────────────────────
            # Compute actual fund metrics if not already present
            fund_metrics: Dict[str, Any] = dict(self.shared_data.get("fund_metrics") or {})
            if "metrics" not in fund_metrics or fund_metrics.get("_source") == "fallback_stub":
                try:
                    fm_result = await self._execute_fund_metrics({
                        "fund_id": fund_context.get("fund_id") or fund_context.get("fundId"),
                        "context": fund_context,
                    })
                    if isinstance(fm_result, dict) and not fm_result.get("error"):
                        fm_data = fm_result.get("fund_metrics", fm_result)
                        fund_metrics.update(fm_data)
                        logger.info("[MEMO] Fund metrics computed: %s", list(fm_data.keys()))
                except Exception as fm_err:
                    logger.warning("[MEMO] Fund metrics computation failed: %s", fm_err)

            # ── Portfolio health ────────────────────────────────────────
            if not self.shared_data.get("portfolio_health"):
                try:
                    ph_result = await self._tool_portfolio_health({
                        "fund_id": fund_context.get("fund_id") or fund_context.get("fundId"),
                    })
                    if isinstance(ph_result, dict) and not ph_result.get("error"):
                        logger.info("[MEMO] Portfolio health computed")
                except Exception as ph_err:
                    logger.warning("[MEMO] Portfolio health failed: %s", ph_err)

            # ── Follow-on strategy ─────────────────────────────────────
            if not self.shared_data.get("followon_strategy"):
                try:
                    fo_result = await self._execute_followon_strategy({
                        "fund_id": fund_context.get("fund_id") or fund_context.get("fundId"),
                    })
                    if isinstance(fo_result, dict) and not fo_result.get("error"):
                        logger.info("[MEMO] Follow-on strategy computed")
                except Exception as fo_err:
                    logger.warning("[MEMO] Follow-on strategy failed: %s", fo_err)

            # ── Fund-level chart data ──────────────────────────────────
            # Compute aggregated charts that memo templates need but that the
            # per-company loop above cannot produce (DPI Sankey, NAV waterfall,
            # heatmap, bar comparison).
            try:
                from app.services.chart_data_service import ChartDataService
                cds = ChartDataService()
                fund_size = ensure_numeric(
                    fund_context.get("fund_size") or fund_context.get("total_committed"),
                    260_000_000,
                )
                # Normalise company dicts so CDS generators see numeric revenue
                for _co in companies:
                    if not ensure_numeric(_co.get("revenue")):
                        _inferred = ensure_numeric(_co.get("inferred_revenue"))
                        if _inferred:
                            _co["revenue"] = _inferred

                if "dpi_sankey" not in fund_metrics:
                    try:
                        fund_metrics["dpi_sankey"] = cds.generate_dpi_sankey(companies, fund_size)
                    except Exception as exc:
                        logger.warning("[MEMO] fund dpi_sankey failed: %s", exc)

                if "waterfall" not in fund_metrics:
                    try:
                        fund_metrics["waterfall"] = cds.generate_waterfall(companies)
                    except Exception as exc:
                        logger.warning("[MEMO] fund waterfall failed: %s", exc)

                if "nav_live" not in fund_metrics:
                    try:
                        fund_metrics["nav_live"] = cds.generate_nav_live(companies)
                    except Exception as exc:
                        logger.warning("[MEMO] fund nav_live failed: %s", exc)

                if "heatmap" not in fund_metrics:
                    try:
                        fund_metrics["heatmap"] = cds.generate_heatmap(companies)
                    except Exception as exc:
                        logger.warning("[MEMO] fund heatmap failed: %s", exc)

                # Bull/bear/base bar chart from scenario tree
                if "fund_scenarios_bar" not in fund_metrics and scenario_all_charts:
                    try:
                        bar = scenario_all_charts.get("scenario_comparison")
                        if bar:
                            fund_metrics["fund_scenarios_bar"] = bar
                    except Exception as exc:
                        logger.warning("[MEMO] fund scenarios bar failed: %s", exc)

            except Exception as fund_exc:
                logger.warning("[MEMO] fund-level chart generation failed: %s", fund_exc)

            # Write back to shared_data
            async with self.shared_data_lock:
                if scenario_analysis:
                    self.shared_data["scenario_analysis"] = scenario_analysis
                if cap_table_history:
                    self.shared_data["cap_table_history"] = cap_table_history
                if revenue_projections:
                    self.shared_data["revenue_projections"] = revenue_projections
                if fund_metrics:
                    self.shared_data["fund_metrics"] = fund_metrics
                if scenario_all_charts:
                    self.shared_data["scenario_all_charts"] = scenario_all_charts
                if fpa_result:
                    self.shared_data["fpa_result"] = fpa_result
                if monte_carlo_result:
                    self.shared_data["monte_carlo_result"] = monte_carlo_result

            logger.info(
                "[MEMO] _populate_memo_service_data done: scenario_analysis=%s, cap_table_history=%s, "
                "revenue_projections=%s, fund_metrics=%s, scenario_all_charts=%s, fpa=%s, monte_carlo=%s",
                list(scenario_analysis.keys()),
                list(cap_table_history.keys()),
                list(revenue_projections.keys()),
                list(fund_metrics.keys()),
                bool(scenario_all_charts),
                bool(fpa_result),
                bool(monte_carlo_result),
            )

        except Exception as e:
            logger.error("[MEMO] _populate_memo_service_data outer error (non-fatal): %s", e, exc_info=True)

    async def _load_companies_for_memo(self) -> None:
        """Pull portfolio companies from DB into shared_data["companies"] if not already present.

        Uses PortfolioService.get_portfolio() when a fund_id is available.
        Normalises DB rows to the same schema that the @ fetch flow produces so
        that LightweightMemoService can treat them identically.
        """
        if self.shared_data.get("companies"):
            return  # already populated by @ fetch — nothing to do

        fund_id = self.shared_data.get("fund_context", {}).get("fund_id")
        if not fund_id:
            logger.debug("[MEMO] _load_companies_for_memo: no fund_id, skipping DB load")
            return

        try:
            from app.services.portfolio_service import PortfolioService
            ps = PortfolioService()
            portfolio = await ps.get_portfolio(fund_id)
            db_companies = portfolio.get("companies", [])
            if not db_companies:
                logger.debug("[MEMO] _load_companies_for_memo: no companies in DB for fund %s", fund_id)
                return

            # Normalise DB rows → shared_data schema
            normalised: List[Dict[str, Any]] = []
            for row in db_companies:
                arr = row.get("arr")
                valuation = row.get("valuation") or row.get("current_valuation")
                normalised.append({
                    "company": row.get("companyName") or row.get("name", "Unknown"),
                    "stage": row.get("stage", ""),
                    "sector": row.get("sector", ""),
                    "revenue": arr,
                    "inferred_revenue": arr,
                    "valuation": valuation,
                    "inferred_valuation": valuation,
                    "burn_rate": row.get("burn_rate"),
                    "runway_months": row.get("runway_months"),
                    "team_size": row.get("employee_count"),
                    "growth_rate": row.get("growth_rate"),
                    "total_funding": row.get("initial_investment"),
                    "ownership_percentage": row.get("ownership_percentage"),
                    "investment_status": row.get("investment_status"),
                    "_source": "portfolio_db",
                })

            async with self.shared_data_lock:
                self.shared_data["companies"] = normalised

            logger.info("[MEMO] _load_companies_for_memo: loaded %d companies from DB", len(normalised))

        except Exception as e:
            logger.warning("[MEMO] _load_companies_for_memo failed (non-fatal): %s", e)

    async def _enrich_companies(self, companies: List[Dict[str, Any]], depth: str = "benchmark") -> None:
        """Fan-out enrichment across all companies via asyncio.gather.

        depth="benchmark" (Tier 1, no network): runs stage_benchmark_fill,
        time_adjusted_estimate, reconstruct_funding_history concurrently for
        every company.  Results are merged back in-place.

        depth="search" (Tier 2): additionally calls gap_resolver for companies
        that still have sparse data after the benchmark pass.
        """
        if not companies:
            return

        try:
            from app.services.micro_skills.benchmark_skills import (
                stage_benchmark_fill,
                time_adjusted_estimate,
                reconstruct_funding_history,
            )
        except ImportError as e:
            logger.warning("[MEMO] benchmark_skills import failed: %s", e)
            return

        async def _enrich_one(company: Dict[str, Any]) -> None:
            try:
                ctx: Dict[str, Any] = {}
                r1 = await stage_benchmark_fill(company, ctx)
                if r1 and r1.updates:
                    for k, v in r1.updates.items():
                        if company.get(k) is None:
                            company[k] = v

                r2 = await time_adjusted_estimate(company, ctx)
                if r2 and r2.updates:
                    for k, v in r2.updates.items():
                        if company.get(k) is None:
                            company[k] = v

                r3 = await reconstruct_funding_history(company, ctx)
                if r3 and r3.updates:
                    for k, v in r3.updates.items():
                        if company.get(k) is None:
                            company[k] = v
            except Exception as enrich_err:
                logger.debug("[MEMO] enrich_one failed for %s: %s", company.get("company"), enrich_err)

        await asyncio.gather(*[_enrich_one(c) for c in companies], return_exceptions=True)
        logger.info("[MEMO] _enrich_companies: enriched %d companies (depth=%s)", len(companies), depth)

        if depth == "search":
            try:
                from app.services.micro_skills.gap_resolver import resolve_gaps
                sparse = [c for c in companies if not c.get("revenue") and not c.get("inferred_revenue")]
                if sparse:
                    await asyncio.gather(*[resolve_gaps(c, {}) for c in sparse], return_exceptions=True)
                    logger.info("[MEMO] _enrich_companies: gap_resolver ran for %d sparse companies", len(sparse))
            except ImportError:
                pass

    async def _run_portfolio_analysis(self) -> None:
        """Aggregate fund-level metrics across all companies in shared_data.

        Computes:
        - fund_nav: sum of ownership% × valuation across all positions
        - cohort groups: companies bucketed by entry stage
        - what-if scenarios: base / bear / bull fund MOIC
        - at_risk list: companies with runway < 6 months

        Results written to shared_data["portfolio_health"] and
        shared_data["fund_scenarios"] so memo templates can consume them.
        """
        companies = self.shared_data.get("companies", [])
        if not companies:
            return

        fund_ctx = self.shared_data.get("fund_context", {})
        fund_size = ensure_numeric(fund_ctx.get("fund_size") or fund_ctx.get("total_committed"), 260_000_000)

        cohorts: Dict[str, List[Dict]] = {}
        total_nav = 0.0
        total_invested = 0.0
        at_risk: List[str] = []
        company_analytics: Dict[str, Any] = {}
        company_returns: Dict[str, Any] = {}

        for c in companies:
            name = c.get("company", "Unknown")
            stage = c.get("stage", "Unknown")
            arr = ensure_numeric(c.get("revenue") or c.get("inferred_revenue") or c.get("arr"))
            valuation = ensure_numeric(c.get("valuation") or c.get("inferred_valuation"))
            ownership_pct = ensure_numeric(c.get("ownership_percentage"), 0.08)  # default 8%
            invested = ensure_numeric(c.get("initial_investment") or c.get("total_funding") or (valuation * 0.1))
            growth = ensure_numeric(c.get("growth_rate"), 0)
            runway = ensure_numeric(c.get("runway_months"), 18)

            nav = valuation * (ownership_pct / 100.0 if ownership_pct > 1 else ownership_pct)
            cost_basis = invested * (ownership_pct / 100.0 if ownership_pct > 1 else ownership_pct)
            moic = (nav / cost_basis) if cost_basis > 0 else 1.0

            total_nav += nav
            total_invested += cost_basis

            if runway < 6:
                at_risk.append(name)

            cohort_key = stage if stage else "Unknown"
            cohorts.setdefault(cohort_key, []).append(c)

            company_analytics[name] = {
                "company_name": name,
                "current_arr": arr,
                "growth_rate": growth,
                "estimated_runway_months": runway,
                "valuation": valuation,
                "ownership_pct": ownership_pct,
                "nav": nav,
            }
            company_returns[name] = {
                "moic": round(moic, 2),
                "nav_contribution": nav,
                "cost_basis": cost_basis,
            }

        # Fund-level scenarios
        fund_tvpi = total_nav / total_invested if total_invested > 0 else 1.0
        bear_moic = fund_tvpi * 0.5
        bull_moic = fund_tvpi * 2.0

        portfolio_health = {
            "total_nav": total_nav,
            "total_invested": total_invested,
            "fund_tvpi": round(fund_tvpi, 2),
            "at_risk_companies": at_risk,
            "company_analytics": company_analytics,
            "company_returns": company_returns,
            "cohort_breakdown": {k: len(v) for k, v in cohorts.items()},
        }

        fund_scenarios = {
            "portfolio_scenarios": [
                {"scenario_name": "Bear", "fund_moic": round(bear_moic, 2),
                 "description": "Market downturn, 50% valuation haircut"},
                {"scenario_name": "Base", "fund_moic": round(fund_tvpi, 2),
                 "description": "Current marks, status quo"},
                {"scenario_name": "Bull", "fund_moic": round(bull_moic, 2),
                 "description": "Market expansion, 2x current valuations"},
            ],
            "fund_size": fund_size,
            "total_nav": total_nav,
        }

        # Build exit_pipeline list for portfolio_review template
        exit_pipeline_list = []
        for c in companies:
            n = c.get("company", "Unknown")
            cr = company_returns.get(n) or {}
            moic_v = cr.get("moic", 0)
            nav_v = company_analytics.get(n, {}).get("nav", 0)
            next_timing = ensure_numeric((c.get("next_round") or {}).get("next_round_timing"), 24)
            if moic_v >= 3.0 and next_timing <= 18:
                exit_pipeline_list.append({
                    "company": n,
                    "expected_moic": moic_v,
                    "expected_proceeds_m": round(nav_v / 1e6, 2),
                    "months_to_exit": int(next_timing),
                })

        # portfolio_analysis: unified key consumed by portfolio_review template + chart builders
        portfolio_analysis = {
            "total_nav_m": round(total_nav / 1e6, 2),
            "total_invested_m": round(total_invested / 1e6, 2),
            "fund_moic": round(fund_tvpi, 2),
            "company_count": len(companies),
            "company_returns": company_returns,
            "company_analytics": company_analytics,
            "cohorts": {k: len(v) for k, v in cohorts.items()},
            "at_risk": at_risk,
            "exit_pipeline": sorted(exit_pipeline_list, key=lambda x: x["expected_proceeds_m"], reverse=True),
            "fund_scenarios": {
                "bear": {"fund_moic": round(bear_moic, 2), "total_proceeds_m": round(total_nav * 0.5 / 1e6, 2)},
                "base": {"fund_moic": round(fund_tvpi, 2), "total_proceeds_m": round(total_nav / 1e6, 2)},
                "bull": {"fund_moic": round(bull_moic, 2), "total_proceeds_m": round(total_nav * 2.0 / 1e6, 2)},
            },
        }

        async with self.shared_data_lock:
            self.shared_data["portfolio_health"] = portfolio_health
            self.shared_data["fund_scenarios"] = fund_scenarios
            self.shared_data["portfolio_analysis"] = portfolio_analysis
            self.shared_data.setdefault("fund_metrics", {}).update({
                "metrics": {
                    "total_nav": total_nav,
                    "total_invested": total_invested,
                    "tvpi": round(fund_tvpi, 2),
                    "total_committed": fund_size,
                },
                "investments": [
                    {
                        "company_name": name,
                        "nav_contribution": analytics["nav"],
                        "invested": analytics.get("nav", 0) / max(company_returns.get(name, {}).get("moic", 1), 0.01),
                        "current_nav": analytics["nav"],
                    }
                    for name, analytics in company_analytics.items()
                ],
            })

        logger.info(
            "[MEMO] _run_portfolio_analysis: %d companies, NAV=$%s, TVPI=%.2fx, at_risk=%s, exit_pipeline=%d",
            len(companies), f"{total_nav / 1e6:,.1f}M", fund_tvpi, at_risk, len(exit_pipeline_list),
        )

    async def _execute_memo_generation(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Generate investment memo using the lightweight 3-shot pipeline.

        Shot 1 (no LLM): Detect type → select template → audit data
        Shot 2 (1 LLM call): Generate all narrative sections at once
        Shot 3 (no LLM): Inject charts/tables from shared_data artifacts

        Returns ``format: 'docs'`` with structured sections for the frontend.
        Falls back to legacy generation if LightweightMemoService unavailable.
        """
        try:
            prompt = inputs.get("prompt", "") or self.shared_data.get("original_prompt", "")
            memo_type = inputs.get("memo_type")

            # Step 0: Load portfolio companies from DB if not already fetched via @
            try:
                await asyncio.wait_for(self._load_companies_for_memo(), timeout=15.0)
            except asyncio.TimeoutError:
                logger.warning("[MEMO] _load_companies_for_memo timed out")

            # Step 0b: Enrich sparse DB companies with stage benchmarks (Tier 1, no network)
            _all_companies = self.shared_data.get("companies", [])
            _db_sourced = [c for c in _all_companies if c.get("_source") == "portfolio_db"]
            if _db_sourced:
                try:
                    await asyncio.wait_for(
                        self._enrich_companies(_db_sourced, depth="benchmark"),
                        timeout=10.0,
                    )
                except asyncio.TimeoutError:
                    logger.warning("[MEMO] _enrich_companies timed out")

            # Step 0c: Fund-level aggregation (NAV, cohorts, scenarios) for portfolio memos
            if len(self.shared_data.get("companies", [])) > 1:
                try:
                    await asyncio.wait_for(self._run_portfolio_analysis(), timeout=10.0)
                except asyncio.TimeoutError:
                    logger.warning("[MEMO] _run_portfolio_analysis timed out")

            # Pre-flight: derive any missing secondary keys from companies
            # Use timeout to prevent hydration from blocking memo generation
            try:
                await asyncio.wait_for(
                    self._hydrate_shared_data_from_companies(),
                    timeout=10.0,
                )
            except asyncio.TimeoutError:
                logger.warning("[MEMO] Hydration timed out after 10s — proceeding with available data")

            # Fix 1: Run the same upstream services that deck uses so LightweightMemoService
            # has scenario_analysis, cap_table_history, and revenue_projections in shared_data.
            await self._populate_memo_service_data()

            # Use lightweight pipeline if available
            if LightweightMemoService is not None:
                memo_svc = LightweightMemoService(self.model_router, self.shared_data)

                # Wrap memo generation in a timeout to prevent infinite hangs
                try:
                    result = await asyncio.wait_for(
                        memo_svc.generate(prompt, memo_type),
                        timeout=180.0,  # 3 minutes max for memo generation
                    )
                except asyncio.TimeoutError:
                    logger.error("[MEMO] Memo generation timed out after 180s")
                    return {
                        "error": "Memo generation timed out. Try again or simplify the request.",
                        "format": "docs",
                        "sections": [],
                        "title": "Memo Generation Timed Out",
                    }

                # Store memo as session artifact for draggable context
                # Use wait_for to prevent lock contention from hanging
                try:
                    async with asyncio.timeout(5.0):
                        async with self.shared_data_lock:
                            if "memo_artifacts" not in self.shared_data:
                                self.shared_data["memo_artifacts"] = []
                            self.shared_data["memo_artifacts"].append({
                                "memo_type": result.get("memo_type", "unknown"),
                                "title": result.get("title", "Untitled"),
                                "generated_at": result.get("metadata", {}).get("generated_at"),
                                "is_resumable": result.get("is_resumable", False),
                                "section_count": len(result.get("sections", [])),
                            })
                except (asyncio.TimeoutError, Exception) as lock_err:
                    logger.warning("[MEMO] Lock acquisition for memo_artifacts timed out: %s", lock_err)

                # Auto-save resumable plan memos to documents table
                if result.get("is_resumable"):
                    await self._save_plan_memo(result)

                return result

            # --- Legacy fallback (original monolithic generation) ---
            logger.warning("[MEMO] LightweightMemoService unavailable, using legacy generation")
            return await self._execute_memo_generation_legacy(inputs)

        except Exception as e:
            logger.error(f"Memo generation error: {e}", exc_info=True)
            return {"error": str(e), "format": "docs", "sections": [], "title": "Memo Generation Failed"}

    async def _try_load_and_hydrate_plan(self) -> None:
        """Attempt to load the most recent plan memo and hydrate shared_data.

        Called when the user's prompt signals plan-resume intent.
        Silently no-ops if no plans are found or database is unavailable.
        """
        try:
            from app.core.adapters import get_document_repo
            doc_repo = get_document_repo()
            if doc_repo is None:
                return

            plans = LightweightMemoService.load_recent_plans(doc_repo, limit=1)
            if not plans:
                logger.info("[PLAN] No saved plans found for resumption")
                return

            plan = plans[0]
            async with self.shared_data_lock:
                LightweightMemoService.hydrate_shared_data(plan, self.shared_data)
            logger.info(
                "[PLAN] Resumed plan '%s' (id=%s) — shared_data hydrated",
                plan.get("title"),
                plan.get("id"),
            )
        except Exception as e:
            logger.warning("[PLAN] Plan loading failed (non-fatal): %s", e)

    async def _save_plan_memo(self, memo: Dict[str, Any]) -> None:
        """Auto-save a resumable plan memo to the documents table. Never raises."""
        try:
            from app.core.adapters import get_document_repo
            doc_repo = get_document_repo()
            if doc_repo is None:
                logger.debug("[MEMO] No document_repo — plan memo not persisted")
                return
            LightweightMemoService.save_to_documents(memo, doc_repo)
        except Exception as e:
            logger.warning("[MEMO] Auto-save plan memo failed (non-fatal): %s", e)

    async def _save_memo_to_documents(self, memo: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Explicitly save a memo (any type) to the documents table."""
        try:
            from app.core.adapters import get_document_repo
            doc_repo = get_document_repo()
            if doc_repo is None:
                return None
            return LightweightMemoService.save_to_documents(memo, doc_repo)
        except Exception as e:
            logger.error("[MEMO] Explicit memo save failed: %s", e, exc_info=True)
            return None

    async def _handle_memo_polish(self, user_feedback: str) -> Dict[str, Any]:
        """Fast path: polish the most recent memo with user feedback (1 LLM call)."""
        try:
            # Retrieve the last generated memo from agent_context
            agent_ctx = self.shared_data.get("agent_context", {})
            memo_sections = agent_ctx.get("memo_sections", [])
            if not memo_sections:
                return {"content": "No memo in context to refine. Generate a memo first.", "format": "analysis"}

            # Reconstruct a minimal memo dict for the polish method
            last_memo = {
                "sections": memo_sections,
                "title": "Current Memo",
                "memo_type": "unknown",
            }

            if LightweightMemoService is not None:
                memo_svc = LightweightMemoService(self.model_router, self.shared_data)
                polished = await memo_svc.polish(last_memo, user_feedback)
                return {
                    "format": "docs",
                    "sections": polished.get("sections", []),
                    "title": polished.get("title", "Refined Memo"),
                    "memo_type": polished.get("memo_type", "unknown"),
                }

            return {"content": "Memo service unavailable for polish.", "format": "analysis"}
        except Exception as e:
            logger.error("[MEMO] Polish failed: %s", e, exc_info=True)
            return {"content": f"Failed to refine memo: {e}", "format": "analysis"}

    async def _handle_memo_save(self) -> Dict[str, Any]:
        """Fast path: save the most recent memo to the documents table."""
        try:
            agent_ctx = self.shared_data.get("agent_context", {})
            memo_sections = agent_ctx.get("memo_sections", [])
            if not memo_sections:
                return {"content": "No memo in context to save.", "format": "analysis"}

            # Build memo dict from context
            artifacts = self.shared_data.get("memo_artifacts", [])
            last_artifact = artifacts[-1] if artifacts else {}
            memo = {
                "sections": memo_sections,
                "title": last_artifact.get("title", "Saved Memo"),
                "memo_type": last_artifact.get("memo_type", "generated_memo"),
                "is_resumable": last_artifact.get("is_resumable", False),
                "metadata": {"generated_at": last_artifact.get("generated_at")},
            }

            row = await self._save_memo_to_documents(memo)
            if row:
                return {"content": f"Memo saved successfully (id: {row.get('id')}).", "format": "analysis"}
            return {"content": "Could not save memo — database unavailable.", "format": "analysis"}
        except Exception as e:
            logger.error("[MEMO] Save action failed: %s", e, exc_info=True)
            return {"content": f"Failed to save memo: {e}", "format": "analysis"}

    async def _execute_memo_generation_legacy(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Legacy monolithic memo generation — kept as fallback."""
        try:
            companies = self.shared_data.get("companies", [])
            fund_context = self.shared_data.get("fund_context", {})
            followon_data = self.shared_data.get("followon_strategy", {})
            scenario_data = self.shared_data.get("scenario_analysis", {})
            fund_metrics_data = self.shared_data.get("fund_metrics", {})
            portfolio_data = self.shared_data.get("portfolio_analysis", {})
            prompt = inputs.get("prompt", "") or self.shared_data.get("original_prompt", "")

            # Determine memo type from prompt context
            prompt_lower = prompt.lower() if prompt else ""
            is_followon_memo = any(kw in prompt_lower for kw in ["follow-on", "followon", "follow on"])
            is_lp_report = any(kw in prompt_lower for kw in ["lp report", "lp quarterly", "quarterly report"])
            is_gp_strategy = any(kw in prompt_lower for kw in ["gp deck", "gp strategy", "strategy report"])

            memo_sections: List[Dict[str, Any]] = []

            # --- Title ---
            if is_followon_memo and companies:
                title = f"Follow-On Investment Memo — {', '.join(c.get('company', '?') for c in companies[:3])}"
            elif is_lp_report:
                fund_name = fund_context.get("fund_name", "Fund")
                title = f"LP Quarterly Report — {fund_name} — Q{((datetime.now().month - 1) // 3) + 1} {datetime.now().year}"
            elif is_gp_strategy:
                title = "GP Strategy & Portfolio Update"
            else:
                title = "Investment Analysis Memo"

            memo_sections.append({"type": "heading1", "content": title})
            memo_sections.append({"type": "paragraph", "content": f"Prepared {datetime.now().strftime('%B %d, %Y')}"})

            # --- Executive Summary ---
            memo_sections.append({"type": "heading2", "content": "Executive Summary"})
            exec_bullets = []
            if companies:
                total_funding = sum(self._get_field_safe(c, "total_funding") for c in companies)
                sectors = sorted(set(c.get("sector", "Unknown") for c in companies))
                exec_bullets.append(f"Analysis of {len(companies)} portfolio companies spanning {', '.join(sectors)} with ${total_funding / 1e6:,.1f}M total funding raised.")

            # Real fund metrics if available
            perf = fund_metrics_data.get("metrics", {}) if isinstance(fund_metrics_data, dict) else {}
            if perf.get("tvpi"):
                exec_bullets.append(f"Fund performance: {perf['tvpi']:.2f}x TVPI, {perf.get('dpi', 0):.2f}x DPI, {perf.get('irr', 0):.1f}% IRR.")
            elif fund_context.get("fund_size"):
                fund_size = fund_context["fund_size"]
                remaining = fund_context.get("remaining_capital", fund_size * 0.4)
                exec_bullets.append(f"Fund size ${fund_size / 1e6:,.0f}M with ${remaining / 1e6:,.0f}M remaining to deploy ({remaining / fund_size * 100:.0f}%).")

            # Follow-on recommendation summary
            if followon_data:
                for company_id, fo in followon_data.items():
                    if isinstance(fo, dict) and fo.get("recommendation"):
                        name = fo.get("company_name", company_id)
                        rec = fo["recommendation"]
                        pro_rata = fo.get("pro_rata_amount", 0)
                        exec_bullets.append(f"{name}: Recommend **{rec}**" + (f" — ${pro_rata / 1e6:,.1f}M pro-rata" if pro_rata else ""))

            if not exec_bullets:
                exec_bullets.append("Comprehensive analysis of portfolio companies with investment recommendations below.")

            memo_sections.append({"type": "list", "items": exec_bullets})

            # --- Per-Company Analysis ---
            for company in companies:
                name = company.get("company", "Unknown")
                memo_sections.append({"type": "heading2", "content": f"Company Analysis: {name}"})

                valuation = self._get_field_safe(company, "valuation")
                revenue = self._get_field_safe(company, "revenue") or self._get_field_safe(company, "inferred_revenue")
                stage = company.get("stage", "Unknown")
                rev_multiple = valuation / revenue if revenue and revenue > 0 else 0

                overview = (
                    f"**{name}** is a {stage} company"
                    + (f" with ${valuation / 1e6:,.1f}M valuation" if valuation else "")
                    + (f" and ${revenue / 1e6:,.1f}M ARR ({rev_multiple:.1f}x revenue multiple)" if revenue else "")
                    + "."
                )
                desc = company.get("product_description", "")
                if desc and desc != "Innovative technology company with strong growth potential.":
                    overview += f"\n\n{desc}"

                memo_sections.append({"type": "paragraph", "content": overview})

                # Key metrics table as list
                metrics_items = []
                if company.get("total_funding"):
                    metrics_items.append(f"Total Funding: ${self._get_field_safe(company, 'total_funding') / 1e6:,.1f}M")
                if company.get("team_size"):
                    metrics_items.append(f"Team Size: {company['team_size']}")
                if company.get("founded_year"):
                    metrics_items.append(f"Founded: {company['founded_year']}")
                gm = company.get("key_metrics", {}).get("gross_margin")
                if gm and isinstance(gm, (int, float)):
                    metrics_items.append(f"Gross Margin: {gm * 100:.0f}%")
                growth = company.get("revenue_growth")
                if growth and isinstance(growth, (int, float)):
                    metrics_items.append(f"Revenue Growth: {growth * 100:.0f}% YoY")
                if metrics_items:
                    memo_sections.append({"type": "list", "items": metrics_items})

                # Cap table Sankey chart if we have cap table history
                cap_history = company.get("cap_table_history") or self.shared_data.get("cap_table_history", {}).get(name)
                if cap_history and isinstance(cap_history, dict):
                    sankey_data = cap_history.get("sankey_data")
                    if sankey_data:
                        memo_sections.append({
                            "type": "chart",
                            "chart": {
                                "type": "sankey",
                                "title": f"{name} — Ownership Flow",
                                "data": sankey_data
                            }
                        })

                # PWERM / scenario chart if available
                company_scenarios = company.get("scenarios") or scenario_data.get(name, {}).get("scenarios")
                if company_scenarios and isinstance(company_scenarios, list) and len(company_scenarios) > 0:
                    probability_data = {
                        "scenarios": [],
                        "breakpoints": [],
                        "xConfig": {"label": "Exit Value ($M)", "type": "log"},
                        "yConfig": {"label": "Return Multiple"}
                    }
                    for sc in company_scenarios[:10]:
                        probability_data["scenarios"].append({
                            "name": sc.get("name", sc.get("scenario_name", "Scenario")),
                            "probability": sc.get("probability", 0),
                            "dataPoints": sc.get("data_points", sc.get("dataPoints", []))
                        })
                    memo_sections.append({
                        "type": "chart",
                        "chart": {
                            "type": "probability_cloud",
                            "title": f"{name} — Exit Scenario Analysis (PWERM)",
                            "data": probability_data
                        }
                    })

            # --- Follow-On Analysis Section ---
            if followon_data:
                memo_sections.append({"type": "heading2", "content": "Follow-On Analysis"})
                for company_id, fo in followon_data.items():
                    if not isinstance(fo, dict):
                        continue
                    name = fo.get("company_name", company_id)
                    memo_sections.append({"type": "heading3", "content": name})
                    details = []

                    # Full investment context (enhanced path)
                    our_invested = fo.get("our_invested", 0)
                    if our_invested:
                        details.append(f"Our Investment: ${our_invested / 1e6:,.1f}M ({fo.get('our_entry_round', 'Unknown')})")
                    current_own = fo.get("current_ownership_pct") or fo.get("current_ownership", 0)
                    if current_own:
                        details.append(f"Current Ownership: {current_own:.1f}%")
                    if fo.get("pro_rata_amount"):
                        details.append(f"Pro-Rata Amount: ${fo['pro_rata_amount'] / 1e6:,.1f}M")
                    if fo.get("ownership_with_followon"):
                        details.append(f"Ownership After Follow-On: {fo['ownership_with_followon']:.1f}%")
                    if fo.get("ownership_without_followon"):
                        details.append(f"Ownership Without Follow-On: {fo['ownership_without_followon']:.1f}%")

                    # Exit impact at various multiples (from enhanced analyze_follow_on)
                    exit_impact = fo.get("exit_impact_at_multiples", {})
                    for mult_key, impact in exit_impact.items():
                        if isinstance(impact, dict) and impact.get("our_proceeds"):
                            details.append(
                                f"At {mult_key} exit: ${impact['our_proceeds'] / 1e6:,.1f}M proceeds "
                                f"({impact.get('our_moic', 0):.1f}x MOIC)"
                            )

                    # Fund return impact
                    fund_impact = fo.get("fund_return_impact", {})
                    if fund_impact.get("marginal_moic_change"):
                        details.append(f"Fund MOIC Impact: {fund_impact['marginal_moic_change']:+.3f}x")

                    if fo.get("recommendation"):
                        details.append(f"**Recommendation: {fo['recommendation']}**")
                    if details:
                        memo_sections.append({"type": "list", "items": details})

                    # Ownership comparison chart (bar chart — before/after follow-on)
                    if current_own and fo.get("ownership_with_followon"):
                        memo_sections.append({
                            "type": "chart",
                            "chart": {
                                "type": "bar",
                                "title": f"{name} — Ownership Scenarios",
                                "data": [
                                    {"name": "Current", "value": current_own},
                                    {"name": "With Follow-On", "value": fo["ownership_with_followon"]},
                                    {"name": "Without Follow-On", "value": fo.get("ownership_without_followon", current_own * 0.8)}
                                ]
                            }
                        })

                    # Scenario cap table waterfall chart (from enhanced path)
                    scenario_caps = fo.get("scenario_cap_tables", {})
                    base_scenario = scenario_caps.get("scenarios", {}).get("base", {})
                    base_exits = base_scenario.get("waterfall_at_exits", [])
                    if base_exits:
                        waterfall_chart_data = []
                        for we in base_exits:
                            if isinstance(we, dict) and we.get("our_proceeds"):
                                waterfall_chart_data.append({
                                    "name": f"${we['exit_value'] / 1e6:,.0f}M",
                                    "value": we["our_proceeds"] / 1e6,
                                    "moic": we.get("our_moic", 0),
                                })
                        if waterfall_chart_data:
                            memo_sections.append({
                                "type": "chart",
                                "chart": {
                                    "type": "bar",
                                    "title": f"{name} — Our Proceeds by Exit Value (Base Scenario)",
                                    "data": waterfall_chart_data,
                                }
                            })

            # --- Fund Metrics Section (for LP reports) ---
            if perf and (perf.get("tvpi") or perf.get("total_nav")):
                memo_sections.append({"type": "heading2", "content": "Fund Performance"})
                fund_items = []
                if perf.get("total_committed"):
                    fund_items.append(f"Fund Size: ${perf['total_committed'] / 1e6:,.0f}M")
                if perf.get("total_invested"):
                    fund_items.append(f"Invested: ${perf['total_invested'] / 1e6:,.0f}M")
                if perf.get("total_nav"):
                    fund_items.append(f"Current NAV: ${perf['total_nav'] / 1e6:,.0f}M")
                if perf.get("tvpi"):
                    fund_items.append(f"TVPI: {perf['tvpi']:.2f}x")
                if perf.get("dpi"):
                    fund_items.append(f"DPI: {perf['dpi']:.2f}x")
                if perf.get("irr"):
                    fund_items.append(f"IRR: {perf['irr']:.1f}%")
                if fund_items:
                    memo_sections.append({"type": "list", "items": fund_items})

                # Fund NAV waterfall chart
                investments = fund_metrics_data.get("investments", [])
                if investments:
                    waterfall_data = []
                    for inv in sorted(investments, key=lambda x: x.get("nav_contribution", 0), reverse=True)[:10]:
                        waterfall_data.append({
                            "name": inv.get("company_name", "Unknown"),
                            "value": inv.get("nav_contribution", 0)
                        })
                    if waterfall_data:
                        memo_sections.append({
                            "type": "chart",
                            "chart": {
                                "type": "waterfall",
                                "title": "NAV Contribution by Company",
                                "data": waterfall_data
                            }
                        })

            # --- Portfolio Health Dashboard (from CompanyHealthScorer) ---
            portfolio_health = self.shared_data.get("portfolio_health", {})
            if not portfolio_health and self.fund_modeling and fund_context.get("fund_id"):
                try:
                    portfolio_health = await self.fund_modeling.analyze_portfolio_companies(
                        fund_id=fund_context["fund_id"]
                    )
                    self.shared_data["portfolio_health"] = portfolio_health
                except Exception as e:
                    logger.warning(f"[MEMO] Portfolio health analysis failed: {e}")

            # company_analytics is a dict keyed by company_id from analyze_portfolio_companies
            raw_analytics = portfolio_health.get("company_analytics", {})
            raw_returns = portfolio_health.get("company_returns", {})
            # Normalise: build a list regardless of whether it's dict or list
            if isinstance(raw_analytics, dict):
                company_analytics = [
                    {"analytics": v, "returns": raw_returns.get(k, {}), "company_name": v.get("company_name", k)}
                    for k, v in raw_analytics.items()
                ]
            elif isinstance(raw_analytics, list):
                company_analytics = raw_analytics
            else:
                company_analytics = []

            if company_analytics:
                memo_sections.append({"type": "heading2", "content": "Portfolio Health Dashboard"})
                # Summary signals across portfolio
                all_signals = []
                health_chart_data = []
                for ca in company_analytics:
                    analytics = ca.get("analytics", ca)
                    returns = ca.get("returns", {})
                    c_name = ca.get("company_name") or analytics.get("company_name", "Unknown")

                    # Collect critical signals
                    for sig in analytics.get("signals", []):
                        if "critical" in sig.lower() or "risk" in sig.lower():
                            all_signals.append(f"{c_name}: {sig}")

                    # Build per-company health row
                    items = []
                    arr_m = analytics.get("current_arr", 0) / 1e6 if analytics.get("current_arr") else 0
                    growth = analytics.get("growth_rate", 0)
                    runway = analytics.get("estimated_runway_months", 0)
                    moic = returns.get("moic", 0)
                    items.append(
                        f"**{c_name}**: ${arr_m:,.1f}M ARR, {growth * 100:.0f}% growth, "
                        f"{runway:.0f}mo runway, {moic:.1f}x MOIC"
                    )
                    memo_sections.append({"type": "list", "items": items})

                    health_chart_data.append({"name": c_name, "value": moic})

                if health_chart_data:
                    memo_sections.append({
                        "type": "chart",
                        "chart": {
                            "type": "bar",
                            "title": "Portfolio MOIC by Company",
                            "data": sorted(health_chart_data, key=lambda x: x["value"], reverse=True),
                        }
                    })

                if all_signals:
                    memo_sections.append({"type": "heading3", "content": "Critical Signals"})
                    memo_sections.append({"type": "list", "items": all_signals[:10]})

            # --- Reserve Forecast Timeline ---
            reserve_forecast = self.shared_data.get("reserve_forecast", {})
            quarters = reserve_forecast.get("quarters", [])
            if quarters:
                memo_sections.append({"type": "heading2", "content": "Reserve Forecast"})
                reserve_items = []
                total_available = reserve_forecast.get("total_available", 0)
                total_obligated = reserve_forecast.get("total_obligated", 0)
                reserve_items.append(f"Available Capital: ${total_available / 1e6:,.1f}M")
                reserve_items.append(f"Total Pro-Rata Obligations: ${total_obligated / 1e6:,.1f}M")
                if total_available > 0:
                    coverage = total_available / total_obligated if total_obligated > 0 else float('inf')
                    reserve_items.append(f"Coverage Ratio: {coverage:.1f}x")
                memo_sections.append({"type": "list", "items": reserve_items})

                # Quarter-by-quarter chart
                reserve_chart = []
                for q in quarters[:8]:  # Show 2 years
                    reserve_chart.append({
                        "name": q.get("quarter", "?"),
                        "value": q.get("cumulative_obligation", 0) / 1e6,
                    })
                if reserve_chart:
                    memo_sections.append({
                        "type": "chart",
                        "chart": {
                            "type": "line",
                            "title": "Cumulative Pro-Rata Obligations ($M)",
                            "data": reserve_chart,
                        }
                    })

            # --- Fund Return Scenarios ---
            fund_scenarios = self.shared_data.get("fund_scenarios", {})
            if not fund_scenarios and self.fund_modeling and fund_context.get("fund_id"):
                try:
                    fund_scenarios = await self.fund_modeling.model_fund_scenarios(
                        fund_id=fund_context["fund_id"]
                    )
                    self.shared_data["fund_scenarios"] = fund_scenarios
                except Exception as e:
                    logger.warning(f"[MEMO] Fund scenario modeling failed: {e}")

            # model_fund_scenarios returns "portfolio_scenarios" key
            scenarios_list = fund_scenarios.get("portfolio_scenarios", []) or fund_scenarios.get("scenarios", [])
            if scenarios_list:
                memo_sections.append({"type": "heading2", "content": "Fund Return Scenarios"})
                scenario_items = []
                scenario_chart = []
                for sc in scenarios_list:
                    sc_name = sc.get("scenario_name", "Unknown")
                    sc_moic = sc.get("fund_moic", 0)
                    sc_dpi = sc.get("fund_dpi", 0)
                    scenario_items.append(
                        f"**{sc_name}**: {sc_moic:.2f}x MOIC, {sc_dpi:.2f}x DPI"
                    )
                    scenario_chart.append({"name": sc_name, "value": sc_moic})

                    # Attribution detail
                    attribution = sc.get("return_attribution", [])
                    for attr in attribution[:3]:
                        if isinstance(attr, dict) and attr.get("marginal_impact"):
                            scenario_items.append(
                                f"  → {attr.get('company_name', '?')}: "
                                f"{attr['marginal_impact']:+.3f}x marginal MOIC"
                            )

                memo_sections.append({"type": "list", "items": scenario_items})
                if scenario_chart:
                    memo_sections.append({
                        "type": "chart",
                        "chart": {
                            "type": "bar",
                            "title": "Fund MOIC by Scenario",
                            "data": scenario_chart,
                        }
                    })

            # --- Exit Pipeline (from plan_exits) ---
            exit_modeling = self.shared_data.get("exit_modeling", {})
            exit_scenarios = exit_modeling.get("scenarios", [])
            if exit_scenarios:
                memo_sections.append({"type": "heading2", "content": "Exit Pipeline"})
                for ex in exit_scenarios:
                    name = ex.get("company_name", ex.get("company", "Unknown"))
                    memo_sections.append({"type": "heading3", "content": name})

                    exit_items = []
                    # Secondary route
                    sec = ex.get("secondary", {})
                    if sec.get("our_proceeds"):
                        exit_items.append(
                            f"Secondary (now): ${sec['our_proceeds'] / 1e6:,.1f}M proceeds at "
                            f"{sec.get('discount', 0.2) * 100:.0f}% discount ({sec.get('moic', 0):.1f}x MOIC)"
                        )

                    # M&A scenarios
                    for ma in ex.get("ma_scenarios", []):
                        if isinstance(ma, dict) and ma.get("our_proceeds"):
                            exit_items.append(
                                f"M&A at {ma.get('label', '?')}: ${ma['our_proceeds'] / 1e6:,.1f}M "
                                f"({ma.get('moic', 0):.1f}x MOIC, {ma.get('fund_dpi_impact', 0):.3f}x fund DPI)"
                            )

                    # IPO timing
                    ipo = ex.get("ipo_timing", {})
                    if ipo.get("months_to_ipo_arr"):
                        exit_items.append(
                            f"IPO-ready in ~{ipo['months_to_ipo_arr']}mo "
                            f"(current ${ipo.get('current_arr', 0) / 1e6:,.1f}M → $100M ARR threshold)"
                        )

                    # Hold vs sell
                    hvs = ex.get("hold_vs_sell", {})
                    if hvs.get("sell_now_moic") and hvs.get("hold_2yr_moic"):
                        signal = hvs.get("recommendation_signal", "hold")
                        exit_items.append(
                            f"**Hold vs Sell**: Sell now {hvs['sell_now_moic']:.1f}x vs hold 2yr {hvs['hold_2yr_moic']:.1f}x "
                            f"→ **{signal.replace('_', ' ').title()}**"
                        )

                    if exit_items:
                        memo_sections.append({"type": "list", "items": exit_items})

                # Exit pipeline bar chart — hold vs sell comparison
                exit_chart_data = []
                for ex in exit_scenarios[:8]:
                    name = ex.get("company_name", ex.get("company", "?"))
                    hvs = ex.get("hold_vs_sell", {})
                    if hvs.get("hold_2yr_moic"):
                        exit_chart_data.append({
                            "name": name,
                            "value": hvs["hold_2yr_moic"],
                        })
                if exit_chart_data:
                    memo_sections.append({
                        "type": "chart",
                        "chart": {
                            "type": "bar",
                            "title": "Projected 2-Year MOIC by Company",
                            "data": sorted(exit_chart_data, key=lambda x: x["value"], reverse=True),
                        }
                    })

            # --- Growth Trajectory Projections (from company health dashboard) ---
            if company_analytics:
                has_projections = False
                growth_chart_data = {"labels": [], "current": [], "projected_12mo": [], "projected_24mo": []}
                for ca in (company_analytics if isinstance(company_analytics, list)
                           else [{"analytics": v, "company_name": v.get("company_name", k)}
                                 for k, v in company_analytics.items()]
                           if isinstance(company_analytics, dict) else []):
                    a = ca.get("analytics", ca) if isinstance(ca, dict) else ca
                    c_name = a.get("company_name") or ca.get("company_name", "")
                    cur_arr = a.get("current_arr", 0)
                    proj_12 = a.get("projected_arr_12mo", 0)
                    proj_24 = a.get("projected_arr_24mo", 0)
                    if cur_arr > 0 and (proj_12 > 0 or proj_24 > 0):
                        growth_chart_data["labels"].append(c_name)
                        growth_chart_data["current"].append(round(cur_arr / 1e6, 2))
                        growth_chart_data["projected_12mo"].append(round(proj_12 / 1e6, 2))
                        growth_chart_data["projected_24mo"].append(round(proj_24 / 1e6, 2))
                        has_projections = True

                if has_projections:
                    memo_sections.append({"type": "heading2", "content": "Growth Trajectory Projections"})
                    memo_sections.append({
                        "type": "chart",
                        "chart": {
                            "type": "bar",
                            "title": "ARR Trajectory: Current → 12mo → 24mo ($M)",
                            "data": {
                                "labels": growth_chart_data["labels"],
                                "datasets": [
                                    {"label": "Current ARR", "data": growth_chart_data["current"], "backgroundColor": "#4285F4"},
                                    {"label": "12mo Projected", "data": growth_chart_data["projected_12mo"], "backgroundColor": "#34A853"},
                                    {"label": "24mo Projected", "data": growth_chart_data["projected_24mo"], "backgroundColor": "#FBBC04"},
                                ]
                            },
                            "options": {"scales": {"y": {"title": {"text": "ARR ($M)"}}}}
                        }
                    })

            # --- Preference Stack Analysis (from scenario cap tables) ---
            scenario_cap_data = self.shared_data.get("scenario_cap_tables", {})
            if not scenario_cap_data:
                # Try to pull from exit modeling scenarios
                for ex in exit_scenarios:
                    sc_caps = ex.get("scenario_cap_tables", {})
                    if sc_caps.get("scenarios"):
                        scenario_cap_data = sc_caps
                        break

            if scenario_cap_data.get("scenarios"):
                memo_sections.append({"type": "heading2", "content": "Preference Stack Analysis"})
                for sc in scenario_cap_data["scenarios"][:4]:
                    sc_name = sc.get("label", sc.get("name", "Scenario"))
                    pref_stack = sc.get("total_preference_stack", 0)
                    be_exit = sc.get("breakeven_exit_value", 0)
                    three_x = sc.get("three_x_exit_value", 0)

                    pref_items = [f"**{sc_name}**"]
                    if pref_stack > 0:
                        pref_items.append(f"Total Preference Stack: ${pref_stack / 1e6:,.1f}M")
                    if be_exit > 0:
                        pref_items.append(f"Breakeven Exit: ${be_exit / 1e6:,.0f}M")
                    if three_x > 0:
                        pref_items.append(f"3x Return Exit: ${three_x / 1e6:,.0f}M")

                    # Waterfall at key exit values
                    wf_exits = sc.get("waterfall_at_exits", [])
                    for wf in wf_exits[:3]:
                        if isinstance(wf, dict) and wf.get("our_proceeds"):
                            pref_consumed = wf.get("pref_consumed", 0)
                            exit_val = wf.get("exit_value", 0)
                            pref_pct = (pref_consumed / exit_val * 100) if exit_val > 0 else 0
                            pref_items.append(
                                f"At ${exit_val / 1e6:,.0f}M exit: "
                                f"${wf['our_proceeds'] / 1e6:,.1f}M proceeds, "
                                f"{pref_pct:.0f}% consumed by preferences"
                            )

                    memo_sections.append({"type": "list", "items": pref_items})

            # --- Risk Analysis ---
            memo_sections.append({"type": "heading2", "content": "Risk Analysis"})
            risks = []
            for company in companies:
                name = company.get("company", "Unknown")
                stage = company.get("stage", "Unknown")
                valuation = self._get_field_safe(company, "valuation")
                revenue = self._get_field_safe(company, "revenue") or self._get_field_safe(company, "inferred_revenue")
                if valuation and revenue and revenue > 0:
                    multiple = valuation / revenue
                    if multiple > 30:
                        risks.append(f"{name}: High valuation multiple ({multiple:.0f}x) relative to revenue — execution risk")
                    elif multiple < 5 and stage in ("Series B", "Series C"):
                        risks.append(f"{name}: Low multiple ({multiple:.0f}x) may indicate growth challenges")
                if company.get("funding_gap_months") and company["funding_gap_months"] > 18:
                    risks.append(f"{name}: {company['funding_gap_months']:.0f} months since last round — may need bridge")

            # Add health-based risks from portfolio analysis
            for ca in company_analytics:
                a = ca.get("analytics", ca) if isinstance(ca, dict) else {}
                for sig in a.get("signals", []):
                    if "critical" in sig.lower() or "risk" in sig.lower():
                        risks.append(f"{ca.get('company_name', a.get('company_name', '?'))}: {sig}")

            if not risks:
                risks = ["Market competition and execution risk across portfolio", "Funding environment may impact follow-on timing"]
            memo_sections.append({"type": "list", "items": risks})

            # --- Recommendations ---
            memo_sections.append({"type": "heading2", "content": "Investment Recommendations"})
            if companies:
                recs = []
                for c in sorted(companies, key=lambda x: self._get_field_safe(x, "valuation"), reverse=True)[:5]:
                    name = c.get("company", "Unknown")
                    val = self._get_field_safe(c, "valuation")
                    fo = followon_data.get(name, {})
                    if isinstance(fo, dict) and fo.get("recommendation"):
                        recs.append(f"**{name}** (${val / 1e6:,.0f}M): {fo['recommendation']}")
                    else:
                        recs.append(f"**{name}**: ${val / 1e6:,.0f}M valuation — further analysis recommended")
                memo_sections.append({"type": "list", "items": recs})

            # Return standardized docs format for frontend
            return {
                "format": "docs",
                "title": title,
                "date": datetime.now().strftime("%B %d, %Y"),
                "sections": memo_sections,
                "metadata": {
                    "word_count": sum(len(str(s.get("content", "") or s.get("items", [])).split()) for s in memo_sections),
                    "section_count": len(memo_sections),
                    "generated_at": datetime.now().isoformat(),
                    "company_count": len(companies),
                    "has_charts": any(s.get("type") == "chart" for s in memo_sections),
                    "memo_type": "followon" if is_followon_memo else "lp_quarterly" if is_lp_report else "gp_strategy" if is_gp_strategy else "investment_analysis"
                }
            }

        except Exception as e:
            logger.error(f"Memo generation error: {e}", exc_info=True)
            return {"error": str(e), "format": "docs", "sections": [], "title": "Memo Generation Failed"}
    
    async def _execute_chart_generation(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Generate charts and visualizations"""
        try:
            companies = self.shared_data.get("companies", [])
            charts = []
            
            # Valuation comparison chart
            if companies:
                charts.append({
                    "type": "bar",
                    "title": "Company Valuations",
                    "data": {
                        "labels": [c.get("company", "Unknown") for c in companies],
                        "datasets": [{
                            "label": "Valuation (USD)",
                            "data": [c.get("valuation", 0) for c in companies],
                            "backgroundColor": "#4285F4"
                        }]
                    },
                    "options": {
                        "scales": {"y": {"beginAtZero": True}},
                        "plugins": {"legend": {"display": True}}
                    }
                })
            
            # Revenue vs Funding scatter plot
            if len(companies) > 1:
                charts.append({
                    "type": "scatter",
                    "title": "Revenue vs Total Funding",
                    "data": {
                        "datasets": [{
                            "label": "Companies",
                            "data": [
                                {
                                    "x": c.get("total_funding", 0),
                                    "y": c.get("revenue", 0),
                                    "label": c.get("company")
                                } for c in companies
                            ],
                            "backgroundColor": "#34A853"
                        }]
                    },
                    "options": {
                        "scales": {
                            "x": {"title": {"text": "Total Funding"}},
                            "y": {"title": {"text": "Revenue"}}
                        }
                    }
                })
            
            # Stage distribution pie chart
            stage_counts = {}
            for company in companies:
                stage = company.get("stage", "Unknown")
                stage_counts[stage] = stage_counts.get(stage, 0) + 1
            
            if stage_counts:
                charts.append({
                    "type": "pie",
                    "title": "Companies by Stage",
                    "data": {
                        "labels": list(stage_counts.keys()),
                        "datasets": [{
                            "data": list(stage_counts.values()),
                            "backgroundColor": [
                                "#4285F4", "#34A853", "#FBBC04", 
                                "#EA4335", "#673AB7", "#00ACC1"
                            ]
                        }]
                    }
                })
            
            # Growth rate comparison
            growth_companies = [c for c in companies if c.get("revenue_growth", 0) > 0]
            if growth_companies:
                charts.append({
                    "type": "horizontalBar",
                    "title": "Revenue Growth Rates",
                    "data": {
                        "labels": [c.get("company") for c in growth_companies],
                        "datasets": [{
                            "label": "Growth Rate (%)",
                            "data": [c.get("revenue_growth", 0) * 100 for c in growth_companies],
                            "backgroundColor": "#FBBC04"
                        }]
                    }
                })
            
            return {
                "charts": {
                    "visualizations": charts,
                    "chart_count": len(charts),
                    "chart_types": list(set(c["type"] for c in charts))
                }
            }
            
        except Exception as e:
            logger.error(f"Chart generation error: {e}")
            return {"error": str(e)}
    
    def _build_investor_pie_chart(self, company_name: str, ownership_summary: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """Construct a pie chart configuration for ownership breakdown."""
        if not ownership_summary:
            return None

        segments = []
        colors = [
            "#1f77b4", "#ff7f0e", "#2ca02c", "#d62728",
            "#9467bd", "#8c564b", "#e377c2", "#7f7f7f",
            "#bcbd22", "#17becf"
        ]

        founders_total = ownership_summary.get("founders_total", 0.0)
        employees_total = ownership_summary.get("employees_total", 0.0)
        investor_breakdown = ownership_summary.get("investor_breakdown", [])

        if founders_total > 0:
            segments.append(("Founders", founders_total))
        if employees_total > 0:
            segments.append(("Employees", employees_total))

        top_investors = investor_breakdown[:5]
        other_investor_total = sum(item.get("ownership", 0.0) for item in investor_breakdown[5:])

        for investor in top_investors:
            name = investor.get("name", "Investor")
            ownership = investor.get("ownership", 0.0)
            if ownership > 0:
                segments.append((name, ownership))

        if other_investor_total > 0:
            segments.append(("Other Investors", other_investor_total))

        if not segments:
            return None

        labels = [name for name, _ in segments]
        data = [ownership for _, ownership in segments]
        background_colors = [colors[i % len(colors)] for i in range(len(segments))]

        return {
            "type": "pie",
            "title": f"{company_name} Ownership Breakdown",
            "data": {
                "labels": labels,
                "datasets": [{
                    "label": "Ownership (%)",
                    "data": data,
                    "backgroundColor": background_colors
                }]
            }
        }

    async def _execute_cap_table_generation(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Generate cap tables with ownership percentages"""
        try:
            companies = self.shared_data.get("companies", [])
            cap_tables = {}
            charts_by_company: Dict[str, Any] = {}
            
            for company in companies:
                company_name = company.get("company", "Unknown")
                
                funding_rounds, inferred_added = self._build_funding_rounds_with_inference(company)

                if not funding_rounds:
                    logger.warning(f"[CAP_TABLE] No funding history available for {company_name}; skipping cap table generation")
                    continue

                if inferred_added:
                    logger.info(f"[CAP_TABLE] Added {inferred_added} inferred rounds for {company_name}")

                # Persist the cleaned rounds back to shared company data for downstream consumers
                company['funding_rounds'] = funding_rounds

                # Use PrePostCapTable service to calculate
                # Pass the full company data with funding_rounds
                company_data_for_cap_table = {
                    "company": company_name,
                    "funding_rounds": funding_rounds,
                    "stage": company.get("stage"),
                    "valuation": company.get("valuation"),
                    "is_yc": company.get("is_yc", False),
                    "geography": company.get("geography", "Unknown"),
                    "founders": company.get("founders", [])
                }
                try:
                    cap_table = self.cap_table_service.calculate_full_cap_table_history(
                        company_data=company_data_for_cap_table
                    )
                    if not cap_table:
                        cap_table = {"history": [], "ownership_evolution": {}}
                except Exception as e:
                    logger.warning(f"Cap table calculation failed for {company_name}: {e}")
                    cap_table = {"history": [], "ownership_evolution": {}}
                
                pie_chart = self._build_investor_pie_chart(company_name, cap_table.get("ownership_summary"))
                if pie_chart:
                    cap_table.setdefault("charts", {})
                    cap_table["charts"]["investor_ownership_pie"] = pie_chart
                    charts_by_company[company_name] = pie_chart

                cap_tables[company_name] = cap_table
            
            # Write cap table data to shared_data so memo templates can find it
            self.shared_data["cap_table_history"] = cap_tables

            return {
                "cap_tables": cap_tables,
                "company_count": len(cap_tables),
                "charts": charts_by_company if charts_by_company else None
            }
            
        except Exception as e:
            logger.error("Cap table generation error: %s", e, exc_info=True)
            return {"error": str(e)}
    
    async def _execute_portfolio_analysis(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze fund portfolio performance — uses real FundModelingService when fund_id available."""
        try:
            context = inputs.get("context", {})
            companies = self.shared_data.get("companies", [])
            fund_id = (
                inputs.get("fund_id")
                or context.get("fund_id")
                or self.shared_data.get("fund_context", {}).get("fund_id")
            )

            # Use real service when fund_id is available
            if fund_id and self.fund_modeling:
                logger.info(f"[PORTFOLIO_ANALYSIS] Real analysis for fund {fund_id}")
                metrics = await self.fund_modeling.calculate_fund_metrics(fund_id)
                optimization = await self.fund_modeling.optimize_portfolio(fund_id)
                pacing = await self.fund_modeling.analyze_pacing(fund_id)

                portfolio_analysis = {
                    "fund_overview": metrics.get("portfolio", {}),
                    "fund_metrics": metrics.get("metrics", {}),
                    "investments": metrics.get("investments", []),
                    "optimization": optimization,
                    "pacing": pacing,
                    "analyzed_companies": []
                }

                # Fit scoring for companies in shared_data (if any)
                for company in companies:
                    portfolio_analysis["analyzed_companies"].append({
                        "name": company.get("company"),
                        "stage": company.get("stage"),
                        "fit_score": self._calculate_fit_score(company, portfolio_analysis)
                    })

                self.shared_data["portfolio_analysis"] = portfolio_analysis
                return {"portfolio_analysis": portfolio_analysis}

            # Fallback: existing prompt-based logic
            stored_fund_context = self.shared_data.get("fund_context", {})
            incoming_fund_context = inputs.get("fund_context") if isinstance(inputs.get("fund_context"), dict) else {}
            fund_context: Dict[str, Any] = {}
            if isinstance(stored_fund_context, dict):
                fund_context.update(stored_fund_context)
            if incoming_fund_context:
                fund_context.update(incoming_fund_context)

            def _coerce_amount(value: Any, default: Optional[float] = None) -> Optional[float]:
                if value in (None, ""):
                    return default
                try:
                    if isinstance(value, (int, float, Decimal)):
                        return float(value)
                    if isinstance(value, str):
                        cleaned = value.replace(",", "").replace("$", "").strip().lower()
                        multiplier = 1.0
                        if cleaned.endswith("mm"):
                            cleaned = cleaned[:-2]; multiplier = 1_000_000
                        elif cleaned.endswith("m"):
                            cleaned = cleaned[:-1]; multiplier = 1_000_000
                        elif cleaned.endswith("b"):
                            cleaned = cleaned[:-1]; multiplier = 1_000_000_000
                        cleaned = cleaned.strip()
                        if not cleaned:
                            return default
                        return float(cleaned) * multiplier
                except (ValueError, TypeError):
                    return default
                return default

            fund_size = _coerce_amount(fund_context.get("fund_size")) or _coerce_amount(context.get("fund_size"))
            deployed_capital = _coerce_amount(fund_context.get("deployed_capital"))
            remaining_capital = _coerce_amount(fund_context.get("remaining_capital"))

            if fund_size is not None:
                if deployed_capital is None and remaining_capital is not None:
                    deployed_capital = max(fund_size - remaining_capital, 0)
                elif remaining_capital is None and deployed_capital is not None:
                    remaining_capital = max(fund_size - deployed_capital, 0)

            portfolio_size = context.get("portfolio_size", 16)
            exits = context.get("exits", 2)

            portfolio_analysis = {
                "fund_overview": {
                    "total_fund_size": fund_size,
                    "deployed_capital": deployed_capital,
                    "remaining_capital": remaining_capital,
                    "deployment_rate": deployed_capital / fund_size if fund_size and deployed_capital else None,
                    "portfolio_companies": portfolio_size,
                    "exits_completed": exits,
                    "active_investments": portfolio_size - exits
                },
                "investment_strategy": {},
                "analyzed_companies": []
            }

            if fund_size and deployed_capital and portfolio_size > 0:
                avg_check_size = deployed_capital / portfolio_size
                portfolio_analysis["investment_strategy"] = {
                    "avg_check_size": avg_check_size,
                    "remaining_investments": int(remaining_capital / avg_check_size) if remaining_capital and avg_check_size > 0 else 0,
                    "capital_per_stage": {
                        "seed": remaining_capital * 0.2,
                        "series_a": remaining_capital * 0.4,
                        "series_b": remaining_capital * 0.4
                    } if remaining_capital else None
                }

            avg_check_size = portfolio_analysis["investment_strategy"].get("avg_check_size")
            for company in companies:
                company_fit = {
                    "name": company.get("company"),
                    "stage": company.get("stage"),
                    "fit_score": self._calculate_fit_score(company, portfolio_analysis)
                }
                if avg_check_size is not None:
                    company_fit["recommended_investment"] = min(avg_check_size, self._get_field_safe(company, "valuation") * 0.1)
                    val = self._get_field_safe(company, "valuation")
                    company_fit["expected_ownership"] = min(0.1, avg_check_size / val) if val > 0 else None
                portfolio_analysis["analyzed_companies"].append(company_fit)

            self.shared_data["portfolio_analysis"] = portfolio_analysis
            return {"portfolio_analysis": portfolio_analysis}

        except Exception as e:
            logger.error(f"Portfolio analysis error: {e}", exc_info=True)
            return {"error": str(e)}
    
    async def _execute_fund_metrics(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate DPI, TVPI, IRR and other fund metrics from real portfolio data."""
        try:
            context = inputs.get("context", {})
            fund_id = (
                inputs.get("fund_id")
                or context.get("fund_id")
                or self.shared_data.get("fund_context", {}).get("fund_id")
            )

            # If we have a fund_id and the real service, use it
            if fund_id and self.fund_modeling:
                logger.info(f"[FUND_METRICS] Calculating real metrics for fund {fund_id}")
                metrics = await self.fund_modeling.calculate_fund_metrics(fund_id)
                pacing = await self.fund_modeling.analyze_pacing(fund_id)

                # Store in shared_data for downstream skills (memo, report)
                self.shared_data["fund_metrics"] = metrics
                self.shared_data["fund_pacing"] = pacing
                return {"fund_metrics": metrics, "fund_pacing": pacing}

            # Fallback: use fund context from prompt (backward-compatible stub)
            fund_context = self.shared_data.get("fund_context", {})
            fund_size = fund_context.get("fund_size") or context.get("fund_size")
            remaining_capital = fund_context.get("remaining_capital") or context.get("remaining_capital")
            dpi = context.get("dpi", 0.5)

            if not fund_size or not remaining_capital:
                return {"error": "No fund_id or fund context provided. Provide fund_id for real metrics."}

            deployed = fund_size - remaining_capital
            distributed = fund_size * dpi
            portfolio_size = context.get("portfolio_size", 16)
            exits = context.get("exits", 2)

            metrics = {
                "metrics": {
                    "total_committed": fund_size,
                    "total_invested": deployed,
                    "total_nav": deployed * 1.3,  # estimate
                    "total_distributed": distributed,
                    "dpi": dpi,
                    "rvpi": remaining_capital / fund_size if fund_size else 0,
                    "tvpi": dpi + (remaining_capital / fund_size) if fund_size else 0,
                    "irr": 0,
                    "deployment_rate": deployed / fund_size if fund_size else 0
                },
                "portfolio": {
                    "company_count": portfolio_size,
                    "active_count": portfolio_size - exits,
                    "exited_count": exits
                },
                "investments": [],
                "_source": "fallback_stub"
            }
            self.shared_data["fund_metrics"] = metrics
            return {"fund_metrics": metrics}

        except Exception as e:
            logger.error(f"Fund metrics calculation error: {e}", exc_info=True)
            return {"error": str(e)}

    # ------------------------------------------------------------------ #
    #  NEW SKILLS: Follow-On Strategy, Round Modeling, Report Generation  #
    # ------------------------------------------------------------------ #

    async def _execute_followon_strategy(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze follow-on / extension / sell decisions for portfolio companies.

        Uses fund_modeling_service.analyze_follow_on() for full scenario analysis
        with waterfall-connected cap tables, exit impact at multiple multiples,
        and fund-level return impact.  Falls back to simple pro-rata when the
        enhanced service is unavailable.
        """
        try:
            from app.core.database import supabase_service
            fund_id = (
                inputs.get("fund_id")
                or self.shared_data.get("fund_context", {}).get("fund_id")
            )
            company_name = inputs.get("company") or inputs.get("company_name")

            # Query portfolio companies
            query = supabase_service.client.table("portfolio_companies").select("*, companies(*)")
            if fund_id:
                query = query.eq("fund_id", fund_id)
            result = query.execute()
            portfolio_companies = result.data or []

            # Filter to requested company if specified
            if company_name:
                company_name_lower = company_name.lower().strip().lstrip("@")
                portfolio_companies = [
                    pc for pc in portfolio_companies
                    if company_name_lower in (pc.get("companies", {}).get("name", "") or "").lower()
                ]

            if not portfolio_companies:
                return {"error": f"No portfolio companies found" + (f" matching '{company_name}'" if company_name else "")}

            followon_results = {}

            # --- Enhanced path: use fund_modeling_service ---
            if self.fund_modeling and fund_id:
                for pc in portfolio_companies:
                    company = pc.get("companies", {}) or {}
                    cid = company.get("id", pc.get("id", "unknown"))
                    name = company.get("name", "Unknown")

                    try:
                        # Full follow-on analysis with exit impact and fund return context
                        fo_analysis = await self.fund_modeling.analyze_follow_on(
                            fund_id=fund_id,
                            company_id=str(cid),
                        )

                        # Scenario cap tables with waterfall at multiple exit values
                        scenario_caps = {}
                        if self.valuation_engine:
                            try:
                                scenario_caps = self.valuation_engine.generate_scenario_cap_tables(
                                    company_data=company,
                                    analytics=fo_analysis.get("analytics"),
                                    our_investment={
                                        "amount": fo_analysis.get("our_invested", pc.get("investment_amount", 0)),
                                        "round": fo_analysis.get("our_entry_round", company.get("stage", "Series A")),
                                    },
                                )
                            except Exception as e:
                                logger.warning(f"[FOLLOWON] Scenario cap tables failed for {name}: {e}")

                        # Cap table history for Sankey charts
                        cap_table_history = {}
                        if self.cap_table_service:
                            try:
                                cap_table_history = self.cap_table_service.calculate_full_cap_table_history(company)
                            except Exception as e:
                                logger.warning(f"[FOLLOWON] Cap table history failed for {name}: {e}")

                        followon_results[name] = {
                            **fo_analysis,
                            "company_name": name,
                            "company_id": cid,
                            "scenario_cap_tables": scenario_caps,
                            "cap_table_history": cap_table_history,
                        }

                    except Exception as e:
                        logger.warning(f"[FOLLOWON] Enhanced analysis failed for {name}, falling back: {e}")
                        # Fall through to simple path below
                        followon_results[name] = self._followon_simple(pc, company, cid, name)

                # Reserve forecast for entire fund
                reserve_forecast = {}
                try:
                    reserve_forecast = await self.fund_modeling.forecast_reserves(fund_id=fund_id)
                except Exception as e:
                    logger.warning(f"[FOLLOWON] Reserve forecast failed: {e}")

                self.shared_data["followon_strategy"] = followon_results
                self.shared_data["reserve_forecast"] = reserve_forecast
                return {"followon_strategy": followon_results, "reserve_forecast": reserve_forecast}

            # --- Fallback path: simple pro-rata when fund_modeling unavailable ---
            for pc in portfolio_companies:
                company = pc.get("companies", {}) or {}
                cid = company.get("id", pc.get("id", "unknown"))
                name = company.get("name", "Unknown")
                followon_results[name] = self._followon_simple(pc, company, cid, name)

            self.shared_data["followon_strategy"] = followon_results
            return {"followon_strategy": followon_results}

        except Exception as e:
            logger.error(f"Follow-on strategy error: {e}", exc_info=True)
            return {"error": str(e)}

    def _followon_simple(self, pc: Dict, company: Dict, cid: str, name: str) -> Dict[str, Any]:
        """Simple pro-rata follow-on analysis (fallback when fund_modeling unavailable)."""
        current_ownership = pc.get("ownership_pct", 0) or 0
        investment_amount = pc.get("investment_amount", 0) or 0
        current_valuation = company.get("current_valuation_usd", 0) or 0

        upcoming_round_size = current_valuation * 0.15 if current_valuation else 5_000_000
        upcoming_pre_money = current_valuation or 10_000_000

        pro_rata_result = {}
        if self.cap_table_service:
            try:
                pro_rata_result = self.cap_table_service.calculate_pro_rata_investment(
                    current_ownership=Decimal(str(current_ownership / 100)),
                    new_money_raised=Decimal(str(upcoming_round_size)),
                    pre_money_valuation=Decimal(str(upcoming_pre_money))
                )
            except Exception as e:
                logger.warning(f"[FOLLOWON] Pro-rata calc failed for {name}: {e}")

        strategy = "pro-rata" if current_ownership >= 5 else "selective"
        recommendation = "follow_on"
        if current_ownership < 2:
            recommendation = "hold"
        elif current_valuation and investment_amount:
            moic = (current_ownership / 100 * current_valuation) / investment_amount if investment_amount > 0 else 0
            if moic > 10:
                recommendation = "consider_partial_sell"
            elif moic < 1:
                recommendation = "hold"

        return {
            "company_name": name,
            "company_id": cid,
            "our_invested": investment_amount,
            "our_entry_round": company.get("stage", "Unknown"),
            "current_ownership_pct": current_ownership,
            "current_valuation": current_valuation,
            "strategy": strategy,
            "recommendation": recommendation,
            "pro_rata_amount": float(pro_rata_result.get("pro_rata_investment_needed", 0)),
            "ownership_with_followon": float(pro_rata_result.get("ownership_with_pro_rata", 0)) * 100,
            "ownership_without_followon": float(pro_rata_result.get("ownership_without_pro_rata", 0)) * 100,
        }

    async def _execute_round_modeling(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Model next round (e.g. 'what is needed for Series D') with dilution + waterfall."""
        try:
            from app.core.database import supabase_service
            company_name = inputs.get("company") or inputs.get("company_name")
            target_raise = inputs.get("raise_amount") or inputs.get("round_size")
            pre_money = inputs.get("pre_money") or inputs.get("pre_money_valuation")

            # Find company in portfolio
            if company_name:
                company_name_lower = company_name.lower().strip().lstrip("@")
                result = supabase_service.client.table("portfolio_companies").select(
                    "*, companies(*)"
                ).execute()
                matches = [
                    pc for pc in (result.data or [])
                    if company_name_lower in (pc.get("companies", {}).get("name", "") or "").lower()
                ]
                if not matches:
                    return {"error": f"Company '{company_name}' not found in portfolio"}
                pc = matches[0]
                company = pc.get("companies", {}) or {}
            else:
                # Use first company from shared_data
                companies = self.shared_data.get("companies", [])
                if not companies:
                    return {"error": "No company specified for round modeling"}
                company = companies[0]
                pc = {}

            name = company.get("name", company.get("company", "Unknown"))
            current_valuation = company.get("current_valuation_usd") or self._get_field_safe(company, "valuation") or 50_000_000
            current_ownership = pc.get("ownership_pct", 0) or 0

            # Default round parameters if not specified
            if not target_raise:
                target_raise = current_valuation * 0.2  # Typical 20% dilution round
            if not pre_money:
                pre_money = current_valuation * 1.5  # Typical up-round

            target_raise = float(target_raise)
            pre_money = float(pre_money)
            post_money = pre_money + target_raise

            # Dilution calculation
            new_investor_pct = target_raise / post_money * 100
            dilution_factor = pre_money / post_money
            ownership_after = current_ownership * dilution_factor

            # Ownership table (before/after)
            ownership_table = {
                "before": {"our_fund": current_ownership, "other_investors": 100 - current_ownership},
                "after": {
                    "our_fund": ownership_after,
                    "new_investor": new_investor_pct,
                    "other_investors": (100 - current_ownership) * dilution_factor
                }
            }

            # Liquidation waterfall at various exit values
            waterfall_scenarios = []
            if self.advanced_cap_table:
                for exit_multiple in [0.5, 1.0, 2.0, 3.0, 5.0, 10.0]:
                    exit_value = post_money * exit_multiple
                    try:
                        waterfall = self.advanced_cap_table.calculate_liquidation_waterfall(
                            exit_value=exit_value,
                            cap_table={"our_fund": ownership_after / 100, "new_investor": new_investor_pct / 100},
                            funding_rounds=[{
                                "round": "New Round",
                                "amount": target_raise,
                                "liquidation_multiple": 1.0,
                                "participating": False,
                                "seniority": 10
                            }]
                        )
                        our_proceeds = 0
                        for dist in waterfall.get("distributions", []):
                            if "our" in str(dist.get("shareholder", "")).lower() or "fund" in str(dist.get("shareholder", "")).lower():
                                our_proceeds += dist.get("amount", 0)
                        if our_proceeds == 0:
                            our_proceeds = exit_value * (ownership_after / 100)

                        waterfall_scenarios.append({
                            "exit_multiple": exit_multiple,
                            "exit_value": exit_value,
                            "our_proceeds": our_proceeds,
                            "our_moic": our_proceeds / (pc.get("investment_amount", 1) or 1)
                        })
                    except Exception as e:
                        logger.warning(f"[ROUND_MODEL] Waterfall calc failed for {exit_multiple}x: {e}")

            round_model = {
                "company_name": name,
                "round_parameters": {
                    "raise_amount": target_raise,
                    "pre_money": pre_money,
                    "post_money": post_money,
                    "new_investor_ownership": new_investor_pct
                },
                "ownership_table": ownership_table,
                "dilution": {
                    "current_ownership": current_ownership,
                    "ownership_after": ownership_after,
                    "dilution_pct": current_ownership - ownership_after
                },
                "waterfall_scenarios": waterfall_scenarios
            }

            self.shared_data["round_modeling"] = round_model
            return {"round_modeling": round_model}

        except Exception as e:
            logger.error(f"Round modeling error: {e}", exc_info=True)
            return {"error": str(e)}

    async def _execute_report_generation(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Generate LP quarterly, follow-on memo, or GP strategy report.

        Uses the lightweight memo pipeline with explicit memo_type routing.
        """
        try:
            prompt = inputs.get("prompt", "") or self.shared_data.get("original_prompt", "")
            report_type = inputs.get("report_type") or inputs.get("type", "investment")

            # Map report types to memo template IDs
            report_type_map = {
                "lp_quarterly": "lp_report",
                "lp_report": "lp_report",
                "followon_memo": "followon",
                "followon": "followon",
                "gp_strategy": "gp_strategy",
                "gp_update": "gp_strategy",
                "investment_analysis": "ic_memo",
            }
            memo_type = report_type_map.get(report_type, report_type)

            # Ensure prerequisite data is in shared_data
            fund_id = self.shared_data.get("fund_context", {}).get("fund_id")
            if fund_id and self.fund_modeling and "fund_metrics" not in self.shared_data:
                metrics = await self.fund_modeling.calculate_fund_metrics(fund_id)
                self.shared_data["fund_metrics"] = metrics

            # Route through unified memo generation (lightweight pipeline)
            memo_result = await self._execute_memo_generation({
                "prompt": prompt,
                "memo_type": memo_type,
            })

            # Tag with report type for frontend/export
            if isinstance(memo_result, dict):
                memo_result["report_type"] = report_type

            return memo_result

        except Exception as e:
            logger.error(f"Report generation error: {e}", exc_info=True)
            return {"error": str(e), "format": "docs"}

    # ── FPA / Modeling skill handlers ──────────────────────────────────

    def _get_fpa_regression_service(self):
        """Lazy-load FPARegressionService."""
        if not hasattr(self, "_fpa_regression_svc"):
            self._fpa_regression_svc = FPARegressionService() if FPARegressionService else None
        return self._fpa_regression_svc

    async def _execute_regression_analysis(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Run linear regression on company metric pairs (e.g. ARR vs time)."""
        try:
            svc = self._get_fpa_regression_service()
            if not svc:
                return {"error": "FPA regression service unavailable"}
            companies = self.shared_data.get("companies", [])
            results = {}
            for company in companies:
                name = company.get("company", "Unknown")
                # Build x (months since founding), y (revenue) from available data
                revenue = self._get_field_safe(company, "revenue")
                growth = self._get_field_safe(company, "growth_rate", 0.5)
                if revenue and revenue > 0:
                    # Generate synthetic history: 6 quarters back using growth rate
                    monthly_growth = (1 + growth) ** (1/12)
                    y = [revenue / (monthly_growth ** (i * 3)) for i in range(6, -1, -1)]
                    x = list(range(len(y)))
                    reg = await svc.linear_regression(x, y)
                    results[name] = {
                        "regression": reg,
                        "current_revenue": revenue,
                        "projected_12m": reg["slope"] * (len(y) + 4) + reg["intercept"]
                    }
            self.shared_data["regression_analysis"] = results
            return {
                "regression_analysis": results,
                "charts": [{"type": "scatter", "title": "Revenue Regression", "data": results}]
            }
        except Exception as e:
            logger.error(f"Regression analysis error: {e}", exc_info=True)
            return {"error": str(e)}

    async def _execute_time_series_forecast(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Forecast revenue/metrics using exponential smoothing."""
        try:
            svc = self._get_fpa_regression_service()
            if not svc:
                return {"error": "FPA regression service unavailable"}
            companies = self.shared_data.get("companies", [])
            periods = inputs.get("periods", 4)  # Default 4 quarters ahead
            results = {}
            for company in companies:
                name = company.get("company", "Unknown")
                revenue = self._get_field_safe(company, "revenue")
                growth = self._get_field_safe(company, "growth_rate", 0.5)
                if revenue and revenue > 0:
                    monthly_growth = (1 + growth) ** (1/12)
                    historical = [
                        {"value": revenue / (monthly_growth ** (i * 3)), "period": f"Q{6-i}"}
                        for i in range(6, -1, -1)
                    ]
                    forecast = await svc.time_series_forecast(historical, periods)
                    results[name] = {
                        "historical": historical,
                        "forecast": forecast,
                        "current_revenue": revenue,
                    }
            self.shared_data["forecast_results"] = results
            return {
                "forecast_results": results,
                "charts": [{"type": "line", "title": "Revenue Forecast", "data": results}]
            }
        except Exception as e:
            logger.error(f"Time series forecast error: {e}", exc_info=True)
            return {"error": str(e)}

    async def _execute_growth_decay_forecast(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Model exponential growth or decay with half-life calculation."""
        try:
            svc = self._get_fpa_regression_service()
            if not svc:
                return {"error": "FPA regression service unavailable"}
            companies = self.shared_data.get("companies", [])
            results = {}
            for company in companies:
                name = company.get("company", "Unknown")
                revenue = self._get_field_safe(company, "revenue")
                growth = self._get_field_safe(company, "growth_rate", 0.5)
                burn = self._get_field_safe(company, "burn_rate", 0)
                if revenue and revenue > 0:
                    monthly_growth = (1 + growth) ** (1/12)
                    data = [revenue / (monthly_growth ** (i * 3)) for i in range(6, -1, -1)]
                    time_periods = list(range(len(data)))
                    decay_result = await svc.exponential_decay(data, time_periods)
                    results[name] = {
                        "growth_decay": decay_result,
                        "current_revenue": revenue,
                        "growth_rate": growth,
                        "burn_rate": burn,
                    }
            self.shared_data["growth_decay_results"] = results
            return {
                "growth_decay_results": results,
                "charts": [{"type": "line", "title": "Growth/Decay Model", "data": results}]
            }
        except Exception as e:
            logger.error(f"Growth/decay forecast error: {e}", exc_info=True)
            return {"error": str(e)}

    async def _execute_monte_carlo(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Run Monte Carlo simulation on company valuations."""
        try:
            svc = self._get_fpa_regression_service()
            if not svc:
                return {"error": "FPA regression service unavailable"}
            companies = self.shared_data.get("companies", [])
            iterations = inputs.get("iterations", 1000)
            results = {}
            for company in companies:
                name = company.get("company", "Unknown")
                revenue = self._get_field_safe(company, "revenue")
                valuation = self._get_field_safe(company, "valuation")
                growth = self._get_field_safe(company, "growth_rate", 0.5)
                if revenue and revenue > 0:
                    base = {"revenue": revenue, "valuation": valuation or revenue * 10, "growth": growth}
                    distributions = {
                        "revenue": {"type": "lognormal", "mean": math.log(max(revenue, 1)), "std": 0.3},
                        "growth": {"type": "normal", "mean": growth, "std": 0.15},
                    }
                    mc_result = await svc.monte_carlo_simulation(base, distributions, iterations)
                    results[name] = mc_result
            self.shared_data["monte_carlo_results"] = results
            return {
                "monte_carlo_results": results,
                "charts": [{"type": "histogram", "title": "Monte Carlo Simulation", "data": results}]
            }
        except Exception as e:
            logger.error(f"Monte Carlo error: {e}", exc_info=True)
            return {"error": str(e)}

    async def _execute_sensitivity_analysis(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Sensitivity / tornado analysis on key valuation drivers."""
        try:
            svc = self._get_fpa_regression_service()
            if not svc:
                return {"error": "FPA regression service unavailable"}
            companies = self.shared_data.get("companies", [])
            results = {}
            for company in companies:
                name = company.get("company", "Unknown")
                revenue = self._get_field_safe(company, "revenue")
                growth = self._get_field_safe(company, "growth_rate", 0.5)
                margin = self._get_field_safe(company, "gross_margin", 0.7)
                multiple = self._get_field_safe(company, "revenue_multiple", 10)
                if revenue and revenue > 0:
                    base_inputs = {
                        "revenue": revenue,
                        "growth_rate": growth,
                        "gross_margin": margin,
                        "multiple": multiple,
                    }
                    variable_ranges = {
                        "revenue": [revenue * 0.5, revenue * 0.75, revenue * 1.25, revenue * 1.5],
                        "growth_rate": [growth * 0.5, growth * 0.75, growth * 1.25, growth * 1.5],
                        "gross_margin": [max(0, margin - 0.15), margin - 0.05, margin + 0.05, min(1, margin + 0.15)],
                        "multiple": [max(1, multiple * 0.5), multiple * 0.75, multiple * 1.25, multiple * 1.5],
                    }
                    def valuation_model(inputs):
                        return inputs["revenue"] * inputs["multiple"] * (1 + inputs["growth_rate"]) * inputs["gross_margin"]
                    sens_result = await svc.sensitivity_analysis(base_inputs, variable_ranges, valuation_model)
                    results[name] = sens_result
            self.shared_data["sensitivity_results"] = results
            return {
                "sensitivity_results": results,
                "charts": [{"type": "tornado", "title": "Sensitivity Analysis", "data": results}]
            }
        except Exception as e:
            logger.error(f"Sensitivity analysis error: {e}", exc_info=True)
            return {"error": str(e)}

    async def _execute_fund_analysis(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Comprehensive fund analysis: metrics + follow-on strategy + portfolio construction."""
        try:
            # Run portfolio analysis
            portfolio_result = await self._execute_portfolio_analysis(inputs)
            # Run fund metrics
            fund_metrics_result = await self._execute_fund_metrics(inputs)
            # Run follow-on strategy
            followon_result = await self._execute_followon_strategy(inputs)

            combined = {
                "fund_analysis": {
                    "portfolio": portfolio_result,
                    "metrics": fund_metrics_result,
                    "followon_strategy": followon_result,
                },
                "charts": [
                    {"type": "pie", "title": "Portfolio Allocation", "data": portfolio_result.get("portfolio_analysis", {})},
                    {"type": "bar", "title": "Fund Metrics", "data": fund_metrics_result.get("fund_metrics", {})},
                ]
            }
            self.shared_data["fund_analysis"] = combined
            return combined
        except Exception as e:
            logger.error(f"Fund analysis error: {e}", exc_info=True)
            return {"error": str(e)}

    async def _execute_portfolio_scenario_modeling(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Model fund-level return scenarios: what if company A exits at 5x while company B bridges.

        Uses fund_modeling_service.model_fund_scenarios() which:
        - Runs CompanyHealthScorer on all portfolio companies
        - Generates scenario cap tables per company (base/decay/bridge/outperform)
        - Builds portfolio-level scenario combinations (power law, stress test, etc.)
        - Computes fund MOIC/DPI, return attribution, and marginal impact per company
        """
        try:
            fund_id = (
                inputs.get("fund_id")
                or self.shared_data.get("fund_context", {}).get("fund_id")
            )
            company_scenarios = inputs.get("company_scenarios")  # Optional user-specified map

            if not self.fund_modeling:
                return {"error": "FundModelingService not available"}

            if not fund_id:
                return {"error": "No fund_id available — set fund context first"}

            result = await self.fund_modeling.model_fund_scenarios(
                fund_id=fund_id,
                company_scenarios=company_scenarios,
            )

            self.shared_data["fund_scenarios"] = result
            return {"fund_scenarios": result}

        except Exception as e:
            logger.error(f"Portfolio scenario modeling error: {e}", exc_info=True)
            return {"error": str(e)}

    async def _execute_company_health_dashboard(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Portfolio health dashboard: growth decay, burn/runway, funding trajectory, signals.

        Uses fund_modeling_service.analyze_portfolio_companies() which runs
        CompanyHealthScorer on every portfolio company and produces:
        - CompanyAnalytics (growth projections, burn, runway, funding prediction, signals)
        - CompanyReturnMetrics (MOIC, IRR on actual dates, cost basis per %)
        - Fund-level summary (total invested, NAV, weighted IRR)
        """
        try:
            fund_id = (
                inputs.get("fund_id")
                or self.shared_data.get("fund_context", {}).get("fund_id")
            )

            if not self.fund_modeling:
                return {"error": "FundModelingService not available"}

            if not fund_id:
                return {"error": "No fund_id available — set fund context first"}

            result = await self.fund_modeling.analyze_portfolio_companies(fund_id=fund_id)

            # Store for memo generation and other downstream skills
            self.shared_data["portfolio_health"] = result

            # Build a summary table for easy display
            summary_rows = []
            for cid, analytics in result.get("company_analytics", {}).items():
                returns = result.get("company_returns", {}).get(cid, {})
                summary_rows.append({
                    "company_name": analytics.get("company_name", "Unknown"),
                    "stage": analytics.get("stage", ""),
                    "current_arr": analytics.get("current_arr", 0),
                    "growth_rate": analytics.get("growth_rate", 0),
                    "growth_trend": analytics.get("growth_trend", "stable"),
                    "runway_months": analytics.get("estimated_runway_months", 0),
                    "valuation_direction": analytics.get("valuation_direction", "flat"),
                    "projected_arr_12mo": analytics.get("projected_arr_12mo", 0),
                    "projected_arr_24mo": analytics.get("projected_arr_24mo", 0),
                    "signals": analytics.get("signals", []),
                    "moic": returns.get("moic", 0),
                    "irr": returns.get("irr", 0),
                    "invested": returns.get("invested", 0),
                    "current_nav": returns.get("current_nav", 0),
                })

            return {
                "portfolio_health": result,
                "summary_table": sorted(summary_rows, key=lambda x: x.get("moic", 0), reverse=True),
                "fund_summary": result.get("fund_summary", {}),
            }

        except Exception as e:
            logger.error(f"Company health dashboard error: {e}", exc_info=True)
            return {"error": str(e)}

    # ------------------------------------------------------------------
    # Phase 3: New skill execution handlers
    # ------------------------------------------------------------------

    async def _execute_deck_quality_validation(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Validate deck quality: hardcoded defaults, estimation markers, consistency."""
        try:
            if DeckQualityValidator is None:
                return {"error": "DeckQualityValidator not available"}
            validator = DeckQualityValidator()
            deck = inputs.get("deck") or self.shared_data.get("deck") or {}
            passed, gates = validator.validate_deck(deck)
            return {
                "passed": passed,
                "gates": [g.__dict__ if hasattr(g, "__dict__") else g for g in gates] if gates else [],
            }
        except Exception as e:
            return {"error": str(e)}

    async def _execute_slide_optimization(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Optimize slide text, bullets, metrics for presentation."""
        try:
            if SlideContentOptimizer is None:
                return {"error": "SlideContentOptimizer not available"}
            optimizer = SlideContentOptimizer()
            content = inputs.get("content") or inputs.get("text") or {}
            constraints = inputs.get("constraints") or {}
            result = optimizer.optimize_text_content(content=content, constraints=constraints)
            return result if isinstance(result, dict) else {"optimized": result}
        except Exception as e:
            return {"error": str(e)}

    async def _execute_formula_evaluation(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Evaluate Excel-like formulas with cell references and functions."""
        try:
            if FormulaEvaluator is None:
                return {"error": "FormulaEvaluator not available"}
            evaluator = FormulaEvaluator()
            formula = inputs.get("formula") or inputs.get("expression", "")
            # Set cell values if provided
            cells = inputs.get("cells") or {}
            for ref, val in cells.items():
                evaluator.set_cell_value(ref, val)
            result = evaluator.evaluate(formula)
            return {"formula": formula, "result": result}
        except Exception as e:
            return {"error": str(e)}

    async def _execute_arithmetic(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Sum, avg, median, percentile, rank, stdev and other calculations."""
        try:
            if ArithmeticEngine is None:
                return {"error": "ArithmeticEngine not available"}
            engine = ArithmeticEngine()
            operation = inputs.get("operation", "sum")
            values = inputs.get("values", [])
            fn = getattr(engine, operation, None)
            if not fn:
                return {"error": f"Unknown operation: {operation}. Available: sum, average, median, min, max, stdev, variance, count"}
            result = fn(*values)
            return {"operation": operation, "values": values, "result": result}
        except Exception as e:
            return {"error": str(e)}

    async def _execute_company_history(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Full company history with funding rounds, investors, DPI Sankey."""
        return await self._tool_company_history(inputs)

    async def _execute_nl_matrix_command(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Natural language to matrix commands."""
        try:
            if NLMatrixController is None:
                return {"error": "NLMatrixController not available"}
            controller = NLMatrixController()
            command = inputs.get("command") or inputs.get("query", "")
            fund_id = inputs.get("fund_id") or self.shared_data.get("fund_context", {}).get("fund_id")
            result = controller.process_nl_command(command=command, fund_id=fund_id)
            return result if isinstance(result, dict) else {"result": result}
        except Exception as e:
            return {"error": str(e)}

    async def _execute_waterfall_calculation(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Liquidation waterfall with investor distributions at exit."""
        return await self._tool_liquidation_waterfall(inputs)

    async def _execute_debt_conversion(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Model SAFE/convertible note conversions."""
        return await self._tool_debt_conversion(inputs)

    async def _execute_market_landscape(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Competitive landscape by sector, geography, stage."""
        return await self._tool_market_landscape(inputs)

    async def _execute_revenue_projection(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Revenue projection with quality-adjusted decay curves."""
        return await self._tool_revenue_projection(inputs)

    async def _execute_compliance_check(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Filing requirements, Form ADV, regulatory calendar."""
        return await self._tool_compliance_check(inputs)

    async def _execute_ma_modeling(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """M&A deal modeling with synergies and integration risk."""
        return await self._tool_ma_workflow(inputs)

    async def _execute_stage_analysis(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze companies across different funding stages"""
        try:
            stages = inputs.get("stages", ["seed", "series_a", "series_b"])
            companies = self.shared_data.get("companies", [])
            
            stage_analysis = {}
            
            for stage in stages:
                stage_key = stage.replace("_", " ").title()
                
                # Calculate stage-specific metrics for each company
                stage_metrics = []
                for company in companies:
                    # Use benchmark valuations and funding amounts for each stage
                    
                    if stage == "seed":
                        stage_val = 10_000_000  # $10M post-money seed
                        stage_funding = 1_500_000  # $1.5M raised
                        stage_rev = 0  # Pre-revenue typically
                        stage_ownership = stage_funding / stage_val  # 15% dilution
                    elif stage == "series_a":
                        stage_val = 50_000_000  # $50M post-money Series A  
                        stage_funding = 8_000_000  # $8M raised
                        stage_rev = 2_000_000  # ~$2M ARR typical at A
                        stage_ownership = stage_funding / stage_val  # 16% dilution
                    else:  # series_b
                        stage_val = 200_000_000  # $200M post-money Series B
                        stage_funding = 25_000_000  # $25M raised
                        stage_rev = 10_000_000  # ~$10M ARR typical at B
                        stage_ownership = stage_funding / stage_val  # 12.5% dilution
                    
                    stage_metrics.append({
                        "company": company.get("company"),
                        "valuation_at_stage": stage_val,
                        "funding_amount": stage_funding,
                        "ownership_given": stage_ownership,
                        "revenue_at_stage": stage_rev,
                        "employees_at_stage": self._estimate_employees_at_stage(stage, company.get("team_size", 10)),
                        "growth_to_next": 3.0 if stage != "series_b" else 2.0  # Growth multiple to next stage
                    })
                
                stage_analysis[stage_key] = {
                    "companies": stage_metrics,
                    "avg_valuation": sum(m["valuation_at_stage"] for m in stage_metrics) / len(stage_metrics) if stage_metrics else 0,
                    "avg_revenue": sum(m["revenue_at_stage"] for m in stage_metrics) / len(stage_metrics) if stage_metrics else 0,
                    "typical_check_size": self._get_typical_check_size(stage),
                    "typical_ownership": self._get_typical_ownership(stage)
                }
            
            return {"stage_analysis": stage_analysis}
            
        except Exception as e:
            logger.error(f"Stage analysis error: {e}")
            return {"error": str(e)}
    
    async def _execute_exit_modeling(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Model exit scenarios and returns WITH FUND OWNERSHIP.

        Enhanced path uses fund_modeling_service.plan_exits() for full exit route
        economics (secondary, M&A at multiple multiples, IPO timing) plus
        valuation_engine.generate_scenario_cap_tables() for waterfall-connected
        proceeds at every exit value.  Every metric carries full investment context:
        cost basis, entry round, preference position, proceeds source.
        """
        try:
            companies = self.shared_data.get("companies", [])
            if not companies:
                logger.warning("No companies found for exit modeling")
                return {"exit_modeling": {"scenarios": [], "error": "No companies to model"}}

            context = inputs.get("context", {})
            fund_context = self.shared_data.get("fund_context", {})
            fund_id = (
                inputs.get("fund_id")
                or fund_context.get("fund_id")
            )
            fund_size = (
                context.get("fund_size")
                or fund_context.get("fund_size")
                or fund_context.get("portfolio_contribution")
                or 126_000_000
            )

            # --- Enhanced path: use fund_modeling + scenario cap tables ---
            if self.fund_modeling and fund_id:
                try:
                    exit_plan = await self.fund_modeling.plan_exits(fund_id=fund_id)
                    exit_scenarios = []

                    # plan_exits returns {"exit_plans": [...], "fund_size": ...}
                    for company_exit in exit_plan.get("exit_plans", []):
                        name = company_exit.get("company_name", "Unknown")

                        # Match to fetched company data for additional context
                        matched_company = next(
                            (c for c in companies
                             if (c.get("company", "") or "").lower() == name.lower()),
                            {}
                        )

                        # Scenario cap tables with full waterfall
                        scenario_caps = {}
                        inv_amount = company_exit.get("secondary", {}).get("value", 0) or 0
                        if self.valuation_engine and matched_company:
                            try:
                                scenario_caps = self.valuation_engine.generate_scenario_cap_tables(
                                    company_data=matched_company,
                                    our_investment={
                                        "amount": inv_amount,
                                        "round": company_exit.get("stage", ""),
                                    },
                                )
                            except Exception as e:
                                logger.warning(f"[EXIT] Scenario cap tables failed for {name}: {e}")

                        exit_scenarios.append({
                            **company_exit,
                            "moat_score": self._calculate_moat_score(matched_company) if matched_company else 0,
                            "scenario_cap_tables": scenario_caps,
                        })

                    enhanced_fund_size = exit_plan.get("fund_size", fund_size)
                    self.shared_data["exit_modeling"] = {
                        "scenarios": exit_scenarios,
                        "fund_size": enhanced_fund_size,
                    }
                    return {"exit_modeling": {"scenarios": exit_scenarios, "fund_size": enhanced_fund_size}}

                except Exception as e:
                    logger.warning(f"[EXIT] Enhanced exit modeling failed, falling back: {e}")

            # --- Fallback path: simple bear/base/bull ---
            typical_check = context.get("typical_check_size")
            if not typical_check:
                stage = companies[0].get("stage", "Series A") if companies else "Series A"
                stage_checks = {"Seed": 2_000_000, "Series A": 10_000_000, "Series B": 20_000_000, "Series C": 40_000_000}
                typical_check = stage_checks.get(stage, 10_000_000)

            exit_scenarios = []
            for company in companies:
                company_name = company.get("company")
                current_val = (
                    company.get("latest_valuation")
                    or company.get("valuation")
                    or company.get("post_money_valuation")
                    or 100_000_000
                )
                stage = company.get("stage", "").lower()
                if "seed" in stage:
                    entry_valuation, our_check_size = 50_000_000, 5_000_000
                elif "series a" in stage or "a" in stage:
                    entry_valuation, our_check_size = 150_000_000, 10_000_000
                elif "series b" in stage or "b" in stage:
                    entry_valuation, our_check_size = current_val or 200_000_000, 15_000_000
                else:
                    entry_valuation, our_check_size = current_val or 300_000_000, 10_000_000

                our_ownership = (our_check_size / (entry_valuation + our_check_size)) * 100
                moat_score = self._calculate_moat_score(company)
                revenue = self._get_field_with_fallback(company, "revenue", 1_000_000)

                scenarios = {
                    "bear": {
                        "exit_valuation": entry_valuation * 2, "probability": 0.3, "timeline_years": 3,
                        "our_invested": our_check_size, "our_entry_round": stage,
                        "our_proceeds": our_check_size * 2, "our_moic": 2.0,
                        "our_profit": our_check_size,
                    },
                    "base": {
                        "exit_valuation": entry_valuation * 5, "probability": 0.5, "timeline_years": 5,
                        "our_invested": our_check_size, "our_entry_round": stage,
                        "our_proceeds": our_check_size * 5, "our_moic": 5.0,
                        "our_profit": our_check_size * 4,
                    },
                    "bull": {
                        "exit_valuation": entry_valuation * 10, "probability": 0.2, "timeline_years": 7,
                        "our_invested": our_check_size, "our_entry_round": stage,
                        "our_proceeds": our_check_size * 10, "our_moic": 10.0,
                        "our_profit": our_check_size * 9,
                    },
                }
                expected_value = sum(s["exit_valuation"] * s["probability"] for s in scenarios.values())

                exit_scenarios.append({
                    "company": company_name,
                    "our_invested": our_check_size,
                    "our_entry_round": stage,
                    "current_valuation": current_val,
                    "current_revenue": revenue,
                    "entry_valuation": entry_valuation,
                    "our_ownership_pct": our_ownership,
                    "moat_score": moat_score,
                    "scenarios": scenarios,
                    "expected_exit_value": expected_value,
                    "expected_multiple": expected_value / entry_valuation if entry_valuation > 0 else 0,
                    "revenue_multiple": current_val / revenue if revenue > 0 else 0,
                    "exit_type": "M&A" if expected_value < 1_000_000_000 else "IPO",
                })

            return {
                "exit_modeling": {
                    "scenarios": exit_scenarios,
                    "fund_size": fund_size,
                    "portfolio_expected_value": sum(s["expected_exit_value"] for s in exit_scenarios),
                }
            }

        except Exception as e:
            logger.error(f"Exit modeling error: {e}")
            return {"error": str(e)}
    
    def _calculate_fit_score(self, company: Dict, portfolio: Dict) -> float:
        """Calculate how well a company fits the fund's strategy"""
        score = 0.5  # Base score
        
        # Stage alignment
        if company.get("stage", "").lower() in ["series a", "series b"]:
            score += 0.2
        
        # Valuation fit
        avg_check = portfolio["investment_strategy"]["avg_check_size"]
        if company.get("valuation", 0) > 0:
            ownership = avg_check / company.get("valuation", 1)
            if 0.05 <= ownership <= 0.15:  # Good ownership range
                score += 0.2
        
        # Sector (AI/ML gets bonus)
        if "ai" in company.get("sector", "").lower() or "ml" in company.get("sector", "").lower():
            score += 0.1
        
        return min(1.0, score)
    
    def _estimate_employees_at_stage(self, stage: str, current_size: int = None) -> int:
        """Estimate employee count at different stages"""
        if current_size is None:
            # Use typical sizes if no current size provided
            typical_sizes = {"seed": 5, "series_a": 25, "series_b": 100}
            return typical_sizes.get(stage, 10)
            
        if stage == "seed":
            return min(5, int(current_size * 0.1))
        elif stage == "series_a":
            return min(25, int(current_size * 0.3))
        else:  # series_b
            return min(100, int(current_size * 0.7))
    
    def _get_typical_check_size(self, stage: str) -> float:
        """Get typical check size for a stage"""
        sizes = {
            "seed": 500_000,
            "series_a": 5_000_000,
            "series_b": 15_000_000
        }
        return sizes.get(stage, 1_000_000)
    
    def _get_typical_ownership(self, stage: str) -> float:
        """Get typical ownership target for a stage"""
        ownership = {
            "seed": 0.10,
            "series_a": 0.15,
            "series_b": 0.10
        }
        return ownership.get(stage, 0.10)
    
    def _calculate_moat_score(self, company: Dict[str, Any]) -> float:
        """Calculate competitive moat score (0-1)"""
        score = 0.0
        
        # Proprietary technology (GPU analysis shows own models)
        if company.get("gross_margin_analysis", {}).get("api_dependency_level") == "own_models":
            score += 0.3
        
        # Customer stickiness (enterprise customers)
        customers = company.get("customers", [])
        if customers and any("fortune 500" in str(c).lower() for c in customers):
            score += 0.2
        
        # Sector defensibility
        sector = company.get("sector", "").lower()
        if "defense" in sector or "healthcare" in sector:
            score += 0.2  # Regulated sectors have moats
        
        # Network effects
        if "platform" in company.get("business_model", "").lower():
            score += 0.1
        
        # Gross margin strength (after GPU costs)
        if company.get("gross_margin", 0) > 0.7:
            score += 0.2
        
        return min(1.0, score)
    
    def _calculate_momentum_score(self, company: Dict[str, Any]) -> float:
        """Calculate growth momentum score (0-1)"""
        score = 0.0
        
        # Revenue growth - use inferred if actual not available
        growth = company.get("revenue_growth")
        if growth is None:
            growth = company.get("inferred_growth")
        
        if growth and growth > 100:
            score += 0.4
        elif growth and growth > 50:
            score += 0.3
        elif growth and growth > 30:
            score += 0.2
        
        # Funding momentum (recent rounds)
        funding_rounds = company.get("funding_rounds", [])
        if funding_rounds:
            latest_date = funding_rounds[0].get("date", "")
            if "2024" in latest_date or "2025" in latest_date:
                score += 0.2  # Recent funding
        
        # Team growth
        team_size = safe_get_value(company.get("team_size", 0))
        if team_size > 100:
            score += 0.2
        elif team_size > 50:
            score += 0.1
        
        # Market timing (AI companies get boost in 2024-2025)
        if "ai" in company.get("sector", "").lower():
            score += 0.2
        
        return min(1.0, score)
    
    def _get_investment_recommendation(self, moat: float, momentum: float, ownership: float) -> str:
        """Generate investment recommendation based on scores"""
        combined_score = (moat * 0.5) + (momentum * 0.3) + (min(ownership / 15, 1.0) * 0.2)
        
        if combined_score > 0.7:
            return "🚀 STRONG BUY - Lead the round"
        elif combined_score > 0.5:
            return "✅ BUY - Participate in round"
        elif combined_score > 0.3:
            return "🔶 CONSIDER - Need more diligence"
        else:
            return "⚠️ PASS - Better opportunities available"
    
    def _company_name_to_row_id(self, matrix_ctx: Dict[str, Any], company_name: str) -> Optional[str]:
        """Resolve company name (or @Name) to matrix rowId. Uses companyNames[i] <-> rowIds[i]."""
        if not matrix_ctx:
            return None
        row_ids = matrix_ctx.get("rowIds") or matrix_ctx.get("row_ids") or []
        company_names = matrix_ctx.get("companyNames") or matrix_ctx.get("company_names") or []
        name_clean = (company_name or "").replace("@", "").strip().lower()
        for i, cn in enumerate(company_names):
            if (cn or "").strip().lower() == name_clean:
                if i < len(row_ids):
                    return row_ids[i]
            if name_clean in (cn or "").lower():
                if i < len(row_ids):
                    return row_ids[i]
        return None
    
    def _column_id_for_field(self, matrix_ctx: Dict[str, Any], field_key: str) -> Optional[str]:
        """Map logical field (valuation, arr, revenue, etc.) to matrix columnId from matrix_context.columns."""
        columns = matrix_ctx.get("columns") or []
        key_lower = (field_key or "").lower()
        for col in columns:
            cid = col.get("id") or col.get("columnId") or ""
            name = (col.get("name") or col.get("label") or "").lower()
            if key_lower in name or key_lower in cid.lower():
                return cid or col.get("id")
        if columns:
            return columns[0].get("id") or columns[0].get("columnId")
        return None
    
    def _get_target_row_ids(
        self, matrix_ctx: Dict[str, Any], entities: Optional[Dict[str, Any]] = None
    ) -> List[Tuple[str, str]]:
        """Return list of (rowId, companyName) for target rows. If entities has companies, those only; else all rows."""
        if not matrix_ctx:
            return []
        row_ids = matrix_ctx.get("rowIds") or matrix_ctx.get("row_ids") or []
        company_names = matrix_ctx.get("companyNames") or matrix_ctx.get("company_names") or []
        companies_requested = (entities or {}).get("companies") or []
        if companies_requested:
            out = []
            for c in companies_requested:
                rid = self._company_name_to_row_id(matrix_ctx, c)
                if rid:
                    out.append((rid, (c.replace("@", "").strip() if isinstance(c, str) else c)))
            return out
        return list(zip(row_ids, company_names)) if len(row_ids) == len(company_names) else []
    
    def _build_grid_commands(self, final_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Build grid_commands from shared_data companies + results and from grid-run-* skill output.
        Pre-computes name->rowId map for O(1) lookups instead of O(n) per company.

        Commands carry execution metadata for frontend workflow chaining:
        - group: execution order (0=edits first, 1+=run services sequentially)
        - depends_on: list of (rowId:actionId) keys this command needs completed first
        - provides: key this command's result can be referenced by
        """
        commands: List[Dict[str, Any]] = []
        matrix_ctx = self.shared_data.get("matrix_context") or final_data.get("matrix_context") or {}
        if not matrix_ctx or not (matrix_ctx.get("rowIds") or matrix_ctx.get("row_ids")):
            pass
        else:
            companies = final_data.get("companies") or self.shared_data.get("companies") or []
            col_valuation = self._column_id_for_field(matrix_ctx, "valuation")
            col_arr = self._column_id_for_field(matrix_ctx, "arr")
            col_revenue = self._column_id_for_field(matrix_ctx, "revenue")
            # Pre-compute name->rowId map: O(n) once instead of O(n) per company
            row_ids = matrix_ctx.get("rowIds") or matrix_ctx.get("row_ids") or []
            company_names = matrix_ctx.get("companyNames") or matrix_ctx.get("company_names") or []
            name_to_row: Dict[str, str] = {}
            for i, cn in enumerate(company_names):
                if i < len(row_ids) and cn:
                    name_to_row[cn.strip().lower()] = row_ids[i]

            # Column lookups for all enrichable metrics
            col_burn = self._column_id_for_field(matrix_ctx, "burnRate") or self._column_id_for_field(matrix_ctx, "burn_rate")
            col_runway = self._column_id_for_field(matrix_ctx, "runway") or self._column_id_for_field(matrix_ctx, "runwayMonths")
            col_cash = self._column_id_for_field(matrix_ctx, "cashInBank") or self._column_id_for_field(matrix_ctx, "cash_in_bank")
            col_gross_margin = self._column_id_for_field(matrix_ctx, "grossMargin") or self._column_id_for_field(matrix_ctx, "gross_margin")
            col_growth_annual = self._column_id_for_field(matrix_ctx, "revenueGrowthAnnual") or self._column_id_for_field(matrix_ctx, "growth_rate")
            col_growth_monthly = self._column_id_for_field(matrix_ctx, "revenueGrowthMonthly")
            col_headcount = self._column_id_for_field(matrix_ctx, "headcount")
            col_sector = self._column_id_for_field(matrix_ctx, "sector")
            col_tam = self._column_id_for_field(matrix_ctx, "tamUsd") or self._column_id_for_field(matrix_ctx, "tam_usd")
            col_sam = self._column_id_for_field(matrix_ctx, "samUsd") or self._column_id_for_field(matrix_ctx, "sam_usd")
            col_som = self._column_id_for_field(matrix_ctx, "somUsd") or self._column_id_for_field(matrix_ctx, "som_usd")

            for company in companies:
                name = company.get("company") or company.get("company_name") or ""
                name_clean = name.replace("@", "").strip().lower()
                # O(1) exact match, then O(n) bidirectional substring match
                row_id = name_to_row.get(name_clean)
                if not row_id:
                    for cn_key, rid in name_to_row.items():
                        if name_clean in cn_key or cn_key in name_clean:
                            row_id = rid
                            break
                if not row_id:
                    logger.warning(f"[GRID_COMMANDS] No row match for '{name}' — skipping grid commands for this company")
                    continue

                # Business model / sector context for reasoning
                biz_model = company.get("business_model") or company.get("sector") or "unknown"
                stage = company.get("funding_stage") or company.get("stage") or ""
                source = company.get("_source_skill") or "company-data-fetcher"

                # Helper: emit edit command with reasoning + source metadata
                def _emit(col_id, value, metric_label="", reasoning=""):
                    if col_id and value is not None:
                        is_inferred = "inferred" in str(reasoning).lower() or metric_label.startswith("Inferred")
                        commands.append({
                            "action": "edit",
                            "rowId": row_id,
                            "columnId": col_id,
                            "value": value,
                            "group": 0,  # Edits always execute first
                            "source_service": source,
                            "reasoning": reasoning or f"Enriched from {source}: {metric_label} for {name}",
                            "confidence": 0.5 if is_inferred else 0.75,
                            "metadata": {
                                "source_service": source,
                                "metric": metric_label,
                                "business_model": biz_model,
                                "stage": stage,
                                "is_inferred": is_inferred,
                            },
                        })

                # Core financials
                val = company.get("valuation") or company.get("inferred_valuation") or company.get("fair_value")
                _emit(col_valuation, val, "Valuation",
                      company.get("valuation_reasoning") or f"{'Inferred' if company.get('inferred_valuation') else 'Extracted'} valuation for {name}")
                arr = company.get("arr") or company.get("inferred_arr") or company.get("revenue") or company.get("inferred_revenue")
                _emit(col_arr, arr, "ARR",
                      company.get("revenue_reasoning") or f"{'Inferred' if company.get('inferred_arr') or company.get('inferred_revenue') else 'Extracted'} revenue for {name}")
                if col_revenue and col_revenue != col_arr:
                    _emit(col_revenue, arr, "Revenue")

                # Operational metrics — from enrichment + business model inference
                burn = company.get("burn_rate") or company.get("burn_rate_monthly") or company.get("inferred_burn_rate")
                _emit(col_burn, burn, "Burn Rate",
                      f"{'Inferred from ' + biz_model + ' model' if company.get('inferred_burn_rate') else 'Extracted'} burn rate")
                runway = company.get("runway_months") or company.get("runway") or company.get("inferred_runway")
                _emit(col_runway, runway, "Runway",
                      f"{'Cash/burn inference' if company.get('inferred_runway') else 'Extracted'} runway in months")
                cash = company.get("cash_balance") or company.get("cash_in_bank") or company.get("inferred_cash")
                _emit(col_cash, cash, "Cash Balance")
                gm = company.get("gross_margin") or company.get("inferred_gross_margin")
                _emit(col_gross_margin, gm, "Gross Margin",
                      company.get("gross_margin_reasoning") or f"{'Inferred from ' + biz_model + ' business model' if company.get('inferred_gross_margin') else 'Extracted'} gross margin")
                growth = company.get("growth_rate") or company.get("revenue_growth_annual_pct") or company.get("inferred_growth_rate")
                _emit(col_growth_annual, growth, "Annual Growth",
                      f"{'Stage-based inference (' + stage + ')' if company.get('inferred_growth_rate') else 'Extracted'} annual growth rate")
                _emit(col_growth_monthly, company.get("revenue_growth_monthly_pct") or company.get("monthly_growth"), "Monthly Growth")
                headcount = company.get("headcount") or company.get("team_size") or company.get("inferred_headcount")
                _emit(col_headcount, headcount, "Headcount",
                      f"{'Inferred from stage + funding' if company.get('inferred_headcount') else 'Extracted'} headcount")
                _emit(col_sector, company.get("sector") or company.get("industry"), "Sector")
                _emit(col_tam, company.get("tam_usd") or company.get("tam"), "TAM")
                _emit(col_sam, company.get("sam_usd") or company.get("sam"), "SAM")
                _emit(col_som, company.get("som_usd") or company.get("som"), "SOM")
        # Merge skill-emitted grid commands and annotate with execution groups
        existing = self.shared_data.get("grid_commands") or []
        # Annotate existing run commands with sequential group ordering
        # so frontend chains them: group 0 = edits, group 1+ = run services per row
        run_group_counter = 1
        run_group_by_action: Dict[str, int] = {}
        for cmd in existing:
            if cmd.get("action") == "run":
                action_id = cmd.get("actionId", "")
                if action_id not in run_group_by_action:
                    run_group_by_action[action_id] = run_group_counter
                    run_group_counter += 1
                cmd.setdefault("group", run_group_by_action[action_id])
                # Mark what this command provides so next group can depend on it
                cmd.setdefault("provides", f"{cmd.get('rowId')}:{action_id}")
            elif "group" not in cmd:
                cmd["group"] = 0  # Default edits to group 0

        # Wire up dependencies: group N+1 run commands depend on group N for same row
        sorted_actions = sorted(run_group_by_action.items(), key=lambda x: x[1])
        for idx in range(1, len(sorted_actions)):
            prev_action = sorted_actions[idx - 1][0]
            cur_action = sorted_actions[idx][0]
            for cmd in existing:
                if cmd.get("action") == "run" and cmd.get("actionId") == cur_action:
                    row_id = cmd.get("rowId")
                    cmd["depends_on"] = [f"{row_id}:{prev_action}"]

        all_cmds = commands + existing
        # Deduplicate: keep last command per (rowId, columnId, action) to avoid redundant writes
        seen: Dict[str, int] = {}
        for i, cmd in enumerate(all_cmds):
            key = f"{cmd.get('rowId')}:{cmd.get('columnId')}:{cmd.get('action')}"
            seen[key] = i
        deduped = [all_cmds[i] for i in sorted(seen.values())]
        # Sort by group for deterministic frontend execution order
        deduped.sort(key=lambda c: (c.get("group", 0), c.get("action") != "edit"))
        return deduped

    def _persist_suggestions_to_db(self, grid_commands: List[Dict[str, Any]], fund_id: Optional[str] = None) -> int:
        """Persist grid edit commands directly to pending_suggestions. No frontend roundtrip."""
        if not grid_commands or not fund_id:
            return 0
        edit_cmds = [c for c in grid_commands if c.get("action") == "edit" and c.get("rowId") and c.get("columnId") and c.get("value") is not None]
        if not edit_cmds:
            return 0
        try:
            from app.core.database import get_supabase_service
            client = get_supabase_service().get_client()
            rows = [{
                "fund_id": fund_id,
                "company_id": cmd["rowId"],
                "column_id": cmd["columnId"],
                "suggested_value": cmd["value"] if isinstance(cmd["value"], dict) else {"value": cmd["value"]},
                "source_service": cmd.get("source_service", "agent"),
                "reasoning": cmd.get("reasoning"),
                "metadata": cmd.get("metadata"),
            } for cmd in edit_cmds]
            result = client.table("pending_suggestions").upsert(rows, on_conflict="fund_id,company_id,column_id").execute()
            count = len(result.data) if result.data else 0
            logger.info(f"[PERSIST_SUGGESTIONS] Wrote {count}/{len(edit_cmds)} suggestions for fund {fund_id}")
            return count
        except Exception as e:
            logger.warning(f"[PERSIST_SUGGESTIONS] DB write failed: {e}")
            return 0

    async def _format_output(
        self,
        results: Dict[str, Any],
        output_format: str,
        prompt: str
    ) -> Dict[str, Any]:
        """Format the final output based on requested format"""
        logger.info(f"[FORMAT_OUTPUT] 🎯 Starting output formatting for format: {output_format}")
        logger.info(f"[FORMAT_OUTPUT] 📊 Results keys: {list(results.keys())}")
        logger.info(f"[FORMAT_OUTPUT] 📊 Shared_data keys: {list(self.shared_data.keys())}")
        
        # CRITICAL DEBUG: Check for deck-storytelling in results
        if 'deck-storytelling' in results:
            logger.info(f"[FORMAT_OUTPUT] ✅ Found deck-storytelling in results!")
            deck_result = results['deck-storytelling']
            logger.info(f"[FORMAT_OUTPUT] 📊 deck-storytelling type: {type(deck_result)}")
            if isinstance(deck_result, dict):
                logger.info(f"[FORMAT_OUTPUT] 📊 deck-storytelling keys: {list(deck_result.keys())}")
                if 'slides' in deck_result:
                    logger.info(f"[FORMAT_OUTPUT] 📊 deck-storytelling has {len(deck_result['slides'])} slides")
                    logger.info(f"[FORMAT_OUTPUT] 📊 First slide: {deck_result['slides'][0] if deck_result['slides'] else 'No slides'}")
            else:
                logger.warning(f"[FORMAT_OUTPUT] ⚠️ deck-storytelling is not a dict: {type(deck_result)}")
        else:
            logger.warning(f"[FORMAT_OUTPUT] ❌ deck-storytelling NOT FOUND in results!")
            logger.warning(f"[FORMAT_OUTPUT] ❌ Available result keys: {list(results.keys())}")
        
        # Log detailed results structure
        for key, value in results.items():
            if isinstance(value, dict):
                logger.info(f"[FORMAT_OUTPUT] 📊 Result '{key}' keys: {list(value.keys())}")
                if 'companies' in value:
                    companies = value['companies']
                    logger.info(f"[FORMAT_OUTPUT] 📊 Result '{key}' has {len(companies)} companies")
                    for i, company in enumerate(companies):
                        if isinstance(company, dict):
                            logger.info(f"[FORMAT_OUTPUT] 📊   Company {i}: {company.get('company', 'NO_COMPANY_FIELD')}")
                        else:
                            logger.warning(f"[FORMAT_OUTPUT] 📊   Company {i}: Invalid type {type(company)}")
                if 'slides' in value:
                    slides = value['slides']
                    logger.info(f"[FORMAT_OUTPUT] 📊 Result '{key}' has {len(slides)} slides")
            else:
                logger.info(f"[FORMAT_OUTPUT] 📊 Result '{key}': {type(value)} - {str(value)[:100]}")
        
        # Combine all results with shared_data
        logger.info(f"[FORMAT_OUTPUT] 🔄 Combining results with shared_data")
        
        
        final_data = {
            **self.shared_data,
            **results
        }
        logger.info(f"[FORMAT_OUTPUT] 🔄 Final_data keys after merge: {list(final_data.keys())}")
        
        # CRITICAL DEBUG: Verify deck-storytelling is in final_data
        if 'deck-storytelling' in final_data:
            logger.info(f"[FORMAT_OUTPUT] ✅ deck-storytelling found in final_data!")
        else:
            logger.warning(f"[FORMAT_OUTPUT] ❌ deck-storytelling NOT in final_data after merge!")
        
        
        # Debug logging to see companies
        logger.info(f"[FORMAT_OUTPUT] 🏢 Companies in shared_data: {len(self.shared_data.get('companies', []))}")
        if self.shared_data.get('companies'):
            for company in self.shared_data.get('companies', []):
                logger.info(f"[FORMAT_OUTPUT] 🏢   - {company.get('company', 'Unknown')}")
        
        logger.info(f"[FORMAT_OUTPUT] 🏢 Companies in final_data: {len(final_data.get('companies', []))}")
        if final_data.get('companies'):
            for company in final_data.get('companies', []):
                logger.info(f"[FORMAT_OUTPUT] 🏢   - {company.get('company', 'Unknown')}")
        
        # Ensure we have companies in the right format
        companies_list = []
        
        # First add companies from shared_data
        if "companies" in final_data:
            companies_list = final_data["companies"]
            logger.info(f"Got {len(companies_list)} companies from final_data")
        
        # Companies should already be in final_data from shared_data merge
        # Just ensure the companies list is present
        if not companies_list and "companies" in final_data:
            companies_list = final_data["companies"]
            logger.info(f"Using {len(companies_list)} companies from final_data")
            
        # Add exit scenarios (bull/base/bear) for each company
        for company in companies_list:
            if company.get("valuation") or company.get("inferred_valuation"):
                try:
                    # Create valuation request for scenarios
                    # Determine stage
                    stage_map = {
                        "Pre-Seed": Stage.PRE_SEED,
                        "Pre Seed": Stage.PRE_SEED,
                        "Seed": Stage.SEED,
                        "Series A": Stage.SERIES_A,
                        "Series B": Stage.SERIES_B,
                        "Series C": Stage.SERIES_C,
                        "Series D+": Stage.LATE,
                        "Growth": Stage.GROWTH,
                        "Late": Stage.LATE,
                        "Late Stage Private": Stage.LATE,
                        "Late Stage": Stage.LATE
                    }
                    company_stage = stage_map.get(company.get("stage", "Series A"), Stage.SERIES_A)
                    
                    # Use inferred_revenue if revenue is None - CRITICAL FIX
                    revenue = ensure_numeric(company.get("revenue"), 0)
                    if revenue == 0:
                        revenue = ensure_numeric(company.get("inferred_revenue"), 0)
                        if revenue == 0:
                            revenue = ensure_numeric(company.get("arr"), 0)
                            if revenue == 0:
                                revenue = ensure_numeric(company.get("inferred_arr"), 1_000_000)
                    
                    # Use inferred_growth_rate if growth_rate is None
                    growth_rate = ensure_numeric(company.get("growth_rate"), 0)
                    if growth_rate == 0:
                        growth_rate = ensure_numeric(company.get("inferred_growth_rate"), 1.5)
                    
                    # Use inferred_valuation if valuation is None - CRITICAL FIX
                    valuation = ensure_numeric(company.get("valuation"), 0)
                    if valuation == 0:
                        valuation = ensure_numeric(company.get("inferred_valuation"), 0)
                        if valuation == 0:
                            # Calculate from total_funding as fallback
                            valuation = ensure_numeric(company.get("total_funding"), 0) * 3
                    
                    # Extract inferred_valuation if available
                    inferred_val = ensure_numeric(company.get("inferred_valuation"), None) if company.get("inferred_valuation") is not None else None
                    val_request = ValuationRequest(
                        company_name=company.get("company", "Unknown"),
                        stage=company_stage,
                        revenue=revenue,
                        growth_rate=growth_rate,
                        last_round_valuation=valuation if valuation and valuation > 0 else None,
                        inferred_valuation=inferred_val,
                        total_raised=self._get_field_safe(company, "total_funding")
                    )
                    
                    # Use FULL PWERM calculation with stage-specific scenarios
                    pwerm_result = await self.valuation_engine._calculate_pwerm(val_request)
                    
                    # Get the full scenario distribution (10+ scenarios)
                    full_scenarios = pwerm_result.scenarios
                    
                    # Also get simplified bear/base/bull for easy display
                    simple_scenarios = self.valuation_engine.generate_simple_scenarios(val_request)
                    
                    # Add both full and simple scenarios to company data
                    company["exit_scenarios"] = simple_scenarios
                    company["full_exit_distribution"] = [
                        {
                            "scenario": s.scenario,
                            "probability": s.probability,
                            "exit_value": s.exit_value,
                            "time_to_exit": s.time_to_exit,
                            "moic": s.moic
                        } for s in full_scenarios
                    ]
                    company["pwerm_valuation"] = pwerm_result.fair_value
                    
                    # Add PWERM scenarios to company for waterfall analysis
                    company["pwerm_scenarios"] = full_scenarios
                    
                    # NOTE: ComprehensiveDealAnalyzer and AdvancedWaterfallCalculator 
                    # will be called during skill execution (_execute_valuation)
                    # to avoid duplicate calculations
                    
                    # The services integration flow:
                    # 1. PWERM scenarios calculated here (stage-specific, 10+ scenarios)
                    # 2. _execute_valuation skill uses these scenarios 
                    # 3. ComprehensiveDealAnalyzer runs waterfall for each scenario
                    # 4. AdvancedWaterfallCalculator calculates breakpoints
                    # 5. Results appear in deal_comparison output
                    
                    logger.info(f"Added PWERM scenarios for {company.get('company', 'Unknown')}: {len(full_scenarios)} scenarios, PWERM value: ${pwerm_result.fair_value:,.0f}")
                    
                except Exception as e:
                    logger.error(f"Failed to generate exit scenarios for {company.get('company', 'Unknown')}: {e}")
            
        # Update final data with companies list (should already be there but ensure it)
        final_data["companies"] = companies_list
        
        # Add citations to final data
        final_data["citations"] = self.citation_manager.get_all_citations()
        
        # Phase 2: Prepare plan_steps and grid_commands for frontend
        plan_steps = final_data.get("plan_steps", [])
        grid_commands = self._build_grid_commands(final_data)
        # NOTE: Do NOT persist suggestions directly here — the frontend handles persistence
        # via handleGridCommandsFromBackend → addServiceSuggestion(). Writing here AND returning
        # grid_commands in the response causes double suggestions in the pending_suggestions table.
        fund_id = (self.shared_data.get("fund_context") or {}).get("fund_id") or (self.shared_data.get("matrix_context") or {}).get("fundId")
        def _add_plan_steps(out: Dict[str, Any]) -> Dict[str, Any]:
            if plan_steps:
                out["plan_steps"] = [
                    {"id": s.get("id"), "label": s.get("label"), "status": s.get("status", "pending"), "detail": s.get("detail"), "explanation": s.get("explanation")}
                    for s in plan_steps
                ]
            if grid_commands:
                out["grid_commands"] = grid_commands
            return out
        
        # Format based on output type
        if output_format == "spreadsheet":
            logger.info(f"[FORMAT_OUTPUT] 📊 Formatting as SPREADSHEET")
            return _add_plan_steps(self._format_spreadsheet(final_data))
        elif output_format == "deck":
            logger.info(f"[FORMAT_OUTPUT] 🎨 Formatting as DECK")
            logger.info(f"[FORMAT_OUTPUT] 🎨 Calling _format_deck with output_format='{output_format}'")
            logger.info(f"[FORMAT_OUTPUT] 🎨 final_data keys: {list(final_data.keys())}")
            logger.info(f"[FORMAT_OUTPUT] 🎨 Has deck-storytelling: {'deck-storytelling' in final_data}")
            
            # Log deck-storytelling data if present
            if 'deck-storytelling' in final_data:
                deck_data = final_data['deck-storytelling']
                logger.info(f"[FORMAT_OUTPUT] 🎨 deck-storytelling data keys: {list(deck_data.keys())}")
                logger.info(f"[FORMAT_OUTPUT] 🎨 deck-storytelling format: {deck_data.get('format')}")
                logger.info(f"[FORMAT_OUTPUT] 🎨 deck-storytelling slides count: {len(deck_data.get('slides') or [])}")
                if deck_data.get('slides'):
                    logger.info(f"[FORMAT_OUTPUT] 🎨 First slide preview: {deck_data['slides'][0] if deck_data['slides'] else 'No slides'}")
                if 'error' in deck_data:
                    logger.error(f"[FORMAT_OUTPUT] 🎨 deck-storytelling has error: {deck_data['error']}")
            
            
            formatted_deck = self._format_deck(final_data)
            logger.info(f"[FORMAT_OUTPUT] 🎨 _format_deck returned: format={formatted_deck.get('format')}, slides_count={len(formatted_deck.get('slides', []))}")
            
            logger.info(f"[FORMAT_OUTPUT] 🎨 formatted_deck keys: {list(formatted_deck.keys())}")
            
            # CRITICAL FIX: Ensure deck is returned at top level, not nested
            # The _format_deck already extracts from deck-storytelling, so this should be flat
            # But let's verify and log
            if formatted_deck.get('format') != 'deck' or 'slides' not in formatted_deck:
                logger.error(f"[FORMAT_OUTPUT] ❌ Invalid deck structure returned from _format_deck!")
                logger.error(f"[FORMAT_OUTPUT] ❌ formatted_deck keys: {list(formatted_deck.keys())}")
                logger.error(f"[FORMAT_OUTPUT] ❌ formatted_deck format: {formatted_deck.get('format')}")
                logger.error(f"[FORMAT_OUTPUT] ❌ formatted_deck slides: {formatted_deck.get('slides')}")
            else:
                logger.info(f"[FORMAT_OUTPUT] ✅ Valid deck structure returned from _format_deck")
            
            logger.info(f"[FORMAT_OUTPUT] 🎨 Returning formatted deck with {len(formatted_deck.get('slides', []))} slides")
            # VALIDATION: Check that slides have content before returning
            deck_slides = formatted_deck.get('slides', [])
            logger.info(f"[DECK_GEN] 🔍 Validating deck before return: {len(deck_slides)} slides")
            empty_slides = []
            for i, slide in enumerate(deck_slides, 1):
                content = slide.get('content', {})
                template = slide.get('template', 'unknown')
                title = content.get('title', '')
                body = content.get('body', '')
                bullets = content.get('bullets', [])
                
                # Check if slide has meaningful content
                is_empty = (
                    (not title or title.strip() in ['', 'Analysis in progress', 'Analysis pending']) and
                    (not body or body.strip() in ['', 'Analysis in progress', 'Analysis pending']) and
                    (not bullets or len(bullets) == 0 or all(b.strip() in ['', 'Analysis in progress'] for b in bullets))
                )
                
                if is_empty:
                    empty_slides.append({
                        'slide_number': i,
                        'template': template,
                        'title': title or 'Untitled'
                    })
                    logger.warning(f"[DECK_GEN] ⚠️  Slide {i} ({template}) appears empty or has only placeholder text")
            
            # Log validation results
            if empty_slides:
                logger.warning(f"[DECK_GEN] ⚠️  Found {len(empty_slides)} empty/placeholder slides out of {len(deck_slides)} total")
                for empty in empty_slides:
                    logger.warning(f"[DECK_GEN]   - Slide {empty['slide_number']}: {empty['template']} - '{empty['title']}'")
            else:
                logger.info(f"[DECK_GEN] ✅ All {len(deck_slides)} slides have content")
            
            # Check if ALL slides are empty - this is a critical error
            if len(empty_slides) == len(deck_slides) and len(deck_slides) > 0:
                logger.error(f"[DECK_GEN] ❌ ALL slides are empty - this is a dummy deck!")
                logger.error(f"[DECK_GEN] ❌ Raising error instead of returning dummy deck")
                raise ValueError(f"All {len(deck_slides)} slides are empty - deck generation failed completely")
            
            logger.info(f"[DECK_GEN] ✅ Returning validated deck with {len(deck_slides)} slides ({len(deck_slides) - len(empty_slides)} with content)")
            return _add_plan_steps(formatted_deck)
        elif output_format == "matrix":
            logger.info(f"[FORMAT_OUTPUT] 📊 Formatting as MATRIX")
            return _add_plan_steps(self._format_matrix(final_data))
        elif output_format == "docs":
            logger.info(f"[FORMAT_OUTPUT] 📝 Formatting as DOCS/MEMO")
            return _add_plan_steps(self._format_docs(final_data))
        else:
            logger.info(f"[FORMAT_OUTPUT] 📝 Formatting as ANALYSIS")
            return _add_plan_steps(self._format_analysis(final_data))
    
    def _format_docs(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Format data for docs/memo output - pass through memo-writer sections.

        The memo-writer skill returns {format: "docs", title, date, sections, metadata}.
        This method extracts that result and wraps it so the frontend receives
        structured sections via memo_updates for the MemoEditor component.

        Regardless of section source, real chart sections are injected from
        enrichment data (scenario_analysis, cap_table_history, exit_modeling,
        fund_metrics) so memos get the same visualizations as deck slides.
        """
        # 1. Check if memo-writer skill produced a result keyed by skill name
        memo_result = data.get("memo-writer") or data.get("memo-generator")

        if isinstance(memo_result, dict) and memo_result.get("sections"):
            sections = memo_result["sections"]
            title = memo_result.get("title", "Investment Memo")
            date = memo_result.get("date", "")
            metadata = memo_result.get("metadata", {})
            logger.info(f"[FORMAT_DOCS] ✅ Found memo-writer sections: {len(sections)} sections")
        else:
            # 2. Check if sections are at the top level (e.g. from shared_data merge)
            sections = data.get("sections", [])
            title = data.get("title", "Investment Memo")
            date = data.get("date", "")
            metadata = data.get("metadata", {})
            if sections:
                logger.info(f"[FORMAT_DOCS] ✅ Found top-level sections: {len(sections)} sections")
            else:
                logger.info(f"[FORMAT_DOCS] No memo sections — building from enrichment data")
                sections = [{"type": "heading1", "content": "Investment Analysis"}]
                title = "Investment Analysis"
                metadata = {"section_count": 0}

        # 3. Inject real charts from enrichment data (same data the deck pipeline uses)
        chart_sections = self._build_chart_sections_from_data(data)
        if chart_sections:
            existing_chart_types = {
                s.get("chart", {}).get("type")
                for s in sections
                if s.get("type") == "chart"
            }
            injected = 0
            for cs in chart_sections:
                ct = cs.get("chart", {}).get("type")
                if ct and ct not in existing_chart_types:
                    sections.append(cs)
                    existing_chart_types.add(ct)
                    injected += 1
            if injected:
                logger.info(f"[FORMAT_DOCS] 📊 Injected {injected} chart(s) from enrichment data")

        # 4. Also inject any pre-built charts sitting in data["charts"]
        existing_chart_types = {
            s.get("chart", {}).get("type") for s in sections if s.get("type") == "chart"
        }
        for chart in data.get("charts", []):
            ct = chart.get("type")
            if ct and ct not in existing_chart_types:
                sections.append({"type": "chart", "chart": chart})
                existing_chart_types.add(ct)

        metadata["has_charts"] = any(s.get("type") == "chart" for s in sections)
        metadata["section_count"] = len(sections)

        return {
            "format": "docs",
            "title": title,
            "date": date,
            "sections": sections,
            "metadata": metadata,
            "memo_updates": {
                "action": "replace",
                "sections": sections
            },
            "citations": data.get("citations", []),
            "charts": data.get("charts", []),
            "companies": data.get("companies", []),
        }

    # ------------------------------------------------------------------
    # Chart section builder — extracts real charts from enrichment data.
    # Same data sources the deck pipeline uses, same chart format as
    # lightweight_memo_service._build_chart().
    # ------------------------------------------------------------------

    def _build_chart_sections_from_data(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Build chart sections from real enrichment data.

        Scans scenario_analysis, cap_table_history, exit_scenarios_data,
        exit_modeling, and fund_metrics for chart-renderable data and returns
        {"type": "chart", "chart": {...}} sections ready for memo injection.
        """
        sections: List[Dict[str, Any]] = []
        companies = data.get("companies", [])

        # --- Cap table sankey ---
        cap_history = data.get("cap_table_history", {})
        if isinstance(cap_history, dict):
            sankey = cap_history.get("sankey_data")
            if sankey:
                sections.append({"type": "chart", "chart": {
                    "type": "sankey", "title": "Cap Table — Ownership Flow", "data": sankey,
                }})
            else:
                for name, ch in cap_history.items():
                    if isinstance(ch, dict) and ch.get("sankey_data"):
                        sections.append({"type": "chart", "chart": {
                            "type": "sankey",
                            "title": f"{name} — Ownership Flow",
                            "data": ch["sankey_data"],
                        }})
                        break

        # --- Cap table evolution (stacked area) ---
        if isinstance(cap_history, dict):
            evolution = cap_history.get("evolution", cap_history.get("rounds", []))
            if not evolution:
                for name, ch in cap_history.items():
                    if isinstance(ch, dict):
                        evolution = ch.get("evolution", ch.get("rounds", []))
                        if evolution:
                            break
            if evolution:
                sections.append({"type": "chart", "chart": {
                    "type": "cap_table_evolution",
                    "title": "Cap Table Evolution",
                    "data": {"evolution": evolution},
                }})

        # --- Probability cloud from scenario_analysis ---
        scenario_analysis = data.get("scenario_analysis", {})
        if isinstance(scenario_analysis, dict):
            for name, sc in scenario_analysis.items():
                if not isinstance(sc, dict):
                    continue
                sc_list = sc.get("scenarios", [])
                if sc_list:
                    sections.append({"type": "chart", "chart": {
                        "type": "probability_cloud",
                        "title": f"{name} — Exit Scenarios",
                        "data": {
                            "scenarios": [
                                {
                                    "name": s.get("name", s.get("scenario_name", "Scenario")),
                                    "probability": s.get("probability", 0),
                                    "dataPoints": s.get("data_points", s.get("dataPoints", [])),
                                }
                                for s in sc_list[:10]
                            ],
                            "breakpoints": sc.get("breakpoints", []),
                            "xConfig": {"label": "Exit Value ($M)", "type": "log"},
                            "yConfig": {"label": "Return Multiple"},
                        },
                    }})
                    break
            # Also check company-level scenarios
            if not any(s.get("chart", {}).get("type") == "probability_cloud" for s in sections):
                for c in companies:
                    sc_list = c.get("scenarios", [])
                    if sc_list:
                        sections.append({"type": "chart", "chart": {
                            "type": "probability_cloud",
                            "title": f"{c.get('company', 'Company')} — Exit Scenarios",
                            "data": {
                                "scenarios": [
                                    {
                                        "name": s.get("name", s.get("scenario_name", "Scenario")),
                                        "probability": s.get("probability", 0),
                                        "dataPoints": s.get("data_points", s.get("dataPoints", [])),
                                    }
                                    for s in sc_list[:10]
                                ],
                                "breakpoints": [],
                                "xConfig": {"label": "Exit Value ($M)", "type": "log"},
                                "yConfig": {"label": "Return Multiple"},
                            },
                        }})
                        break

        # --- Bull/bear/base bar chart from exit scenarios ---
        exit_data = data.get("exit_scenarios_data", data.get("exit_modeling", {}))
        if isinstance(exit_data, dict):
            for name, company_exits in exit_data.items():
                if not isinstance(company_exits, dict):
                    continue
                scenarios = company_exits.get("scenarios", [])
                if scenarios:
                    sections.append({"type": "chart", "chart": {
                        "type": "bar",
                        "title": f"{name} — Exit Values by Scenario",
                        "data": [
                            {
                                "name": s.get("scenario_name", s.get("name", "Unknown")),
                                "value": (s.get("exit_value", 0) or 0) / 1e6,
                            }
                            for s in scenarios[:6]
                        ],
                    }})
                    break

        # --- DPI Sankey ---
        fund_metrics = data.get("fund_metrics", {})
        dpi_sankey = fund_metrics.get("dpi_sankey") if isinstance(fund_metrics, dict) else None
        if not dpi_sankey and isinstance(exit_data, dict):
            dpi_sankey = exit_data.get("dpi_sankey")
        if dpi_sankey:
            sections.append({"type": "chart", "chart": {
                "type": "dpi_sankey",
                "title": "Fund Distribution Flow",
                "data": dpi_sankey,
            }})

        # --- Breakpoint chart ---
        if isinstance(scenario_analysis, dict):
            for name, sc in scenario_analysis.items():
                if isinstance(sc, dict):
                    breakpoints = sc.get("breakpoints", [])
                    if breakpoints:
                        sections.append({"type": "chart", "chart": {
                            "type": "breakpoint_chart",
                            "title": f"{name} — Breakpoint Distribution",
                            "data": {"breakpoints": breakpoints},
                        }})
                        break

        # --- Scatter multiples (if 2+ companies) ---
        if len(companies) >= 2:
            points = []
            for c in companies:
                val = safe_get_value(c.get("valuation"), 0)
                rev = (
                    safe_get_value(c.get("revenue"))
                    or safe_get_value(c.get("arr"))
                    or safe_get_value(c.get("inferred_revenue"))
                    or 1
                )
                growth = (
                    safe_get_value(c.get("growth_rate"))
                    or safe_get_value(c.get("revenue_growth_annual_pct"))
                    or 0
                )
                multiple = val / rev if val and rev > 0 else 0
                points.append({
                    "name": c.get("company", "Unknown"),
                    "x": growth * 100 if growth < 5 else growth,
                    "y": multiple,
                    "arr": rev,
                    "stage": c.get("stage", "series_a"),
                })
            if any(p["y"] > 0 for p in points):
                sections.append({"type": "chart", "chart": {
                    "type": "scatter_multiples",
                    "title": "Revenue Multiple vs Growth",
                    "data": {"companies": points},
                }})

        # --- Revenue forecasting (bull/bear/base) line chart ---
        revenue_projections = data.get("revenue_projections", {})
        if not revenue_projections and companies:
            # Build forecasting data from company revenue + growth
            for c in companies:
                rev = (
                    safe_get_value(c.get("revenue"))
                    or safe_get_value(c.get("arr"))
                    or safe_get_value(c.get("inferred_revenue"))
                    or 0
                )
                growth = (
                    safe_get_value(c.get("growth_rate"))
                    or safe_get_value(c.get("revenue_growth_annual_pct"))
                    or 0
                )
                if rev > 0 and growth > 0:
                    name = c.get("company", "Company")
                    growth_frac = growth if growth < 5 else growth / 100
                    base_pts = []
                    bull_pts = []
                    bear_pts = []
                    for yr in range(6):
                        base_rev = rev * ((1 + growth_frac * max(0.3, 1 - yr * 0.12)) ** yr)
                        bull_rev = rev * ((1 + growth_frac * 1.3 * max(0.3, 1 - yr * 0.10)) ** yr)
                        bear_rev = rev * ((1 + growth_frac * 0.5 * max(0.3, 1 - yr * 0.15)) ** yr)
                        base_pts.append({"x": yr, "y": round(base_rev / 1e6, 2)})
                        bull_pts.append({"x": yr, "y": round(bull_rev / 1e6, 2)})
                        bear_pts.append({"x": yr, "y": round(bear_rev / 1e6, 2)})
                    revenue_projections[name] = {
                        "base": base_pts, "bull": bull_pts, "bear": bear_pts,
                        "current_revenue": rev,
                    }
        if revenue_projections:
            for name, proj in revenue_projections.items():
                if isinstance(proj, dict) and proj.get("base"):
                    sections.append({"type": "chart", "chart": {
                        "type": "line",
                        "title": f"{name} — Revenue Forecast (Bull/Bear/Base)",
                        "data": {
                            "labels": [f"Year {p['x']}" for p in proj["base"]],
                            "datasets": [
                                {
                                    "label": "Bull Case",
                                    "data": [p["y"] for p in proj.get("bull", proj["base"])],
                                    "borderColor": "#59a14f",
                                    "backgroundColor": "rgba(89,161,79,0.1)",
                                    "fill": False,
                                    "borderDash": [5, 3],
                                },
                                {
                                    "label": "Base Case",
                                    "data": [p["y"] for p in proj["base"]],
                                    "borderColor": "#4e79a7",
                                    "backgroundColor": "rgba(78,121,167,0.1)",
                                    "fill": False,
                                },
                                {
                                    "label": "Bear Case",
                                    "data": [p["y"] for p in proj.get("bear", proj["base"])],
                                    "borderColor": "#e15759",
                                    "backgroundColor": "rgba(225,87,89,0.1)",
                                    "fill": False,
                                    "borderDash": [5, 3],
                                },
                            ],
                        },
                    }})
                    break  # One forecasting chart per memo

        # --- Investor/team-enriched cap table pie chart ---
        if not any(s.get("chart", {}).get("type") == "pie" for s in sections):
            for c in companies:
                cap = c.get("cap_table") or c.get("ownership_breakdown") or {}
                investors_raw = c.get("investors") or c.get("key_investors") or []
                if cap and isinstance(cap, dict):
                    pie_segments = []
                    for holder, pct in cap.items():
                        val = safe_get_value(pct, 0)
                        if val > 0:
                            pie_segments.append({"name": str(holder), "value": val})
                    if pie_segments:
                        sections.append({"type": "chart", "chart": {
                            "type": "pie",
                            "title": f"{c.get('company', 'Company')} — Ownership Breakdown",
                            "data": pie_segments,
                        }})
                        break
                elif investors_raw and isinstance(investors_raw, list):
                    # Build approximate ownership pie from investor list + stage heuristics
                    stage = (c.get("stage") or "series_a").lower().replace(" ", "_")
                    founder_pct_map = {"seed": 70, "series_a": 55, "series_b": 40, "series_c": 30, "series_d": 25}
                    esop_pct = 10
                    founder_pct = founder_pct_map.get(stage, 35)
                    investor_pool = 100 - founder_pct - esop_pct
                    inv_list = investors_raw[:6] if isinstance(investors_raw[0], str) else [
                        i.get("name", i.get("investor", "Investor")) for i in investors_raw[:6]
                    ]
                    per_investor = investor_pool / max(len(inv_list), 1)
                    pie_segments = [{"name": "Founders", "value": founder_pct}, {"name": "ESOP", "value": esop_pct}]
                    for inv_name in inv_list:
                        pie_segments.append({"name": str(inv_name), "value": round(per_investor, 1)})
                    sections.append({"type": "chart", "chart": {
                        "type": "pie",
                        "title": f"{c.get('company', 'Company')} — Estimated Ownership",
                        "data": pie_segments,
                    }})
                    break

        # --- Cap table waterfall (ownership evolution through rounds) ---
        if not any(s.get("chart", {}).get("type") == "cap_table_waterfall" for s in sections):
            if isinstance(cap_history, dict):
                for name, ch in cap_history.items():
                    if isinstance(ch, dict):
                        evolution = ch.get("evolution", ch.get("rounds", []))
                        if evolution and isinstance(evolution, list) and len(evolution) >= 2:
                            sections.append({"type": "chart", "chart": {
                                "type": "cap_table_waterfall",
                                "title": f"{name} — Ownership Through Rounds",
                                "data": {"cap_table_evolution": evolution},
                            }})
                            break

        # --- J-curve / growth-decay projection ---
        fund_deployment = data.get("fund_deployment", {})
        if isinstance(fund_deployment, dict) and fund_deployment.get("j_curve"):
            j_curve_data = fund_deployment["j_curve"]
            sections.append({"type": "chart", "chart": {
                "type": "line",
                "title": "Fund J-Curve — Cumulative Net Cash Flow",
                "data": {
                    "labels": [str(p.get("year", p.get("quarter", i))) for i, p in enumerate(j_curve_data)],
                    "datasets": [{
                        "label": "Cumulative Net Cash Flow ($M)",
                        "data": [round(safe_get_value(p.get("cumulative", p.get("value", 0)), 0) / 1e6, 2) for p in j_curve_data],
                        "borderColor": "#4e79a7",
                        "backgroundColor": "rgba(78,121,167,0.15)",
                        "fill": True,
                    }],
                },
            }})
        elif not any("J-Curve" in (s.get("chart", {}).get("title") or "") for s in sections):
            # Build synthetic J-curve from fund context if available
            fc = data.get("fund_context", {})
            fund_size = safe_get_value(fc.get("fund_size"), 0)
            if fund_size > 0:
                # Standard VC J-curve: deploy over ~4 years, returns start year 3-4
                j_pts = []
                for yr in range(11):
                    if yr <= 4:
                        deploy = fund_size * min(yr * 0.22, 0.90)
                        returns = fund_size * max(0, (yr - 2) * 0.05)
                    else:
                        deploy = fund_size * 0.95
                        returns = fund_size * (0.15 + (yr - 4) * 0.25)
                    j_pts.append(round((returns - deploy) / 1e6, 1))
                sections.append({"type": "chart", "chart": {
                    "type": "line",
                    "title": "Projected Fund J-Curve",
                    "data": {
                        "labels": [f"Year {y}" for y in range(11)],
                        "datasets": [{
                            "label": "Cumulative Net ($M)",
                            "data": j_pts,
                            "borderColor": "#4e79a7",
                            "backgroundColor": "rgba(78,121,167,0.15)",
                            "fill": True,
                        }],
                    },
                }})

        # --- Growth decay curves per company ---
        for c in companies[:3]:
            rev = (
                safe_get_value(c.get("revenue"))
                or safe_get_value(c.get("arr"))
                or safe_get_value(c.get("inferred_revenue"))
                or 0
            )
            growth = (
                safe_get_value(c.get("growth_rate"))
                or safe_get_value(c.get("revenue_growth_annual_pct"))
                or 0
            )
            if rev > 0 and growth > 0:
                name = c.get("company", "Company")
                growth_frac = growth if growth < 5 else growth / 100
                # Growth decay: growth rate decreases over time (SaaS reversion)
                growth_pts = []
                rev_pts = []
                current_rev = rev
                current_growth = growth_frac
                for yr in range(7):
                    growth_pts.append(round(current_growth * 100, 1))
                    rev_pts.append(round(current_rev / 1e6, 2))
                    current_rev = current_rev * (1 + current_growth)
                    current_growth = current_growth * 0.75  # 25% decay per year
                sections.append({"type": "chart", "chart": {
                    "type": "line",
                    "title": f"{name} — Growth Decay Projection",
                    "data": {
                        "labels": [f"Year {y}" for y in range(7)],
                        "datasets": [
                            {
                                "label": "Revenue ($M)",
                                "data": rev_pts,
                                "borderColor": "#4e79a7",
                                "yAxisID": "y",
                            },
                            {
                                "label": "Growth Rate (%)",
                                "data": growth_pts,
                                "borderColor": "#e15759",
                                "borderDash": [5, 3],
                                "yAxisID": "y1",
                            },
                        ],
                    },
                }})
                break  # One growth decay chart

        # --- Portfolio treemap (if 2+ companies) ---
        if len(companies) >= 2 and not any(
            s.get("chart", {}).get("type") == "treemap" for s in sections
        ):
            treemap_data = []
            for c in companies:
                val = (
                    safe_get_value(c.get("valuation"))
                    or safe_get_value(c.get("nav"))
                    or safe_get_value(c.get("inferred_revenue"))
                    or 0
                )
                if val > 0:
                    treemap_data.append({
                        "name": c.get("company", "Unknown"),
                        "value": val,
                        "sector": c.get("sector", "Other"),
                    })
            if treemap_data:
                sections.append({"type": "chart", "chart": {
                    "type": "treemap",
                    "title": "Portfolio Allocation by Valuation",
                    "data": treemap_data,
                }})

        return sections

    def _format_spreadsheet(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Format data for spreadsheet output with commands"""
        # Check if we already have spreadsheet data from skill execution
        if "format" in data and data["format"] == "spreadsheet" and "commands" in data:
            # Skill already generated the spreadsheet, use it directly
            return {
                "format": "spreadsheet",
                "commands": data["commands"],
                "metadata": data.get("metadata", {}),
                "data": data,
                "citations": data.get("citations", []),
                "charts": data.get("charts", []),
                "hasFormulas": True,
                "hasCharts": True
            }
        
        # Fallback to generating commands if no skill result
        companies = data.get("companies", [])
        commands = []
        
        # Generate header commands
        headers = self._generate_spreadsheet_columns(data)
        for i, header in enumerate(headers):
            cell = f"{chr(65 + i)}1"
            commands.append(f'sheet.write("{cell}", "{header}").style("bold", true).style("backgroundColor", "#f0f0f0")')
        
        # Generate data commands
        rows = self._generate_spreadsheet_rows(data)
        for row_idx, row in enumerate(rows, start=2):
            for col_idx, value in enumerate(row):
                cell = f"{chr(65 + col_idx)}{row_idx}"
                if isinstance(value, (int, float)) and value > 1000:
                    commands.append(f'sheet.write("{cell}", {value}).format("currency")')
                elif isinstance(value, float) and 0 < value < 1:
                    commands.append(f'sheet.write("{cell}", {value}).format("percentage")')
                else:
                    commands.append(f'sheet.write("{cell}", "{value}")')
        
        # Add formulas
        if len(rows) > 0:
            last_row = len(rows) + 1
            commands.append(f'sheet.formula("E{last_row + 1}", "=SUM(E2:E{last_row})").style("bold", true)')
            commands.append(f'sheet.formula("F{last_row + 1}", "=AVERAGE(F2:F{last_row})").style("bold", true)')
            commands.append(f'sheet.formula("G{last_row + 1}", "=SUM(G2:G{last_row})").style("bold", true)')
        
        return {
            "format": "spreadsheet",
            "commands": commands,
            "data": data,
            "columns": headers,
            "rows": rows,
            "citations": data.get("citations", []),
            "hasFormulas": True,
            "hasCharts": True
        }
    
    def _build_minimal_deck(self, reason: str, companies: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Construct a minimal, but well-formed fallback deck when upstream data is missing."""
        company_names = ", ".join([c.get("company", "Unknown Company") for c in companies]) or "Selected Companies"
        generated_at = datetime.now().isoformat()
        slides = [
            {
                "id": "fallback-title",
                "order": 1,
                "template": "title",
                "content": {
                    "title": "Deck Generation Fallback",
                    "subtitle": company_names,
                    "body": "Automated storytelling failed, so this minimal deck was assembled to keep the workflow moving."
                }
            },
            {
                "id": "fallback-summary",
                "order": 2,
                "template": "summary",
                "content": {
                    "title": "What Happened",
                    "bullets": [
                        "Primary deck storyteller returned an error or empty slides.",
                        "Upstream data is preserved so you can re-run once upstream issues are resolved.",
                        "Minimal slides ensure downstream tooling continues to work."
                    ],
                    "callouts": [
                        {
                            "label": "Reason",
                            "value": reason or "Unknown",
                            "context": "Captured during deck formatting"
                        }
                    ]
                }
            },
            {
                "id": "fallback-next-steps",
                "order": 3,
                "template": "action_plan",
                "content": {
                    "title": "Next Steps",
                    "actions": [
                        "Review logs for the reported error message.",
                        "Verify company data completeness (revenue, metrics, slide templates).",
                        "Re-run deck generation after addressing data/model issues."
                    ],
                    "notes": f"Fallback generated at {generated_at}."
                }
            }
        ]
        
        metadata = {
            "generated_at": generated_at,
            "fallback_reason": reason,
            "is_fallback": True
        }
        
        return {
            "format": "deck",
            "slides": slides,
            "theme": "professional",
            "metadata": metadata,
            "citations": [],
            "charts": [],
            "companies": companies
        }
    
    def _normalize_slide_format(self, slides: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Normalize slide format: convert 'type' field to 'template' if present
        
        This ensures compatibility with frontend which expects 'template' field.
        Handles legacy formats that may use 'type' instead of 'template'.
        Also ensures all required fields (id, order, content, template) are present.
        """
        if not slides:
            return []
        
        normalized = []
        for idx, slide in enumerate(slides, start=1):
            if not isinstance(slide, dict):
                logger.warning(f"[NORMALIZE_SLIDES] Skipping non-dict slide: {type(slide)}")
                continue
            
            normalized_slide = slide.copy()
            
            # If slide has 'type' but no 'template', convert it
            if "type" in normalized_slide and "template" not in normalized_slide:
                normalized_slide["template"] = normalized_slide.pop("type")
                logger.debug(f"[NORMALIZE_SLIDES] Converted 'type' to 'template' for slide {normalized_slide.get('id', 'unknown')}")
            
            # Ensure 'template' field exists (default to 'text' if missing)
            if "template" not in normalized_slide:
                normalized_slide["template"] = "text"
                logger.warning(f"[NORMALIZE_SLIDES] Added default 'template' field to slide {normalized_slide.get('id', 'unknown')}")
            
            # CRITICAL: Ensure required fields for frontend compatibility
            # Frontend expects: id, order, template, content
            if "id" not in normalized_slide:
                normalized_slide["id"] = f"slide-{idx}" if normalized_slide.get("order") is None else f"slide-{normalized_slide.get('order', idx)}"
                logger.warning(f"[NORMALIZE_SLIDES] Added missing 'id' field to slide: {normalized_slide['id']}")
            
            if "order" not in normalized_slide:
                normalized_slide["order"] = idx
                logger.warning(f"[NORMALIZE_SLIDES] Added missing 'order' field to slide {normalized_slide.get('id')}: {idx}")
            
            # Ensure 'content' field exists (frontend requires it)
            if "content" not in normalized_slide:
                # Try to extract content from top-level fields
                content = {}
                if "title" in normalized_slide:
                    content["title"] = normalized_slide.pop("title")
                if "subtitle" in normalized_slide:
                    content["subtitle"] = normalized_slide.pop("subtitle")
                if "body" in normalized_slide:
                    content["body"] = normalized_slide.pop("body")
                if "bullets" in normalized_slide:
                    content["bullets"] = normalized_slide.pop("bullets")
                
                # If no content was found, create minimal content
                if not content:
                    content = {
                        "title": normalized_slide.get("id", f"Slide {idx}"),
                        "body": "Content not available"
                    }
                
                normalized_slide["content"] = content
                logger.warning(f"[NORMALIZE_SLIDES] Added missing 'content' field to slide {normalized_slide.get('id')}")
            
            # Ensure content is a dict (not None or other type)
            if not isinstance(normalized_slide.get("content"), dict):
                normalized_slide["content"] = {
                    "title": normalized_slide.get("id", f"Slide {idx}"),
                    "body": str(normalized_slide.get("content", "Content not available"))
                }
                logger.warning(f"[NORMALIZE_SLIDES] Fixed invalid 'content' type for slide {normalized_slide.get('id')}")
            
            normalized.append(normalized_slide)
        
        return normalized
    
    def _format_deck(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Format data for deck output"""
        logger.critical(f"[FORMAT_DECK] ⚫⚫⚫ _format_deck CALLED with keys: {list(data.keys())} ⚫⚫⚫")
        logger.info(f"[FORMAT_DECK] Starting deck formatting, data keys: {list(data.keys())}")
        logger.info(f"[FORMAT_DECK] Data contains deck-storytelling: {'deck-storytelling' in data}")
        
        
        # CRITICAL FIX: Check if deck-storytelling skill already generated the deck
        # The skill result is stored under the skill name key, not at top level!
        if "deck-storytelling" in data and isinstance(data["deck-storytelling"], dict):
            deck_data = data["deck-storytelling"]
            logger.info(f"[FORMAT_DECK] Found deck-storytelling data with keys: {list(deck_data.keys())}")
            
            
            # Check if deck-storytelling returned an error
            if "error" in deck_data:
                error_msg = deck_data.get('error', 'Unknown error')
                error_type = deck_data.get('error_type', 'Unknown')
                logger.error(f"[FORMAT_DECK] ❌ deck-storytelling returned ERROR: {error_msg}")
                logger.error(f"[FORMAT_DECK] ❌ Error type: {error_type}")
                
                # If error is due to missing companies, try to wait/retry instead of falling back
                if "companies_not_found" in error_msg.lower() or error_type == "companies_not_found":
                    logger.warning(f"[FORMAT_DECK] ⚠️ deck-storytelling failed due to missing companies")
                    logger.warning(f"[FORMAT_DECK] ⚠️ Available companies in data: {len(data.get('companies', []))}")
                    logger.warning(f"[FORMAT_DECK] ⚠️ This suggests company-data-fetcher may not have completed yet")
                    # Don't fall back immediately - let the error propagate so caller can retry
                    # Only fall back if we truly cannot get companies
                    if data.get('companies'):
                        logger.info(f"[FORMAT_DECK] ⚠️ Companies found in data, but deck-storytelling didn't see them")
                        logger.info(f"[FORMAT_DECK] ⚠️ This may be a timing issue - falling back to basic slides")
                    else:
                        logger.error(f"[FORMAT_DECK] ❌ No companies available anywhere - cannot generate deck")
                        # Return error instead of fallback
                        return {
                            "format": "deck",
                            "error": error_msg,
                            "error_type": error_type,
                            "slides": [],
                            "theme": "professional",
                            "metadata": {
                                "generated_at": datetime.now().isoformat(),
                                "is_fallback": False,
                                "is_error": True,
                                "error": error_msg
                            },
                            "companies": data.get("companies", []),
                            "citations": data.get("citations", []),
                            "charts": data.get("charts", [])
                        }
                else:
                    logger.warning(f"[FORMAT_DECK] ⚠️ deck-storytelling failed with non-company error, falling back to _generate_slides()")
                # Fall through to fallback generation for other errors
            
            logger.info(f"[FORMAT_DECK] deck_data.format = {deck_data.get('format')}")
            logger.info(f"[FORMAT_DECK] deck_data has slides: {'slides' in deck_data}")
            logger.info(f"[FORMAT_DECK] deck_data.slides type: {type(deck_data.get('slides'))}")
            logger.info(f"[FORMAT_DECK] deck_data.slides length: {len(deck_data.get('slides', []))}")
            
            # Only use deck-storytelling result if it has slides and no error
            validation_passed = (deck_data.get("format") == "deck" and 
                "slides" in deck_data and 
                len(deck_data.get("slides", [])) > 0 and
                "error" not in deck_data)
            
            
            if validation_passed:
                logger.info(f"[FORMAT_DECK] ✅ Using deck-storytelling result with {len(deck_data.get('slides', []))} slides")
                # Normalize slide format (type -> template)
                normalized_slides = self._normalize_slide_format(deck_data.get("slides") or [])
                result = {
                    "format": "deck",
                    "slides": normalized_slides,
                    "theme": deck_data.get("theme", "professional"),
                    "metadata": deck_data.get("metadata", {}),
                    "citations": deck_data.get("citations", []),
                    "charts": deck_data.get("charts", []),
                    "companies": data.get("companies", [])  # Include companies for frontend
                }
                logger.info(f"[FORMAT_DECK] ✅ Returning deck-storytelling result with {len(result['slides'])} slides")
                logger.info(f"[FORMAT_DECK] ✅ Result structure: format={result['format']}, has_slides={len(result.get('slides', []))}, has_companies={len(result.get('companies', []))}")
                logger.info(f"[FORMAT_DECK] Final result slides: {len(result.get('slides', []))}")
                logger.info(f"[FORMAT_DECK] Slide IDs: {[s.get('id') for s in result.get('slides', [])[:3]]}")
                
                # DEFENSIVE CHECK: Ensure format field is always present
                if 'format' not in result:
                    logger.warning(f"[FORMAT_DECK] ⚠️ Missing format field, adding it")
                    result['format'] = 'deck'
                
                # Ensure all required fields are present and properly typed
                result.setdefault('theme', 'professional')
                result.setdefault('metadata', {})
                result.setdefault('citations', [])
                result.setdefault('charts', [])
                result.setdefault('companies', [])
                
                # Ensure all list fields are actually lists (not None)
                if result['slides'] is None:
                    result['slides'] = []
                if result['citations'] is None:
                    result['citations'] = []
                if result['charts'] is None:
                    result['charts'] = []
                if result['companies'] is None:
                    result['companies'] = []
                
                # CHANGED: Don't raise exception, just log warning and fall through to fallback
                if not result.get('slides') or len(result.get('slides', [])) == 0:
                    logger.warning(f"[FORMAT_DECK] ⚠️ deck-storytelling result has EMPTY slides!")
                    logger.warning(f"[FORMAT_DECK] ⚠️ Companies available: {len(data.get('companies', []))}")
                    logger.warning(f"[FORMAT_DECK] ⚠️ Falling through to fallback deck generation")
                else:
                    logger.info(f"[FORMAT_DECK] ✅ Final validation passed: returning {len(result['slides'])} slides")
                    return result
            else:
                slides_data = deck_data.get('slides') or []
                if not isinstance(slides_data, list):
                    slides_data = []
                slides_len = len(slides_data)
                logger.warning(f"[FORMAT_DECK] ⚠️ deck-storytelling found but invalid: format={deck_data.get('format')}, has_slides={'slides' in deck_data}, slides_len={slides_len}")
        
        # Fallback: Check if we already have deck data at top level (legacy support)
        if "format" in data and data["format"] == "deck" and "slides" in data:
            # Normalize slides to ensure it's always a list
            legacy_slides = data.get("slides") or []
            if not isinstance(legacy_slides, list):
                legacy_slides = []
            logger.info(f"[FORMAT_DECK] Using legacy deck data with {len(legacy_slides)} slides")
            # Normalize slide format (type -> template)
            normalized_slides = self._normalize_slide_format(legacy_slides)
            # Skill already generated the deck, use it directly
            legacy_result = {
                "format": "deck",
                "slides": normalized_slides,
                "theme": data.get("theme", "professional"),
                "metadata": data.get("metadata", {}),
                "citations": data.get("citations", []),
                "charts": data.get("charts", []),
                "companies": data.get("companies", [])
            }
            
            # DEFENSIVE CHECK: Ensure format field is always present
            if 'format' not in legacy_result:
                logger.warning(f"[FORMAT_DECK] ⚠️ Missing format field in legacy result, adding it")
                legacy_result['format'] = 'deck'
            
            # Ensure all required fields are present and properly typed
            legacy_result.setdefault('theme', 'professional')
            legacy_result.setdefault('metadata', {})
            legacy_result.setdefault('citations', [])
            legacy_result.setdefault('charts', [])
            legacy_result.setdefault('companies', [])
            
            # Ensure all list fields are actually lists (not None)
            if legacy_result['slides'] is None:
                legacy_result['slides'] = []
            if legacy_result['citations'] is None:
                legacy_result['citations'] = []
            if legacy_result['charts'] is None:
                legacy_result['charts'] = []
            if legacy_result['companies'] is None:
                legacy_result['companies'] = []
            
            # CRITICAL VALIDATION: Ensure legacy slides are not empty
            if not legacy_result.get('slides') or len(legacy_result.get('slides', [])) == 0:
                logger.error(f"[FORMAT_DECK] ❌ CRITICAL ERROR: Legacy deck data has EMPTY slides!")
                logger.error(f"[FORMAT_DECK] ❌ Companies available: {len(data.get('companies', []))}")
                logger.error(f"[FORMAT_DECK] ❌ Data keys: {list(data.keys())}")
                fallback_reason = "legacy_deck_empty"
                logger.warning(f"[FORMAT_DECK] ⚠️ Returning minimal fallback deck due to {fallback_reason}")
                minimal_deck = self._build_minimal_deck(fallback_reason, data.get('companies', []))
                # Ensure minimal deck is also normalized
                if minimal_deck.get('slides'):
                    minimal_deck['slides'] = self._normalize_slide_format(minimal_deck['slides'])
                return minimal_deck
            
            logger.info(f"[FORMAT_DECK] ✅ Legacy validation passed: returning {len(legacy_result['slides'])} slides")
            return legacy_result
        
        # CRITICAL: Only use fallback if deck-storytelling was never attempted or truly failed
        # If deck-storytelling exists but returned empty/error, we should NOT fall back automatically
        deck_storytelling_attempted = "deck-storytelling" in data
        logger.info(f"[FORMAT_DECK] deck-storytelling attempted: {deck_storytelling_attempted}")
        
        if deck_storytelling_attempted:
            deck_data = data.get("deck-storytelling", {})
            if isinstance(deck_data, dict):
                has_error = deck_data.get("error") is not None
                has_empty_slides = not deck_data.get("slides") or len(deck_data.get("slides", [])) == 0
                
                if has_error or has_empty_slides:
                    error_msg = deck_data.get("error", "Unknown error")
                    logger.error(f"[FORMAT_DECK] ❌ deck-storytelling was attempted but failed: {error_msg}")
                    logger.error(f"[FORMAT_DECK] ❌ This indicates a real problem - deck-storytelling should have generated slides")
                    logger.error(f"[FORMAT_DECK] ❌ Companies available: {len(data.get('companies', []))}")
                    logger.error(f"[FORMAT_DECK] ❌ DO NOT use fallback slides - return error instead")
                    
                    # Return error result instead of fallback
                    error_result = {
                        "format": "deck",
                        "error": f"deck-storytelling failed: {error_msg}",
                        "error_type": deck_data.get("error_type", "deck_generation_failed"),
                        "slides": [],
                        "theme": "professional",
                        "metadata": {
                            "generated_at": datetime.now().isoformat(),
                            "is_fallback": False,
                            "is_error": True,
                            "error": error_msg,
                            "deck_storytelling_attempted": True
                        },
                        "companies": data.get("companies", []) if data.get("companies") is not None else [],
                        "citations": data.get("citations", []) if data.get("citations") is not None else [],
                        "charts": data.get("charts", []) if data.get("charts") is not None else []
                    }
                    
                    # Ensure all list fields are actually lists (not None)
                    if error_result['slides'] is None:
                        error_result['slides'] = []
                    if error_result['citations'] is None:
                        error_result['citations'] = []
                    if error_result['charts'] is None:
                        error_result['charts'] = []
                    if error_result['companies'] is None:
                        error_result['companies'] = []
                    
                    # Normalize slides (even if empty, ensures consistent structure)
                    error_result['slides'] = self._normalize_slide_format(error_result['slides'])
                    
                    return error_result
        
        # Only generate fallback slides if deck-storytelling was never attempted
        if not deck_storytelling_attempted:
            logger.warning(f"[FORMAT_DECK] ⚠️ deck-storytelling was NOT attempted - this should not happen for deck format!")
            logger.warning(f"[FORMAT_DECK] ⚠️ Generating fallback slides as last resort")
            try:
                fallback_slides = self._generate_slides(data)
                if fallback_slides is None:
                    logger.error(f"[FORMAT_DECK] ❌ _generate_slides returned None!")
                    fallback_slides = []
                # Defensive check: ensure fallback_slides is a list
                if not isinstance(fallback_slides, list):
                    logger.error(f"[FORMAT_DECK] ❌ _generate_slides returned non-list: {type(fallback_slides)}")
                    fallback_slides = []
                logger.info(f"[FORMAT_DECK] Generated {len(fallback_slides)} fallback slides")
            except Exception as e:
                logger.error(f"[FORMAT_DECK] ❌ Error generating fallback slides: {e}")
                import traceback
                logger.error(f"[FORMAT_DECK] Traceback: {traceback.format_exc()}")
                fallback_slides = []
        else:
            # deck-storytelling was attempted but we shouldn't reach here
            logger.error(f"[FORMAT_DECK] ❌ Logic error: deck-storytelling was attempted but we're in fallback path")
            fallback_slides = []
        
        # Determine why fallback was used
        fallback_reasons = []
        if not self.tavily_api_key:
            fallback_reasons.append("tavily_api_key_missing")
        if not self.model_router or not any([
            self.model_router.anthropic_key,
            self.model_router.openai_key,
            self.model_router.google_key,
            self.model_router.groq_key
        ]):
            fallback_reasons.append("llm_api_keys_missing")
        if not fallback_reasons:
            fallback_reasons.append("deck_storytelling_unavailable")
        
        # Log fallback reasons for debugging
        logger.info(f"[FORMAT_DECK] Fallback reasons: {fallback_reasons}")
        logger.info(f"[FORMAT_DECK] Tavily available: {bool(self.tavily_api_key)}")
        _llm_available = bool(self.model_router and any([
            self.model_router.anthropic_key,
            self.model_router.openai_key,
            self.model_router.google_key,
            self.model_router.groq_key
        ]))
        logger.info(f"[FORMAT_DECK] LLM available: {_llm_available}")
        
        fallback_result = {
            "format": "deck",
            "slides": fallback_slides,
            "theme": "professional",
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "is_fallback": True,
                "fallback_reasons": fallback_reasons,
                "tavily_available": bool(self.tavily_api_key),
                "llm_available": bool(self.model_router and any([
                    self.model_router.anthropic_key,
                    self.model_router.openai_key,
                    self.model_router.google_key,
                    self.model_router.groq_key
                ]))
            },
            "citations": data.get("citations", []) if data.get("citations") is not None else [],
            "charts": data.get("charts", []) if data.get("charts") is not None else [],
            "companies": data.get("companies", []) if data.get("companies") is not None else []
        }
        
        # DEFENSIVE CHECK: Ensure format field is always present
        if 'format' not in fallback_result:
            logger.warning(f"[FORMAT_DECK] ⚠️ Missing format field in fallback result, adding it")
            fallback_result['format'] = 'deck'
        
        # CRITICAL VALIDATION: Ensure fallback slides are not empty
        if not fallback_result.get('slides') or len(fallback_result.get('slides', [])) == 0:
            logger.error(f"[FORMAT_DECK] ❌ CRITICAL ERROR: Fallback slide generation returned EMPTY slides!")
            logger.error(f"[FORMAT_DECK] ❌ Companies available: {len(data.get('companies', []))}")
            logger.error(f"[FORMAT_DECK] ❌ Data keys: {list(data.keys())}")
            logger.error(f"[FORMAT_DECK] ❌ Fallback slides generated: {len(fallback_slides)}")
            
            # Try to generate minimal error deck
            logger.error(f"[FORMAT_DECK] ❌ Attempting to generate minimal error deck...")
            error_slides = [
                {
                    "id": "error-slide-1",
                    "order": 1,
                    "template": "title",
                    "content": {
                        "title": "Deck Generation Error",
                        "subtitle": "Unable to generate slides",
                        "body": "An error occurred during deck generation. Please try again."
                    }
                }
            ]
            fallback_result['slides'] = error_slides
            fallback_result['metadata']['error'] = 'fallback_generation_failed'
            logger.warning(f"[FORMAT_DECK] ⚠️ Generated minimal error deck with {len(error_slides)} slides")
        
        logger.info(f"[FORMAT_DECK] ✅ Fallback validation passed: returning {len(fallback_result['slides'])} slides")
        
        # CRITICAL: Normalize slides before returning (ensure type -> template conversion)
        if fallback_result.get('slides'):
            fallback_result['slides'] = self._normalize_slide_format(fallback_result['slides'])
        
        # Ensure all required fields are present and properly typed
        fallback_result.setdefault('format', 'deck')
        fallback_result.setdefault('slides', [])
        fallback_result.setdefault('theme', 'professional')
        fallback_result.setdefault('metadata', {})
        fallback_result.setdefault('citations', [])
        fallback_result.setdefault('charts', [])
        fallback_result.setdefault('companies', [])
        
        # Ensure all list fields are actually lists (not None)
        if fallback_result['slides'] is None:
            fallback_result['slides'] = []
        if fallback_result['citations'] is None:
            fallback_result['citations'] = []
        if fallback_result['charts'] is None:
            fallback_result['charts'] = []
        if fallback_result['companies'] is None:
            fallback_result['companies'] = []
        
        # Final return - all code paths above ensure this returns a properly formatted dict
        # with normalized slides, consistent field types, and all required fields present
        logger.info(f"[FORMAT_DECK] ✅ Final return: format={fallback_result.get('format')}, slides={len(fallback_result.get('slides', []))}, companies={len(fallback_result.get('companies', []))}")
        return fallback_result
    
    def _format_matrix(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Format data for matrix output"""
        # If data already has matrix structure from MatrixQueryOrchestrator, use it
        if "columns" in data and "rows" in data:
            return {
                "format": "matrix",
                "columns": data.get("columns", []),
                "rows": data.get("rows", []),
                "formulas": data.get("formulas", {}),
                "metadata": data.get("metadata", {}),
                "citations": data.get("citations", []),
                "charts": data.get("charts", [])
            }
        
        # Fallback to original formatting
        return {
            "format": "matrix",
            "type": "matrix",
            "data": data,
            "dimensions": self._generate_matrix_dimensions(data),
            "citations": data.get("citations", []),
            "charts": data.get("charts", [])
        }
    
    def _format_analysis(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Format data for analysis output"""
        # Extract skill results from their skill names
        formatted = {
            "type": "analysis",
            "companies": data.get("companies", [])
        }
        
        # Extract results from skill outputs
        if "deal-comparer" in data and isinstance(data["deal-comparer"], dict):
            formatted["comparison"] = data["deal-comparer"].get("deal_comparison", {})
        
        if "cap-table-generator" in data and isinstance(data["cap-table-generator"], dict):
            formatted["cap_tables"] = data["cap-table-generator"].get("cap_tables", {})
        
        if "portfolio-analyzer" in data and isinstance(data["portfolio-analyzer"], dict):
            formatted["portfolio_analysis"] = data["portfolio-analyzer"].get("portfolio_analysis", {})
        
        if "fund-metrics-calculator" in data and isinstance(data["fund-metrics-calculator"], dict):
            formatted["fund_metrics"] = data["fund-metrics-calculator"].get("fund_metrics", {})
        
        if "stage-analyzer" in data and isinstance(data["stage-analyzer"], dict):
            formatted["stage_analysis"] = data["stage-analyzer"].get("stage_analysis", {})
        
        if "exit-modeler" in data and isinstance(data["exit-modeler"], dict):
            formatted["exit_modeling"] = data["exit-modeler"].get("exit_modeling", {})

        if "portfolio-scenario-modeler" in data and isinstance(data["portfolio-scenario-modeler"], dict):
            formatted["fund_scenarios"] = data["portfolio-scenario-modeler"].get("fund_scenarios", {})

        if "company-health-dashboard" in data and isinstance(data["company-health-dashboard"], dict):
            formatted["portfolio_health"] = data["company-health-dashboard"].get("portfolio_health", {})
            formatted["portfolio_health_summary"] = data["company-health-dashboard"].get("summary_table", [])

        if "followon-strategy" in data and isinstance(data["followon-strategy"], dict):
            formatted["followon_strategy"] = data["followon-strategy"].get("followon_strategy", {})
            formatted["reserve_forecast"] = data["followon-strategy"].get("reserve_forecast", {})

        if "valuation-engine" in data and isinstance(data["valuation-engine"], dict):
            formatted["valuations"] = data["valuation-engine"].get("valuations", {})
        
        # Generate comprehensive summary
        formatted["summary"] = self._generate_comprehensive_summary(formatted)
        
        # Add citations and charts
        formatted["citations"] = data.get("citations", [])
        formatted["charts"] = data.get("charts", [])
        
        # Remove empty sections
        return {k: v for k, v in formatted.items() if v}
    
    def _generate_spreadsheet_columns(self, data: Dict[str, Any]) -> List[str]:
        """Generate column headers for spreadsheet"""
        return [
            "Company", "Stage", "Business Model", "Sector",
            "Revenue", "Valuation", "Total Funding", 
            "Team Size", "Founded", "Gross Margin", "Growth Rate"
        ]
    
    def _generate_spreadsheet_rows(self, data: Dict[str, Any]) -> List[List[Any]]:
        """Generate rows for spreadsheet"""
        rows = []
        companies = data.get("companies", [])
        
        for company in companies:
            row = [
                company.get("company", ""),
                company.get("stage", ""),
                company.get("business_model", ""),
                company.get("sector", ""),
                company.get("revenue", 0),
                company.get("valuation", 0),
                company.get("total_funding", 0),
                company.get("team_size", 0),
                company.get("founded_year", ""),
                company.get("key_metrics", {}).get("gross_margin", 0),
                company.get("revenue_growth", 0)
            ]
            rows.append(row)
        
        return rows
    
    def _generate_slides(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate frontend-compatible fallback slides"""
        raw_companies = data.get("companies", []) or []
        companies = [c for c in raw_companies if isinstance(c, dict)]
        slides: List[Dict[str, Any]] = []

        def add_slide(slide_type: str, content: Dict[str, Any]) -> None:
            slides.append({
                "id": f"fallback-slide-{len(slides) + 1}",
                "order": len(slides) + 1,
                "template": slide_type,  # Use "template" to match deck-storytelling format
                "content": content
            })

        add_slide(
            "title",
            {
                "title": "Investment Analysis",
                "subtitle": f"{len(companies)} companies analyzed" if companies else "No company data available",
                "date": datetime.now().strftime("%B %d, %Y")
            }
        )

        if not companies:
            add_slide(
                "summary",
                {
                    "title": "Awaiting Company Inputs",
                    "body": "No company records were returned by the upstream skills. Re-run the request or provide explicit tickers/handles to generate a deck.",
                    "bullets": [
                        "Example prompt: Compare @Mercury and @Brex for a $500M growth fund",
                        "Ensure the backend unified-brain service is running"
                    ]
                }
            )
            return slides

        total_funding = sum(safe_get_value(c.get("total_funding"), 0) or 0 for c in companies)
        total_valuation = sum(safe_get_value(c.get("valuation"), 0) or 0 for c in companies)
        avg_valuation = total_valuation / len(companies) if companies else 0

        add_slide(
            "overview",
            {
                "title": "Market Overview",
                "metrics": {
                    "Total Funding Raised": self._format_money(total_funding),
                    "Aggregate Valuation": self._format_money(total_valuation),
                    "Average Valuation": self._format_money(avg_valuation),
                    "Companies Analyzed": str(len(companies))
                },
                "bullets": [
                    "Fallback deck generated from available structured data",
                    "Charts and detailed storytelling require successful deck skill execution"
                ]
            }
        )

        for idx, company in enumerate(companies[:4]):
            revenue = self._get_field_with_fallback(company, 'revenue', 0)
            valuation = self._get_field_with_fallback(company, 'valuation', 0)
            funding = safe_get_value(company.get('total_funding'), 0) or 0
            metrics = {
                "Stage": company.get('stage', 'Unknown'),
                "Valuation": self._format_money(valuation),
                "Revenue": self._format_money(revenue),
                "Total Funding": self._format_money(funding),
                "Headcount": str(company.get('team_size', 'Unknown')),
                "Headquarters": company.get('headquarters', company.get('geography', 'Unknown'))
            }

            bullets = []
            if company.get('business_model'):
                bullets.append(company['business_model'])
            if company.get('customers'):
                customers = company['customers']
                if isinstance(customers, list):
                    bullets.append(f"Customers: {', '.join(customers[:3])}{'…' if len(customers) > 3 else ''}")
            if company.get('recent_news'):
                news = company['recent_news']
                if isinstance(news, list) and news:
                    bullets.append(f"Latest: {news[0]}")

            add_slide(
                "company_profile",
                {
                    "title": company.get('company', f"Company {idx + 1}"),
                    "subtitle": company.get('sector') or company.get('category'),
                    "body": company.get('description') or company.get('summary'),
                    "metrics": metrics,
                    "bullets": bullets,
                    "notes": company.get('website_url')
                }
            )

        valuations = [
            max(0, float(safe_get_value(company.get('valuation'), 0) or 0))
            for company in companies[:6]
        ]
        labels = [company.get('company', f"Company {idx + 1}") for idx, company in enumerate(companies[:6])]

        chart_data = {
            "type": "bar",
            "title": "Reported Valuations",
            "data": {
                "labels": labels,
                "datasets": [
                    {
                        "label": "Valuation ($M)",
                        "data": [round(v / 1_000_000, 2) for v in valuations],
                        "backgroundColor": "rgba(99, 102, 241, 0.8)"
                    }
                ]
            },
            "options": {
                "plugins": {
                    "tooltip": {
                        "callbacks": {
                            "label": "function(ctx){return ctx.dataset.label + ': $' + ctx.parsed.y + 'M';}"
                        }
                    }
                }
            }
        }

        add_slide(
            "chart",
            {
                "title": "Valuation Comparison",
                "chart_data": chart_data,
                "notes": "Values shown in millions of USD."
            }
        )

        add_slide(
            "summary",
            {
                "title": "Next Steps",
                "bullets": [
                    "Refine source data so deck-storytelling skill can render full narrative",
                    "Export as PDF/PPT once a structured deck is available",
                    "Use dedicated TAM and PWERM prompts for deeper sections"
                ]
            }
        )

        return slides
    
    def _generate_matrix_dimensions(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Generate matrix dimensions"""
        return {
            "x_axis": "companies",
            "y_axis": "metrics"
        }
    
    def _calculate_followon_scenarios(self, 
                                      initial_investment: float,
                                      initial_ownership: float,
                                      exit_multiple: float,
                                      rounds_to_exit: int = 3,
                                      dilution_per_round: float = 0.20,
                                      reserve_ratio: float = 2.0) -> Dict[str, Any]:
        """
        Calculate investment returns with and without follow-on investments
        WITH DETAILED PER-ROUND REQUIREMENTS
        
        Args:
            initial_investment: Initial check size
            initial_ownership: Initial ownership percentage (0-1)
            exit_multiple: Expected exit multiple on post-money valuation
            rounds_to_exit: Number of funding rounds until exit
            dilution_per_round: Dilution per funding round (0-1)
            reserve_ratio: Multiple of initial for reserves (2.0 = 2x)
        
        Returns:
            Dict with scenarios for with/without follow-on, including per-round details
        """
        scenarios = {}
        
        # Track round-by-round details
        round_details = []
        
        # Scenario 1: No follow-on (get diluted each round)
        no_followon_ownership = initial_ownership
        for _ in range(rounds_to_exit):
            no_followon_ownership *= (1 - dilution_per_round)
        
        # Calculate exit value based on diluted ownership
        implied_post_money = initial_investment / initial_ownership
        exit_valuation = implied_post_money * exit_multiple * ((1 + 0.5) ** rounds_to_exit)  # Assume 50% growth per round
        no_followon_proceeds = exit_valuation * no_followon_ownership
        no_followon_multiple = no_followon_proceeds / initial_investment
        
        scenarios["no_followon"] = {
            "capital_deployed": initial_investment,
            "final_ownership": no_followon_ownership,
            "exit_proceeds": no_followon_proceeds,
            "multiple": no_followon_multiple,
            "irr": ((no_followon_multiple ** (1 / (rounds_to_exit * 1.5))) - 1) * 100  # Assume 18mo per round
        }
        
        # Scenario 2: With follow-on (maintain pro-rata up to reserves)
        with_followon_ownership = initial_ownership
        total_invested = initial_investment
        remaining_reserves = initial_investment * (reserve_ratio - 1)
        
        # Track ownership through rounds
        current_ownership = initial_ownership
        
        for round_num in range(rounds_to_exit):
            # Calculate round details
            round_name = ["Series B", "Series C", "Series D", "Series E"][round_num] if round_num < 4 else f"Round {round_num + 1}"
            current_valuation = implied_post_money * ((1 + 0.5) ** (round_num + 1))
            round_size = current_valuation * dilution_per_round / (1 - dilution_per_round)
            
            # Calculate pro-rata amount to maintain ownership
            pro_rata_amount = round_size * current_ownership
            
            # Determine actual investment this round
            if pro_rata_amount <= remaining_reserves:
                # Can maintain full ownership
                actual_investment = pro_rata_amount
                total_invested += pro_rata_amount
                remaining_reserves -= pro_rata_amount
                new_ownership = current_ownership  # Maintained
                dilution_this_round = 0
            else:
                # Partial or no follow-on
                if remaining_reserves > 0:
                    actual_investment = remaining_reserves
                    total_invested += remaining_reserves
                    # Calculate partial dilution
                    participation_rate = remaining_reserves / pro_rata_amount if pro_rata_amount > 0 else 0
                    dilution_this_round = dilution_per_round * (1 - participation_rate)
                    new_ownership = current_ownership * (1 - dilution_this_round)
                    remaining_reserves = 0
                else:
                    # No reserves left, full dilution
                    actual_investment = 0
                    dilution_this_round = dilution_per_round
                    new_ownership = current_ownership * (1 - dilution_per_round)
            
            # Store round details
            round_details.append({
                "round": round_name,
                "pre_money_valuation": current_valuation * (1 - dilution_per_round),
                "post_money_valuation": current_valuation,
                "round_size": round_size,
                "ownership_before": current_ownership,
                "ownership_after": new_ownership,
                "dilution": dilution_this_round,
                "pro_rata_required": pro_rata_amount,
                "actual_investment": actual_investment,
                "participation_rate": (actual_investment / pro_rata_amount * 100) if pro_rata_amount > 0 else 0
            })
            
            current_ownership = new_ownership
        
        with_followon_ownership = current_ownership
        with_followon_proceeds = exit_valuation * with_followon_ownership
        with_followon_multiple = with_followon_proceeds / total_invested if total_invested > 0 else 0
        
        scenarios["with_followon"] = {
            "capital_deployed": total_invested,
            "final_ownership": with_followon_ownership,
            "exit_proceeds": with_followon_proceeds,
            "multiple": with_followon_multiple,
            "irr": ((with_followon_multiple ** (1 / (rounds_to_exit * 1.5))) - 1) * 100,  # Assume 18mo per round
            "round_details": round_details  # NEW: Detailed per-round requirements
        }
        
        # Calculate delta
        scenarios["delta"] = {
            "additional_capital": total_invested - initial_investment,
            "ownership_preserved": with_followon_ownership - no_followon_ownership,
            "additional_proceeds": with_followon_proceeds - no_followon_proceeds,
            "multiple_delta": with_followon_multiple - no_followon_multiple,
            "follow_on_decision": "FOLLOW" if with_followon_multiple > no_followon_multiple else "PASS"
        }
        
        # Add summary of follow-on requirements
        scenarios["followon_summary"] = {
            "total_reserves_needed": sum(r["pro_rata_required"] for r in round_details),
            "total_reserves_allocated": initial_investment * (reserve_ratio - 1),
            "can_maintain_ownership": sum(r["pro_rata_required"] for r in round_details) <= initial_investment * (reserve_ratio - 1),
            "rounds_with_full_participation": sum(1 for r in round_details if r["participation_rate"] >= 100),
            "rounds_with_partial_participation": sum(1 for r in round_details if 0 < r["participation_rate"] < 100),
            "rounds_with_no_participation": sum(1 for r in round_details if r["participation_rate"] == 0)
        }
        
        return scenarios
    
    def _calculate_investor_specific_exit_scenarios(self,
                                                   company_data: Dict[str, Any],
                                                   our_investment: float,
                                                   check_size: float,
                                                   stage: str) -> Dict[str, Any]:
        """
        Calculate comprehensive exit scenarios using actual cap table reconstruction
        Shows exactly where our investment sits in the preference stack
        
        Args:
            company_data: Company data with funding rounds
            our_investment: Our proposed investment amount
            check_size: Our check size (may differ from investment amount)
            stage: Company stage for dynamic calculations
            
        Returns:
            Dict with investor-specific exit scenarios and breakpoints
        """

        # Guard against zero/negative investment inputs which create undefined ratios
        valuation_for_defaults = company_data.get('valuation', 100_000_000) or 100_000_000
        minimum_check_size = max(valuation_for_defaults * 0.05, 1_000_000)

        if not our_investment or our_investment <= 0:
            our_investment = minimum_check_size

        if not check_size or check_size <= 0:
            check_size = our_investment

        our_investment = max(our_investment, minimum_check_size)
        check_size = max(check_size, our_investment)

        # Persist normalized value for downstream consumers
        company_data.setdefault('optimal_check_size', check_size)

        # 1. Get full cap table reconstruction
        try:
            cap_table_data = self.cap_table_service.calculate_full_cap_table_history(company_data)
            if not cap_table_data:
                cap_table_data = {"history": [], "ownership_evolution": {}, "current_cap_table": {}}
        except Exception as e:
            logger.warning(f"Cap table calculation failed: {e}")
            cap_table_data = {"history": [], "ownership_evolution": {}, "current_cap_table": {}}
        
        # 2. Extract current ownership distribution
        current_ownership = cap_table_data.get('current_cap_table', {})
        
        # Calculate common ownership percentage (founders + employees)
        common_ownership_pct = sum(
            v for k, v in current_ownership.items() 
            if any(term in k for term in ['Founder', 'Employee', 'Common', 'Option'])
        ) / 100.0 if current_ownership else 0.25  # Default 25% if no data
        
        # 3. Get funding history for preference stack
        funding_rounds = company_data.get('funding_rounds', [])
        total_liquidation_prefs = sum(
            round_data.get('amount', 0) * round_data.get('liquidation_preference', 1.0)
            for round_data in funding_rounds
        )
        
        # Add our investment to the stack
        our_liquidation_pref = our_investment * 1.0  # Assume 1x non-participating
        total_prefs_with_us = total_liquidation_prefs + our_liquidation_pref
        
        # 4. Calculate dynamic common threshold
        # This is where common starts receiving meaningful proceeds
        meaningful_common_amounts = {
            'Seed': 500_000,
            'Series A': 2_000_000,
            'Series B': 5_000_000, 
            'Series C': 10_000_000,
            'Series D': 20_000_000
        }
        
        meaningful_amount = meaningful_common_amounts.get(stage, 2_000_000)
        
        # Common threshold = liquidation prefs + (meaningful amount / common ownership %)
        if common_ownership_pct > 0:
            common_threshold = total_prefs_with_us + (meaningful_amount / common_ownership_pct)
        else:
            common_threshold = total_prefs_with_us * 2  # Fallback
            
        # 5. Calculate our entry and exit ownership
        current_valuation = company_data.get('valuation', 100_000_000)
        post_money = current_valuation + our_investment
        our_entry_ownership = our_investment / post_money
        
        # Estimate dilution through future rounds using benchmark-driven approach
        rounds_to_exit = {'Seed': 4, 'Series A': 3, 'Series B': 2, 'Series C': 1}.get(stage, 2)
        
        # Get quality-adjusted dilution from benchmarks (not fixed 18%)
        from app.services.intelligent_gap_filler import IntelligentGapFiller
        gap_filler = IntelligentGapFiller(fund_profile={})
        
        # Calculate quality score to determine dilution rate
        quality_mult = 1.0
        investors = company_data.get('investors', [])
        tier1_vcs = ['sequoia', 'a16z', 'benchmark', 'accel', 'greylock']
        tier2_vcs = ['menlo', 'redpoint', 'norwest', 'ivp', 'sapphire']
        
        has_tier1 = any(t1 in str(investors).lower() for t1 in tier1_vcs)
        has_tier2 = any(t2 in str(investors).lower() for t2 in tier2_vcs)
        
        # Performance relative to benchmarks affects dilution
        revenue = company_data.get('revenue') or company_data.get('arr') or company_data.get('inferred_revenue', 0)
        valuation = company_data.get('valuation', 100_000_000)
        current_multiple = self._safe_divide(valuation, revenue, default=20)
        
        # Dilution varies based on company quality and investor interest
        if has_tier1 and current_multiple < 15:
            # High quality, reasonable valuation = less dilution
            dilution_per_round = 0.15
        elif has_tier2 or current_multiple < 20:
            # Good quality = standard dilution  
            dilution_per_round = 0.18
        else:
            # Lower quality or high valuation = more dilution
            dilution_per_round = 0.22
            
        our_exit_ownership_no_followon = our_entry_ownership * ((1 - dilution_per_round) ** rounds_to_exit)
        
        # With follow-on (maintain pro-rata)
        our_exit_ownership_with_followon = our_entry_ownership
        
        # 6. Build investor stack (who gets paid in what order)
        investor_stack = []
        
        # Add existing investors from funding rounds
        for round_data in funding_rounds:
            investor_stack.append({
                'investor': round_data.get('lead_investor', f"{round_data.get('round', 'Unknown')} Investors"),
                'amount': round_data.get('amount', 0),
                'liquidation_preference': round_data.get('amount', 0) * round_data.get('liquidation_preference', 1.0),
                'participating': round_data.get('participating', False),
                'seniority': funding_rounds.index(round_data)  # Lower index = earlier round = less senior
            })
            
        # Add our investment to the stack
        investor_stack.append({
            'investor': 'Our Investment',
            'amount': our_investment,
            'liquidation_preference': our_liquidation_pref,
            'participating': False,
            'seniority': len(funding_rounds),  # We're the newest = most senior NOW
            'current_position': 1,  # Senior position today
            'position_after_next_round': 2,  # Will be pushed down
            'position_at_exit': 1 + rounds_to_exit  # Final position depends on rounds
        })
        
        # Sort by seniority (LIFO - last in, first out)
        # Higher seniority number = more recent investment = gets paid first
        investor_stack.sort(key=lambda x: x['seniority'], reverse=True)
        
        # 7. Calculate proceeds at various exit values WITH future rounds
        exit_scenarios = []
        exit_multiples = [0.5, 1.0, 2.0, 3.0, 5.0, 10.0]
        
        # Project future rounds that will be senior to us
        future_rounds_prefs = 0
        if rounds_to_exit > 0:
            # Each future round raises progressively more
            next_round_size = total_liquidation_prefs * 1.5  # Typical next round is 1.5x previous
            for i in range(rounds_to_exit):
                future_rounds_prefs += next_round_size
                next_round_size *= 1.8  # Each subsequent round ~80% larger
        
        # Total preferences INCLUDING future rounds
        total_prefs_at_exit = total_prefs_with_us + future_rounds_prefs
        
        for multiple in exit_multiples:
            exit_value = current_valuation * multiple
            
            # Calculate waterfall distribution WITH future investors
            remaining = exit_value
            our_proceeds = 0
            common_proceeds = 0
            
            # First pay future investors (they're senior to us)
            if remaining >= future_rounds_prefs:
                remaining -= future_rounds_prefs
            else:
                # Not enough to pay future investors, we get nothing
                remaining = 0
            
            # Then pay existing stack (including us)
            if remaining > 0:
                for investor in investor_stack:
                    liq_pref = investor['liquidation_preference']
                    if remaining >= liq_pref:
                        if investor['investor'] == 'Our Investment':
                            our_proceeds = liq_pref
                        remaining -= liq_pref
                    else:
                        # Pro-rata distribution if not enough
                        if investor['investor'] == 'Our Investment':
                            our_proceeds = remaining * (liq_pref / sum(i['liquidation_preference'] for i in investor_stack))
                        remaining = 0
                        break
                    
            # Distribute remaining to common (simplified - ignores participation)
            if remaining > 0:
                # Our share of common (if we convert)
                our_common_share = remaining * our_exit_ownership_no_followon
                # Compare to staying as preferred
                if our_common_share > our_proceeds:
                    our_proceeds = our_common_share
                    
                # Common shareholders get their portion
                common_proceeds = remaining * common_ownership_pct
                
            exit_scenarios.append({
                'exit_value': exit_value,
                'exit_multiple': multiple,
                'our_proceeds': our_proceeds,
                'our_moic': our_proceeds / check_size if check_size > 0 else 0,
                'common_proceeds': common_proceeds,
                'liquidation_prefs_paid': min(exit_value, total_prefs_with_us),
                'conversion_triggered': our_proceeds > our_liquidation_pref
            })
            
        # 8. Identify INVESTOR-FOCUSED breakpoints (defensive positioning)
        def safe_ratio(numerator: float, denominator: float) -> Optional[float]:
            if denominator and denominator > 0:
                value = numerator / denominator
                return value if math.isfinite(value) else None
            return None

        conversion_threshold_ratio = safe_ratio(future_rounds_prefs + our_liquidation_pref, our_exit_ownership_no_followon)

        # Calculate breakpoints WITHOUT pro rata (no follow-on investment)
        breakpoints_no_pro_rata = {
            # Defensive breakpoints - what exit values we need to get our money back
            'return_of_capital_after_dilution': safe_ratio(check_size, our_exit_ownership_no_followon),
            'breakeven_after_future_prefs': future_rounds_prefs + our_liquidation_pref,  # Exit value needed after future rounds take their prefs

            # Return thresholds - dynamic based on actual investment
            'exit_for_1x_return': safe_ratio(check_size + future_rounds_prefs, our_exit_ownership_no_followon),
            'exit_for_2x_return': safe_ratio(check_size * 2 + future_rounds_prefs, our_exit_ownership_no_followon),
            'exit_for_3x_return': safe_ratio(check_size * 3 + future_rounds_prefs, our_exit_ownership_no_followon),
            'exit_for_5x_return': safe_ratio(check_size * 5 + future_rounds_prefs, our_exit_ownership_no_followon),
            'exit_for_10x_return': safe_ratio(check_size * 10 + future_rounds_prefs, our_exit_ownership_no_followon),
        }
        
        # Calculate breakpoints WITH pro rata (maintaining ownership through follow-on investments)
        # With pro rata, we maintain our_entry_ownership, so calculations are simpler
        breakpoints_with_pro_rata = {
            'exit_for_1x_return': safe_ratio(check_size, our_exit_ownership_with_followon),
            'exit_for_2x_return': safe_ratio(check_size * 2, our_exit_ownership_with_followon),
            'exit_for_3x_return': safe_ratio(check_size * 3, our_exit_ownership_with_followon),
            'exit_for_5x_return': safe_ratio(check_size * 5, our_exit_ownership_with_followon),
            'exit_for_10x_return': safe_ratio(check_size * 10, our_exit_ownership_with_followon),
        }
        
        breakpoints = {
            # Without pro rata (no follow-on)
            **{f'{k}_no_pro_rata': v for k, v in breakpoints_no_pro_rata.items()},
            # With pro rata (maintaining ownership)
            **{f'{k}_with_pro_rata': v for k, v in breakpoints_with_pro_rata.items()},
            # Legacy fields for backward compatibility (use no_pro_rata values)
            'return_of_capital_after_dilution': breakpoints_no_pro_rata['return_of_capital_after_dilution'],
            'breakeven_after_future_prefs': future_rounds_prefs + our_liquidation_pref,
            'exit_for_2x_return': breakpoints_no_pro_rata['exit_for_2x_return'],
            'exit_for_3x_return': breakpoints_no_pro_rata['exit_for_3x_return'],
            'target_3x_exit': breakpoints_no_pro_rata['exit_for_3x_return'],  # For backward compatibility
            'target_2x_exit': breakpoints_no_pro_rata['exit_for_2x_return'],  # For backward compatibility
            'target_1x_exit': breakpoints_no_pro_rata['exit_for_1x_return'],  # For backward compatibility

            # Stack position - where we sit in the preference stack
            'total_prefs_ahead_of_us': future_rounds_prefs,  # How much gets paid before us
            'total_prefs_including_us': total_prefs_with_us,  # Total liquidation preferences
            'total_prefs_at_exit': total_prefs_at_exit,  # Including projected future rounds

            # Conversion decision points
            'conversion_threshold': conversion_threshold_ratio if conversion_threshold_ratio is not None else (total_prefs_at_exit * 2),
            'conversion_always_better': total_prefs_at_exit * 1.5,  # Above this, always convert

            # Reserve planning for follow-on
            'next_round_requirement': next_round_size * our_entry_ownership if rounds_to_exit > 0 else 0,
            'total_reserves_to_maintain_ownership': sum(next_round_size * ((1-dilution_per_round)**i) * our_entry_ownership * (1.8**i) for i in range(rounds_to_exit)),

            # Key exit values (dynamic based on current valuation)
            'exit_at_2x_valuation': current_valuation * 2,
            'exit_at_5x_valuation': current_valuation * 5,
            'exit_at_10x_valuation': current_valuation * 10
        }
        
        return {
            'cap_table_data': cap_table_data,
            'investor_stack': investor_stack,
            'exit_scenarios': exit_scenarios,
            'breakpoints': breakpoints,
            'ownership_analysis': {
                'common_ownership_pct': common_ownership_pct * 100,
                'our_entry_ownership': our_entry_ownership * 100,
                'our_exit_ownership_no_followon': our_exit_ownership_no_followon * 100,
                'our_exit_ownership_with_followon': our_exit_ownership_with_followon * 100
            },
            'preference_analysis': {
                'total_existing_preferences': total_liquidation_prefs,
                'our_preference': our_liquidation_pref,
                'total_with_us': total_prefs_with_us,
                'our_position_in_stack': 1,  # Current position (senior)
                'position_at_exit': 1 + rounds_to_exit,  # Position after future rounds
                'dilution_scenario': f"{dilution_per_round*100:.0f}% per round based on {'Tier 1 investors' if has_tier1 else 'standard quality'}"
            }
        }
    
    def _assess_scenario_probabilities(self, win_lose_scenarios: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Assess probabilities for win/lose scenarios in competitive landscape analysis
        """
        try:
            if not win_lose_scenarios:
                return {
                    "total_scenarios": 0,
                    "win_probability": 0.0,
                    "lose_probability": 0.0,
                    "scenario_breakdown": []
                }
            
            total_scenarios = len(win_lose_scenarios)
            win_count = 0
            lose_count = 0
            scenario_breakdown = []
            
            for scenario in win_lose_scenarios:
                scenario_type = scenario.get('type', 'unknown').lower()
                probability = scenario.get('probability', 0.0)
                
                if 'win' in scenario_type or 'success' in scenario_type:
                    win_count += 1
                elif 'lose' in scenario_type or 'failure' in scenario_type:
                    lose_count += 1
                
                scenario_breakdown.append({
                    "type": scenario_type,
                    "probability": probability,
                    "description": scenario.get('description', '')
                })
            
            win_probability = win_count / total_scenarios if total_scenarios > 0 else 0.0
            lose_probability = lose_count / total_scenarios if total_scenarios > 0 else 0.0
            
            return {
                "total_scenarios": total_scenarios,
                "win_probability": win_probability,
                "lose_probability": lose_probability,
                "scenario_breakdown": scenario_breakdown,
                "confidence": "medium" if total_scenarios >= 3 else "low"
            }
            
        except Exception as e:
            logger.warning(f"Error assessing scenario probabilities: {e}")
            return {
                "total_scenarios": 0,
                "win_probability": 0.0,
                "lose_probability": 0.0,
                "scenario_breakdown": [],
                "error": str(e)
            }

    def _generate_fallback_pwerm_scenarios(self, company_data: Dict[str, Any], base_valuation: float) -> List[Dict[str, Any]]:
        """Generate fallback PWERM scenarios when not available from valuation service"""
        stage = company_data.get('stage', 'Series A')
        
        # Basic scenario templates based on stage
        if stage in ['Seed', 'Pre-Seed']:
            scenarios = [
                {"scenario": "IPO", "probability": 0.02, "exit_value": base_valuation * 50, "time_to_exit": 8},
                {"scenario": "Strategic Acquisition", "probability": 0.08, "exit_value": base_valuation * 15, "time_to_exit": 5},
                {"scenario": "Good Exit", "probability": 0.20, "exit_value": base_valuation * 5, "time_to_exit": 4},
                {"scenario": "Modest Exit", "probability": 0.30, "exit_value": base_valuation * 2, "time_to_exit": 3},
                {"scenario": "Break-even", "probability": 0.25, "exit_value": base_valuation * 1, "time_to_exit": 2},
                {"scenario": "Loss", "probability": 0.15, "exit_value": base_valuation * 0.5, "time_to_exit": 2}
            ]
        elif stage in ['Series A', 'Series B']:
            scenarios = [
                {"scenario": "IPO", "probability": 0.05, "exit_value": base_valuation * 20, "time_to_exit": 6},
                {"scenario": "Strategic Premium", "probability": 0.15, "exit_value": base_valuation * 10, "time_to_exit": 4},
                {"scenario": "Strategic Exit", "probability": 0.25, "exit_value": base_valuation * 5, "time_to_exit": 3},
                {"scenario": "PE Buyout", "probability": 0.25, "exit_value": base_valuation * 3, "time_to_exit": 3},
                {"scenario": "Modest Return", "probability": 0.20, "exit_value": base_valuation * 1.5, "time_to_exit": 2},
                {"scenario": "Flat/Down", "probability": 0.10, "exit_value": base_valuation * 0.8, "time_to_exit": 2}
            ]
        else:  # Series C+
            scenarios = [
                {"scenario": "IPO", "probability": 0.15, "exit_value": base_valuation * 10, "time_to_exit": 4},
                {"scenario": "Large Strategic", "probability": 0.25, "exit_value": base_valuation * 5, "time_to_exit": 3},
                {"scenario": "PE Buyout", "probability": 0.30, "exit_value": base_valuation * 3, "time_to_exit": 2},
                {"scenario": "Secondary Sale", "probability": 0.20, "exit_value": base_valuation * 2, "time_to_exit": 2},
                {"scenario": "Flat Exit", "probability": 0.10, "exit_value": base_valuation * 1.2, "time_to_exit": 1}
            ]
        
        return scenarios

    def _generate_probability_cloud_data(self, company_data: Dict[str, Any], check_size: float) -> Dict[str, Any]:
        """
        Generate probability cloud data for frontend visualization.
        This creates the exact structure expected by renderProbabilityCloud in TableauLevelCharts.
        """
        try:
            # Get PWERM scenarios if available
            scenarios = company_data.get('pwerm_scenarios', [])
            if not scenarios:
                # Generate scenarios if not already calculated
                stage_map = {
                    "Pre-Seed": Stage.PRE_SEED,
                    "Pre Seed": Stage.PRE_SEED,
                    "Seed": Stage.SEED,
                    "Series A": Stage.SERIES_A,
                    "Series B": Stage.SERIES_B,
                    "Series C": Stage.SERIES_C,
                    "Growth": Stage.GROWTH,
                    "Late": Stage.LATE
                }
                
                company_stage = stage_map.get(company_data.get("stage", "Series A"), Stage.SERIES_A)
                
                # Use inferred_revenue if revenue is None - CRITICAL FIX
                revenue = ensure_numeric(company_data.get("revenue"), 0)
                if revenue == 0:
                    revenue = ensure_numeric(company_data.get("inferred_revenue"), 0)
                    if revenue == 0:
                        revenue = ensure_numeric(company_data.get("arr"), 0)
                        if revenue == 0:
                            revenue = ensure_numeric(company_data.get("inferred_arr"), 1_000_000)
                
                # Use inferred_growth_rate if growth_rate is None
                growth_rate = ensure_numeric(company_data.get("growth_rate"), 0)
                if growth_rate == 0:
                    growth_rate = ensure_numeric(company_data.get("inferred_growth_rate"), 1.5)
                
                # Use inferred_valuation if valuation is None - CRITICAL FIX
                valuation = ensure_numeric(company_data.get("valuation"), 0)
                if valuation == 0:
                    valuation = ensure_numeric(company_data.get("inferred_valuation"), 0)
                    if valuation == 0:
                        # Calculate from total_funding as fallback
                        valuation = ensure_numeric(company_data.get("total_funding"), 0) * 3
                
                # Extract inferred_valuation if available
                inferred_val = ensure_numeric(company_data.get("inferred_valuation"), None) if company_data.get("inferred_valuation") is not None else None
                val_request = ValuationRequest(
                    company_name=company_data.get("company", "Unknown"),
                    stage=company_stage,
                    revenue=revenue,
                    growth_rate=growth_rate,
                    last_round_valuation=valuation if valuation and valuation > 0 else None,
                    inferred_valuation=inferred_val,
                    total_raised=self._get_field_safe(company_data, "total_funding")
                )
                
                scenarios = self.valuation_engine._generate_exit_scenarios(val_request)
                self.valuation_engine.annotate_scenarios_with_returns(scenarios, val_request)
            
            # Calculate our investment metrics
            valuation = company_data.get('valuation', 100_000_000)
            our_entry_ownership = check_size / valuation if valuation > 0 else 0.08
            
            # Model cap table evolution for each scenario
            our_investment = {
                'amount': check_size,
                'ownership': our_entry_ownership
            }
            for scenario in scenarios:
                self.valuation_engine.model_cap_table_evolution(
                    scenario, 
                    company_data, 
                    our_investment
                )
            
            # Calculate breakpoint distributions (probability ranges)
            breakpoint_distributions = self.valuation_engine.calculate_breakpoint_distributions(scenarios)
            
            # Generate return curves for scenarios
            self.valuation_engine.generate_return_curves(scenarios, our_investment)
            
            # Format breakpoint clouds for frontend
            breakpoint_clouds = []
            
            # Return of capital (defensive position)
            if 'our_breakeven' in breakpoint_distributions:
                dist = breakpoint_distributions['our_breakeven']
                breakpoint_clouds.append({
                    'type': 'return_of_capital',
                    'label': 'Return of Capital',
                    'median': dist['median'],
                    'p10_p90': [dist['p10'], dist['p90']],
                    'p25_p75': [dist['p25'], dist['p75']],
                    'color': '#ef4444'  # Red for breakeven
                })
            
            # 3x return threshold
            if 'our_3x' in breakpoint_distributions:
                dist = breakpoint_distributions['our_3x']
                breakpoint_clouds.append({
                    'type': '3x_return',
                    'label': '3x Return',
                    'median': dist['median'],
                    'p10_p90': [dist['p10'], dist['p90']],
                    'p25_p75': [dist['p25'], dist['p75']],
                    'color': '#22c55e'  # Green for good returns
                })
            
            # Liquidation preference satisfaction
            if 'liquidation_satisfied' in breakpoint_distributions:
                dist = breakpoint_distributions['liquidation_satisfied']
                breakpoint_clouds.append({
                    'type': 'liquidation_cleared',
                    'label': 'Liquidation Cleared',
                    'median': dist['median'],
                    'p10_p90': [dist['p10'], dist['p90']],
                    'p25_p75': [dist['p25'], dist['p75']],
                    'color': '#3b82f6'  # Blue for liquidation
                })
            
            # Format scenario curves (top 10 most probable)
            sorted_scenarios = sorted(scenarios, key=lambda s: s.probability, reverse=True)[:10]
            scenario_curves = []
            
            for scenario in sorted_scenarios:
                if hasattr(scenario, 'return_curve') and scenario.return_curve:
                    # Determine color based on exit type
                    if scenario.exit_type and 'IPO' in scenario.exit_type:
                        color = '#10b981'  # Green for IPO
                    elif scenario.exit_type and 'Downside' in scenario.exit_type:
                        color = '#ef4444'  # Red for downside
                    else:
                        color = '#f59e0b'  # Amber for M&A
                    
                    # Convert return_curve format to list of {x, y} points
                    curve_points = []
                    if isinstance(scenario.return_curve, dict) and 'exit_values' in scenario.return_curve and 'return_multiples' in scenario.return_curve:
                        exit_values = scenario.return_curve['exit_values']
                        return_multiples = scenario.return_curve['return_multiples']
                        # Ensure both are lists and same length
                        if isinstance(exit_values, list) and isinstance(return_multiples, list) and len(exit_values) == len(return_multiples):
                            for i in range(len(exit_values)):
                                curve_points.append({
                                    'x': exit_values[i],
                                    'y': return_multiples[i]
                                })
                    
                    # Only add if we have valid curve points
                    if curve_points and len(curve_points) > 0:
                        # Frontend expects return_curve with exit_values and return_multiples arrays
                        exit_values = [pt['x'] for pt in curve_points]
                        return_multiples = [pt['y'] for pt in curve_points]
                        
                        scenario_curves.append({
                            'name': scenario.scenario,  # Frontend expects 'name', not 'scenario'
                            'scenario': scenario.scenario,  # Keep for backwards compatibility
                            'probability': scenario.probability,
                            'exit_type': scenario.exit_type or 'M&A',
                            'return_curve': {  # Frontend expects return_curve structure
                                'exit_values': exit_values,
                                'return_multiples': return_multiples
                            },
                            'curve': curve_points,  # Keep for backwards compatibility
                            'color': color
                        })
            
            # Calculate decision zones based on breakpoints
            # Frontend expects decision_zones with 'range' array [start, end] instead of 'start' and 'end'
            decision_zones = []
            
            # Loss zone (below our investment)
            loss_threshold = breakpoint_distributions.get('our_breakeven', {}).get('p25', check_size * 2)
            decision_zones.append({
                'range': [0, loss_threshold],  # Frontend expects 'range' array
                'start': 0,  # Keep for backwards compatibility
                'end': loss_threshold,
                'label': 'Loss Zone',
                'color': '#fee2e2',  # Light red
                'opacity': 0.1,
                'description': f'Below ${loss_threshold/1e6:.0f}M - likely loss of capital'
            })
            
            # Defensive zone (1x-3x return)
            defensive_start = loss_threshold
            defensive_end = breakpoint_distributions.get('our_3x', {}).get('median', check_size * 10)
            decision_zones.append({
                'range': [defensive_start, defensive_end],  # Frontend expects 'range' array
                'start': defensive_start,  # Keep for backwards compatibility
                'end': defensive_end,
                'label': 'Defensive Returns',
                'color': '#fef3c7',  # Light yellow
                'opacity': 0.1,
                'description': f'${defensive_start/1e6:.0f}M-${defensive_end/1e6:.0f}M - moderate returns'
            })
            
            # Conversion zone (high returns, convert to common)
            conversion_threshold = breakpoint_distributions.get('conversion_point', {}).get('median', valuation * 5)
            if conversion_threshold < defensive_end * 2:
                decision_zones.append({
                    'range': [defensive_end, 10_000_000_000],  # Frontend expects 'range' array
                    'start': defensive_end,  # Keep for backwards compatibility
                    'end': 10_000_000_000,  # Use 10B instead of infinity
                    'label': 'Home Run Zone',
                    'color': '#dcfce7',  # Light green
                    'opacity': 0.1,
                    'description': f'Above ${defensive_end/1e6:.0f}M - convert to common for maximum returns'
                })
            
            # Add configuration for chart scales
            config = {
                'x_axis': {
                    'type': 'log',
                    'min': 10_000_000,  # $10M
                    'max': 10_000_000_000,  # $10B
                    'label': 'Exit Value ($M)'
                },
                'y_axis': {
                    'type': 'linear',
                    'min': 0,
                    'max': 20,
                    'label': 'Return Multiple (MOIC)'
                }
            }
            
            # Add insights about the probability cloud
            insights = {
                'median_exit': f"${breakpoint_distributions.get('our_breakeven', {}).get('median', valuation * 3)/1e6:.0f}M",
                'breakeven_range': f"${breakpoint_distributions.get('our_breakeven', {}).get('p25', valuation)/1e6:.0f}M - ${breakpoint_distributions.get('our_breakeven', {}).get('p75', valuation * 5)/1e6:.0f}M",
                'probability_of_3x': sum(s.probability for s in scenarios if hasattr(s, 'moic') and s.moic and s.moic >= 3.0) * 100,
                'most_likely_scenario': sorted_scenarios[0].scenario if sorted_scenarios else "Unknown"
            }
            
            return {
                'scenario_curves': scenario_curves,
                'breakpoint_clouds': breakpoint_clouds,
                'decision_zones': decision_zones,
                'config': config,
                'insights': insights
            }
            
        except Exception as e:
            logger.error(f"Error generating probability cloud data: {e}")
            # Return minimal valid structure
            return {
                'scenario_curves': [],
                'breakpoint_clouds': [],
                'decision_zones': [],
                'config': {
                    'x_axis': {'type': 'log', 'min': 10_000_000, 'max': 10_000_000_000},
                    'y_axis': {'type': 'linear', 'min': 0, 'max': 20}
                },
                'insights': {}
            }
    
    def _generate_summary(self, data: Dict[str, Any]) -> str:
        """Generate summary for analysis"""
        return "Analysis complete"
    
    def _generate_comprehensive_summary(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Generate comprehensive summary of all analysis"""
        summary = {
            "overview": "",
            "key_findings": [],
            "recommendations": []
        }
        
        # Companies overview
        companies = data.get("companies", [])
        if companies:
            summary["overview"] = f"Analyzed {len(companies)} companies: {', '.join(c.get('company', '') for c in companies)}"
        
        # Key findings from each skill
        if "deal_comparison" in data:
            comp = data["deal_comparison"]
            if comp and "companies" in comp and comp["companies"]:
                top = comp["companies"][0]
                summary["key_findings"].append(f"Top ranked: {top.get('name')} with score {top.get('score')}")
        
        if "fund_metrics" in data:
            metrics = data["fund_metrics"]
            if metrics and "performance_metrics" in metrics:
                perf = metrics["performance_metrics"]
                summary["key_findings"].append(f"Fund Performance: {perf.get('dpi', 0):.1f}x DPI, {perf.get('tvpi', 0):.1f}x TVPI")
        
        if "portfolio_analysis" in data:
            portfolio = data["portfolio_analysis"]
            if portfolio and "fund_overview" in portfolio:
                overview = portfolio["fund_overview"]
                summary["key_findings"].append(
                    f"Portfolio: {overview.get('portfolio_companies')} companies, "
                    f"{overview.get('exits_completed')} exits, "
                    f"${overview.get('remaining_capital', 0)/1_000_000:.0f}M to deploy"
                )
        
        # Recommendations
        if "portfolio_analysis" in data:
            analyzed = data["portfolio_analysis"].get("analyzed_companies", [])
            for company_fit in analyzed:
                if company_fit.get("fit_score", 0) > 0.7:
                    summary["recommendations"].append(
                        f"Invest ${company_fit.get('recommended_investment', 0)/1_000_000:.1f}M in {company_fit.get('name')} "
                        f"for {company_fit.get('expected_ownership', 0)*100:.1f}% ownership"
                    )
        
        return summary
    
    async def _extract_comprehensive_profile(
        self, 
        company_name: str, 
        search_results: List[Dict[str, Any]],
        linkedin_identifier: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Extract comprehensive company profile using Claude
        This is the core extraction method that was in the missing part of the file
        
        Args:
            company_name: Name of the company (without @)
            search_results: List of Tavily search results
            linkedin_identifier: Optional LinkedIn identifier for validation
            
        Returns:
            Comprehensive company profile with all extracted data
        """
        # Initialize extracted_data to avoid undefined variable errors
        extracted_data = {"company": company_name}
        
        try:
            # Combine all search results into context
            all_content = []
            all_urls = set()
            
            for result in search_results:
                if result and 'results' in result:
                    for r in result['results']:
                        content = r.get('content', '')
                        url = r.get('url', '')
                        title = r.get('title', '')
                        published_date = r.get('published_date', r.get('publishedDate', ''))
                        
                        # Include publication date in the formatted content for Claude
                        date_info = f"\nPUBLISHED: {published_date}" if published_date else ""
                        all_content.append(f"[{title}]\nURL: {url}{date_info}\n{content}")
                        
                        # Extract domains mentioned in content
                        import re
                        domains = re.findall(
                            r'(?:https?://)?(?:www\.)?([a-zA-Z0-9\-]+\.(?:com|io|ai|co|dev|app|group|org|net|tech|xyz|vc|one|uk|eu))',
                            content + ' ' + url
                        )
                        all_urls.update(domains)
            
            combined_content = "\n\n---\n\n".join(all_content[:20])  # Increased to 20 results for better investor coverage
            logger.info(f"[CLAUDE_INPUT][{company_name}] Sending {len(combined_content)} chars to Claude from {len(all_content)} documents")
            logger.info(f"[CLAUDE_INPUT][{company_name}] Content breakdown: {len(all_content[:20])} docs included (max 20)")
            
            # Log published dates presence
            docs_with_dates = sum(1 for doc in all_content[:20] if "PUBLISHED:" in doc)
            logger.info(f"[CLAUDE_INPUT][{company_name}] {docs_with_dates}/{len(all_content[:20])} documents have PUBLISHED dates")

            funding_snippets = []
            for block in all_content:
                lower_block = block.lower()
                if any(keyword in lower_block for keyword in [" raised ", " funding ", "series ", "$", " round", "million", "billion"]):
                    funding_snippets.append(block[:500].replace("\n", " ").strip())
                if len(funding_snippets) >= 10:  # Capture MORE funding snippets
                    break
            if funding_snippets:
                for idx, snippet in enumerate(funding_snippets, start=1):
                    logger.info(f"[FUNDING_SNIPPET][{company_name}] #{idx}: {snippet}")
                logger.info(f"[FUNDING_DETECTION] Found {len(funding_snippets)} funding snippets for {company_name} - Claude MUST extract these")
            else:
                logger.warning(f"[FUNDING_DETECTION] NO funding snippets detected for {company_name} in {len(all_content)} search results")
            
            # Get fund context from shared data
            fund_context_str = ""
            if self.shared_data.get('fund_context'):
                fund_ctx = self.shared_data['fund_context']
                fund_size = fund_ctx.get('fund_size', 0)
                if fund_size:
                    fund_context_str = f"\n**FUND CONTEXT**: Evaluating for a ${fund_size/1e6:.0f}M fund"
                    # Detect fund stage from original prompt
                    if 'seed fund' in self.shared_data.get('prompt', '').lower():
                        fund_context_str += " focused on SEED stage investments"
                        fund_context_str += "\n**FILTER**: Prioritize seed/pre-seed companies. Series C+ are likely irrelevant."
                        fund_context_str += "\n**STAGE HINT**: If the company is from Y Combinator or similar accelerator, they are likely Seed stage."
                    
                    # Add disambiguation priority guidance
                    fund_context_str += "\n**DISAMBIGUATION PRIORITY**: When search results contain ambiguous company names, prioritize the HIGH-GROWTH TECH COMPANY that aligns with this fund's investment thesis and stage focus. Use LinkedIn company pages as a strong disambiguation signal."
            
            # Use Claude to extract structured data with explicit two-step reasoning
            extraction_prompt = f"""STEP 1: IDENTIFY THE CORRECT COMPANY
{fund_context_str}

Search Results (may contain multiple companies with similar names):
{combined_content[:30000]}  # Increased to 30k chars to capture investor data

Domains found in content: {', '.join(list(all_urls)[:20])}

FIRST, determine which specific "{company_name}" company to extract data for:

IDENTIFICATION RULES:
1. LinkedIn company pages (linkedin.com/company/*) are the STRONGEST signal - if you see one, that's the company
2. Official domain matching company name ({company_name.lower()}.com/io/ai/co.uk) = strong signal
3. Pick the HIGH-GROWTH TECH COMPANY (startup, funded, B2B software/AI)
4. REJECT: OS features, crypto exchanges, products/platforms with similar names
5. REJECT: Companies from wrong geography/stage if fund has geographic/stage focus
6. If still uncertain, pick the one mentioned in TechCrunch/Crunchbase/pitch decks most

Once you identify which company, note:
- Their website URL
- Their business model (one sentence)
- Their LinkedIn URL (if found)

STEP 2: EXTRACT DATA FOR ONLY THAT COMPANY

Now extract and return a JSON object with the following structure. BE SPECIFIC, not generic:

{{
    "company": "{company_name}",
    "website_url": "actual company website URL if found, otherwise null",
    "business_model": "ULTRA-SPECIFIC description of what they do. Examples: 'AI-powered medical consultation analysis', 'ML infrastructure and model serving platform', 'Defense contractor drone detection system', 'B2B payments automation for SMBs'. NEVER use generic terms like SaaS, Software, Platform alone",
    "sector": "Technology sector/category. Examples: 'AI/ML', 'Infrastructure', 'Developer Tools', 'Security', 'Data Analytics', 'Automation', 'Payments'. This is WHAT type of technology they build.",
    "vertical": "Target customer industry - WHO they sell to. Examples: 'Healthcare' (sells to hospitals/clinics), 'Financial Services' (sells to banks/fintech), 'Retail' (sells to stores), 'Manufacturing' (sells to factories), 'Legal' (sells to law firms), 'Education' (sells to schools). Use 'Horizontal' ONLY if they sell across ALL industries. Use 'SMB' or 'Enterprise' if that's their primary segmentation.",
    "category": "Business model category. REQUIRED - MUST be exactly one of: 'ai_first', 'ai_saas', 'saas', 'rollup', 'marketplace', 'services', 'tech_enabled_services', 'hardware', 'gtm_software', 'deeptech_hardware', 'materials', 'manufacturing', 'industrial'. DO NOT leave empty or use other values.",
    "stage": "Seed/Series A/Series B/etc",
    "founded_year": 2020,
    "headquarters": "City, Country",
    "team_size": 50,
    "founders": [
        {{
            "name": "Full name of founder (e.g., 'John Smith', not 'J. Smith')",
            "role": "Current role (CEO, CTO, COO, etc)",
            "background": "Previous companies, education, notable achievements",
            "linkedin_url": "LinkedIn profile URL if mentioned (look for linkedin.com/in/...)",
            "previous_exits": "Any previous successful exits or acquisitions",
            "technical_founder": true/false,
            "domain_expertise": "Years of experience in this industry/domain"
        }}
    ],
    "funding_rounds": [
        {{
            "date": "2021-05",
            "round": "Series A", 
            "amount": 124000000,
            "valuation": null,
            "investors": ["Jaan Tallinn"]
        }},
        {{
            "date": "2022-04",
            "round": "Series B", 
            "amount": 580000000,
            "valuation": null,
            "investors": ["Jaan Tallinn", "Sam Bankman-Fried", "James McClave"]
        }}
    ],
    "total_funding": 15000000,
    "latest_valuation": 50000000,
    "revenue": 5000000,
    "revenue_growth": 2.5,
    "customers": ["Customer 1", "Customer 2"],
    "competitors": ["Competitor 1", "Competitor 2"],
    "incumbents": ["Legacy Player 1", "Established Company 2"],
    "key_metrics": {{
        "arr": 5000000,
        "mrr": 400000,
        "gross_margin": 0.75,
        "burn_rate": 500000,
        "runway_months": 18,
        "ltv_cac_ratio": 3.5
    }},
    "acquisitions": ["Company acquired if any"],
    "product_description": "Detailed description of what the product does",
    "target_market": "Who they sell to",
    "pricing_model": "How they charge (per seat, usage-based, etc)",
    "technology_stack": ["Tech 1", "Tech 2"],
    "recent_news": ["Recent development 1", "Recent development 2"],
    "unit_economics": {{
        "unit_of_work": "What is one unit of value? (e.g., 'one presentation generated', 'one API call', 'one month of access', 'one document processed')",
        "compute_intensity": "What happens computationally? (e.g., 'generates 50 slides with AI', 'searches 100M documents', 'processes video stream', 'stores and queries data')",
        "target_segment": "prosumer|SME|mid-market|enterprise|Fortune 500",
        "pricing_per_unit": "Estimated price they charge per unit if known",
        "gpu_cost_estimate": "Rough GPU/compute cost for that unit of work"
    }},
}}

CRITICAL: Only extract data from search results about the company you identified in STEP 1. IGNORE all other companies with similar names.

EXTRACTION RULES:
1. For website_url, use the domain you identified in STEP 1
2. IGNORE search results that mention different companies (check for conflicting details, different founders, different products)
3. If search results mention multiple companies, only extract data that CLEARLY matches the company from STEP 1

BUSINESS MODEL EXTRACTION (MOST IMPORTANT):
- Read the search results carefully to understand WHAT THE COMPANY ACTUALLY DOES (the one you identified)
- Look for phrases like "builds", "develops", "provides", "helps", "enables"
- Extract the SPECIFIC product/service, not generic categories
- BAD: "SaaS", "Software", "Platform", "Technology company"
- GOOD: "AI medical scribe for doctor consultations", "Infrastructure for ML model deployment", "Automated drone detection for airports"

SECTOR EXTRACTION:
- Identify the INDUSTRY or VERTICAL they serve
- BAD: "Technology", "Software", "IT"  
- GOOD: "Healthcare AI", "Defense Tech", "FinTech Infrastructure", "LegalTech", "EdTech", "AgTech"

FOUNDER EXTRACTION (CRITICAL FOR INVESTMENT DECISION):
- Extract founders ONLY from the company you identified in STEP 1
- Cross-check founder names with the company domain/LinkedIn to ensure they match
- Look for founder names, NOT just titles (e.g., "Sarah Chen", not "the CEO")
- Search for: "founded by", "co-founder", "CEO", "CTO", "leadership team"
- Look for LinkedIn mentions: "linkedin.com/in/[username]"
- Extract work history: "previously at Google", "former VP at Microsoft", "ex-McKinsey"
- Note any previous exits: "sold previous company to", "acquisition by", "successful exit"
- Identify if technical: "PhD in CS", "former engineer at", "built the technology"
- IMPORTANT: Return empty array [] if no founders found for the identified company, but TRY HARD to find them first
- IGNORE founders from other companies with similar names in the search results

COMPETITOR AND INCUMBENT EXTRACTION (CRITICAL FOR MARKET ANALYSIS):
- **Competitors**: Direct competitors mentioned in the text
  - Look for: "competing with [Company]", "versus [Company]", "vs [Company]", "alternative to [Company]"
  - Look for: "competes with [Company]", "rival to [Company]", "competitor [Company]"
  - These are usually other startups or newer companies in the same space
- **Incumbents**: Established/legacy players being disrupted
  - Look for: "disrupting [Company]", "replacing [Company]", "taking on [Company]"
  - Look for: "challenging [Company]", "going after [Company]'s market"
  - These are usually large established companies (Microsoft, Oracle, SAP, Salesforce, etc.)
- IMPORTANT: Keep these as SEPARATE fields - competitors and incumbents serve different analysis purposes

5. **FUNDING EXTRACTION IS MANDATORY** - Extract ALL funding rounds, but ONLY for the company you identified in STEP 1:
   
   - Cross-check funding announcements with the company domain/founders to ensure they match
   - IGNORE funding from other companies with similar names
   
   EXAMPLE 1: If you see "Series A: Amount Raised: $124M, Date: May 2021, Lead Investors: Jaan Tallinn"
   → Extract as: {{"round": "Series A", "amount": 124000000, "date": "2021-05", "investors": ["Jaan Tallinn"]}}
   
   EXAMPLE 2: If you see "{company_name} raised $15 million in Series B funding"
   → Extract as: {{"round": "Series B", "amount": 15000000, "date": "", "investors": []}}
   
   EXAMPLE 3: If you see "Apr 2022 | Sam Bankman-Fried | $580m | Series B"
   → Extract as: {{"round": "Series B", "amount": 580000000, "date": "2022-04", "investors": ["Sam Bankman-Fried"]}}
   
   **CRITICAL RULES FOR DATES**:
   - **LOOK FOR THE `PUBLISHED: YYYY-MM-DD` LINE** at the start of each article block
   - **USE THE ARTICLE'S "PUBLISHED:" DATE** - Extract the EXACT date from the `PUBLISHED:` line
   - Example: If you see `PUBLISHED: 2024-09-15` and the article mentions "{company_name} raised $X", use "2024-09-15" as the round date
   - **THIS IS MANDATORY** - The `PUBLISHED:` line is RIGHT THERE in the text, extract it!
   - Only use empty string `""` if the `PUBLISHED:` line is completely missing from the article block
   - DO NOT parse dates from article text like "in Q1" or "last year" - use the PUBLISHED date first
   
   **PARSING RULES**:
   - "$124M" = 124000000 (multiply millions by 1,000,000)
   - "$580m" = 580000000 (lowercase m also means million)
   - If no investors: use []
   - **EXTRACT THE ROUND ANYWAY**
   
6. INVESTORS EXTRACTION: Look for phrases like "led by", "participated", "investors include", "backed by", "raised from"
   - Extract ALL investor names mentioned in connection with funding rounds
   - Common patterns: "Series A led by Accel", "Goldman Sachs participated", "investors include Sequoia"
   - Use empty array [] if no investors mentioned (but still include the round!)
   
7. Set null for any field you cannot find data for (EXCEPT funding_rounds and investors - use [] not null)
8. For funding: Extract what you CAN find - incomplete rounds are better than no rounds

TAM/MARKET SIZE EXTRACTION (CRITICAL):
- Look for phrases like: "market size", "TAM", "total addressable market", "market valued at", "market worth"
- Extract the EXACT number and year (e.g., "$50 billion market by 2025")
- Look for analyst firms: Gartner, IDC, Forrester, McKinsey, CB Insights
- Include the EXACT quote as citation (e.g., "The global AI market is expected to reach $1.8 trillion by 2030")
- For labor TAM: Look for "X million workers", "average salary", "labor costs", "workforce spending"

Return ONLY the JSON object, no other text."""

            # LOG BEFORE MODEL ROUTER CALL
            prompt_length = len(extraction_prompt)
            logger.info(f"[EXTRACT_PROFILE][{company_name}] 📞 Calling model router for comprehensive profile extraction")
            logger.info(f"[EXTRACT_PROFILE][{company_name}] 📏 Extraction prompt length: {prompt_length:,} chars")
            logger.info(f"[EXTRACT_PROFILE][{company_name}] 📝 Prompt preview (first 500 chars): {extraction_prompt[:500]}...")
            
            try:
                result = await self.model_router.get_completion(
                    prompt=extraction_prompt,
                    capability=ModelCapability.STRUCTURED,
                    max_tokens=8000,  # Increased to ensure funding_rounds array is not truncated
                    temperature=0.1,
                    json_mode=True,
                    fallback_enabled=True,
                    caller_context=f"_extract_comprehensive_profile({company_name})"
                )
                response_text = result.get('response', '{}')
                
                # LOG AFTER SUCCESSFUL MODEL ROUTER CALL
                model_used = result.get('model', 'unknown')
                latency = result.get('latency', 0)
                cost = result.get('cost', 0)
                logger.info(f"[EXTRACT_PROFILE][{company_name}] ✅ Model router call successful")
                logger.info(f"[EXTRACT_PROFILE][{company_name}] 📊 Model: {model_used} | Latency: {latency:.2f}s | Cost: ${cost:.4f}")
                logger.info(f"[EXTRACT_PROFILE][{company_name}] 📏 Response length: {len(response_text):,} chars")
                
            except Exception as e:
                # LOG MODEL ROUTER FAILURE
                logger.error(f"[EXTRACT_PROFILE][{company_name}] ❌ Model router call FAILED: {type(e).__name__}: {str(e)}")
                import traceback
                logger.error(f"[EXTRACT_PROFILE][{company_name}] 🔴 Stack trace:\n{traceback.format_exc()}")
                
                # Return partial extracted data instead of empty dict
                logger.warning(f"[EXTRACT_PROFILE][{company_name}] ⚠️  Returning partial data due to model router failure")
                partial_data = {
                    "company": company_name,
                    "extraction_error": str(e),
                    "extraction_partial": True
                }
                # Try to extract whatever we can from search results without LLM
                if search_results:
                    # Basic extraction from first result
                    first_result = None
                    for sr in search_results:
                        if sr and sr.get('results'):
                            first_result = sr['results'][0]
                            break
                    if first_result:
                        partial_data['website_url'] = first_result.get('url', '')
                        partial_data['description'] = first_result.get('content', '')[:500] if first_result.get('content') else ''
                
                return partial_data
            
            response_text = result.get('response', '{}')
            
            # Log raw response details
            logger.info(f"[CLAUDE_RESPONSE][{company_name}] Raw response length: {len(response_text)} chars")
            logger.info(f"[CLAUDE_RESPONSE][{company_name}] Response starts with: {response_text[:200]}")
            logger.info(f"[CLAUDE_RESPONSE][{company_name}] Response ends with: {response_text[-200:]}")
            
            # Check if response was truncated
            if len(response_text) >= 7500:
                logger.warning(f"[CLAUDE_RESPONSE][{company_name}] Response is {len(response_text)} chars, close to 8000 token limit - may be truncated")
            
            logger.info(f"[CLAUDE_RESPONSE][{company_name}] Attempting to parse JSON response")
            
            # Clean and parse JSON
            import json
            # Remove any markdown formatting if present
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0]
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0]
            
            try:
                extracted_data = json.loads(response_text)
                
                # LOG FUNDING EXTRACTION RESULTS IMMEDIATELY
                funding_rounds = extracted_data.get('funding_rounds', [])
                
                # Detect token truncation if funding_rounds is missing but response is long
                if not funding_rounds and len(response_text) >= 6000:
                    logger.error(f"[TOKEN_TRUNCATION] Response was {len(response_text)} chars for {company_name}, likely truncated before funding_rounds!")
                    logger.error(f"[TOKEN_TRUNCATION] Found {len(funding_snippets)} funding snippets in search results")
                
                if funding_rounds and isinstance(funding_rounds, list) and len(funding_rounds) > 0:
                    logger.info(f"[FUNDING_SUCCESS] ✅ Claude extracted {len(funding_rounds)} funding rounds for {company_name}")
                    for idx, round_data in enumerate(funding_rounds, 1):
                        logger.info(f"  Round {idx}: {round_data.get('round')} - ${round_data.get('amount', 0)/1e6:.1f}M on {round_data.get('date', 'unknown date')}")
                else:
                    logger.error(f"[FUNDING_FAILURE] ❌ Claude extracted ZERO funding rounds for {company_name} despite {len(funding_snippets)} snippets in search results!")
                    logger.error(f"[FUNDING_FAILURE] Extracted data keys: {list(extracted_data.keys())}")
                
                # Add quality summary
                logger.info(f"[EXTRACTION_SUMMARY][{company_name}] Extracted fields present: {', '.join(k for k in extracted_data.keys() if extracted_data.get(k))}")
                logger.info(f"[EXTRACTION_SUMMARY][{company_name}] business_model: {extracted_data.get('business_model', 'MISSING')[:100]}")
                logger.info(f"[EXTRACTION_SUMMARY][{company_name}] stage: {extracted_data.get('stage', 'MISSING')}")
                logger.info(f"[EXTRACTION_SUMMARY][{company_name}] total_funding: ${extracted_data.get('total_funding', 0):,.0f}")
                logger.info(f"[EXTRACTION_SUMMARY][{company_name}] founders: {len(extracted_data.get('founders', []))} found")
                
                if 'founders' in extracted_data:
                    logger.info(
                        f"[FOUNDERS_RAW] Extracted founders for {company_name}: {extracted_data.get('founders')}"
                    )
                
                # CRITICAL FIX: Ensure funding_rounds have proper structure with investors as list, NEVER None
                if "funding_rounds" in extracted_data and isinstance(extracted_data["funding_rounds"], list):
                    for round_data in extracted_data["funding_rounds"]:
                        if isinstance(round_data, dict):
                            # ROOT CAUSE FIX: Infer missing amounts and dates from stage
                            if round_data.get("amount") is None and round_data.get("round"):
                                stage_amounts = {
                                    "pre-seed": 1_500_000,
                                    "seed": 3_000_000, 
                                    "series a": 15_000_000,
                                    "series b": 50_000_000,
                                    "series c": 100_000_000,
                                    "series d": 200_000_000
                                }
                                round_name = str(round_data.get("round", "")).lower()
                                inferred_amount = stage_amounts.get(round_name, 10_000_000)
                                round_data["amount"] = inferred_amount
                                logger.info(f"[FUNDING_INFERENCE] Inferred {round_name} amount: ${inferred_amount:,.0f}")
                            
                            # ROOT CAUSE FIX: Infer missing dates
                            if round_data.get("date") is None and round_data.get("round"):
                                # Use current date minus estimated time since round
                                from datetime import datetime, timedelta
                                stage_timing = {
                                    "pre-seed": 24,  # months ago
                                    "seed": 18,
                                    "series a": 12, 
                                    "series b": 9,
                                    "series c": 6,
                                    "series d": 3
                                }
                                round_name = str(round_data.get("round", "")).lower()
                                months_ago = stage_timing.get(round_name, 12)
                                inferred_date = datetime.now() - timedelta(days=months_ago * 30)
                                round_data["date"] = inferred_date.strftime("%Y-%m-%d")
                                logger.info(f"[FUNDING_INFERENCE] Inferred {round_name} date: {round_data['date']}")
                            
                            # Ensure investors is ALWAYS a list, never None or null
                            if "investors" not in round_data or round_data.get("investors") is None:
                                round_data["investors"] = []
                                logger.warning(f"No investors found for {company_name} {round_data.get('round', 'unknown round')}")
                            elif not isinstance(round_data.get("investors"), list):
                                # If investors is a string or other type, wrap it in a list
                                investor_val = round_data.get("investors")
                                if investor_val:
                                    round_data["investors"] = [investor_val] if isinstance(investor_val, str) else []
                                else:
                                    round_data["investors"] = []
                
                # IMPORTANT: Never override specific business models with generic ones
                # Validate that we got specific descriptions, not generic
                if extracted_data.get("business_model") in ["SaaS", "Software", "Technology", "Tech"]:
                    logger.warning(f"Got generic business model for {company_name}: {extracted_data.get('business_model')}")
                    # Try to extract from search content directly
                    for result in search_results:
                        if result and 'results' in result:
                            for r in result['results'][:2]:
                                content = r.get('content', '').lower()
                                # Look for specific keywords to improve categorization
                                if 'ai' in content and 'code' in content:
                                    extracted_data["business_model"] = "AI-powered development tools"
                                    break
                                elif 'healthcare' in content and ('ai' in content or 'ml' in content):
                                    extracted_data["business_model"] = "Healthcare AI platform"
                                    break
                                elif 'proptech' in content or 'property' in content:
                                    extracted_data["business_model"] = "PropTech platform"
                                    break
                                elif 'fintech' in content or 'payments' in content:
                                    extracted_data["business_model"] = "FinTech platform"
                                    break
                
            except json.JSONDecodeError:
                logger.error(f"Failed to parse Claude response: {response_text[:500]}")
                # Still try to get basic data
                extracted_data = {
                    "company": company_name,
                    "website_url": None
                }
                
                # Try to find website URL from search results
                for url in all_urls:
                    if company_name.lower() in url.lower():
                        extracted_data["website_url"] = f"https://{url}"
                        break
            
            # Ensure we have essential fields
            if not extracted_data.get("company"):
                extracted_data["company"] = company_name
            
            # Ensure vertical field exists - use sector as fallback
            if not extracted_data.get("vertical"):
                if extracted_data.get("sector"):
                    extracted_data["vertical"] = extracted_data["sector"]
                else:
                    # Try to detect from business_model or category
                    business_model_lower = extracted_data.get("business_model", "").lower()
                    if "healthcare" in business_model_lower or "medical" in business_model_lower:
                        extracted_data["vertical"] = "Healthcare"
                    elif "fintech" in business_model_lower or "payment" in business_model_lower:
                        extracted_data["vertical"] = "FinTech"
                    elif "legal" in business_model_lower:
                        extracted_data["vertical"] = "LegalTech"
                    elif "defense" in business_model_lower:
                        extracted_data["vertical"] = "DefenseTech"
                    elif "property" in business_model_lower or "real estate" in business_model_lower:
                        extracted_data["vertical"] = "PropTech"
                    else:
                        # Default to Horizontal for cross-industry tools
                        extracted_data["vertical"] = "Horizontal"
                
            logger.info(f"Extracted profile for {company_name}: {extracted_data.get('business_model', 'Unknown')}, vertical: {extracted_data.get('vertical', 'Unknown')}, category: {extracted_data.get('category', 'Unknown')}")
            
            return extracted_data
            
        except Exception as e:
            logger.error(f"Error extracting comprehensive profile for {company_name}: {e}")
            return {
                "company": company_name,
                "error": str(e)
            }
    
    def _get_exit_founder_ownership(self, company: Dict) -> float:
        """Get projected founder ownership at exit based on stage"""
        stage = company.get('stage', 'Series A')
        if 'Seed' in stage:
            return 55
        elif 'Series A' in stage:
            return 38
        elif 'Series B' in stage:
            return 27
        elif 'Series C' in stage:
            return 14
        else:
            return 20
    
    def _get_rounds_to_exit(self, company: Dict) -> str:
        """Get number of rounds to exit based on stage"""
        stage = company.get('stage', 'Series A')
        if 'Seed' in stage:
            return "3-4 rounds"
        elif 'Series A' in stage:
            return "2-3 rounds"
        elif 'Series B' in stage:
            return "1-2 rounds"
        elif 'Series C' in stage:
            return "1 round"
        else:
            return "IPO ready"
    
    def _get_cap_table_labels(self, company: Dict) -> List[str]:
        """Get stakeholder names from ACTUAL cap table for pie chart"""
        # Use actual cap table if available
        cap_table = company.get('cap_table', {})
        if cap_table:
            # Return stakeholder names (already sorted by ownership in cap_table)
            return [holder for holder, pct in cap_table.items() if pct > 0.5]
        
        # Fallback to generic labels if no cap table
        stage = company.get('stage', 'Series A')
        if 'Seed' in stage or 'Pre-Seed' in stage:
            return ['Founders', 'Seed Investors', 'Employee Pool']
        elif 'Series A' in stage:
            return ['Founders', 'Seed Investors', 'Series A Investors', 'Employee Pool']
        elif 'Series B' in stage:
            return ['Founders', 'Series A Investors', 'Series B Investors', 'Employee Pool']
        else:
            return ['Founders', 'Early Investors', 'Late Investors', 'Employee Pool']
    
    def _create_proper_cap_table_datasets(self, company: Dict) -> List[Dict[str, Any]]:
        """Create PIE chart data from ACTUAL cap table, not hardcoded"""
        # Get ACTUAL cap table from calculations
        cap_table_history = company.get('cap_table_history', {})
        current_cap_table = company.get('cap_table', {})
        
        # If we have calculated cap table, use it
        if current_cap_table:
            # Convert to pie chart format
            labels = []
            data = []
            colors = [
                'rgba(59, 130, 246, 0.9)',   # Blue - Founders
                'rgba(251, 146, 60, 0.9)',   # Orange - Employees
                'rgba(16, 185, 129, 0.9)',   # Green - Series A
                'rgba(239, 68, 68, 0.9)',    # Red - Series B
                'rgba(139, 92, 246, 0.9)',   # Purple - Series C
                'rgba(236, 72, 153, 0.9)',   # Pink - Others
            ]
            
            for idx, (holder, pct) in enumerate(current_cap_table.items()):
                if pct > 0.5:  # Only show holders with >0.5% ownership
                    labels.append(holder)
                    data.append(round(pct, 1))
            
            # Return PIE chart format
            return [{
                'label': 'Ownership %',
                'data': data,
                'backgroundColor': colors[:len(labels)],
                'labels': labels  # For pie chart
            }]
        
        # Fallback to generic ownership if no cap table
        datasets = []
        stage = company.get('stage', 'Series A')
        
        # Generic ownership based on stage
        if 'Seed' in stage:
            founder_data = [55]  # Current ownership
        elif 'Series A' in stage:
            founder_data = [38]
        elif 'Series B' in stage:
            founder_data = [27]
        elif 'Series C' in stage:
            founder_data = [14]
        else:
            founder_data = [20]
        
        datasets.append({
            'label': 'Founders',
            'data': founder_data,
            'backgroundColor': 'rgba(59, 130, 246, 0.9)',  # Blue
        })
        
        # Build investor datasets based on stage
        if 'Seed' in stage:
            # Only seed investors so far
            datasets.append({
                'label': 'Seed Investors',
                'data': [0, 10, 10, 8, 7, 7],
                'backgroundColor': 'rgba(139, 92, 246, 0.9)',  # Purple
            })
            # Our investment
            datasets.append({
                'label': 'Our Investment',
                'data': [0, 0, 6, 5, 4, 4],
                'backgroundColor': 'rgba(34, 197, 94, 0.9)',  # Green
            })
            # Future Series A
            datasets.append({
                'label': 'Series A (Future)',
                'data': [0, 0, 0, 12, 10, 10],
                'backgroundColor': 'rgba(236, 72, 153, 0.7)',  # Pink (lighter for future)
            })
            # Future Series B
            datasets.append({
                'label': 'Series B (Future)',
                'data': [0, 0, 0, 0, 12, 12],
                'backgroundColor': 'rgba(156, 163, 175, 0.7)',  # Gray (lighter for future)
            })
        
        elif 'Series A' in stage:
            # Seed investors
            datasets.append({
                'label': 'Seed Investors',
                'data': [10, 8, 7, 6, 5, 5],
                'backgroundColor': 'rgba(139, 92, 246, 0.9)',
            })
            # Series A investors
            datasets.append({
                'label': 'Series A Investors',
                'data': [0, 17, 15, 13, 10, 10],
                'backgroundColor': 'rgba(236, 72, 153, 0.9)',
            })
            # Our investment
            datasets.append({
                'label': 'Our Investment',
                'data': [0, 0, 8, 7, 6, 6],
                'backgroundColor': 'rgba(34, 197, 94, 0.9)',
            })
            # Future Series B
            datasets.append({
                'label': 'Series B (Future)',
                'data': [0, 0, 0, 14, 12, 12],
                'backgroundColor': 'rgba(99, 102, 241, 0.7)',
            })
            # Future Series C
            datasets.append({
                'label': 'Series C (Future)',
                'data': [0, 0, 0, 0, 14, 14],
                'backgroundColor': 'rgba(156, 163, 175, 0.7)',
            })
        
        elif 'Series B' in stage:
            # Earlier investors
            datasets.append({
                'label': 'Seed Investors',
                'data': [8, 7, 5, 4, 3, 3],
                'backgroundColor': 'rgba(139, 92, 246, 0.9)',
            })
            datasets.append({
                'label': 'Series A Investors',
                'data': [10, 9, 13, 11, 8, 8],
                'backgroundColor': 'rgba(236, 72, 153, 0.9)',
            })
            datasets.append({
                'label': 'Series B Investors',
                'data': [0, 12, 25, 23, 17, 17],
                'backgroundColor': 'rgba(99, 102, 241, 0.9)',
            })
            # Our investment
            datasets.append({
                'label': 'Our Investment',
                'data': [0, 0, 0, 10, 9, 9],
                'backgroundColor': 'rgba(34, 197, 94, 0.9)',
            })
            # Future Series C
            datasets.append({
                'label': 'Series C (Future)',
                'data': [0, 0, 0, 0, 18, 18],
                'backgroundColor': 'rgba(156, 163, 175, 0.7)',
            })
        
        else:
            # Series C and beyond - simplified
            datasets.append({
                'label': 'Early Investors',
                'data': [20, 15, 12, 10, 10],
                'backgroundColor': 'rgba(139, 92, 246, 0.9)',
            })
            datasets.append({
                'label': 'Growth Investors',
                'data': [18, 25, 30, 28, 28],
                'backgroundColor': 'rgba(99, 102, 241, 0.9)',
            })
            datasets.append({
                'label': 'Our Investment',
                'data': [0, 0, 15, 14, 14],
                'backgroundColor': 'rgba(34, 197, 94, 0.9)',
            })
            datasets.append({
                'label': 'Late Stage (Future)',
                'data': [0, 0, 0, 10, 10],
                'backgroundColor': 'rgba(156, 163, 175, 0.7)',
            })
        
        return datasets

    def _build_cap_table_chart_from_history(
        self,
        cap_table_data: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """Build stacked chart payload directly from real cap table history"""
        history = cap_table_data.get('history') or []
        if not history:
            return None

        owners: List[str] = []

        def _collect_owners(ownership: Dict[str, Any]):
            for owner in ownership or {}:
                if owner and owner not in owners:
                    owners.append(owner)

        first_snapshot = history[0]
        _collect_owners(first_snapshot.get('pre_money_ownership', {}))
        for snapshot in history:
            _collect_owners(snapshot.get('pre_money_ownership', {}))
            _collect_owners(snapshot.get('post_money_ownership', {}))

        current_cap_table = cap_table_data.get('current_cap_table') or {}
        _collect_owners(current_cap_table)

        final_cap_table = cap_table_data.get('final_cap_table_at_exit')
        if isinstance(final_cap_table, dict):
            _collect_owners(final_cap_table)

        if not owners:
            return None

        def _owner_sort_key(owner: str) -> Tuple[int, str]:
            lowered = owner.lower()
            if 'founder' in lowered:
                return (0, owner)
            if 'employee' in lowered or 'option' in lowered or 'esop' in lowered:
                return (1, owner)
            if 'our ' in lowered or 'our fund' in lowered:
                return (2, owner)
            return (3, owner)

        owners.sort(key=_owner_sort_key)

        labels: List[str] = []
        series: Dict[str, List[float]] = {owner: [] for owner in owners}

        def _append(label: str, ownership: Dict[str, Any]):
            labels.append(label)
            for owner in owners:
                value = ownership.get(owner, 0) if ownership else 0
                try:
                    numeric_val = float(value)
                except (TypeError, ValueError):
                    numeric_val = 0.0
                series[owner].append(round(numeric_val, 2))

        pre_money = first_snapshot.get('pre_money_ownership')
        if pre_money:
            _append('Initial', pre_money)

        for snapshot in history:
            round_name = snapshot.get('round_name', 'Round')
            post_money = snapshot.get('post_money_ownership') or {}
            _append(round_name, post_money)

        if current_cap_table:
            _append('Current', current_cap_table)

        if isinstance(final_cap_table, dict) and final_cap_table:
            _append('Exit', final_cap_table)

        significant = {
            owner: values
            for owner, values in series.items()
            if any(abs(v) > 0.01 for v in values)
        }
        if not significant:
            return None

        color_cycle = [
            'rgba(59, 130, 246, 0.9)',
            'rgba(251, 146, 60, 0.9)',
            'rgba(16, 185, 129, 0.9)',
            'rgba(139, 92, 246, 0.9)',
            'rgba(236, 72, 153, 0.9)',
            'rgba(107, 114, 128, 0.9)',
            'rgba(249, 115, 22, 0.9)',
            'rgba(14, 116, 144, 0.9)'
        ]

        datasets: List[Dict[str, Any]] = []
        color_index = 0
        for owner in owners:
            if owner not in significant:
                continue

            lowered = owner.lower()
            if 'founder' in lowered:
                color = 'rgba(59, 130, 246, 0.9)'
            elif 'employee' in lowered or 'option' in lowered or 'esop' in lowered:
                color = 'rgba(251, 146, 60, 0.9)'
            elif 'our ' in lowered or 'our fund' in lowered:
                color = 'rgba(16, 185, 129, 0.9)'
            else:
                color = color_cycle[color_index % len(color_cycle)]
                color_index += 1

            datasets.append({
                'label': owner,
                'data': significant[owner],
                'backgroundColor': color
            })

        return {
            'labels': labels,
            'datasets': datasets
        }


    async def _generate_comprehensive_business_analysis(self, companies: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate comprehensive business analysis for all companies in one model call"""
        try:
            if not companies or len(companies) < 2:
                return {}
            
            company_a = companies[0]
            company_b = companies[1]
            
            name_a = company_a.get('company', 'Company A')
            name_b = company_b.get('company', 'Company B')
            
            # Extract available citations from market research
            available_citations = await self._extract_available_citations(companies)
            
            # Collect all relevant data for both companies
            analysis_data = {
                "company_a": {
                    "name": name_a,
                    "acv": self._calculate_acv(company_a),
                    "gross_margin": safe_get_value(company_a.get("gross_margin", company_a.get("inferred_gross_margin", 0.7))),
                    "ltv_cac": company_a.get("ltv_cac_ratio", 3.0),
                    "payback_months": company_a.get('cac_payback_months', 18),
                    "yoy_growth": company_a.get('revenue_growth', 0),
                    "burn_multiple": self._calculate_burn_multiple(company_a),
                    "rule_of_40": self._calculate_rule_of_40(company_a),
                    "business_model": company_a.get('business_model', ''),
                    "product_description": company_a.get('product_description', ''),
                    "target_market": company_a.get('target_market', ''),
                    "pricing_model": company_a.get('pricing_model', ''),
                    "customer_segment": company_a.get('customer_segment', ''),
                    "geography": company_a.get('geography', ''),
                    "customers": company_a.get('customers', []),
                    "unit_economics": company_a.get('unit_economics', {}),
                    "gpu_metrics": company_a.get('gpu_metrics', {}),
                    "stage": company_a.get('stage', '')
                },
                "company_b": {
                    "name": name_b,
                    "acv": self._calculate_acv(company_b),
                    "gross_margin": safe_get_value(company_b.get("gross_margin", company_b.get("inferred_gross_margin", 0.7))),
                    "ltv_cac": company_b.get("ltv_cac_ratio", 3.0),
                    "payback_months": company_b.get('cac_payback_months', 18),
                    "yoy_growth": company_b.get('revenue_growth', 0),
                    "burn_multiple": self._calculate_burn_multiple(company_b),
                    "rule_of_40": self._calculate_rule_of_40(company_b),
                    "business_model": company_b.get('business_model', ''),
                    "product_description": company_b.get('product_description', ''),
                    "target_market": company_b.get('target_market', ''),
                    "pricing_model": company_b.get('pricing_model', ''),
                    "customer_segment": company_b.get('customer_segment', ''),
                    "geography": company_b.get('geography', ''),
                    "customers": company_b.get('customers', []),
                    "unit_economics": company_b.get('unit_economics', {}),
                    "gpu_metrics": company_b.get('gpu_metrics', {}),
                    "stage": company_b.get('stage', '')
                }
            }
            
            # Build citations section for prompt
            citations_text = ""
            if available_citations:
                citations_text = "\n\nAVAILABLE SOURCES FOR CITATIONS:\n"
                for i, citation in enumerate(available_citations, 1):
                    citations_text += f"[{i}] {citation.get('title', citation.get('source', 'Unknown'))} - {citation.get('url', 'No URL')}\n"
                citations_text += "\nUse these sources to support your claims with inline citations like [1], [2], etc.\n"

            prompt = f"""You are analyzing {name_a} vs {name_b} for a growth fund investment decision.

CRITICAL REQUIREMENTS:
1. Use inline citations [1], [2], [3] for ALL factual claims using available sources
2. Include specific numbers: "$X market size", "Y% growth rate", "Z customers"
3. Reference actual competitors by name when available
4. Cite analyst reports when discussing market size
5. Explain WHY metrics matter (e.g., "70% gross margin indicates strong pricing power")
6. Compare to industry benchmarks with specific numbers

INSTITUTIONAL-GRADE ANALYSIS STRUCTURE:
- Market Opportunity: TAM/SAM/SOM with sources, growth drivers with data
- Business Model: Unit economics with actual numbers, pricing power indicators  
- Competitive Position: Named competitors, differentiation with evidence
- Financial Health: Key metrics with context (e.g., "Rule of 40 score of 65 vs industry avg 40")
- Risk Factors: Specific concerns with mitigation strategies

AVAILABLE SOURCES FOR CITATIONS:{citations_text}

Company A ({name_a}):
- ACV: ${analysis_data['company_a']['acv']/1000:.0f}K
- Gross Margin: {analysis_data['company_a']['gross_margin']*100:.0f}%
- LTV/CAC: {analysis_data['company_a']['ltv_cac']:.1f}x (Payback: {analysis_data['company_a']['payback_months']} months)
- YoY Growth: {analysis_data['company_a']['yoy_growth']*100:.0f}%
- Burn Multiple: {analysis_data['company_a']['burn_multiple']:.1f}x
- Business Model: {analysis_data['company_a']['business_model']}
- Product Description: {analysis_data['company_a']['product_description']}
- Target Market: {analysis_data['company_a']['target_market']}
- Pricing Model: {analysis_data['company_a']['pricing_model']}
- Customer Segment: {analysis_data['company_a']['customer_segment']}
- Geography: {analysis_data['company_a']['geography']}
- Customers: {analysis_data['company_a']['customers'][:5] if isinstance(analysis_data['company_a']['customers'], list) else analysis_data['company_a']['customers']}
- Unit Economics: {analysis_data['company_a']['unit_economics']}
- GPU Metrics: {analysis_data['company_a']['gpu_metrics']}
- Stage: {analysis_data['company_a']['stage']}

Company B ({name_b}):
- ACV: ${analysis_data['company_b']['acv']/1000:.0f}K
- Gross Margin: {analysis_data['company_b']['gross_margin']*100:.0f}%
- LTV/CAC: {analysis_data['company_b']['ltv_cac']:.1f}x (Payback: {analysis_data['company_b']['payback_months']} months)
- YoY Growth: {analysis_data['company_b']['yoy_growth']*100:.0f}%
- Burn Multiple: {analysis_data['company_b']['burn_multiple']:.1f}x
- Business Model: {analysis_data['company_b']['business_model']}
- Product Description: {analysis_data['company_b']['product_description']}
- Target Market: {analysis_data['company_b']['target_market']}
- Pricing Model: {analysis_data['company_b']['pricing_model']}
- Customer Segment: {analysis_data['company_b']['customer_segment']}
- Geography: {analysis_data['company_b']['geography']}
- Customers: {analysis_data['company_b']['customers'][:5] if isinstance(analysis_data['company_b']['customers'], list) else analysis_data['company_b']['customers']}
- Unit Economics: {analysis_data['company_b']['unit_economics']}
- GPU Metrics: {analysis_data['company_b']['gpu_metrics']}
- Stage: {analysis_data['company_b']['stage']}

Explain WHY each company has their specific metrics based on their actual business:

1. ACV Analysis:
   - {name_a}: WHY do they have ${analysis_data['company_a']['acv']/1000:.0f}K ACV? What about their business model, target market, and pricing drives this specific number?
   - {name_b}: WHY do they have ${analysis_data['company_b']['acv']/1000:.0f}K ACV? What about their business model, target market, and pricing drives this specific number?

2. Margin Analysis:
   - {name_a}: WHY do they have {analysis_data['company_a']['gross_margin']*100:.0f}% margin? What about their cost structure, business model, and unit economics drives this specific margin?
   - {name_b}: WHY do they have {analysis_data['company_b']['gross_margin']*100:.0f}% margin? What about their cost structure, business model, and unit economics drives this specific margin?

3. Customer Economics Analysis:
   - {name_a}: WHY do they have {analysis_data['company_a']['ltv_cac']:.1f}x LTV/CAC? What about their customer acquisition, business model, and target market drives this specific ratio?
   - {name_b}: WHY do they have {analysis_data['company_b']['ltv_cac']:.1f}x LTV/CAC? What about their customer acquisition, business model, and target market drives this specific ratio?

4. Growth Analysis:
   - {name_a}: WHY do they have {analysis_data['company_a']['yoy_growth']*100:.0f}% growth and {analysis_data['company_a']['burn_multiple']:.1f}x burn multiple? What about their market dynamics, business model, and execution drives these specific numbers?
   - {name_b}: WHY do they have {analysis_data['company_b']['yoy_growth']*100:.0f}% growth and {analysis_data['company_b']['burn_multiple']:.1f}x burn multiple? What about their market dynamics, business model, and execution drives these specific numbers?

5. Investment Implications:
   - Which company's business model is more attractive for a growth fund and why?
   - What are the key business risks and opportunities for each?

Focus on WHY their specific business creates these specific metrics. If you don't have enough business context, say so.

EXAMPLES OF PROPER CITATIONS:
- "Market growing at 40% CAGR [1] with TAM of $50B [2]"
- "Company has 500 customers including Fortune 500 [3]"
- "Competing with Salesforce, HubSpot [4] in $20B market [5]"
- "70% gross margin vs 60% industry average [6] indicates pricing power"

Return your analysis with inline citations for ALL factual claims.
"""

            try:
                result = await self.model_router.get_completion(
                    prompt=prompt,
                    capability=ModelCapability.ANALYSIS,
                    max_tokens=2000,
                    temperature=0.3,
                    fallback_enabled=True
                )
                
                if result and result.get('response'):
                    analysis_text = result['response']
                    # Parse the structured response
                    sections = analysis_text.split('\n\n')
                    analysis = {
                        "unit_economics": {},
                        "margin_analysis": {},
                        "customer_economics": {},
                        "growth_efficiency": {},
                        "investment_implications": ""
                    }
                    
                    current_section = None
                    for section in sections:
                        if 'Unit Economics Analysis:' in section:
                            current_section = "unit_economics"
                        elif 'Margin Structure Analysis:' in section:
                            current_section = "margin_analysis"
                        elif 'Customer Economics Analysis:' in section:
                            current_section = "customer_economics"
                        elif 'Growth Efficiency Analysis:' in section:
                            current_section = "growth_efficiency"
                        elif 'Investment Implications:' in section:
                            current_section = "investment_implications"
                        
                        if current_section and current_section != "investment_implications":
                            # Extract company-specific analysis
                            if f"{name_a} ACV Analysis:" in section:
                                analysis[current_section][name_a] = section.split(f"{name_a} ACV Analysis:")[1].split(f"{name_b} ACV Analysis:")[0].strip()
                            elif f"{name_b} ACV Analysis:" in section:
                                analysis[current_section][name_b] = section.split(f"{name_b} ACV Analysis:")[1].split("Comparative ACV Insights:")[0].strip()
                            elif f"{name_a} Margin Analysis:" in section:
                                analysis[current_section][name_a] = section.split(f"{name_a} Margin Analysis:")[1].split(f"{name_b} Margin Analysis:")[0].strip()
                            elif f"{name_b} Margin Analysis:" in section:
                                analysis[current_section][name_b] = section.split(f"{name_b} Margin Analysis:")[1].split("Comparative Margin Insights:")[0].strip()
                            elif f"{name_a} Customer Analysis:" in section:
                                analysis[current_section][name_a] = section.split(f"{name_a} Customer Analysis:")[1].split(f"{name_b} Customer Analysis:")[0].strip()
                            elif f"{name_b} Customer Analysis:" in section:
                                analysis[current_section][name_b] = section.split(f"{name_b} Customer Analysis:")[1].split("Comparative Customer Insights:")[0].strip()
                            elif f"{name_a} Growth Analysis:" in section:
                                analysis[current_section][name_a] = section.split(f"{name_a} Growth Analysis:")[1].split(f"{name_b} Growth Analysis:")[0].strip()
                            elif f"{name_b} Growth Analysis:" in section:
                                analysis[current_section][name_b] = section.split(f"{name_b} Growth Analysis:")[1].split("Comparative Growth Insights:")[0].strip()
                        elif current_section == "investment_implications":
                            analysis[current_section] = section.replace('Investment Implications:', '').strip()
                    
                    return analysis
            except Exception as e:
                logger.error(f"Error in analysis: {e}")
            
            # Fallback if Claude fails
            return {
                "unit_economics": {
                    name_a: f"${analysis_data['company_a']['acv']/1000:.0f}K ACV indicates {analysis_data['company_a']['customer_segment'].lower() if analysis_data['company_a']['customer_segment'] else 'mid-market'} focus",
                    name_b: f"${analysis_data['company_b']['acv']/1000:.0f}K ACV indicates {analysis_data['company_b']['customer_segment'].lower() if analysis_data['company_b']['customer_segment'] else 'mid-market'} focus"
                },
                "margin_analysis": {
                    name_a: f"{analysis_data['company_a']['gross_margin']*100:.0f}% margin reflects {analysis_data['company_a']['business_model'].lower() if analysis_data['company_a']['business_model'] else 'standard'} cost structure",
                    name_b: f"{analysis_data['company_b']['gross_margin']*100:.0f}% margin reflects {analysis_data['company_b']['business_model'].lower() if analysis_data['company_b']['business_model'] else 'standard'} cost structure"
                },
                "customer_economics": {
                    name_a: f"{analysis_data['company_a']['ltv_cac']:.1f}x LTV/CAC indicates {analysis_data['company_a']['customer_segment'].lower() if analysis_data['company_a']['customer_segment'] else 'standard'} customer acquisition efficiency",
                    name_b: f"{analysis_data['company_b']['ltv_cac']:.1f}x LTV/CAC indicates {analysis_data['company_b']['customer_segment'].lower() if analysis_data['company_b']['customer_segment'] else 'standard'} customer acquisition efficiency"
                },
                "growth_efficiency": {
                    name_a: f"{analysis_data['company_a']['yoy_growth']*100:.0f}% growth indicates {analysis_data['company_a']['target_market'].lower() if analysis_data['company_a']['target_market'] else 'market'} traction",
                    name_b: f"{analysis_data['company_b']['yoy_growth']*100:.0f}% growth indicates {analysis_data['company_b']['target_market'].lower() if analysis_data['company_b']['target_market'] else 'market'} traction"
                },
                "investment_implications": f"Both companies offer different risk/return profiles based on their business models."
            }
        except Exception as e:
            logger.error(f"Comprehensive business analysis error: {e}")
            return {}
    
    def _safe_generate_recommendation(self, company: Dict[str, Any]) -> Dict[str, str]:
        """Safely generate investment recommendation with error handling"""
        try:
            return self._generate_investment_recommendation(company)
        except Exception as e:
            logger.error(f"[DECK_GEN] ❌ Investment recommendation generation failed for {company.get('company', 'Unknown')}: {e}")
            return {
                "recommendation": "HOLD",
                "action": "Further analysis required",
                "reasoning": "Analysis in progress"
            }
    
    async def _generate_investment_thesis_comparison(self, companies: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate comparative investment thesis analysis for deck slides"""
        try:
            if not companies or len(companies) < 2:
                return {}
            
            company_a = companies[0]
            company_b = companies[1]
            
            name_a = company_a.get('company', 'Company A')
            name_b = company_b.get('company', 'Company B')
            
            # Extract key investment thesis elements
            thesis_a = self._generate_investment_thesis(company_a)
            thesis_b = self._generate_investment_thesis(company_b)
            
            # Generate comparative analysis using ModelRouter
            comparison_prompt = f"""
            Compare the investment theses for two companies and provide a structured analysis:

            Company A ({name_a}):
            - Thesis: {thesis_a.get('thesis', 'N/A')}
            - Action: {thesis_a.get('action', 'N/A')}
            - Score: {thesis_a.get('total_score', 0)}

            Company B ({name_b}):
            - Thesis: {thesis_b.get('thesis', 'N/A')}
            - Action: {thesis_b.get('action', 'N/A')}
            - Score: {thesis_b.get('total_score', 0)}

            Provide a comparative analysis with:
            1. Key differentiators between the companies
            2. Relative strengths and weaknesses
            3. Investment recommendation with reasoning
            4. Risk factors for each company

            Format as structured analysis with clear sections.
            """
            
            result = await self.model_router.get_completion(
                prompt=comparison_prompt,
                capability=ModelCapability.ANALYSIS,
                max_tokens=1500,
                temperature=0.3,
                fallback_enabled=True
            )
            
            if result and result.get('response'):
                analysis_text = result['response']
                
                # Parse the structured response
                sections = analysis_text.split('\n\n')
                comparison = {
                    "company_a": {
                        "name": name_a,
                        "thesis": thesis_a.get('thesis', ''),
                        "action": thesis_a.get('action', ''),
                        "score": thesis_a.get('total_score', 0)
                    },
                    "company_b": {
                        "name": name_b,
                        "thesis": thesis_b.get('thesis', ''),
                        "action": thesis_b.get('action', ''),
                        "score": thesis_b.get('total_score', 0)
                    },
                    "differentiators": [],
                    "strengths_weaknesses": {},
                    "recommendation": "",
                    "risk_factors": {}
                }
                
                current_section = None
                for section in sections:
                    if 'differentiators' in section.lower() or 'differentiation' in section.lower():
                        current_section = "differentiators"
                    elif 'strengths' in section.lower() and 'weaknesses' in section.lower():
                        current_section = "strengths_weaknesses"
                    elif 'recommendation' in section.lower():
                        current_section = "recommendation"
                    elif 'risk' in section.lower():
                        current_section = "risk_factors"
                    
                    if current_section == "differentiators":
                        # Extract bullet points
                        lines = section.split('\n')
                        for line in lines:
                            if line.strip().startswith('•') or line.strip().startswith('-'):
                                comparison["differentiators"].append(line.strip()[1:].strip())
                    elif current_section == "recommendation":
                        comparison["recommendation"] = section.replace('Recommendation:', '').strip()
                
                return comparison
            
            # Fallback if ModelRouter fails
            return {
                "company_a": {
                    "name": name_a,
                    "thesis": thesis_a.get('thesis', ''),
                    "action": thesis_a.get('action', ''),
                    "score": thesis_a.get('total_score', 0)
                },
                "company_b": {
                    "name": name_b,
                    "thesis": thesis_b.get('thesis', ''),
                    "action": thesis_b.get('action', ''),
                    "score": thesis_b.get('total_score', 0)
                },
                "differentiators": [
                    f"{name_a} focuses on {company_a.get('business_model', 'technology')}",
                    f"{name_b} focuses on {company_b.get('business_model', 'technology')}"
                ],
                "recommendation": f"Both companies offer different value propositions based on their business models and market positioning.",
                "risk_factors": {
                    name_a: "Market competition and execution risk",
                    name_b: "Market competition and execution risk"
                }
            }
            
        except Exception as e:
            logger.error(f"Investment thesis comparison error: {e}")
            return {}
    
    async def _extract_available_citations(self, companies: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Extract available citations from market research data"""
        citations = []
        
        try:
            # Check if we have market research data
            if hasattr(self, 'market_research_cache'):
                for company in companies:
                    company_name = company.get('company', '')
                    if company_name in self.market_research_cache:
                        research = self.market_research_cache[company_name]
                        
                        # Extract from raw search results
                        raw_results = research.get('raw_search_results', [])
                        for result in raw_results:
                            if result.get('answer'):
                                citations.append({
                                    'title': f"Market Research - {company_name}",
                                    'source': result.get('query', 'Market Research'),
                                    'url': 'https://tavily.com/search',  # Generic URL for Tavily searches
                                    'content': result.get('answer', '')[:200]
                                })
            
            # Add company data citations
            for company in companies:
                company_name = company.get('company', '')
                website = company.get('website_url', '')
                if website:
                    citations.append({
                        'title': f"{company_name} - Company Website",
                        'source': f"{company_name} official website",
                        'url': website,
                        'content': f"Company information and business model data for {company_name}"
                    })
            
            # Add industry benchmark citations
            citations.extend([
                {
                    'title': "Cambridge Associates VC Index Q2 2024",
                    'source': "Cambridge Associates",
                    'url': "https://www.cambridgeassociates.com/",
                    'content': "Venture capital benchmark data and performance metrics"
                },
                {
                    'title': "BLS Occupational Employment Statistics",
                    'source': "Bureau of Labor Statistics",
                    'url': "https://www.bls.gov/oes/",
                    'content': "Labor market data and employment statistics"
                }
            ])
            
        except Exception as e:
            logger.error(f"Error extracting citations: {e}")
        
        return citations[:10]  # Limit to 10 citations for prompt

    def _create_cap_table_datasets(self, waterfall_data: Dict[str, Any], labels: List[str], company_name: str) -> List[Dict[str, Any]]:
        """[DEPRECATED] Old function for backward compatibility"""
        # This function is deprecated - use _create_proper_cap_table_datasets instead
        logger.warning("Using deprecated _create_cap_table_datasets function")
        return [
            {
                'label': 'Founders',
                'data': [100, 85, 70, 55, 40, 40],
                'backgroundColor': 'rgba(59, 130, 246, 0.8)'
            },
            {
                'label': 'Investors',
                'data': [0, 10, 20, 30, 42, 42],
                'backgroundColor': 'rgba(16, 185, 129, 0.8)'
            },
            {
                'label': 'Employees',
                'data': [0, 5, 10, 15, 18, 18],
                'backgroundColor': 'rgba(251, 146, 60, 0.8)'
            }
        ]
    
    async def _get_service_calculated_fields(self, company_data: Dict[str, Any], missing_fields: List[str]) -> Dict[str, Any]:
        """Get field values from services instead of hardcoded defaults
        
        This method replaces hardcoded defaults with actual service calculations,
        ensuring all data comes from intelligent inference services.
        """
        try:
            # Use IntelligentGapFiller for stage-appropriate benchmarks
            if missing_fields:
                logger.info(f"Getting service-calculated fields for {missing_fields}")
                
                # Call the intelligent gap filler to infer missing fields
                inferred_data = await self.gap_filler.infer_from_stage_benchmarks(
                    company_data=company_data,
                    missing_fields=missing_fields
                )
                
                # Process inferred data
                result = {}
                for field in missing_fields:
                    if field in inferred_data:
                        inference = inferred_data[field]
                        if hasattr(inference, 'value'):
                            result[field] = inference.value
                        else:
                            result[field] = inference
                    else:
                        # If gap filler doesn't have this field, log warning
                        logger.warning(f"Field {field} not provided by IntelligentGapFiller")
                        # Do NOT provide hardcoded default - let it be None
                        result[field] = None
                
                return result
            
            return {}
            
        except Exception as e:
            logger.error(f"Error getting service-calculated fields: {e}")
            # Return empty dict on error - no hardcoded fallbacks
            return {}
    
    async def _ensure_companies_have_inferred_data(self, companies: List[Dict]) -> List[Dict]:
        """
        Ensure all companies have inferred data fields populated.
        This is critical for companies that bypass the normal fetch pipeline.
        """
        if not companies:
            return companies
            
        logger.info(f"[INFERENCE_ENRICHMENT] Ensuring {len(companies)} companies have inferred data")
        
        for company in companies:
            company_name = company.get('company', 'Unknown')
            
            # Check if already has inferred data (quick check)
            if company.get('inferred_revenue') and company.get('inferred_valuation'):
                logger.info(f"[INFERENCE_ENRICHMENT] {company_name} already has inferred data, skipping")
                continue
            
            logger.info(f"[INFERENCE_ENRICHMENT] Processing inference for {company_name}")
            
            # Define numeric fields that need inference
            numeric_fields = [
                "revenue", "growth_rate", "valuation", "team_size", 
                "burn_rate", "runway_months", "gross_margin", "total_funding",
                "customer_count", "ltv_cac_ratio", "net_retention"
            ]
            
            for field in numeric_fields:
                inferred_field = f"inferred_{field}"
                
                # Get the extracted value
                actual_value = company.get(field)
                if field == "revenue":
                    # Special handling for revenue - check ARR too
                    actual_value = actual_value or company.get("arr")
                
                # Check what we already have
                existing_inferred = company.get(inferred_field)
                
                # CRITICAL FIX: Treat None, empty string, or 0 as missing
                # For team_size specifically, 0 should always be treated as missing
                # For other fields, preserve original behavior (0 is also treated as missing)
                is_missing = (actual_value is None or actual_value == "" or actual_value == 0)
                
                # Establish the final value using hierarchy
                if not is_missing:
                    # We have extracted data - use it for both fields
                    final_value = actual_value
                    company[field] = final_value
                    company[inferred_field] = final_value
                    logger.info(f"[INFERENCE_ENRICHMENT] Using extracted {field} = {final_value} for {company_name}")
                elif existing_inferred is not None and existing_inferred != 0:
                    # We have inferred data - use it
                    final_value = existing_inferred
                    company[field] = final_value
                    company[inferred_field] = final_value
                    logger.info(f"[INFERENCE_ENRICHMENT] Using already inferred {field} = {final_value} for {company_name}")
                else:
                    # Need to calculate a value - ALWAYS infer team_size when missing
                    stage = company.get("stage", "Seed")
                    service_fields = await self._get_service_calculated_fields(
                        company, 
                        [field]  # This will call infer_from_stage_benchmarks for team_size
                    )
                    calculated_value = service_fields.get(field)
                    
                    if calculated_value is not None and calculated_value != 0:
                        company[field] = calculated_value
                        company[inferred_field] = calculated_value
                        logger.info(f"[INFERENCE_ENRICHMENT] Using service-calculated {field} = {calculated_value} for {company_name}")
                    else:
                        # Use stage-based default as last resort
                        default_value = self._get_stage_default(field, stage)
                        company[field] = default_value
                        company[inferred_field] = default_value
                        logger.warning(f"[INFERENCE_ENRICHMENT] Using stage default {field} = {default_value} for {company_name}")
            
            # CRITICAL: Ensure inferred_valuation ALWAYS exists
            if not company.get('inferred_valuation') or company.get('inferred_valuation') == 0:
                # Try valuation first, then use stage default
                if company.get('valuation') and company.get('valuation') != 0:
                    company['inferred_valuation'] = company['valuation']
                else:
                    stage = company.get('stage', 'Seed')
                    default_val = self._get_stage_default('valuation', stage)
                    company['inferred_valuation'] = default_val
                    company['valuation'] = default_val
                    logger.warning(f"[INFERENCE_ENRICHMENT] Forced inferred_valuation = {default_val} for {company_name} at stage {stage}")
            
            # Also ensure inferred_revenue ALWAYS exists
            if not company.get('inferred_revenue') or company.get('inferred_revenue') == 0:
                if company.get('revenue') and company.get('revenue') != 0:
                    company['inferred_revenue'] = company['revenue']
                elif company.get('arr') and company.get('arr') != 0:
                    company['inferred_revenue'] = company['arr']
                else:
                    stage = company.get('stage', 'Seed')
                    default_val = self._get_stage_default('revenue', stage)
                    company['inferred_revenue'] = default_val
                    company['revenue'] = default_val
                    logger.warning(f"[INFERENCE_ENRICHMENT] Forced inferred_revenue = {default_val} for {company_name} at stage {stage}")
            
            # Ensure growth metrics exist for deck projections
            if not company.get('projected_growth_rate') or not company.get('growth_rate'):
                self._ensure_growth_metrics(company)
        
        # Final validation step - ensure no None values in critical fields
        validated_companies = [validate_company_data(company) for company in companies]
        logger.info(f"[INFERENCE_ENRICHMENT] Completed inference enrichment and validation for {len(validated_companies)} companies")
        return validated_companies
    
    def _validate_chart_data(self, chart_data: Dict[str, Any]) -> tuple[bool, str]:
        """
        Validate chart_data structure before prerendering
        
        Args:
            chart_data: Chart data to validate
            
        Returns:
            Tuple of (is_valid, error_message)
        """
        if not chart_data:
            return False, "Chart data is None or empty"
        
        chart_type = chart_data.get('type')
        if not chart_type:
            return False, "Chart data missing 'type' field"
        
        data = chart_data.get('data')
        if not data:
            return False, f"Chart data missing 'data' field for type {chart_type}"
        
        # Type-specific validation
        if chart_type == 'sankey':
            if not isinstance(data, dict):
                return False, "Sankey data must be a dict"
            if not data.get('nodes') or not isinstance(data['nodes'], list):
                return False, "Sankey data missing 'nodes' array"
            if not data.get('links') or not isinstance(data['links'], list):
                return False, "Sankey data missing 'links' array"
            # Validate nodes have required fields
            for i, node in enumerate(data['nodes']):
                if not isinstance(node, dict):
                    return False, f"Sankey node {i} is not a dict"
                if 'id' not in node and 'name' not in node:
                    return False, f"Sankey node {i} missing 'id' or 'name'"
            # Validate links have required fields
            for i, link in enumerate(data['links']):
                if not isinstance(link, dict):
                    return False, f"Sankey link {i} is not a dict"
                if 'source' not in link or 'target' not in link or 'value' not in link:
                    return False, f"Sankey link {i} missing 'source', 'target', or 'value'"
        
        elif chart_type == 'heatmap':
            if not isinstance(data, dict):
                return False, "Heatmap data must be a dict"
            if not data.get('dimensions') or not isinstance(data['dimensions'], list):
                return False, "Heatmap data missing 'dimensions' array"
            if not data.get('companies') or not isinstance(data['companies'], list):
                return False, "Heatmap data missing 'companies' array"
            if not data.get('scores') or not isinstance(data['scores'], list):
                return False, "Heatmap data missing 'scores' array"
            # Validate scores array matches companies
            if len(data['scores']) != len(data['companies']):
                return False, f"Heatmap scores array length ({len(data['scores'])}) doesn't match companies ({len(data['companies'])})"
        
        elif chart_type == 'line':
            if not isinstance(data, dict):
                return False, "Line chart data must be a dict"
            if not data.get('labels') or not isinstance(data['labels'], list):
                return False, "Line chart data missing 'labels' array"
            if not data.get('datasets') or not isinstance(data['datasets'], list):
                return False, "Line chart data missing 'datasets' array"
        
        elif chart_type == 'pie':
            if not isinstance(data, dict):
                return False, "Pie chart data must be a dict"
            if not data.get('labels') or not isinstance(data['labels'], list):
                return False, "Pie chart data missing 'labels' array"
            if not data.get('datasets') or not isinstance(data['datasets'], list):
                return False, "Pie chart data missing 'datasets' array"
        
        return True, ""
    
    async def _prerender_complex_chart(self, chart_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Pre-render complex charts to PNG images
        
        Args:
            chart_data: Chart data with type and data fields
            
        Returns:
            Updated chart_data with pre-rendered image if applicable, or original chart_data if prerender fails
        """
        if not chart_renderer:
            logger.warning("[CHART_PRERENDER] chart_renderer_service unavailable, skipping pre-render")
            return chart_data
        
        try:
            # Validate chart data structure first
            is_valid, error_msg = self._validate_chart_data(chart_data)
            if not is_valid:
                logger.warning(f"[CHART_PRERENDER] Chart data validation failed: {error_msg}. Skipping prerender, using raw data.")
                return chart_data
            
            chart_type = chart_data.get('type')
            if not chart_type:
                return chart_data
            
            # Check if this chart type should be pre-rendered
            if chart_renderer.should_prerender_chart(chart_type):
                logger.info(f"[CHART_PRERENDER] Pre-rendering {chart_type} chart")
                
                # Render chart to PNG
                base64_image = await chart_renderer.render_tableau_chart(
                    chart_type=chart_type,
                    chart_data=chart_data,
                    width=800,
                    height=400
                )
                
                if base64_image:
                    # Replace chart_data with image
                    logger.info(f"[CHART_PRERENDER] Successfully prerendered {chart_type} chart")
                    return {
                        "type": "image",
                        "src": f"data:image/png;base64,{base64_image}",
                        "alt": chart_data.get('title', f"{chart_type} chart"),
                        "original_type": chart_type,
                        "original_data": chart_data
                    }
                else:
                    logger.warning(f"[CHART_PRERENDER] Failed to pre-render {chart_type} chart, keeping original data")
            
            return chart_data
            
        except Exception as e:
            logger.error(f"[CHART_PRERENDER] Error pre-rendering chart: {e}")
            import traceback
            logger.error(f"[CHART_PRERENDER] Stack trace: {traceback.format_exc()}")
            # Return original chart_data on error so frontend can render it
            return chart_data
    
    @staticmethod
    def _format_billions(
        value: Optional[Union[int, float]],
        zero_label: str = "$0B",
        none_label: str = "N/A",
        precision: int = 1
    ) -> str:
        """Safely format monetary values in billions without raising on None."""
        if value is None:
            return none_label
        
        try:
            numeric = float(value)
        except (TypeError, ValueError):
            return none_label
        
        if not math.isfinite(numeric):
            return none_label
        
        if numeric == 0:
            return zero_label
        
        return f"${numeric/1e9:.{precision}f}B"
    
    def _extract_deal_charts(self, deal_comparisons: List[Any]) -> List[Dict[str, Any]]:
        """Extract charts from ComprehensiveDealAnalyzer deal_comparisons"""
        all_charts = []
        for deal in deal_comparisons:
            if hasattr(deal, 'charts') and deal.charts:
                charts_dict = deal.charts if isinstance(deal.charts, dict) else {}
                for chart_name, chart_data in charts_dict.items():
                    if chart_data and isinstance(chart_data, dict):
                        formatted_chart = {
                            "type": chart_data.get("type", "bar"),
                            "title": chart_data.get("title", f"{deal.company_name} - {chart_name}"),
                            "data": chart_data.get("data", {}),
                            "company": deal.company_name if hasattr(deal, 'company_name') else "Unknown"
                        }
                        all_charts.append(formatted_chart)
        return all_charts
    
    def _format_founder_history(self, founder_profile: Dict[str, Any]) -> str:
        """Format founder background with work history and previous companies"""
        work_history = founder_profile.get('work_history', [])
        previous_companies = founder_profile.get('previous_companies', [])
        
        background_parts = []
        
        # Add work history (last 3 roles)
        if work_history and isinstance(work_history, list):
            for role in work_history[:3]:
                if isinstance(role, dict):
                    company_name = role.get('company', '')
                    title = role.get('title', '')
                    if company_name and title:
                        background_parts.append(f"{title} at {company_name}")
        
        # Add previous companies if no work history
        if not background_parts and previous_companies:
            if isinstance(previous_companies, list):
                background_parts.append(f"Previously at {', '.join(previous_companies[:3])}")
            else:
                background_parts.append(f"Previously at {previous_companies}")
        
        return "; ".join(background_parts) if background_parts else ""
    
    def _format_business_model_bullets(self, company: Dict[str, Any]) -> List[str]:
        """Convert business model data into formatted bullets"""
        bullets = []
        
        one_liner = company.get('one_liner', '')
        if one_liner:
            bullets.append(f"• {one_liner}")
        
        product_desc = company.get('product_description', '')
        if product_desc:
            # Truncate if too long
            desc = product_desc[:200] + "..." if len(product_desc) > 200 else product_desc
            bullets.append(f"• Product: {desc}")
        
        target_market = company.get('target_market', company.get('who_they_sell_to', ''))
        if target_market:
            bullets.append(f"• Target Market: {target_market}")
        
        pricing = company.get('pricing_model', '')
        if pricing:
            bullets.append(f"• Pricing Model: {pricing}")
        
        business_model = company.get('business_model', '')
        if business_model and business_model != product_desc:
            bullets.append(f"• Business Model: {business_model[:150]}")
        
        return bullets
    
    def _format_fund_fit_bullets(self, deal: Any, company: Dict[str, Any]) -> List[str]:
        """Extract and format fund fit analysis into bullets"""
        bullets = []
        
        if hasattr(deal, 'fund_fit_score') and deal.fund_fit_score is not None:
            bullets.append(f"• Fund Fit Score: {deal.fund_fit_score:.1f}/100")
        
        fund_fit_details = company.get('fund_fit_details', {})
        if fund_fit_details:
            recommendation = fund_fit_details.get('recommendation', '')
            if recommendation:
                bullets.append(f"• Recommendation: {recommendation}")
            
            reasons = fund_fit_details.get('reasons', [])
            if isinstance(reasons, list):
                for reason in reasons[:3]:
                    if reason:
                        bullets.append(f"• {reason}")
            elif isinstance(reasons, str):
                bullets.append(f"• {reasons}")
        
        # Add fund economics if available
        fund_context = self.shared_data.get('fund_context', {})
        investment_amount = fund_context.get('investment_amount', 0)
        if investment_amount > 0 and hasattr(deal, 'ownership_target'):
            bullets.append(f"• Check Size: ${investment_amount/1e6:.1f}M for {deal.ownership_target*100:.1f}% ownership")
        
        return bullets
    
    def _format_analysis_to_bullets(self, analysis: Dict[str, Any]) -> List[str]:
        """Convert comprehensive_analysis JSON structure to readable bullets"""
        bullets = []
        
        if not analysis or not isinstance(analysis, dict):
            return bullets
        
        # Extract unit economics insights
        unit_econ = analysis.get('unit_economics', {})
        if unit_econ and isinstance(unit_econ, dict):
            for company_name, analysis_text in unit_econ.items():
                if isinstance(analysis_text, str) and analysis_text:
                    bullets.append(f"• {company_name} Unit Economics: {analysis_text[:150]}")
        
        # Extract margin analysis
        margin = analysis.get('margin_analysis', {})
        if margin and isinstance(margin, dict):
            for company_name, margin_text in margin.items():
                if isinstance(margin_text, str) and margin_text:
                    bullets.append(f"• {company_name} Margins: {margin_text[:150]}")
        
        # Extract investment implications
        implications = analysis.get('investment_implications', '')
        if implications:
            if isinstance(implications, str):
                bullets.append(f"• Key Insight: {implications[:200]}")
            elif isinstance(implications, list):
                for impl in implications[:3]:
                    if impl:
                        bullets.append(f"• {impl[:150]}")
        
        # Extract competitive analysis
        competitive = analysis.get('competitive_analysis', {})
        if competitive and isinstance(competitive, dict):
            for key, value in competitive.items():
                if isinstance(value, str) and value:
                    bullets.append(f"• {key.replace('_', ' ').title()}: {value[:150]}")
        
        return bullets
    
    def _format_comparative_analysis_to_bullets(self, comparative_analysis: Dict[str, Any]) -> List[str]:
        """Convert comparative_analysis JSON to readable narrative bullets"""
        bullets = []
        
        if not comparative_analysis or not isinstance(comparative_analysis, dict):
            return bullets
        
        # Extract key comparison points
        strengths = comparative_analysis.get('strengths', {})
        if strengths and isinstance(strengths, dict):
            for company, strength_list in strengths.items():
                if isinstance(strength_list, list):
                    for strength in strength_list[:2]:
                        if strength:
                            bullets.append(f"• {company} Strength: {strength[:150]}")
        
        risks = comparative_analysis.get('risks', {})
        if risks and isinstance(risks, dict):
            for company, risk_list in risks.items():
                if isinstance(risk_list, list):
                    for risk in risk_list[:2]:
                        if risk:
                            bullets.append(f"• {company} Risk: {risk[:150]}")
        
        recommendation = comparative_analysis.get('recommendation', '')
        if recommendation:
            bullets.append(f"• Recommendation: {recommendation[:200]}")
        
        return bullets
    
    # ------------------------------------------------------------------
    # New tools: company list reasoning, @ cell enrichment, inline todos, CRM sync
    # ------------------------------------------------------------------

    async def _tool_reason_company_list(self, inputs: dict) -> dict:
        """Ranked reasoning over a set of companies: relevance, gaps, next steps."""
        company_names = inputs.get("companies", [])
        objective = inputs.get("objective", "investment analysis")

        # Gather whatever we already have in shared_data
        companies = self.shared_data.get("companies", [])
        matched = []
        for name in company_names:
            clean = name.replace("@", "").strip().lower()
            match = next(
                (c for c in companies
                 if (c.get("company", "") or "").lower() == clean
                 or (c.get("name", "") or "").lower() == clean),
                None,
            )
            matched.append(match or {"company": name, "_missing": True})

        # Build a compact summary for each company
        summaries = []
        gaps = []
        for c in matched:
            cname = c.get("company") or c.get("name", "?")
            if c.get("_missing"):
                gaps.append(cname)
                summaries.append(f"- **{cname}**: No data fetched yet — needs enrichment")
                continue
            rev = c.get("arr") or c.get("revenue") or c.get("inferred_revenue")
            val = c.get("valuation") or c.get("inferred_valuation")
            stage = c.get("stage", "Unknown")
            desc = (c.get("description") or c.get("product_description") or "")[:120]
            missing_fields = [k for k in ("arr", "valuation", "stage", "total_funding") if not c.get(k)]
            if missing_fields:
                gaps.append(f"{cname} missing: {', '.join(missing_fields)}")
            summaries.append(
                f"- **{cname}** ({stage}): {desc}  |  Rev: ${(rev or 0)/1e6:.1f}M  Val: ${(val or 0)/1e6:.0f}M"
            )

        # Build todos for gaps
        todos = []
        for g in gaps:
            todos.append({
                "type": "action_item",
                "title": f"Fill gaps: {g}",
                "description": f"Run enrichment or web search to fill missing data for {g}",
                "priority": "high",
            })

        return {
            "reasoning": f"## Company List Analysis ({objective})\n\n" + "\n".join(summaries),
            "gaps": gaps,
            "suggestions": todos,
            "company_count": len(matched),
            "data_complete_count": len([c for c in matched if not c.get("_missing")]),
        }

    async def _tool_build_company_list(self, inputs: dict) -> dict:
        """LEGACY — delegates to source_companies with discover_web=true.

        Kept for backward compatibility. All new sourcing should go through
        source_companies directly, which has the same rubric-driven pipeline
        but with 2-round web discovery and proper DB+web merge.
        """
        from app.services.sourcing_service import generate_rubric

        criteria = inputs.get("criteria", "")
        sector = inputs.get("sector", "")
        stage = inputs.get("stage", "")
        geography = inputs.get("geography", "")
        max_results = min(inputs.get("max_results", 10), 20)

        thesis = " ".join(filter(None, [criteria, sector, stage, geography]))
        if not thesis.strip():
            return {"error": "At least one search criterion required (criteria, sector, stage, or geography)"}

        # Generate rubric to get weights + filters
        rubric = generate_rubric(
            thesis_description=thesis,
            weight_overrides=inputs.get("scoring_rubric") or None,
            target_stage=stage or None,
            filters={"sector": sector, "geography": geography} if (sector or geography) else None,
        )

        logger.info(
            "[BUILD_COMPANY_LIST] delegating to source_companies(discover_web=true) intent=%s",
            rubric.get("intent"),
        )

        # Delegate to source_companies with web discovery enabled
        result = await self._tool_source_companies({
            "filters": rubric.get("filters", {}),
            "custom_weights": rubric.get("weights"),
            "target_stage": stage or rubric.get("target_stage"),
            "discover_web": True,
            "thesis": thesis,
            "min_web_threshold": 5,
            "max_results": max_results,
            "persist_results": inputs.get("persist", True),
            "display": "ranked_list",
        })

        # Add grid commands if requested
        add_to_matrix = inputs.get("add_to_matrix", False)
        if add_to_matrix and result.get("companies"):
            grid_commands = []
            for c in result["companies"]:
                grid_commands.append({
                    "action": "add_row",
                    "companyName": c.get("name", ""),
                    "company_id": c.get("company_id"),
                    "cellValues": {
                        "arr": c.get("arr"),
                        "valuation": c.get("valuation"),
                        "stage": c.get("stage"),
                        "sector": c.get("sector"),
                        "headcount": c.get("employee_count"),
                        "totalRaised": c.get("total_funding"),
                        "description": c.get("description"),
                    },
                    "source_service": "build_company_list",
                })
            result["grid_commands"] = result.get("grid_commands", []) + grid_commands

        return result

    async def _tool_score_and_rank_companies(self, inputs: dict) -> dict:
        """Score companies from shared_data by configurable rubric and return ranked list.

        Dimensions: fund_fit, growth, moat, capital_efficiency, momentum.
        Optionally run full valuations. Returns ranked list + grid_commands for scores.
        """
        company_names = inputs.get("companies") or []
        rubric = inputs.get("scoring_rubric") or {}
        run_valuations = inputs.get("run_valuations", False)
        fund_id = inputs.get("fund_id") or self.shared_data.get("fund_id")

        # Gather companies from shared_data
        shared_companies = self.shared_data.get("companies", [])
        if not company_names:
            # Score all companies in shared_data
            targets = shared_companies
        else:
            name_set = {n.lower().strip().lstrip("@") for n in company_names}
            targets = [
                c for c in shared_companies
                if isinstance(c, dict)
                and (c.get("company") or c.get("name") or "").lower().strip() in name_set
            ]

        if not targets:
            return {"error": "No companies found in shared_data to score", "companies": []}

        # Rubric weights
        weights = {
            "fund_fit": rubric.get("fund_fit", 0.25),
            "growth": rubric.get("growth", 0.20),
            "moat": rubric.get("moat", 0.15),
            "capital_efficiency": rubric.get("capital_efficiency", 0.20),
            "momentum": rubric.get("momentum", 0.20),
        }

        scored = []
        for c in targets:
            name = c.get("company") or c.get("name") or "Unknown"
            scores = {}

            # Fund fit
            ff = c.get("fund_fit_score") or 0
            if hasattr(ff, "value"):
                ff = ff.value
            scores["fund_fit"] = min(10, float(ff or 0))

            # Growth
            gr = c.get("growth_rate") or 0
            if hasattr(gr, "value"):
                gr = gr.value
            gr = float(gr or 0)
            scores["growth"] = min(10, max(0, gr * 3.3)) if gr > 0 else 2

            # Moat
            moat = c.get("moat_score") or c.get("switching_costs") or 5
            if hasattr(moat, "value"):
                moat = moat.value
            scores["moat"] = min(10, float(moat or 5))

            # Capital efficiency (revenue / total funding)
            rev = c.get("arr") or c.get("revenue") or c.get("inferred_revenue") or 0
            if hasattr(rev, "value"):
                rev = rev.value
            funding = c.get("total_funding") or c.get("total_raised") or 0
            if hasattr(funding, "value"):
                funding = funding.value
            if funding and float(funding) > 0:
                ratio = float(rev or 0) / float(funding)
                scores["capital_efficiency"] = min(10, ratio * 10)
            else:
                scores["capital_efficiency"] = 3

            # Momentum — based on recency of funding + growth rate
            months_since = c.get("months_since_last_raise") or 24
            if hasattr(months_since, "value"):
                months_since = months_since.value
            recency = max(0, 10 - (int(months_since or 24) / 3))
            scores["momentum"] = min(10, (recency + scores["growth"]) / 2)

            composite = sum(scores[d] * weights[d] for d in weights)
            scored.append({
                "name": name,
                "company_id": c.get("id") or c.get("company_id"),
                "scores": {k: round(v, 1) for k, v in scores.items()},
                "composite_score": round(composite, 2),
                "arr": rev,
                "valuation": c.get("valuation") or c.get("inferred_valuation"),
                "stage": c.get("stage"),
                "sector": c.get("sector"),
            })

        scored.sort(key=lambda x: x["composite_score"], reverse=True)

        # Build grid commands to write scores
        grid_commands = []
        for rank, s in enumerate(scored, 1):
            cid = s.get("company_id") or self._find_company_id(s["name"])
            if cid:
                grid_commands.append({
                    "action": "edit",
                    "rowId": cid,
                    "columnId": "composite_score",
                    "value": s["composite_score"],
                    "source_service": "score_and_rank",
                    "reasoning": f"Rank #{rank}: " + ", ".join(f"{k}={v}" for k, v in s["scores"].items()),
                })

        return {
            "ranked_companies": scored,
            "count": len(scored),
            "scoring_weights": weights,
            "grid_commands": grid_commands,
        }

    async def _tool_find_comparables(self, inputs: dict) -> dict:
        """Find comparable companies by sector/stage/business model/revenue range/geography.

        Searches DB + web for similar companies. Returns similarity-scored list.
        Persists new discoveries to DB.
        """
        target_company = inputs.get("company", "").strip().lstrip("@")
        if not target_company:
            return {"error": "company name required"}

        fund_id = inputs.get("fund_id") or self.shared_data.get("fund_id")
        max_results = min(inputs.get("max_results", 10), 20)

        # Resolve target from shared_data or DB
        target_data = None
        for c in self.shared_data.get("companies", []):
            if isinstance(c, dict) and (c.get("company") or c.get("name") or "").lower().strip() == target_company.lower():
                target_data = c
                break

        if not target_data:
            try:
                from app.services.portfolio_service import portfolio_service
                target_data = await portfolio_service.get_company_by_name(target_company, fund_id)
            except Exception:
                pass

        if not target_data:
            target_data = {"company": target_company}

        # Build comp criteria from target
        target_sector = target_data.get("sector", "")
        target_stage = target_data.get("stage", "")
        target_bm = target_data.get("business_model", "")
        target_arr = target_data.get("arr") or target_data.get("current_arr_usd") or target_data.get("inferred_revenue") or 0
        if hasattr(target_arr, "value"):
            target_arr = target_arr.value
        target_arr = float(target_arr or 0)
        target_geo = target_data.get("hq_location") or target_data.get("hq") or ""

        # DB search for similar companies
        db_comps = []
        try:
            from app.services.portfolio_service import portfolio_service
            # Search by sector
            if target_sector:
                resp = await portfolio_service.search_companies_db(target_sector, limit=max_results * 2)
                db_comps.extend(resp.get("companies", []))
            # Search by business model
            if target_bm:
                resp = await portfolio_service.search_companies_db(target_bm, limit=max_results)
                seen_ids = {c.get("company_id") for c in db_comps}
                for c in resp.get("companies", []):
                    if c.get("company_id") not in seen_ids:
                        db_comps.append(c)
        except Exception as e:
            logger.warning(f"[FIND_COMPARABLES] DB search failed: {e}")

        # Remove the target itself
        db_comps = [c for c in db_comps if (c.get("name") or "").lower().strip() != target_company.lower()]

        # Web search for additional comps
        web_comps = []
        if len(db_comps) < max_results and hasattr(self, '_execute_web_search'):
            comp_query = f"{target_company} competitors similar companies {target_sector} {target_stage}".strip()
            try:
                wr = await self._execute_web_search({"query": comp_query, "max_results": 8})
                if isinstance(wr, dict):
                    # LLM extract names
                    snippets = "\n".join(
                        f"- {r.get('title', '')}: {(r.get('snippet') or '')[:200]}"
                        for r in wr.get("results", [])[:10]
                    )
                    try:
                        from app.services.model_router import ModelCapability
                        extract_result = await self.model_router.get_completion(
                            prompt=(
                                f"From these search results about companies similar to {target_company}, "
                                f"extract competitor/comparable company names.\n\n{snippets}\n\n"
                                f"Return JSON: {{\"companies\": [\"Name1\", \"Name2\", ...]}}"
                            ),
                            system_prompt="Extract company names. Return valid JSON only.",
                            capability=ModelCapability.FAST,
                            max_tokens=300,
                            temperature=0.0,
                            json_mode=True,
                            caller_context="find_comparables_extract",
                        )
                        raw = extract_result.get("response", "{}") if isinstance(extract_result, dict) else str(extract_result)
                        parsed = json.loads(raw) if isinstance(raw, str) else raw
                        existing_names = {c.get("name", "").lower() for c in db_comps} | {target_company.lower()}
                        for name in parsed.get("companies", []):
                            if name.lower().strip() not in existing_names and 2 < len(name) < 60:
                                web_comps.append(name.strip())
                    except Exception:
                        pass
            except Exception as e:
                logger.warning(f"[FIND_COMPARABLES] Web search failed: {e}")

        # Enrich web-found companies (limited)
        if web_comps:
            semaphore = asyncio.Semaphore(3)

            async def enrich_comp(name: str) -> dict:
                async with semaphore:
                    try:
                        result = await self._execute_company_fetch({"company": name, "fund_id": fund_id})
                        companies_list = result.get("companies", []) if isinstance(result, dict) else []
                        cd = companies_list[0] if companies_list else (result if isinstance(result, dict) else {})
                        arr_v = cd.get("arr") or cd.get("revenue") or cd.get("inferred_revenue") or 0
                        if hasattr(arr_v, "value"):
                            arr_v = arr_v.value
                        return {
                            "name": name,
                            "company_id": cd.get("id"),
                            "sector": cd.get("sector", ""),
                            "stage": cd.get("stage", ""),
                            "arr": arr_v,
                            "valuation": cd.get("valuation") or cd.get("inferred_valuation"),
                            "business_model": cd.get("business_model", ""),
                            "hq": cd.get("hq_location", ""),
                            "description": (cd.get("description") or "")[:200],
                            "source": "web_enriched",
                        }
                    except Exception:
                        return {"name": name, "source": "web_failed", "error": True}

            enrich_tasks = [enrich_comp(n) for n in web_comps[:max(0, max_results - len(db_comps))]]
            enriched = await asyncio.gather(*enrich_tasks)
            enriched = [e for e in enriched if not e.get("error")]
            db_comps.extend(enriched)

        # Similarity scoring
        for c in db_comps:
            sim = 0.0
            total_weight = 0.0

            # Sector match (weight 3)
            c_sector = (c.get("sector") or "").lower()
            if target_sector and c_sector:
                from difflib import SequenceMatcher
                sim += 3 * SequenceMatcher(None, target_sector.lower(), c_sector).ratio()
            total_weight += 3

            # Stage match (weight 2)
            c_stage = (c.get("stage") or "").lower()
            if target_stage and c_stage:
                sim += 2 * (1.0 if target_stage.lower() == c_stage else 0.3)
            total_weight += 2

            # Revenue range (weight 2) — within 5x
            c_arr = c.get("arr") or 0
            if hasattr(c_arr, "value"):
                c_arr = c_arr.value
            c_arr = float(c_arr or 0)
            if target_arr > 0 and c_arr > 0:
                ratio = min(target_arr, c_arr) / max(target_arr, c_arr)
                sim += 2 * ratio
            total_weight += 2

            # Business model (weight 2)
            c_bm = (c.get("business_model") or "").lower()
            if target_bm and c_bm:
                from difflib import SequenceMatcher
                sim += 2 * SequenceMatcher(None, target_bm.lower(), c_bm).ratio()
            total_weight += 2

            # Geography (weight 1)
            c_geo = (c.get("hq") or c.get("hq_location") or "").lower()
            if target_geo and c_geo:
                sim += 1 * (1.0 if target_geo.lower() in c_geo or c_geo in target_geo.lower() else 0.2)
            total_weight += 1

            c["similarity_score"] = round(sim / total_weight * 10, 1) if total_weight > 0 else 0

        db_comps.sort(key=lambda x: x.get("similarity_score", 0), reverse=True)
        final = db_comps[:max_results]

        return {
            "target": target_company,
            "target_profile": {
                "sector": target_sector,
                "stage": target_stage,
                "arr": target_arr,
                "business_model": target_bm,
                "geography": target_geo,
            },
            "comparables": final,
            "count": len(final),
        }

    async def _tool_add_companies_to_matrix(self, inputs: dict) -> dict:
        """Batch add companies to the matrix grid.

        Persists each company to DB and emits add_row grid commands with pre-populated cells.
        Triggers background enrichment for sparse fields.
        """
        company_names = inputs.get("companies") or []
        fund_id = inputs.get("fund_id") or self.shared_data.get("fund_id")

        if not company_names:
            return {"error": "companies list required"}

        grid_commands = []
        results = []
        semaphore = asyncio.Semaphore(3)

        async def add_one(name: str) -> dict:
            async with semaphore:
                name = name.strip().lstrip("@")
                # Check if already in shared_data
                existing = None
                for c in self.shared_data.get("companies", []):
                    if isinstance(c, dict) and (c.get("company") or c.get("name") or "").lower() == name.lower():
                        existing = c
                        break

                if not existing:
                    # Fetch and enrich
                    try:
                        result = await self._execute_company_fetch({"company": name, "fund_id": fund_id})
                        companies_list = result.get("companies", []) if isinstance(result, dict) else []
                        existing = companies_list[0] if companies_list else {}
                    except Exception as e:
                        return {"name": name, "error": str(e)}

                # Build grid command
                arr_val = existing.get("arr") or existing.get("revenue") or existing.get("inferred_revenue")
                if hasattr(arr_val, "value"):
                    arr_val = arr_val.value
                val = existing.get("valuation") or existing.get("inferred_valuation")
                if hasattr(val, "value"):
                    val = val.value

                company_id = existing.get("id") or existing.get("company_id") or self._find_company_id(name)

                cmd = {
                    "action": "add_row",
                    "companyName": name,
                    "company_id": company_id,
                    "cellValues": {
                        "arr": arr_val,
                        "valuation": val,
                        "stage": existing.get("stage"),
                        "sector": existing.get("sector"),
                        "headcount": existing.get("employee_count") or existing.get("team_size"),
                        "totalRaised": existing.get("total_funding") or existing.get("total_raised"),
                        "burnRate": existing.get("burn_rate"),
                        "runway": existing.get("runway_months"),
                        "description": (existing.get("description") or existing.get("product_description") or "")[:200],
                    },
                    "source_service": "add_companies_to_matrix",
                }
                return {"name": name, "company_id": company_id, "grid_command": cmd}

        tasks = [add_one(n) for n in company_names[:20]]
        raw_results = await asyncio.gather(*tasks)

        for r in raw_results:
            if r.get("error"):
                results.append({"name": r["name"], "status": "failed", "error": r["error"]})
            else:
                results.append({"name": r["name"], "company_id": r.get("company_id"), "status": "added"})
                grid_commands.append(r["grid_command"])

        return {
            "results": results,
            "added_count": len(grid_commands),
            "failed_count": sum(1 for r in results if r["status"] == "failed"),
            "grid_commands": grid_commands,
        }

    async def _tool_remove_company_from_matrix(self, inputs: dict) -> dict:
        """Archive or soft-delete a company from the matrix grid.

        Emits a delete grid_command for the frontend to remove the row.
        """
        company_name = (inputs.get("company") or "").strip().lstrip("@")
        if not company_name:
            return {"error": "company name required"}

        company_id = inputs.get("company_id") or self._find_company_id(company_name)
        if not company_id:
            return {"error": f"Could not find company_id for '{company_name}'"}

        grid_commands = [{
            "action": "delete",
            "rowId": company_id,
            "companyName": company_name,
            "source_service": "remove_company_from_matrix",
        }]

        return {
            "removed": company_name,
            "company_id": company_id,
            "grid_commands": grid_commands,
        }

    async def _tool_refresh_company_data(self, inputs: dict) -> dict:
        """Re-fetch and re-enrich a company. Emits suggestion diffs for changed fields."""
        company_name = (inputs.get("company") or "").strip().lstrip("@")
        if not company_name:
            return {"error": "company name required"}

        fund_id = inputs.get("fund_id") or self.shared_data.get("fund_id")

        # Get old data from shared_data
        old_data = None
        for c in self.shared_data.get("companies", []):
            if isinstance(c, dict) and (c.get("company") or c.get("name") or "").lower() == company_name.lower():
                old_data = c
                break

        # Force fresh fetch (bypass cache)
        cache_key = company_name.lower().strip()
        self._company_cache.pop(cache_key, None)

        try:
            result = await self._execute_company_fetch({"company": company_name, "fund_id": fund_id})
            companies_list = result.get("companies", []) if isinstance(result, dict) else []
            new_data = companies_list[0] if companies_list else {}
        except Exception as e:
            return {"error": f"Re-fetch failed: {e}"}

        # Diff old vs new to create suggestion commands
        diff_fields = []
        grid_commands = []
        company_id = new_data.get("id") or self._find_company_id(company_name)
        compare_fields = ["arr", "revenue", "valuation", "stage", "sector", "employee_count",
                          "team_size", "total_funding", "burn_rate", "runway_months", "growth_rate"]

        for field in compare_fields:
            old_val = old_data.get(field) if old_data else None
            new_val = new_data.get(field)
            if hasattr(old_val, "value"):
                old_val = old_val.value
            if hasattr(new_val, "value"):
                new_val = new_val.value
            if new_val is not None and new_val != old_val:
                diff_fields.append({"field": field, "old": old_val, "new": new_val})
                if company_id:
                    # Map field to grid column
                    col_map = {"arr": "arr", "revenue": "arr", "valuation": "valuation",
                               "stage": "stage", "sector": "sector", "employee_count": "headcount",
                               "team_size": "headcount", "total_funding": "totalRaised",
                               "burn_rate": "burnRate", "runway_months": "runway", "growth_rate": "revenueGrowthAnnual"}
                    col_id = col_map.get(field, field)
                    grid_commands.append({
                        "action": "edit",
                        "rowId": company_id,
                        "columnId": col_id,
                        "value": new_val,
                        "source_service": "refresh_company_data",
                        "reasoning": f"Updated {field}: {old_val} → {new_val}",
                    })

        return {
            "company": company_name,
            "refreshed": True,
            "changes": diff_fields,
            "change_count": len(diff_fields),
            "grid_commands": grid_commands,
        }

    async def _tool_bulk_score_matrix(self, inputs: dict) -> dict:
        """Score all companies in the grid by rubric, write to score column.

        Reads grid snapshot, scores each company, emits edit grid_commands.
        """
        rubric = inputs.get("scoring_rubric") or {}
        fund_id = inputs.get("fund_id") or self.shared_data.get("fund_id")

        matrix_ctx = self.shared_data.get("matrix_context") or {}
        grid_snap = matrix_ctx.get("gridSnapshot", {}) if isinstance(matrix_ctx.get("gridSnapshot"), dict) else {}
        rows = grid_snap.get("rows", [])

        if not rows:
            return {"error": "No grid data available. Grid must be open with companies."}

        # Build company-like dicts from grid rows for scoring
        targets = []
        for row in rows:
            cells = row.get("cells") or row.get("cellValues") or {}
            targets.append({
                "company": row.get("companyName") or row.get("company_name") or "",
                "id": row.get("id") or row.get("rowId"),
                "arr": cells.get("arr"),
                "valuation": cells.get("valuation"),
                "growth_rate": cells.get("revenueGrowthAnnual") or cells.get("growth_rate"),
                "stage": cells.get("stage"),
                "sector": cells.get("sector"),
                "total_funding": cells.get("totalRaised"),
                "employee_count": cells.get("headcount"),
                "burn_rate": cells.get("burnRate"),
                "fund_fit_score": cells.get("composite_score"),
            })

        # Reuse score_and_rank logic
        self.shared_data.setdefault("companies", []).extend(targets)
        result = await self._tool_score_and_rank_companies({
            "companies": [t["company"] for t in targets if t["company"]],
            "scoring_rubric": rubric,
            "fund_id": fund_id,
        })

        # Clean up injected entries
        for t in targets:
            if t in self.shared_data.get("companies", []):
                self.shared_data["companies"].remove(t)

        return result

    async def _tool_search_extract(self, inputs: dict) -> dict:
        """Targeted web search + structured extraction combo.

        Searches for specific data points and returns extracted structured data.
        Chains: web_search → LLM extraction → grid suggestions.
        """
        query = inputs.get("query", "")
        company = inputs.get("company", "")
        extract_fields = inputs.get("extract_fields", [])

        if not query and not company:
            return {"error": "query or company required"}

        # If company provided, use granular search
        if company:
            field_schema = {}
            default_fields = ["revenue", "valuation", "funding", "stage", "employee_count", "description"]
            for f in (extract_fields or default_fields):
                field_schema[f] = "number, string, or null"
            return await self._granular_search_and_extract(
                company_name=company,
                search_suffix=query or "company data funding revenue",
                extract_fields=field_schema,
                extraction_instructions="Extract all available company data. Use exact numbers when found.",
            )

        # Generic query: web search + extraction
        try:
            search_result = await self._execute_tool("web_search", {
                "query": query,
                "search_depth": "basic",
            })
            results = search_result.get("results", []) if isinstance(search_result, dict) else []
        except Exception as e:
            return {"query": query, "results": [], "error": str(e)}

        # Extract company names from results if no specific fields requested
        extracted_items = []
        for r in results[:5]:
            extracted_items.append({
                "title": r.get("title", ""),
                "url": r.get("url", ""),
                "snippet": (r.get("content") or r.get("snippet") or "")[:300],
            })

        return {
            "query": query,
            "results": extracted_items,
            "count": len(extracted_items),
        }

    async def _tool_enrich_sparse_grid(self, inputs: dict) -> dict:
        """Auto-detect companies with sparse data in the grid and generate suggestions.

        Scans grid for companies with many empty fields, runs web search + benchmarks
        to fill gaps, and writes results as pending_suggestions.
        """
        fund_id = inputs.get("fund_id")
        min_empty = inputs.get("min_empty_fields", 3)

        matrix_ctx = self.shared_data.get("matrix_context") or {}
        grid_rows = matrix_ctx.get("rows") or matrix_ctx.get("gridData") or []

        if not grid_rows:
            return {"error": "No grid data available", "enriched": 0}

        # Identify sparse companies
        CHECK_FIELDS = ["arr", "currentValuationUsd", "headcount", "totalRaised",
                        "burnRateMonthlyUsd", "revenueGrowthAnnualPct", "stage"]
        sparse_companies = []
        for row in grid_rows:
            name = row.get("name") or row.get("companyName") or ""
            if not name:
                continue
            empty_count = sum(1 for f in CHECK_FIELDS if not row.get(f))
            if empty_count >= min_empty:
                sparse_companies.append({
                    "name": name,
                    "id": row.get("id") or row.get("companyId"),
                    "empty_fields": empty_count,
                    "existing_data": row,
                })

        if not sparse_companies:
            return {"message": "All companies have sufficient data", "enriched": 0, "checked": len(grid_rows)}

        # Sort by sparseness (most empty first)
        sparse_companies.sort(key=lambda x: x["empty_fields"], reverse=True)

        # Enrich top sparse companies using micro-skills (max 10)
        enriched_count = 0
        results = []
        for sc in sparse_companies[:10]:
            try:
                empty_fields = [f for f in CHECK_FIELDS if not sc["existing_data"].get(f)]
                result = await self._tool_enrich_field({
                    "company_name": sc["name"],
                    "fields": empty_fields,
                })
                suggestions_written = result.get("suggestions_persisted", 0)
                fields_found = result.get("fields_found", [])
                enriched_count += 1
                results.append({
                    "name": sc["name"],
                    "suggestions": suggestions_written,
                    "fields_found": fields_found,
                    "success": True,
                })
            except Exception as e:
                results.append({"name": sc["name"], "error": str(e), "success": False})

        return {
            "checked": len(grid_rows),
            "sparse_found": len(sparse_companies),
            "enriched": enriched_count,
            "results": results,
        }

    async def _tool_enrich_cell(self, inputs: dict) -> dict:
        """Deep-enrich a single company cell. No @ prefix needed."""
        company_name = (inputs.get("company") or "").replace("@", "").strip()
        column = inputs.get("column", "")
        current_value = inputs.get("current_value")

        if not company_name:
            return {"error": "company name required"}

        # Try lightweight diligence first
        ld_memo_sections = []
        try:
            ld_result = await self._execute_lightweight_diligence({"company_name": company_name})
            company_data = ld_result.get("company", {}) if isinstance(ld_result, dict) else {}
            ld_memo_sections = ld_result.get("memo_sections", [])
        except Exception as e:
            logger.warning(f"[ENRICH_CELL] Lightweight diligence failed for {company_name}: {e}")
            company_data = {}

        # Extract relevant field
        field_map = {
            "valuation": ["valuation", "inferred_valuation"],
            "arr": ["arr", "revenue", "inferred_revenue"],
            "revenue": ["revenue", "arr", "inferred_revenue"],
            "stage": ["stage"],
            "sector": ["sector"],
            "description": ["description", "product_description"],
            "total_funding": ["total_funding"],
            "headcount": ["team_size", "employee_count"],
            "gross_margin": ["gross_margin"],
            "business_model": ["business_model"],
            "team_size": ["team_size", "employee_count"],
            "target_market": ["target_market"],
            "pricing_model": ["pricing_model"],
            "hq_location": ["hq_location", "hq"],
        }
        candidates = field_map.get(column, [column])
        enriched_value = None
        for field in candidates:
            v = company_data.get(field)
            if v is not None:
                enriched_value = v
                break

        if enriched_value is None:
            return {
                "suggestion": None,
                "message": f"Could not find data for {company_name}.{column}",
                "memo_sections": ld_memo_sections,
            }

        return {
            "suggestion": {
                "action": "edit",
                "rowId": company_name,
                "columnId": column,
                "value": enriched_value,
                "reasoning": f"Enriched from web search for {company_name}",
                "confidence": company_data.get("confidence", 0.6),
                "source_service": "agent.enrich_cell",
            },
            "grid_command": {
                "action": "edit",
                "rowId": company_name,
                "columnId": column,
                "value": enriched_value,
                "reasoning": f"Enriched from web search for {company_name}",
                "confidence": company_data.get("confidence", 0.6),
            },
            "memo_sections": ld_memo_sections,
        }

    async def _tool_write_to_memo(self, inputs: dict) -> dict:
        """Primary output tool. Appends a section to the memo canvas.

        The agent calls this incrementally to build analysis. Each call
        streams a section to the frontend MemoEditor in real-time.
        """
        section = {
            "id": f"sec_{len(self.shared_data.get('memo_sections', []))}",
            "title": inputs.get("section_title", ""),
            "text": inputs.get("text", ""),
            "type": "heading2" if inputs.get("section_title") else "paragraph",
            "content": inputs.get("text", ""),
            "created_at": datetime.now().isoformat(),
        }
        # Optional chart
        if inputs.get("chart_type") and inputs.get("chart_data"):
            section["chart"] = {
                "type": inputs["chart_type"],
                "data": inputs["chart_data"],
                "title": inputs.get("section_title", ""),
            }
            section["type"] = "chart"
        # Optional table
        if inputs.get("table"):
            section["table"] = inputs["table"]
            if not section.get("chart"):
                section["type"] = "table"

        async with self.shared_data_lock:
            if "memo_sections" not in self.shared_data:
                self.shared_data["memo_sections"] = []
            self.shared_data["memo_sections"].append(section)

        return {
            "success": True,
            "section_id": section["id"],
            "memo_sections": [section],
        }

    async def _tool_emit_todo(self, inputs: dict) -> dict:
        """Emit an inline todo that appears in chat and feeds into the suggestions panel."""
        todo = {
            "type": "action_item",
            "title": inputs.get("title", ""),
            "description": inputs.get("description", ""),
            "priority": inputs.get("priority", "medium"),
            "company": inputs.get("company", ""),
            "due": inputs.get("due", ""),
        }

        # Persist to pending_suggestions so it survives page refresh
        fund_id = self.shared_data.get("fund_context", {}).get("fundId")
        company_id = inputs.get("company", "") or "portfolio"
        if fund_id:
            try:
                supabase_url = settings.SUPABASE_URL
                supabase_key = settings.SUPABASE_SERVICE_ROLE_KEY or settings.SUPABASE_ANON_KEY
                if supabase_url and supabase_key:
                    from supabase import create_client
                    sb = create_client(supabase_url, supabase_key)
                    sb.table("pending_suggestions").upsert({
                        "fund_id": fund_id,
                        "company_id": company_id,
                        "column_id": "_todo",
                        "suggested_value": json.dumps(todo),
                        "source_service": "agent.emit_todo",
                        "reasoning": inputs.get("description", ""),
                        "metadata": {"tool": "emit_todo", "priority": todo["priority"]},
                    }, on_conflict="fund_id,company_id,column_id").execute()
            except Exception as e:
                logger.warning(f"[TOOL] Failed to persist todo: {e}")

        return {"suggestion": todo, "todo": todo}

    async def _tool_sync_crm(self, inputs: dict) -> dict:
        """Sync companies from matrix to CRM (Attio/Affinity via MCP)."""
        try:
            from app.services.crm import get_crm_provider
        except ImportError:
            return {"error": "CRM module not available"}

        direction = inputs.get("direction", "push")
        matrix_context = self.shared_data.get("matrix_context") or {}
        grid_snapshot = matrix_context.get("gridSnapshot") or {}
        rows = grid_snapshot.get("rows", [])

        # Filter to requested companies if specified
        requested = inputs.get("companies")
        if requested:
            clean_names = {n.replace("@", "").strip().lower() for n in requested}
            rows = [r for r in rows if (r.get("companyName") or "").lower() in clean_names]

        if not rows:
            return {"error": "No companies found in matrix to sync", "synced": 0}

        provider = get_crm_provider()
        if direction == "pull":
            try:
                crm_data = await provider.pull_all(limit=100)
                return {"pulled": crm_data, "count": len(crm_data.get("companies", []))}
            except Exception as e:
                return {"error": f"CRM pull failed: {e}"}

        # Default: push to CRM
        try:
            result = await provider.sync_companies_from_matrix(rows)
            return {
                "synced": result.created + result.updated,
                "created": result.created,
                "updated": result.updated,
                "errors": result.errors,
                "success": result.success,
            }
        except Exception as e:
            return {"error": f"CRM sync failed: {e}"}

    # ------------------------------------------------------------------
    # New tools: batch ops, currency, CRM granular
    # ------------------------------------------------------------------

    async def _tool_batch_valuate(self, inputs: dict) -> dict:
        """Run valuations across multiple companies in parallel batches."""
        company_names = inputs.get("companies", [])
        method = inputs.get("method")
        if not company_names:
            # Fall back to portfolio
            company_names = list(
                self.shared_data.get("matrix_context", {}).get("companyNames", [])
            )
        if not company_names:
            return {"error": "No companies specified and no portfolio loaded"}

        import asyncio
        results = []
        errors = []
        # Process in batches of 5
        for batch_start in range(0, len(company_names), 5):
            batch = company_names[batch_start:batch_start + 5]
            batch_tasks = []
            for name in batch:
                val_inputs = {"company_id": name}
                if method:
                    val_inputs["method"] = method
                batch_tasks.append(self._tool_valuation(val_inputs))
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            for name, res in zip(batch, batch_results):
                if isinstance(res, Exception):
                    errors.append({"company": name, "error": str(res)})
                elif isinstance(res, dict) and res.get("error"):
                    errors.append({"company": name, "error": res["error"]})
                else:
                    results.append({"company": name, "valuation": res})

        return {
            "completed": len(results),
            "failed": len(errors),
            "results": results,
            "errors": errors,
            "total": len(company_names),
        }

    async def _tool_batch_enrich(self, inputs: dict) -> dict:
        """Enrich multiple companies in parallel — wrapper around resolve_data_gaps."""
        company_names = inputs.get("companies", [])
        fields = inputs.get("fields")
        if not company_names:
            company_names = list(
                self.shared_data.get("matrix_context", {}).get("companyNames", [])
            )
        if not company_names:
            return {"error": "No companies specified and no portfolio loaded"}

        gap_inputs = {"companies": company_names[:20]}
        if fields:
            gap_inputs["needed_fields"] = fields
        return await self._tool_resolve_gaps(gap_inputs)

    async def _tool_convert_currency(self, inputs: dict) -> dict:
        """Convert all monetary values in shared_data to target currency."""
        target = inputs.get("target_currency", "USD").upper()
        source = inputs.get("source_currency", "").upper()

        try:
            from app.services.fx_intelligence_service import FXIntelligenceService
            fx = FXIntelligenceService()
        except ImportError:
            return {"error": "FX service not available"}

        companies = self.shared_data.get("companies", [])
        conversions = []
        monetary_fields = [
            "revenue", "arr", "inferred_revenue", "valuation", "inferred_valuation",
            "total_funding", "last_round_amount", "burn_rate",
        ]

        for company in companies:
            company_ccy = source or (company.get("currency") or company.get("reporting_currency") or "USD").upper()
            if company_ccy == target:
                continue

            rate = await fx.get_rate(company_ccy, target)
            if not rate:
                continue

            converted_fields = {}
            for field in monetary_fields:
                val = company.get(field)
                if val is not None:
                    try:
                        original = float(val)
                        converted = original * rate
                        company[field] = converted
                        company[f"_{field}_original_{company_ccy}"] = original
                        converted_fields[field] = converted
                    except (TypeError, ValueError):
                        pass

            if converted_fields:
                company["currency"] = target
                conversions.append({
                    "company": company.get("company") or company.get("name"),
                    "from": company_ccy,
                    "to": target,
                    "rate": rate,
                    "fields_converted": list(converted_fields.keys()),
                })

        return {
            "target_currency": target,
            "conversions": conversions,
            "companies_converted": len(conversions),
            "companies_skipped": len(companies) - len(conversions),
        }

    async def _tool_crm_search(self, inputs: dict) -> dict:
        """Search CRM for companies, deals, or notes."""
        try:
            from app.services.crm import get_crm_provider
        except ImportError:
            return {"error": "CRM module not available"}

        query = inputs.get("query", "")
        entity_type = inputs.get("entity_type", "companies")

        provider = get_crm_provider()
        try:
            if entity_type == "companies":
                results = await provider.search_companies(query, limit=10)
                return {"companies": [r.__dict__ if hasattr(r, '__dict__') else r for r in results], "count": len(results)}
            elif entity_type == "deals":
                results = await provider.list_deals(limit=20)
                # Filter by query
                filtered = [d for d in results if query.lower() in (getattr(d, 'name', '') or '').lower()] if query else results
                return {"deals": [d.__dict__ if hasattr(d, '__dict__') else d for d in filtered], "count": len(filtered)}
            elif entity_type == "notes":
                results = await provider.list_notes(query, limit=20)
                return {"notes": [n.__dict__ if hasattr(n, '__dict__') else n for n in results], "count": len(results)}
            else:
                return {"error": f"Unknown entity_type: {entity_type}. Use: companies, deals, notes"}
        except Exception as e:
            return {"error": f"CRM search failed: {e}"}

    async def _tool_crm_log_interaction(self, inputs: dict) -> dict:
        """Log a meeting, call, email, or note to CRM."""
        try:
            from app.services.crm import get_crm_provider
            from app.services.crm.base import CRMNote
        except ImportError:
            return {"error": "CRM module not available"}

        company = inputs.get("company", "")
        note_type = inputs.get("note_type", "note")
        content = inputs.get("content", "")
        title = inputs.get("title", f"{note_type.capitalize()} - {company}")

        if not company or not content:
            return {"error": "company and content are required"}

        provider = get_crm_provider()
        try:
            # Find the company in CRM first
            matches = await provider.search_companies(company, limit=1)
            if not matches:
                return {"error": f"Company '{company}' not found in CRM. Sync first with sync_crm."}

            company_ext_id = matches[0].external_id if hasattr(matches[0], 'external_id') else str(matches[0].get('external_id', ''))

            note = CRMNote(
                external_id="",
                company_external_id=company_ext_id,
                title=title,
                body=content,
            )
            result = await provider.add_note(note)
            return {
                "logged": True,
                "note_type": note_type,
                "company": company,
                "title": title,
            }
        except Exception as e:
            return {"error": f"Failed to log interaction: {e}"}

    async def _tool_crm_pipeline_update(self, inputs: dict) -> dict:
        """Update deal pipeline stage in CRM."""
        try:
            from app.services.crm import get_crm_provider
            from app.services.crm.base import CRMDeal
        except ImportError:
            return {"error": "CRM module not available"}

        company = inputs.get("company", "")
        stage = inputs.get("stage", "")
        deal_value = inputs.get("deal_value")
        notes = inputs.get("notes", "")

        if not company or not stage:
            return {"error": "company and stage are required"}

        valid_stages = {"sourced", "screening", "dd", "ic_review", "term_sheet", "closed", "passed"}
        if stage not in valid_stages:
            return {"error": f"Invalid stage: {stage}. Use: {', '.join(sorted(valid_stages))}"}

        provider = get_crm_provider()
        try:
            matches = await provider.search_companies(company, limit=1)
            if not matches:
                return {"error": f"Company '{company}' not found in CRM. Sync first with sync_crm."}

            company_ext_id = matches[0].external_id if hasattr(matches[0], 'external_id') else str(matches[0].get('external_id', ''))

            deal = CRMDeal(
                external_id="",
                company_external_id=company_ext_id,
                name=f"{company} - Investment",
                stage=stage,
                value=deal_value,
            )
            result = await provider.upsert_deal(deal)
            return {
                "updated": True,
                "company": company,
                "stage": stage,
                "deal_value": deal_value,
            }
        except Exception as e:
            return {"error": f"Failed to update pipeline: {e}"}

        # =========================================================================
        # Phase 7: Enhanced Agent Skills — bulk ops, LP queries, team comparison
        # =========================================================================

    def _identify_data_gaps(self, company_data: Dict[str, Any]) -> List[str]:
        """Return list of missing or low-confidence fields for dynamic enrichment."""
        gaps = []
        critical_fields = [
            ('revenue', 'arr', 'current_arr_usd'),
            ('valuation', 'current_valuation_usd'),
            ('employee_count', 'team_size', 'headcount'),
            ('total_raised', 'total_funding'),
            ('burn_rate', 'burn_rate_monthly_usd'),
            ('growth_rate', 'revenue_growth_annual_pct'),
        ]
        for field_group in critical_fields:
            found = False
            for field in field_group if isinstance(field_group, tuple) else [field_group]:
                val = company_data.get(field)
                if val is not None and val != 0:
                    found = True
                    break
                # Check inferred version
                inferred = company_data.get(f'inferred_{field}')
                confidence = company_data.get(f'{field}_confidence', 0)
                if inferred is not None and confidence >= 0.5:
                    found = True
                    break
            if not found:
                gaps.append(field_group[0] if isinstance(field_group, tuple) else field_group)
        return gaps

    def _calculate_search_depth(self, gaps: List[str]) -> int:
        """More gaps = more Tavily searches. Min 4, max 12."""
        base = 4
        gap_boost = min(len(gaps) * 2, 8)
        return min(base + gap_boost, 12)

    def _build_gap_specific_queries(self, company_name: str, gaps: List[str]) -> List[str]:
        """Build targeted Tavily queries based on specific data gaps."""
        search_name = self._clean_company_name_for_search(company_name)
        base_queries = [
            f'"{search_name}" startup company funding valuation revenue',
            f'"{search_name}" company business model team founders',
            f'"{search_name}" Series A B C funding round investors',
            f'"{search_name}" founder CEO CTO background',
        ]
        gap_queries: Dict[str, str] = {
            'revenue': f'"{search_name}" annual revenue ARR MRR 2024 2025',
            'valuation': f'"{search_name}" valuation post-money Series round 2024 2025',
            'employee_count': f'"{search_name}" employees team size headcount LinkedIn',
            'total_raised': f'"{search_name}" total funding raised Crunchbase',
            'burn_rate': f'"{search_name}" burn rate runway cash monthly expenses',
            'growth_rate': f'"{search_name}" revenue growth rate YoY percentage',
        }
        for gap in gaps:
            if gap in gap_queries:
                base_queries.append(gap_queries[gap])
        return base_queries[:self._calculate_search_depth(gaps)]

    async def _execute_bulk_valuation(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Run valuation engine across all portfolio companies in parallel."""
        fund_id = inputs.get('fund_id')
        if not fund_id:
            return {"error": "fund_id required for bulk valuation"}

        try:
            from app.services.portfolio_service import PortfolioService
            ps = PortfolioService()
            portfolio = ps.get_portfolio(fund_id)
            companies = portfolio.get('companies', []) if portfolio else []
            if not companies:
                return {"error": "No companies found in portfolio", "fund_id": fund_id}

            async def value_one(company: Dict) -> Dict:
                try:
                    result = await self._execute_valuation({
                        'company_name': company.get('name', ''),
                        'company': company.get('name', ''),
                        'revenue': company.get('current_arr_usd') or company.get('arr') or 0,
                        'arr': company.get('current_arr_usd') or company.get('arr') or 0,
                        'sector': company.get('sector', ''),
                        'stage': company.get('funnel_status', ''),
                        'growth_rate': company.get('revenue_growth_annual_pct', 0),
                        'fund_id': fund_id,
                        'company_id': company.get('id'),
                    })
                    return {"company_id": company.get('id'), "name": company.get('name'), "result": result, "success": True}
                except Exception as e:
                    return {"company_id": company.get('id'), "name": company.get('name'), "error": str(e), "success": False}

            # Limit concurrency to 5
            semaphore = asyncio.Semaphore(5)
            async def limited(company: Dict) -> Dict:
                async with semaphore:
                    return await value_one(company)

            tasks = [limited(c) for c in companies]
            results = await asyncio.gather(*tasks)

            succeeded = [r for r in results if r.get('success')]
            failed = [r for r in results if not r.get('success')]

            # Write successful valuations as pending_suggestions
            for r in succeeded:
                val = r.get('result', {})
                valuation = val.get('valuation') or val.get('fair_value') or val.get('estimated_valuation')
                if valuation and r.get('company_id'):
                    try:
                        await self._add_pending_suggestion_to_db(
                            fund_id=fund_id,
                            company_id=r['company_id'],
                            column_id='valuation',
                            suggested_value=valuation,
                            source_service='valuation_engine.bulk',
                            reasoning=f"Bulk valuation: {val.get('method', 'composite')} method"
                        )
                    except Exception as e:
                        logger.warning(f"[BULK_VALUATION] Failed to write suggestion for {r.get('name')}: {e}")

            return {
                "total": len(results),
                "succeeded": len(succeeded),
                "failed": len(failed),
                "results": results,
            }
        except Exception as e:
            logger.error(f"[BULK_VALUATION] Error: {e}")
            return {"error": str(e)}

    async def _execute_multi_enrich(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Enrich multiple companies using targeted micro-skill searches.

        Delegates to _tool_enrich_field per company — detects empty fields,
        runs focused searches, and persists suggestions automatically.
        """
        company_names = inputs.get('companies', inputs.get('company_names', []))
        fund_id = inputs.get('fund_id')

        # Pick up proactive enrichment targets from shared_data
        proactive_targets = self.shared_data.get("proactive_enrich_companies", [])
        sparse_targets = self.shared_data.get("sparse_enrich_companies", [])
        if proactive_targets:
            for name in proactive_targets:
                if name not in company_names:
                    company_names.append(name)
        if sparse_targets:
            for item in sparse_targets:
                name = item if isinstance(item, str) else item.get("name", "")
                if name and name not in company_names:
                    company_names.append(name)

        if not company_names and fund_id:
            try:
                from app.services.portfolio_service import PortfolioService
                ps = PortfolioService()
                portfolio = ps.get_portfolio(fund_id)
                companies = portfolio.get('companies', []) if portfolio else []
                company_names = [c.get('name') for c in companies if c.get('name')]
            except Exception:
                pass

        if not company_names:
            return {"error": "No companies to enrich"}

        semaphore = asyncio.Semaphore(5)
        async def enrich_one(name: str) -> Dict:
            async with semaphore:
                try:
                    # _tool_enrich_field auto-detects empty fields, searches, persists suggestions
                    result = await self._tool_enrich_field({
                        "company_name": name,
                    })
                    return {
                        "company": name,
                        "fields_found": result.get("fields_found", []),
                        "suggestions_persisted": result.get("suggestions_persisted", 0),
                        "success": True,
                    }
                except Exception as e:
                    logger.error(f"[MULTI_ENRICH] {name} failed: {e}")
                    return {"company": name, "error": str(e), "success": False}

        tasks = [enrich_one(name) for name in company_names[:20]]
        results = await asyncio.gather(*tasks)

        succeeded = sum(1 for r in results if r.get('success'))
        total_suggestions = sum(r.get("suggestions_persisted", 0) for r in results if r.get("success"))
        logger.info(f"[MULTI_ENRICH] Complete: {succeeded}/{len(results)} enriched, {total_suggestions} suggestions")
        return {
            "total": len(results),
            "succeeded": succeeded,
            "failed": len(results) - succeeded,
            "suggestions_persisted": total_suggestions,
            "companies": results,
        }

    async def _execute_lp_query_response(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Answer LP questions using fund data + portfolio metrics."""
        query = inputs.get('query', inputs.get('prompt', ''))
        fund_id = inputs.get('fund_id')

        try:
            from app.services.portfolio_service import PortfolioService
            ps = PortfolioService()
            fund_metrics = ps.get_portfolio_metrics(fund_id) if fund_id else {}
            pacing = ps.get_portfolio_pacing(fund_id) if fund_id else {}
        except Exception:
            fund_metrics = {}
            pacing = {}

        # Build shared data for memo generation
        shared_data = {
            'fund_metrics': fund_metrics,
            'fund_context': {
                'fund_id': fund_id,
                'fund_metrics': fund_metrics,
                'pacing': pacing,
            },
            'query_summary': query[:80] if query else 'LP Query',
        }

        try:
            from app.services.lightweight_memo_service import LightweightMemoService
            memo_svc = LightweightMemoService()
            memo = await memo_svc.generate(
                user_prompt=query,
                shared_data=shared_data,
                template_id='lp_quarterly_enhanced' if 'dpi' in query.lower() or 'quarterly' in query.lower() else 'bespoke_lp',
            )
            return {"memo": memo, "fund_metrics": fund_metrics, "template": "lp_response"}
        except Exception as e:
            logger.error(f"[LP_QUERY] Error: {e}")
            return {"error": str(e), "fund_metrics": fund_metrics}

    async def _execute_team_comparison(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Compare founding teams across 2-4 companies."""
        company_names = inputs.get('companies', inputs.get('company_names', []))
        if len(company_names) < 2:
            return {"error": "Need at least 2 companies for team comparison"}

        # Fetch data for each company
        team_data = []
        for name in company_names[:4]:
            try:
                result = await self._execute_company_fetch({'company': name})
                companies = result.get('companies', [])
                if companies:
                    team_data.append(companies[0])
            except Exception as e:
                team_data.append({'company': name, 'error': str(e)})

        # Score teams on 5 dimensions
        dimensions = ['technical', 'domain', 'execution', 'fundraising', 'leadership']
        radar_data = []
        for dim in dimensions:
            entry = {'dimension': dim.title()}
            for td in team_data:
                name = td.get('company', td.get('name', 'Unknown'))
                # Use existing scores or estimate from data
                score = td.get(f'{dim}_score', 5)
                if dim == 'technical':
                    score = min(10, 5 + (1 if td.get('cto_technical') else 0) + (1 if td.get('repeat_founders') else 0))
                elif dim == 'fundraising':
                    raised = td.get('total_raised', 0) or 0
                    score = min(10, 3 + int(raised / 10_000_000))
                entry[name] = score
            radar_data.append(entry)

        return {
            "companies": team_data,
            "radar_data": {
                "dimensions": radar_data,
                "companies": [td.get('company', td.get('name', '')) for td in team_data],
            },
            "template": "team_comparison",
        }

    async def _execute_followon_deep_dive(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Deep follow-on analysis with cap table evolution and breakpoints."""
        company_name = inputs.get('company', inputs.get('company_name', ''))
        follow_on_amount = inputs.get('follow_on_amount', inputs.get('amount', 0))
        fund_id = inputs.get('fund_id')

        # Get company data
        try:
            result = await self._execute_company_fetch({'company': company_name})
            company_data = result.get('companies', [{}])[0] if result.get('companies') else {}
        except Exception:
            company_data = {'company': company_name}

        # Get cap table history if available
        cap_table_evolution = []
        try:
            rounds = company_data.get('funding_rounds', [])
            if rounds:
                founder_pct = 100.0
                our_pct = 0.0
                other_pct = 0.0
                esop_pct = 0.0
                for r in rounds:
                    dilution = r.get('dilution', 0.15)
                    founder_pct *= (1 - dilution)
                    esop_pct = min(15, esop_pct + 2)
                    our_new = dilution * 0.3 if 'our' in str(r.get('investors', '')).lower() else 0
                    our_pct = our_pct * (1 - dilution) + our_new * 100
                    other_pct = 100 - founder_pct - esop_pct - our_pct
                    cap_table_evolution.append({
                        'round': r.get('round', r.get('type', 'Unknown')),
                        'founders': round(founder_pct, 1),
                        'esop': round(esop_pct, 1),
                        'our_fund': round(our_pct, 1),
                        'others': round(max(0, other_pct), 1),
                    })
        except Exception:
            pass

        # Model follow-on scenarios
        current_ownership = company_data.get('ownership_percentage', 10) or 10
        scenarios = [
            {'name': 'No Follow-On', 'ownership': round(current_ownership * 0.8, 1), 'moic': 'N/A'},
            {'name': 'Pro-Rata', 'ownership': round(current_ownership, 1), 'moic': 'N/A'},
            {'name': 'Super Pro-Rata', 'ownership': round(current_ownership * 1.2, 1), 'moic': 'N/A'},
        ]

        return {
            "company": company_data,
            "cap_table_evolution": cap_table_evolution,
            "follow_on_amount": follow_on_amount,
            "scenarios": scenarios,
            "current_ownership": current_ownership,
            "template": "followon_deep_dive",
        }

    async def _execute_competitive_landscape_memo(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Generate competitive landscape memo with scatter positioning."""
        company_name = inputs.get('company', inputs.get('company_name', ''))

        # Fetch company + competitors
        try:
            result = await self._execute_company_fetch({'company': company_name})
            company_data = result.get('companies', [{}])[0] if result.get('companies') else {}
        except Exception:
            company_data = {'company': company_name}

        # Search for competitors
        competitors = []
        if self.tavily_api_key:
            try:
                search_name = self._clean_company_name_for_search(company_name)
                comp_result = await self._tavily_search(f'{search_name} competitors alternatives market landscape')
                if comp_result and comp_result.get('results'):
                    for r in comp_result['results'][:5]:
                        competitors.append({
                            'source': r.get('url', ''),
                            'title': r.get('title', ''),
                            'snippet': r.get('content', '')[:200],
                        })
            except Exception:
                pass

        return {
            "company": company_data,
            "competitors": competitors,
            "template": "competitive_landscape",
        }

    async def _add_pending_suggestion_to_db(
        self,
        fund_id: str,
        company_id: str,
        column_id: str,
        suggested_value: Any,
        source_service: str,
        reasoning: str = "",
        ) -> None:
        """Write a pending suggestion to the database."""
        try:
            from app.core.database import get_supabase_service
            sb = get_supabase_service()
            if not sb:
                return
            import json
            sb.from_('pending_suggestions').upsert({
                'fund_id': fund_id,
                'company_id': company_id,
                'column_id': column_id,
                'suggested_value': {"value": suggested_value},
                'source_service': source_service,
                'reasoning': reasoning,
            }, on_conflict='fund_id,company_id,column_id').execute()
        except Exception as e:
            logger.warning(f"[PENDING_SUGGESTION] Write failed: {e}")

    # ------------------------------------------------------------------
    # Shared-data hydration: derive secondary keys from companies
    # ------------------------------------------------------------------

    async def _hydrate_shared_data_from_companies(self) -> None:
        """Derive secondary shared_data keys from companies when missing.

        After resolve_data_gaps (or any tool) populates shared_data["companies"],
        downstream consumers like memo templates expect keys such as
        revenue_projections, scenario_analysis, fund_metrics, and
        cap_table_history.  If those keys are absent we synthesise lightweight
        versions from the company dicts so memos never render blank.

        This is intentionally *additive* — it never overwrites keys that a
        real analysis tool has already populated.

        Lock strategy: hold the lock only briefly to snapshot inputs and to
        write back results.  All computation happens *outside* the lock so
        other coroutines (memo artifact storage, process_request_stream) are
        never blocked for more than a few microseconds.
        """
        # ── Step 1: snapshot inputs under a brief lock ──
        async with self.shared_data_lock:
            companies = list(self.shared_data.get("companies", []))
            if not companies:
                return
            fund_ctx = dict(self.shared_data.get("fund_context", {}))
            # Record which keys already exist so we don't recompute them
            has_rev_proj = bool(self.shared_data.get("revenue_projections"))
            has_scenarios = bool(self.shared_data.get("scenario_analysis"))
            has_fund_metrics = bool(self.shared_data.get("fund_metrics"))
            has_cap_hist = bool(self.shared_data.get("cap_table_history"))

        # ── Step 2: compute derived data WITHOUT holding the lock ──
        derived: Dict[str, Any] = {}

        # -- 1. revenue_projections (per-company growth curves) ----------
        if not has_rev_proj:
            rev_proj: dict = {}
            for c in companies:
                name = c.get("company") or c.get("name") or ""
                if not name:
                    continue
                base_rev = (
                    _num(c.get("arr"))
                    or _num(c.get("revenue"))
                    or _num(c.get("inferred_revenue"))
                    or 0
                )
                stage = c.get("stage") or "Unknown"
                growth = _num(c.get("growth_rate")) or _stage_default_growth(stage)
                # Fallback: if no revenue at all, estimate from stage benchmarks
                if base_rev <= 0:
                    stage_rev_defaults = {
                        "Pre-Seed": 100_000, "Seed": 500_000, "Series A": 3_000_000,
                        "Series B": 15_000_000, "Series C": 50_000_000, "Series D": 100_000_000,
                    }
                    base_rev = stage_rev_defaults.get(stage, 1_000_000)
                    logger.info("[HYDRATE] %s: no revenue data, using stage default $%s for %s", name, f"{base_rev:,.0f}", stage)
                projections = []
                rev = base_rev
                gr = growth
                for yr in range(1, 6):
                    rev = rev * (1 + gr)
                    projections.append({"year": yr, "revenue": round(rev, 2), "growth_rate": round(gr, 4)})
                    gr = max(gr * 0.85, 0.05)  # decay
                rev_proj[name] = {
                    "company": name,
                    "base_revenue": base_rev,
                    "initial_growth": growth,
                    "years": 5,
                    "projections": projections,
                    "_inferred": base_rev == 0,
                }
            if rev_proj:
                derived["revenue_projections"] = rev_proj
            logger.info("[HYDRATE] revenue_projections: %d/%d companies", len(rev_proj), len(companies))

        # -- 2. scenario_analysis (basic exit multiples) -----------------
        if not has_scenarios:
            scenarios: dict = {}
            for c in companies:
                name = c.get("company") or c.get("name") or ""
                if not name:
                    continue
                val = _num(c.get("valuation")) or _num(c.get("inferred_valuation")) or 0
                rev = (
                    _num(c.get("arr"))
                    or _num(c.get("revenue"))
                    or _num(c.get("inferred_revenue"))
                    or 0
                )
                stage = c.get("stage") or "Unknown"
                # Fallback: estimate valuation from stage if missing
                if val <= 0:
                    stage_val_defaults = {
                        "Pre-Seed": 5_000_000, "Seed": 20_000_000, "Series A": 100_000_000,
                        "Series B": 400_000_000, "Series C": 1_000_000_000, "Series D": 3_000_000_000,
                    }
                    val = stage_val_defaults.get(stage, 50_000_000)
                    logger.info("[HYDRATE] %s: no valuation, using stage default $%s for %s", name, f"{val:,.0f}", stage)
                multiple = round(val / rev, 1) if rev > 0 else 10.0
                scenarios[name] = {
                    "company": name,
                    "current_valuation": val,
                    "current_revenue": rev,
                    "revenue_multiple": multiple,
                    "exit_scenarios": [
                        {"label": "Bear", "exit_multiple": round(multiple * 0.5, 1), "exit_value": round(val * 1.5, 0), "probability": 0.25},
                        {"label": "Base", "exit_multiple": round(multiple, 1), "exit_value": round(val * 3, 0), "probability": 0.50},
                        {"label": "Bull", "exit_multiple": round(multiple * 2, 1), "exit_value": round(val * 7, 0), "probability": 0.20},
                        {"label": "Outlier", "exit_multiple": round(multiple * 4, 1), "exit_value": round(val * 15, 0), "probability": 0.05},
                    ],
                }
            if scenarios:
                derived["scenario_analysis"] = scenarios
            logger.info("[HYDRATE] scenario_analysis: %d/%d companies", len(scenarios), len(companies))

        # -- 3. fund_metrics (from fund_context + companies) -------------
        if not has_fund_metrics and fund_ctx:
            fund_size = _num(fund_ctx.get("fund_size")) or _num(fund_ctx.get("fundSize")) or 0
            deployed = _num(fund_ctx.get("deployed_capital")) or _num(fund_ctx.get("deployedCapital")) or 0
            remaining = _num(fund_ctx.get("remaining_capital")) or _num(fund_ctx.get("remainingCapital")) or (fund_size - deployed if fund_size else 0)
            if fund_size > 0:
                derived["fund_metrics"] = {
                    "metrics": {
                        "total_committed": fund_size,
                        "total_invested": deployed,
                        "total_nav": round(deployed * 1.3, 0),
                        "total_distributed": 0,
                        "dpi": 0,
                        "rvpi": round(remaining / fund_size, 2) if fund_size else 0,
                        "tvpi": round((deployed * 1.3) / deployed, 2) if deployed else 0,
                        "irr": 0,
                        "deployment_rate": round(deployed / fund_size, 2) if fund_size else 0,
                    },
                    "portfolio": {
                        "company_count": len(companies),
                        "active_count": len(companies),
                        "exited_count": 0,
                    },
                    "_source": "hydrated_from_companies",
                }

        # -- 4. cap_table_history (lightweight from funding data) --------
        if not has_cap_hist:
            cap_hist: dict = {}
            for c in companies:
                name = c.get("company") or c.get("name") or ""
                if not name:
                    continue
                val = _num(c.get("valuation")) or _num(c.get("inferred_valuation")) or 0
                total_raised = _num(c.get("total_funding")) or _num(c.get("total_raised")) or 0
                stage = c.get("stage") or "Unknown"
                # Always generate cap table — use stage defaults if no valuation
                founder_pct = _stage_founder_ownership(stage)
                if val > 0 and total_raised > 0:
                    investor_pct = min(total_raised / val, 0.60)
                else:
                    investor_pct = 1 - founder_pct - 0.10
                esop_pct = round(1.0 - founder_pct - investor_pct, 4)
                if esop_pct < 0:
                    esop_pct = 0.10
                    investor_pct = round(1.0 - founder_pct - esop_pct, 4)
                cap_hist[name] = {
                    "company": name,
                    "current_cap_table": {
                        "Founders": round(founder_pct * 100, 1),
                        "Investors": round(investor_pct * 100, 1),
                        "Option Pool": round(esop_pct * 100, 1),
                    },
                    "total_raised": total_raised,
                    "num_rounds": _stage_round_count(stage),
                    "founder_ownership": round(founder_pct * 100, 1),
                    "_source": "hydrated_from_companies",
                }
            if cap_hist:
                derived["cap_table_history"] = cap_hist
            logger.info("[HYDRATE] cap_table_history: %d/%d companies", len(cap_hist), len(companies))

        # ── Step 3: write results back under a brief lock ──
        if derived:
            async with self.shared_data_lock:
                for key, value in derived.items():
                    # Additive: don't overwrite if another coroutine populated it
                    if not self.shared_data.get(key):
                        self.shared_data[key] = value
                        entry_count = len(value) if isinstance(value, dict) else 1
                        logger.info("[HYDRATE] Derived %s (%d entries)", key, entry_count)
                    else:
                        logger.info("[HYDRATE] Skipped %s — already populated", key)
        else:
            logger.warning("[HYDRATE] No derived data produced from %d companies", len(companies))

    # ------------------------------------------------------------------
    # Phase 9: Gap resolver tool + missing skill handlers
    # ------------------------------------------------------------------

    async def _tool_resolve_gaps(self, inputs: dict) -> dict:
        """Auto-detect and fill missing company data using benchmarks + parallel searches.

        This is the key tool that prevents the agent from ever saying "no data".
        Tier 1: Stage benchmarks (instant) → Tier 2: Parallel Tavily searches → Tier 3: Valuations.
        All results persist to pending_suggestions and emit grid_commands.
        """
        from app.services.micro_skills.gap_resolver import resolve_gaps

        company_names = inputs.get("companies") or []
        needed_fields = inputs.get("needed_fields")
        fund_ctx = self.shared_data.get("fund_context", {})
        fund_id = inputs.get("fund_id") or fund_ctx.get("fundId", "")

        # Build company list from shared_data or inputs
        companies = []
        existing_companies = self.shared_data.get("companies", [])

        if company_names:
            for name in company_names:
                name = name.strip().lstrip("@")
                existing = next(
                    (c for c in existing_companies if c.get("name", "").lower() == name.lower()),
                    None,
                )
                if existing:
                    companies.append(existing)
                else:
                    companies.append({"name": name})
        elif existing_companies:
            companies = existing_companies
        else:
            return {"error": "No companies specified or in shared_data"}

        # Get portfolio + build UUID map + grid cell values
        portfolio_companies = []
        matrix_ctx = self.shared_data.get("matrix_context") or {}
        grid_rows = matrix_ctx.get("gridSnapshot", {}).get("rows", []) if isinstance(matrix_ctx.get("gridSnapshot"), dict) else []
        _grid_id_map: Dict[str, str] = {}
        _grid_cell_map: Dict[str, Dict] = {}
        for row in grid_rows[:50]:
            cells = row.get("cells") or row.get("cellValues") or {}
            _rn = (row.get("companyName") or row.get("company_name") or "").strip().lower()
            _ri = row.get("id") or ""
            if _rn and _ri:
                _grid_id_map[_rn] = _ri
                _grid_cell_map[_rn] = {
                    "id": _ri,
                    "stage": self._extract_str(cells, "stage", "investment_stage"),
                    "sector": self._extract_str(cells, "sector", "category"),
                    "arr": self._extract_numeric(cells, "arr", "currentArrUsd"),
                    "valuation": self._extract_numeric(cells, "currentValuationUsd", "valuation"),
                    "total_funding": self._extract_numeric(cells, "totalFunding", "total_funding"),
                    "last_round_amount": self._extract_numeric(cells, "lastRoundAmount", "last_round_amount"),
                    "team_size": self._extract_numeric(cells, "teamSize", "team_size"),
                }
            portfolio_companies.append({
                "name": row.get("companyName") or row.get("company_name") or "",
                "stage": self._extract_str(cells, "stage", "investment_stage"),
                "sector": self._extract_str(cells, "sector", "category"),
                "arr": self._extract_numeric(cells, "arr", "currentArrUsd"),
                "valuation": self._extract_numeric(cells, "currentValuationUsd", "valuation"),
            })

        # Stamp UUIDs and seed grid values onto companies before gap resolution
        # Preserve originals in _grid_values so correction detection can compare later
        for company in companies:
            _cn = (company.get("name") or company.get("company") or "").strip().lower()
            if not _cn:
                continue
            if not (company.get("id") or company.get("company_id")):
                _uid = _grid_id_map.get(_cn, "")
                if _uid:
                    company["id"] = _uid
                    logger.info("[GAP_RESOLVER_PREP] Stamped UUID %s onto %s", _uid[:8], _cn)
            _gv = _grid_cell_map.get(_cn)
            if _gv:
                company["_grid_values"] = dict(_gv)  # preserve originals for correction detection
                for _k, _v in _gv.items():
                    if _v and not company.get(_k):
                        company[_k] = _v

        # Wire up Tavily search and LLM extract functions
        async def tavily_fn(query: str) -> dict:
            return await self._tavily_search(query)

        async def llm_fn(prompt: str, system: str) -> str:
            result = await self.model_router.get_completion(
                prompt=prompt,
                system_prompt=system,
                capability=ModelCapability.FAST,
                max_tokens=400,
                temperature=0.0,
                json_mode=True,
                caller_context="micro_skill_extract",
            )
            return result.get("response", "{}") if isinstance(result, dict) else str(result)

        # Run gap resolver
        result = await resolve_gaps(
            companies=companies,
            needed_fields=needed_fields,
            fund_id=fund_id,
            tavily_search_fn=tavily_fn if self.tavily_api_key else None,
            llm_extract_fn=llm_fn,
            portfolio_companies=portfolio_companies if portfolio_companies else None,
        )

        # Store enriched companies back in shared_data
        async with self.shared_data_lock:
            self.shared_data["companies"] = result["companies"]

        # Derive secondary keys (revenue_projections, scenario_analysis, etc.)
        # so downstream tools like generate_memo have data to work with.
        await self._hydrate_shared_data_from_companies()

        return {
            "companies_enriched": len(result["companies"]),
            "total_fields_filled": result["total_fields_filled"],
            "total_suggestions_persisted": result["total_suggestions_persisted"],
            "skills_run": result["skills_run"],
            "memo_sections": result["memo_sections"],
            "grid_commands": result["grid_commands"],
            "chart_data": result["chart_data"],
            "company_summaries": [
                {
                    "name": c.get("name", ""),
                    "stage": c.get("stage", ""),
                    "arr": c.get("arr") or c.get("inferred_revenue"),
                    "valuation": c.get("valuation") or c.get("inferred_valuation"),
                    "team_size": c.get("team_size"),
                }
                for c in result["companies"]
            ],
        }

    async def _tool_enrich_proactive(self, inputs: dict) -> dict:
        """Auto-fetch and enrich a company mentioned without @. Delegates to gap resolver."""
        company_name = inputs.get("company_name", "").strip().lstrip("@")
        if not company_name:
            return {"error": "company_name is required"}

        return await self._tool_resolve_gaps({
            "companies": [company_name],
            "fund_id": inputs.get("fund_id"),
        })

    # ------------------------------------------------------------------
    # enrich_field: single flexible tool for any column / field
    # ------------------------------------------------------------------

    def _find_company_id(self, company_name: str) -> str:
        """Look up company UUID from shared_data or grid snapshot."""
        name_lower = company_name.lower().strip().lstrip("@")
        # Check shared_data companies
        for c in self.shared_data.get("companies", []):
            if isinstance(c, dict):
                cn = (c.get("name") or c.get("company") or "").lower()
                if cn == name_lower:
                    return c.get("id") or c.get("company_id") or ""
        # Check grid snapshot
        matrix_ctx = self.shared_data.get("matrix_context") or {}
        grid_snap = matrix_ctx.get("gridSnapshot", {}) if isinstance(matrix_ctx.get("gridSnapshot"), dict) else {}
        for row in grid_snap.get("rows", []):
            if (row.get("companyName") or row.get("company_name") or "").lower().strip() == name_lower:
                return row.get("id") or row.get("rowId") or ""
        return ""

    async def _persist_company_to_db(self, company_data: Dict[str, Any], fund_id: Optional[str] = None) -> Dict[str, Any]:
        """Persist enriched company data to the companies table via portfolio_service.upsert_company.

        Called after _execute_company_fetch to durably store enriched data.
        Deduplicates by name, only writes non-None fields, stashes overflow into extra_data JSONB.

        Returns upsert result dict with id, name, created flag, and updated fields.
        """
        company_name = (
            company_data.get("company")
            or company_data.get("name")
            or company_data.get("requested_company")
            or ""
        ).strip()
        if not company_name:
            logger.warning("[PERSIST_COMPANY] No company name in data, skipping persist")
            return {"error": "no company name"}

        resolved_fund_id = fund_id or self.shared_data.get("fund_id")

        try:
            from app.services.portfolio_service import portfolio_service
            result = await portfolio_service.upsert_company(
                name=company_name,
                data=company_data,
                fund_id=resolved_fund_id,
            )
            if result.get("error"):
                logger.warning(f"[PERSIST_COMPANY] upsert failed for '{company_name}': {result['error']}")
            else:
                action = "Created" if result.get("created") else "Updated"
                logger.info(
                    f"[PERSIST_COMPANY] {action} '{company_name}' "
                    f"(id={result.get('id')}), fields={result.get('updated_fields', [])}"
                )
            return result
        except Exception as e:
            logger.error(f"[PERSIST_COMPANY] Exception persisting '{company_name}': {e}")
            return {"error": str(e)}

    def _detect_empty_fields(self, company_name: str) -> List[str]:
        """Scan the grid for empty cells for a given company. Returns column IDs."""
        name_lower = company_name.lower().strip().lstrip("@")
        matrix_ctx = self.shared_data.get("matrix_context") or {}
        grid_snap = matrix_ctx.get("gridSnapshot", {}) if isinstance(matrix_ctx.get("gridSnapshot"), dict) else {}
        for row in grid_snap.get("rows", []):
            rn = (row.get("companyName") or row.get("company_name") or "").lower().strip()
            if rn != name_lower:
                continue
            cells = row.get("cells") or row.get("cellValues") or {}
            empty = []
            # Check enrichable columns
            for col_id in [
                "arr", "valuation", "headcount", "totalRaised", "burnRate",
                "runway", "grossMargin", "cashInBank", "sector", "stage",
                "description", "revenueGrowthAnnual", "lastRoundAmount",
            ]:
                val = cells.get(col_id)
                if val is None or val == "" or val == 0:
                    empty.append(col_id)
            return empty
        return []

    async def _tool_enrich_field(self, inputs: dict) -> dict:
        """Search for specific data fields for a company using targeted micro-skills.

        Accepts any column ID — routes to the right search skill or builds a custom query.
        If fields is empty/missing, auto-detects empty columns from the grid.
        """
        from app.services.micro_skills.search_skills import find_field
        from app.services.micro_skills.suggestion_emitter import (
            emit_suggestions, emit_batch, build_grid_commands,
        )

        company_name = inputs.get("company_name", "").strip().lstrip("@")
        if not company_name:
            return {"error": "company_name required"}

        fields = inputs.get("fields") or []
        context = inputs.get("context", "")

        # Auto-detect empty fields from grid if none specified
        if not fields:
            fields = self._detect_empty_fields(company_name)
            if not fields:
                return {"message": f"No empty fields detected for {company_name}", "grid_commands": []}

        # Wire Tavily + LLM (same pattern as _tool_resolve_gaps)
        async def tavily_fn(query: str) -> dict:
            return await self._tavily_search(query)

        async def llm_fn(prompt: str, system: str) -> str:
            result = await self.model_router.get_completion(
                prompt=prompt,
                system_prompt=system,
                capability=ModelCapability.FAST,
                max_tokens=400,
                temperature=0.0,
                json_mode=True,
                caller_context="enrich_field",
            )
            return result.get("response", "{}") if isinstance(result, dict) else str(result)

        # Run find_field (parallel skill dispatch)
        skill_results = await find_field(
            company_name=company_name,
            fields=fields,
            tavily_search_fn=tavily_fn if self.tavily_api_key else None,
            llm_extract_fn=llm_fn,
            context=context,
        )

        # Aggregate results
        all_grid_commands = []
        all_field_updates = {}
        all_citations = []
        all_memo_sections = []
        total_confidence = 0.0

        for r in skill_results:
            if not r.has_data():
                continue

            # Build grid commands
            cmds = build_grid_commands(r, company_name)
            all_grid_commands.extend(cmds)

            # Collect field updates
            all_field_updates.update(r.field_updates)
            all_citations.extend(r.citations)
            total_confidence = max(total_confidence, r.confidence)

            if r.memo_section:
                all_memo_sections.append(r.memo_section)

        # Merge into shared_data companies
        async with self.shared_data_lock:
            for c in self.shared_data.get("companies", []):
                if isinstance(c, dict):
                    cn = (c.get("name") or c.get("company") or "").lower()
                    if cn == company_name.lower():
                        for k, v in all_field_updates.items():
                            if v is not None and not c.get(k):
                                c[k] = v
                        break

        # Persist suggestions to DB
        fund_ctx = self.shared_data.get("fund_context", {})
        fund_id = fund_ctx.get("fundId", "")
        company_id = self._find_company_id(company_name)
        suggestions_persisted = 0
        if fund_id and company_id:
            suggestions_persisted = await emit_batch(
                skill_results, company_id, fund_id, company_name,
            )

        return {
            "company": company_name,
            "fields_requested": fields,
            "fields_found": list(all_field_updates.keys()),
            "field_updates": all_field_updates,
            "grid_commands": all_grid_commands,
            "memo_sections": all_memo_sections,
            "citations": [
                c.to_dict() if hasattr(c, 'to_dict') else {"url": c}
                for c in all_citations[:10]
            ],
            "confidence": total_confidence,
            "suggestions_persisted": suggestions_persisted,
        }

    async def _tool_search_field_for_companies(self, inputs: dict) -> dict:
        """Search for a specific field across multiple companies in parallel.

        Example: 'find founders for Mercury, Ramp, Deel' → runs enrich_field
        for each company concurrently, aggregates grid_commands + suggestions.
        """
        field = inputs.get("field", "")
        company_names = inputs.get("company_names") or []
        context = inputs.get("context", "")

        if not field:
            return {"error": "field is required (e.g. 'founders', 'arr', 'headcount')"}
        if not company_names:
            return {"error": "company_names is required (list of company names)"}

        # Cap at 10 companies per batch to limit Tavily usage
        company_names = [n.strip().lstrip("@") for n in company_names[:10]]

        # Normalise field to list for enrich_field
        fields = [field] if isinstance(field, str) else field

        # Run enrich_field in parallel for each company
        tasks = [
            self._tool_enrich_field({
                "company_name": name,
                "fields": fields,
                "context": context,
            })
            for name in company_names
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Aggregate
        all_grid_commands = []
        all_field_updates = {}
        errors = []
        total_persisted = 0

        for i, r in enumerate(results):
            name = company_names[i]
            if isinstance(r, Exception):
                errors.append({"company": name, "error": str(r)})
                continue
            if isinstance(r, dict) and r.get("error"):
                errors.append({"company": name, "error": r["error"]})
                continue
            if isinstance(r, dict):
                all_grid_commands.extend(r.get("grid_commands", []))
                all_field_updates[name] = r.get("field_updates", {})
                total_persisted += r.get("suggestions_persisted", 0)

        return {
            "field": field,
            "companies_searched": len(company_names),
            "companies_found": len(company_names) - len(errors),
            "field_updates": all_field_updates,
            "grid_commands": all_grid_commands,
            "suggestions_persisted": total_persisted,
            "errors": errors if errors else None,
        }

    async def _tool_run_projection(self, inputs: dict) -> dict:
        """Run revenue/ARR projection with growth decay curves and scenario bands."""
        from app.services.micro_skills.benchmark_skills import (
            _ensure_numeric, _normalize_stage, _months_since_date,
            STAGE_BENCHMARKS, GROWTH_DECAY,
        )

        company_name = inputs.get("company", "")
        years = inputs.get("years", 5)

        # Get company data from shared_data
        companies = self.shared_data.get("companies", [])
        company = next(
            (c for c in companies if c.get("name", "").lower() == company_name.lower()),
            {},
        ) if company_name else (companies[0] if companies else {})

        stage = _normalize_stage(company.get("stage", ""))
        bench = STAGE_BENCHMARKS.get(stage, STAGE_BENCHMARKS["Series A"])

        base_arr = _ensure_numeric(
            company.get("arr") or company.get("revenue") or company.get("inferred_revenue"),
            bench["arr_median"],
        )
        growth = _ensure_numeric(company.get("growth_rate"), bench["growth_rate"])

        # Build bull/base/bear scenarios with growth decay
        scenarios = {}
        for scenario_name, growth_mult in [("bull", 1.3), ("base", 1.0), ("bear", 0.6)]:
            projections = []
            current = base_arr
            for year in range(1, years + 1):
                decay = GROWTH_DECAY.get(year, 0.3)
                annual_growth = growth * growth_mult * decay
                current = current * (1 + annual_growth)
                projections.append({
                    "year": year,
                    "arr": round(current),
                    "growth_rate": round(annual_growth, 2),
                })
            scenarios[scenario_name] = projections

        # Time to $100M
        time_to_100m = None
        for p in scenarios["base"]:
            if p["arr"] >= 100_000_000:
                time_to_100m = p["year"]
                break

        name = company.get("name", company_name or "company")
        return {
            "company": name,
            "base_arr": base_arr,
            "scenarios": scenarios,
            "time_to_100m": time_to_100m,
            "memo_sections": [{
                "type": "heading2", "content": f"{name} — Revenue Projections",
            }, {
                "type": "list", "items": [
                    f"Base ARR: ${base_arr/1e6:.1f}M",
                    f"Bull case ({years}yr): ${scenarios['bull'][-1]['arr']/1e6:.0f}M",
                    f"Base case ({years}yr): ${scenarios['base'][-1]['arr']/1e6:.0f}M",
                    f"Bear case ({years}yr): ${scenarios['bear'][-1]['arr']/1e6:.0f}M",
                ] + ([f"Time to $100M ARR: {time_to_100m} years (base)"] if time_to_100m else []),
            }],
            "chart_config": {
                "type": "line",
                "title": f"{name} — Revenue Projection",
                "datasets": [
                    {"label": "Bull", "data": [{"x": p["year"], "y": p["arr"]} for p in scenarios["bull"]], "borderColor": "#10b981"},
                    {"label": "Base", "data": [{"x": p["year"], "y": p["arr"]} for p in scenarios["base"]], "borderColor": "#3b82f6"},
                    {"label": "Bear", "data": [{"x": p["year"], "y": p["arr"]} for p in scenarios["bear"]], "borderColor": "#ef4444"},
                ],
            },
        }

    async def _execute_proactive_enrich(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Skill handler for proactive-enricher. Delegates to gap resolver."""
        company_name = inputs.get("company", inputs.get("company_name", ""))
        return await self._tool_resolve_gaps({
            "companies": [company_name] if company_name else [],
            "fund_id": inputs.get("fund_id"),
        })

    async def _execute_company_list_search(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Skill handler for company-list-builder. Delegates to _tool_build_company_list."""
        return await self._tool_build_company_list(inputs)

    async def _execute_search_extract(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Skill handler for search-extract-combo. Delegates to _tool_search_extract."""
        return await self._tool_search_extract(inputs)

    async def _execute_projection_model(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Skill handler for projection-modeler. Delegates to _tool_run_projection."""
        return await self._tool_run_projection(inputs)

    async def _execute_sparse_grid_enrich(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Skill handler for sparse-grid-enricher. Delegates to _tool_enrich_sparse_grid."""
        return await self._tool_enrich_sparse_grid(inputs)

    def is_ready(self) -> bool:
        """Return whether critical dependencies initialized successfully."""
        return bool(self._is_ready)
    
    def readiness_status(self) -> Dict[str, Any]:
        """Expose readiness diagnostics for API layer logging."""
        return {
            "ready": bool(self._is_ready),
            "error": self._readiness_error
        }
    
    # ------------------------------------------------------------------
    # Micro-skill tool handlers
    # Each is self-contained: reads grid → runs skills → emits suggestions
    # ------------------------------------------------------------------

    async def _tool_run_followon_analysis(self, inputs: dict) -> dict:
        """Loop all portfolio companies and flag who needs follow-on capital.

        Runs next_round_model + time_adjusted_estimate per company using grid data.
        Flags URGENT (<threshold mo runway) and WATCH (threshold–15mo) companies.
        Persists runway/burn suggestions and emits grid_commands.
        """
        from app.services.micro_skills.benchmark_skills import (
            next_round_model,
            time_adjusted_estimate,
            _normalize_stage,
            _ensure_numeric,
        )
        from app.services.micro_skills.suggestion_emitter import emit_batch, build_grid_commands

        fund_ctx = self.shared_data.get("fund_context", {})
        fund_id = inputs.get("fund_id") or fund_ctx.get("fundId", "")
        threshold = int(inputs.get("runway_threshold_months") or 9)

        # Pull companies from grid
        matrix_ctx = self.shared_data.get("matrix_context") or {}
        grid_snapshot = matrix_ctx.get("gridSnapshot") or {}
        grid_rows = (
            grid_snapshot.get("rows", [])
            if isinstance(grid_snapshot, dict)
            else grid_snapshot if isinstance(grid_snapshot, list) else []
        )
        if not grid_rows:
            return {"error": "No portfolio grid data available. Send matrix context with gridSnapshot."}

        urgent = []
        watch = []
        healthy = []
        all_grid_commands: list = []
        total_persisted = 0

        for row in grid_rows:
            name = row.get("companyName") or row.get("company_name") or "Unknown"
            company_id = row.get("id") or row.get("companyId") or ""
            cells = row.get("cells") or row.get("cellValues") or {}

            # Build company_data from grid cells
            company_data: Dict[str, Any] = {"name": name}
            for k, v in cells.items():
                val = v.get("value", v) if isinstance(v, dict) else v
                if val is not None and val != "" and val != "N/A":
                    company_data[k] = val

            # Normalize field names from grid column IDs to benchmark_skills field names
            if not company_data.get("stage"):
                company_data["stage"] = (
                    company_data.pop("investmentStage", None)
                    or company_data.pop("funding_stage", None)
                    or ""
                )
            if not company_data.get("arr"):
                company_data["arr"] = company_data.pop("currentArrUsd", None) or 0
            if not company_data.get("burn_rate"):
                company_data["burn_rate"] = company_data.pop("burnRate", None) or 0
            if not company_data.get("runway_months"):
                company_data["runway_months"] = company_data.pop("runway", None) or 0
            if not company_data.get("last_round_date"):
                company_data["last_round_date"] = company_data.pop("lastRoundDate", None) or ""
            if not company_data.get("last_round_amount"):
                company_data["last_round_amount"] = company_data.pop("lastRoundAmount", None) or 0
            if not company_data.get("valuation"):
                company_data["valuation"] = company_data.pop("currentValuationUsd", None) or 0
            if not company_data.get("total_funding"):
                company_data["total_funding"] = company_data.pop("totalFunding", None) or 0

            # Run skills
            from app.services.micro_skills import MicroSkillResult
            results: list[MicroSkillResult] = []
            try:
                time_result = await time_adjusted_estimate(company_data)
                if time_result.has_data():
                    time_result.merge_into(company_data)
                    results.append(time_result)
            except Exception as e:
                logger.warning(f"[FOLLOWON] time_adjusted failed for {name}: {e}")

            try:
                round_result = await next_round_model(company_data)
                if round_result.has_data():
                    results.append(round_result)
            except Exception as e:
                logger.warning(f"[FOLLOWON] next_round_model failed for {name}: {e}")

            # Determine runway and flag
            runway = _ensure_numeric(
                company_data.get("runway_months")
                or (time_result.field_updates.get("runway_months") if time_result.has_data() else None),
                0
            )
            down_round_risk = round_result.field_updates.get("down_round_risk", "UNKNOWN") if results else "UNKNOWN"
            next_round_months = round_result.field_updates.get("next_round_months", 0) if results else 0

            entry = {
                "company": name,
                "runway_months": runway,
                "down_round_risk": down_round_risk,
                "next_round_in_months": next_round_months,
                "stage": company_data.get("stage", ""),
            }

            if runway > 0 and runway < threshold:
                entry["flag"] = "URGENT"
                urgent.append(entry)
            elif runway >= threshold and runway < 15:
                entry["flag"] = "WATCH"
                watch.append(entry)
            else:
                entry["flag"] = "OK"
                healthy.append(entry)

            # Emit suggestions to DB and collect grid_commands
            if results:
                if fund_id and company_id:
                    try:
                        total_persisted += await emit_batch(results, company_id, fund_id, name)
                    except Exception as e:
                        logger.warning(f"[FOLLOWON] emit_batch failed for {name}: {e}")
                for r in results:
                    all_grid_commands.extend(build_grid_commands(r, name))

        summary_lines = []
        if urgent:
            summary_lines.append(
                f"URGENT ({len(urgent)} companies, <{threshold}mo runway): "
                + ", ".join(e["company"] for e in urgent)
            )
        if watch:
            summary_lines.append(
                f"WATCH ({len(watch)} companies, {threshold}-15mo runway): "
                + ", ".join(e["company"] for e in watch)
            )
        if healthy:
            summary_lines.append(f"OK ({len(healthy)} companies well-funded)")

        return {
            "urgent": urgent,
            "watch": watch,
            "healthy": healthy,
            "summary": " | ".join(summary_lines) or "No runway data found in grid",
            "total_suggestions_persisted": total_persisted,
            "grid_commands": all_grid_commands,
            "memo_sections": [
                {
                    "type": "followon_analysis",
                    "heading": "Follow-on Capital Analysis",
                    "items": summary_lines,
                    "urgent": urgent,
                    "watch": watch,
                }
            ],
        }

    async def _tool_run_portfolio_health(self, inputs: dict) -> dict:
        """Run full benchmark analysis across all portfolio companies.

        Stage fit, ARR vs benchmark, burn, growth outliers, valuation coherence.
        Emits per-company grid suggestions for missing or out-of-range fields.
        Delegates to resolve_gaps (Tier 1 only — no web search for speed).
        """
        from app.services.micro_skills.gap_resolver import resolve_gaps
        from app.services.micro_skills import FOLLOWON_FIELDS, CORE_FIELDS

        fund_ctx = self.shared_data.get("fund_context", {})
        fund_id = inputs.get("fund_id") or fund_ctx.get("fundId", "")
        fields = inputs.get("fields") or FOLLOWON_FIELDS

        # Pull companies from grid
        matrix_ctx = self.shared_data.get("matrix_context") or {}
        grid_snapshot = matrix_ctx.get("gridSnapshot") or {}
        grid_rows = (
            grid_snapshot.get("rows", [])
            if isinstance(grid_snapshot, dict)
            else grid_snapshot if isinstance(grid_snapshot, list) else []
        )
        if not grid_rows:
            return {"error": "No portfolio grid data available. Send matrix context with gridSnapshot."}

        # Build company list from grid rows, stamping UUIDs
        companies = []
        for row in grid_rows:
            name = row.get("companyName") or row.get("company_name") or "Unknown"
            company_id = row.get("id") or row.get("companyId") or ""
            cells = row.get("cells") or row.get("cellValues") or {}

            company: Dict[str, Any] = {"name": name}
            if company_id:
                company["id"] = company_id

            # Map grid column IDs → micro-skill field names
            field_map = {
                "stage": ["stage", "investmentStage", "funding_stage"],
                "sector": ["sector", "category"],
                "arr": ["arr", "currentArrUsd"],
                "valuation": ["valuation", "currentValuationUsd"],
                "total_funding": ["total_funding", "totalFunding"],
                "last_round_amount": ["last_round_amount", "lastRoundAmount"],
                "last_round_date": ["last_round_date", "lastRoundDate"],
                "burn_rate": ["burn_rate", "burnRate"],
                "runway_months": ["runway_months", "runway"],
                "team_size": ["team_size", "teamSize", "headcount"],
                "growth_rate": ["growth_rate", "revenueGrowthAnnual"],
                "gross_margin": ["gross_margin", "grossMargin"],
            }
            for skill_field, grid_keys in field_map.items():
                for gk in grid_keys:
                    raw = cells.get(gk)
                    val = raw.get("value", raw) if isinstance(raw, dict) else raw
                    if val is not None and val != "" and val != "N/A":
                        company[skill_field] = val
                        break

            companies.append(company)

        # Run gap resolver — Tier 1 only (no tavily_search_fn → instant benchmarks only)
        result = await resolve_gaps(
            companies=companies,
            needed_fields=fields,
            fund_id=fund_id,
            tavily_search_fn=None,   # Tier 1 only for speed
            llm_extract_fn=None,
            portfolio_companies=companies,
        )

        # Store enriched companies back in shared_data
        async with self.shared_data_lock:
            self.shared_data["companies"] = result["companies"]

        return {
            "companies_analyzed": len(result["companies"]),
            "total_fields_filled": result["total_fields_filled"],
            "total_suggestions_persisted": result["total_suggestions_persisted"],
            "skills_run": result["skills_run"],
            "memo_sections": result["memo_sections"],
            "grid_commands": result["grid_commands"],
            "chart_data": result["chart_data"],
            "company_summaries": [
                {
                    "name": c.get("name", ""),
                    "stage": c.get("stage", ""),
                    "arr": c.get("arr") or c.get("inferred_revenue"),
                    "valuation": c.get("valuation") or c.get("inferred_valuation"),
                    "runway_months": c.get("runway_months"),
                    "down_round_risk": c.get("down_round_risk"),
                }
                for c in result["companies"]
            ],
        }

    async def _tool_enrich_sparse_companies(self, inputs: dict) -> dict:
        """Find companies with missing core fields and enrich them via web search.

        Filters grid to companies with >= min_missing gaps, then runs the full
        gap resolver (Tier 1 benchmarks + Tier 2 parallel Tavily searches).
        Emits grid suggestions with source citations.
        """
        from app.services.micro_skills import CORE_FIELDS, detect_missing

        min_missing = int(inputs.get("min_missing") or 3)
        company_names_filter = [n.strip().lstrip("@") for n in (inputs.get("companies") or [])]
        fields = inputs.get("fields") or CORE_FIELDS

        # Pull companies from grid
        matrix_ctx = self.shared_data.get("matrix_context") or {}
        grid_snapshot = matrix_ctx.get("gridSnapshot") or {}
        grid_rows = (
            grid_snapshot.get("rows", [])
            if isinstance(grid_snapshot, dict)
            else grid_snapshot if isinstance(grid_snapshot, list) else []
        )
        if not grid_rows:
            return {"error": "No portfolio grid data available. Send matrix context with gridSnapshot."}

        # Build company list and filter to sparse ones
        candidate_companies = []
        for row in grid_rows:
            name = row.get("companyName") or row.get("company_name") or "Unknown"
            if company_names_filter and name not in company_names_filter:
                continue
            company_id = row.get("id") or row.get("companyId") or ""
            cells = row.get("cells") or row.get("cellValues") or {}

            company: Dict[str, Any] = {"name": name}
            if company_id:
                company["id"] = company_id

            field_map = {
                "stage": ["stage", "investmentStage", "funding_stage"],
                "sector": ["sector", "category"],
                "description": ["description"],
                "arr": ["arr", "currentArrUsd"],
                "valuation": ["valuation", "currentValuationUsd"],
                "total_funding": ["total_funding", "totalFunding"],
                "last_round_amount": ["last_round_amount", "lastRoundAmount"],
                "last_round_date": ["last_round_date", "lastRoundDate"],
                "burn_rate": ["burn_rate", "burnRate"],
                "runway_months": ["runway_months", "runway"],
                "team_size": ["team_size", "teamSize", "headcount"],
                "growth_rate": ["growth_rate", "revenueGrowthAnnual"],
                "gross_margin": ["gross_margin", "grossMargin"],
                "founders": ["founders"],
                "hq_location": ["hq_location", "hqLocation"],
                "competitors": ["competitors"],
            }
            for skill_field, grid_keys in field_map.items():
                for gk in grid_keys:
                    raw = cells.get(gk)
                    val = raw.get("value", raw) if isinstance(raw, dict) else raw
                    if val is not None and val != "" and val != "N/A":
                        company[skill_field] = val
                        break

            missing = detect_missing(company, fields)
            if len(missing) >= min_missing:
                company["_missing_fields"] = missing
                candidate_companies.append(company)

        if not candidate_companies:
            return {
                "message": f"No companies with {min_missing}+ missing fields found",
                "companies_checked": len(grid_rows),
                "grid_commands": [],
            }

        logger.info(
            f"[ENRICH_SPARSE] Found {len(candidate_companies)} sparse companies "
            f"(min_missing={min_missing}): {[c['name'] for c in candidate_companies[:10]]}"
        )

        # Delegate to _tool_resolve_gaps which runs full Tier 1+2 pipeline
        fund_ctx = self.shared_data.get("fund_context", {})
        fund_id = inputs.get("fund_id") or fund_ctx.get("fundId", "")

        return await self._tool_resolve_gaps({
            "companies": [c["name"] for c in candidate_companies],
            "needed_fields": fields,
            "fund_id": fund_id,
        })

    async def __aenter__(self):
        """Async context manager entry - ensures Tavily session is properly initialized"""
        # Session will be created lazily on first use via _tavily_search
        # This avoids creating sessions that may never be used
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit - ensures proper cleanup of Tavily session"""
        if self.session:
            try:
                await self.session.close()
                logger.info("[TAVILY] Session closed via context manager")
            except Exception as close_error:
                logger.warning(f"[TAVILY] Error closing session in context manager: {close_error}")
            finally:
                self.session = None


# Singleton instance getter
_orchestrator_instance = None

def get_unified_orchestrator() -> UnifiedMCPOrchestrator:
    """Get or create singleton orchestrator instance"""
    global _orchestrator_instance
    if _orchestrator_instance is None:
        _orchestrator_instance = UnifiedMCPOrchestrator()
    return _orchestrator_instance


# For backwards compatibility
SingleAgentOrchestrator = UnifiedMCPOrchestrator
